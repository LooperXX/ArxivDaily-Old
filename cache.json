{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.02242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "Transformer has been widely adopted in Neural Machine Translation (NMT)\nbecause of its large capacity and parallel training of sequence generation.\nHowever, the deployment of Transformer is challenging because different\nscenarios require models of different complexities and scales. Naively training\nmultiple Transformers is redundant in terms of both computation and memory. In\nthis paper, we propose a novel Scalable Transformers, which naturally contains\nsub-Transformers of different scales and have shared parameters. Each\nsub-Transformer can be easily obtained by cropping the parameters of the\nlargest Transformer. A three-stage training scheme is proposed to tackle the\ndifficulty of training the Scalable Transformers, which introduces additional\nsupervisions from word-level and sequence-level self-distillation. Extensive\nexperiments were conducted on WMT EN-De and En-Fr to validate our proposed\nScalable Transformers.",
          "link": "http://arxiv.org/abs/2106.02242",
          "publishedOn": "2021-06-21T02:07:37.440Z",
          "wordCount": 582,
          "title": "Scalable Transformers for Neural Machine Translation. (arXiv:2106.02242v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_V/0/1/0/all/0/1\">Vanessa Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinbauer_T/0/1/0/all/0/1\">Thomas Kleinbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>",
          "description": "Hate speech and profanity detection suffer from data sparsity, especially for\nlanguages other than English, due to the subjective nature of the tasks and the\nresulting annotation incompatibility of existing corpora. In this study, we\nidentify profane subspaces in word and sentence representations and explore\ntheir generalization capability on a variety of similar and distant target\ntasks in a zero-shot setting. This is done monolingually (German) and\ncross-lingually to closely-related (English), distantly-related (French) and\nnon-related (Arabic) tasks. We observe that, on both similar and distant target\ntasks and across all languages, the subspace-based representations transfer\nmore effectively than standard BERT representations in the zero-shot setting,\nwith improvements between F1 +10.9 and F1 +42.9 over the baselines across all\ntested monolingual and cross-lingual scenarios.",
          "link": "http://arxiv.org/abs/2106.07505",
          "publishedOn": "2021-06-21T02:07:37.433Z",
          "wordCount": 596,
          "title": "Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces. (arXiv:2106.07505v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "End-to-end speech translation models have become a new trend in research due\nto their potential of reducing error propagation. However, these models still\nsuffer from the challenge of data scarcity. How to effectively use unlabeled or\nother parallel corpora from machine translation is promising but still an open\nproblem. In this paper, we propose Cross Speech-Text Network (XSTNet), an\nend-to-end model for speech-to-text translation. XSTNet takes both speech and\ntext as input and outputs both transcription and translation text. The model\nbenefits from its three key design aspects: a self-supervised pre-trained\nsub-network as the audio encoder, a multi-task training objective to exploit\nadditional parallel bilingual text, and a progressive training procedure. We\nevaluate the performance of XSTNet and baselines on the MuST-C En-X and\nLibriSpeech En-Fr datasets. In particular, XSTNet achieves state-of-the-art\nresults on all language directions with an average BLEU of 28.8, outperforming\nthe previous best method by 3.2 BLEU. Code, models, cases, and more detailed\nanalysis are available at https://github.com/ReneeYe/XSTNet.",
          "link": "http://arxiv.org/abs/2104.10380",
          "publishedOn": "2021-06-21T02:07:37.426Z",
          "wordCount": 623,
          "title": "End-to-end Speech Translation via Cross-modal Progressive Training. (arXiv:2104.10380v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuoyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xin Lei</a>",
          "description": "In this paper, we propose an open source, production first, and production\nready speech recognition toolkit called WeNet in which a new two-pass approach\nis implemented to unify streaming and non-streaming end-to-end (E2E) speech\nrecognition in a single model. The main motivation of WeNet is to close the gap\nbetween the research and the production of E2E speechrecognition models. WeNet\nprovides an efficient way to ship ASR applications in several real-world\nscenarios, which is the main difference and advantage to other open source E2E\nspeech recognition toolkits. In our toolkit, a new two-pass method is\nimplemented. Our method propose a dynamic chunk-based attention strategy of the\nthe transformer layers to allow arbitrary right context length modifies in\nhybrid CTC/attention architecture. The inference latency could be easily\ncontrolled by only changing the chunk size. The CTC hypotheses are then\nrescored by the attention decoder to get the final result. Our experiments on\nthe AISHELL-1 dataset using WeNet show that, our model achieves 5.03\\% relative\ncharacter error rate (CER) reduction in non-streaming ASR compared to a\nstandard non-streaming transformer. After model quantification, our model\nperform reasonable RTF and latency.",
          "link": "http://arxiv.org/abs/2102.01547",
          "publishedOn": "2021-06-21T02:07:37.376Z",
          "wordCount": 690,
          "title": "WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit. (arXiv:2102.01547v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Sheshera Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>",
          "description": "Query by Example is a well-known information retrieval task in which a\ndocument is chosen by the user as the search query and the goal is to retrieve\nrelevant documents from a large collection. However, a document often covers\nmultiple aspects of a topic. To address this scenario we introduce the task of\nfaceted Query by Example in which users can also specify a finer grained aspect\nin addition to the input query document. We focus on the application of this\ntask in scientific literature search. We envision models which are able to\nretrieve scientific papers analogous to a query scientific paper along\nspecifically chosen rhetorical structure elements as one solution to this\nproblem. In this work, the rhetorical structure elements, which we refer to as\nfacets, indicate backgrounds, methods, or results of a scientific paper. We\nintroduce and describe an expert annotated test collection to evaluate models\ntrained to perform this task. Our test collection consists of a diverse set of\n50 query documents, drawn from computational linguistics and machine learning\nvenues. We carefully followed the annotation guideline used by TREC for depth-k\npooling (k = 100 or 250) and the resulting data collection consists of graded\nrelevance scores with high annotation agreement. The data is freely available\nfor research purposes.",
          "link": "http://arxiv.org/abs/2103.12906",
          "publishedOn": "2021-06-21T02:07:37.356Z",
          "wordCount": 700,
          "title": "CSFCube -- A Test Collection of Computer Science Research Articles for Faceted Query by Example. (arXiv:2103.12906v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10259",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cattiau_J/0/1/0/all/0/1\">Julie Cattiau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1\">Angad Chandorkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>",
          "description": "While current state-of-the-art Automatic Speech Recognition (ASR) systems\nachieve high accuracy on typical speech, they suffer from significant\nperformance degradation on disordered speech and other atypical speech\npatterns. Personalization of ASR models, a commonly applied solution to this\nproblem, is usually performed in a server-based training environment posing\nproblems around data privacy, delayed model-update times, and communication\ncost for copying data and models between mobile device and server\ninfrastructure. In this paper, we present an approach to on-device based ASR\npersonalization with very small amounts of speaker-specific data. We test our\napproach on a diverse set of 100 speakers with disordered speech and find\nmedian relative word error rate improvement of 71% with only 50 short\nutterances required per speaker. When tested on a voice-controlled home\nautomation platform, on-device personalized models show a median task success\nrate of 81%, compared to only 40% of the unadapted models.",
          "link": "http://arxiv.org/abs/2106.10259",
          "publishedOn": "2021-06-21T02:07:37.348Z",
          "wordCount": 604,
          "title": "On-Device Personalization of Automatic Speech Recognition Models for Disordered Speech. (arXiv:2106.10259v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>",
          "description": "Punctuation prediction for automatic speech recognition (ASR) output\ntranscripts plays a crucial role for improving the readability of the ASR\ntranscripts and for improving the performance of downstream natural language\nprocessing applications. However, achieving good performance on punctuation\nprediction often requires large amounts of labeled speech transcripts, which is\nexpensive and laborious. In this paper, we propose a Discriminative\nSelf-Training approach with weighted loss and discriminative label smoothing to\nexploit unlabeled speech transcripts. Experimental results on the English\nIWSLT2011 benchmark test set and an internal Chinese spoken language dataset\ndemonstrate that the proposed approach achieves significant improvement on\npunctuation prediction accuracy over strong baselines including BERT, RoBERTa,\nand ELECTRA models. The proposed Discriminative Self-Training approach\noutperforms the vanilla self-training approach. We establish a new\nstate-of-the-art (SOTA) on the IWSLT2011 test set, outperforming the current\nSOTA model by 1.3% absolute gain on F$_1$.",
          "link": "http://arxiv.org/abs/2104.10339",
          "publishedOn": "2021-06-21T02:07:37.341Z",
          "wordCount": 601,
          "title": "Discriminative Self-training for Punctuation Prediction. (arXiv:2104.10339v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>",
          "description": "In the traditional cascading architecture for spoken language understanding\n(SLU), it has been observed that automatic speech recognition errors could be\ndetrimental to the performance of natural language understanding. End-to-end\n(E2E) SLU models have been proposed to directly map speech input to desired\nsemantic frame with a single model, hence mitigating ASR error propagation.\nRecently, pre-training technologies have been explored for these E2E models. In\nthis paper, we propose a novel joint textual-phonetic pre-training approach for\nlearning spoken language representations, aiming at exploring the full\npotentials of phonetic information to improve SLU robustness to ASR errors. We\nexplore phoneme labels as high-level speech features, and design and compare\npre-training tasks based on conditional masked language model objectives and\ninter-sentence relation objectives. We also investigate the efficacy of\ncombining textual and phonetic information during fine-tuning. Experimental\nresults on spoken language understanding benchmarks, Fluent Speech Commands and\nSNIPS, show that the proposed approach significantly outperforms strong\nbaseline models and improves robustness of spoken language understanding to ASR\nerrors.",
          "link": "http://arxiv.org/abs/2104.10357",
          "publishedOn": "2021-06-21T02:07:37.334Z",
          "wordCount": 637,
          "title": "Pre-training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning. (arXiv:2104.10357v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.01183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shengfei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Linghao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1\">Huixiong Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huanhuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "In recent years, great success has been achieved in the field of natural\nlanguage processing (NLP), thanks in part to the considerable amount of\nannotated resources. For named entity recognition (NER), most languages do not\nhave such an abundance of labeled data as English, so the performances of those\nlanguages are relatively lower. To improve the performance, we propose a\ngeneral approach called Back Attention Network (BAN). BAN uses a translation\nsystem to translate other language sentences into English and then applies a\nnew mechanism named back attention knowledge transfer to obtain task-specific\ninformation from pre-trained high-resource languages NER model. This strategy\ncan transfer high-layer features of well-trained model and enrich the semantic\nrepresentations of the original language. Experiments on three different\nlanguage datasets indicate that the proposed approach outperforms other\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/1906.01183",
          "publishedOn": "2021-06-21T02:07:37.325Z",
          "wordCount": 608,
          "title": "Back Attention Knowledge Transfer for Low-Resource Named Entity Recognition. (arXiv:1906.01183v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dookun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Bum Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>",
          "description": "Digital assistants are experiencing rapid growth due to their ability to\nassist users with day-to-day tasks where most dialogues are happening\nmulti-turn. However, evaluating multi-turn dialogues remains challenging,\nespecially at scale. We suggest a context-sensitive method to estimate the\nturn-level satisfaction for dialogue considering various types of user\npreferences. The costs of interactions between users and dialogue systems are\nformulated using a budget consumption concept. We assume users have an initial\ninteraction budget for a dialogue formed based on the task complexity and that\neach turn has a cost. When the task is completed, or the budget has been\nexhausted, users quit the dialogue. We demonstrate our method's effectiveness\nby extensive experimentation with a simulated dialogue platform and real\nmulti-turn dialogues.",
          "link": "http://arxiv.org/abs/2103.01287",
          "publishedOn": "2021-06-21T02:07:37.307Z",
          "wordCount": 594,
          "title": "DEUS: A Data-driven Approach to Estimate User Satisfaction in Multi-turn Dialogues. (arXiv:2103.01287v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruirui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Chelsea J.-T. Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Hongda Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elibol_O/0/1/0/all/0/1\">Oguz Elibol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>",
          "description": "By implicitly recognizing a user based on his/her speech input, speaker\nidentification enables many downstream applications, such as personalized\nsystem behavior and expedited shopping checkouts. Based on whether the speech\ncontent is constrained or not, both text-dependent (TD) and text-independent\n(TI) speaker recognition models may be used. We wish to combine the advantages\nof both types of models through an ensemble system to make more reliable\npredictions. However, any such combined approach has to be robust to incomplete\ninputs, i.e., when either TD or TI input is missing. As a solution we propose a\nfusion of embeddings network foenet architecture, combining joint learning with\nneural attention. We compare foenet with four competitive baseline methods on a\ndataset of voice assistant inputs, and show that it achieves higher accuracy\nthan the baseline and score fusion methods, especially in the presence of\nincomplete inputs.",
          "link": "http://arxiv.org/abs/2106.10169",
          "publishedOn": "2021-06-21T02:07:37.247Z",
          "wordCount": 603,
          "title": "Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition. (arXiv:2106.10169v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaken_E/0/1/0/all/0/1\">Elad Ben Zaken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>",
          "description": "We show that with small-to-medium training data, fine-tuning only the bias\nterms (or a subset of the bias terms) of pre-trained BERT models is competitive\nwith (and sometimes better than) fine-tuning the entire model. For larger data,\nbias-only fine-tuning is competitive with other sparse fine-tuning methods.\nBesides their practical utility, these findings are relevant for the question\nof understanding the commonly-used process of finetuning: they support the\nhypothesis that finetuning is mainly about exposing knowledge induced by\nlanguage-modeling training, rather than learning new task-specific linguistic\nknowledge.",
          "link": "http://arxiv.org/abs/2106.10199",
          "publishedOn": "2021-06-21T02:07:37.239Z",
          "wordCount": 520,
          "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_A/0/1/0/all/0/1\">Atsushi Fujita</a>",
          "description": "In deep neural network modeling, the most common practice is to stack a\nnumber of recurrent, convolutional, or feed-forward layers in order to obtain\nhigh-quality continuous space representations which in turn improves the\nquality of the network's prediction. Conventionally, each layer in the stack\nhas its own parameters which leads to a significant increase in the number of\nmodel parameters. In this paper, we propose to share parameters across all\nlayers thereby leading to a recurrently stacked neural network model. We report\non an extensive case study on neural machine translation (NMT), where we apply\nour proposed method to an encoder-decoder based neural network model, i.e., the\nTransformer model, and experiment with three Japanese--English translation\ndatasets. We empirically demonstrate that the translation quality of a model\nthat recurrently stacks a single layer 6 times, despite having significantly\nfewer parameters, approaches that of a model that stacks 6 layers where each\nlayer has different parameters. We also explore the limits of recurrent\nstacking where we train extremely deep NMT models. This paper also examines the\nutility of our recurrently stacked model as a student model through transfer\nlearning via leveraging pre-trained parameters and knowledge distillation, and\nshows that it compensates for the performance drops in translation quality that\nthe direct training of recurrently stacked model brings. We also show how\ntransfer learning helps in faster decoding on top of the already reduced number\nof parameters due to recurrent stacking. Finally, we analyze the effects of\nrecurrently stacked layers by visualizing the attentions of models that use\nrecurrently stacked layers and models that do not.",
          "link": "http://arxiv.org/abs/2106.10002",
          "publishedOn": "2021-06-21T02:07:37.141Z",
          "wordCount": 740,
          "title": "Recurrent Stacking of Layers in Neural Networks: An Application to Neural Machine Translation. (arXiv:2106.10002v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litvak_M/0/1/0/all/0/1\">Marina Litvak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanetik_N/0/1/0/all/0/1\">Natalia Vanetik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiacheng Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Siya Qi</a>",
          "description": "Due to the subjectivity of the summarization, it is a good practice to have\nmore than one gold summary for each training document. However, many modern\nlarge-scale abstractive summarization datasets have only one-to-one samples\nwritten by different human with different styles. The impact of this phenomenon\nis understudied. We formulate the differences among possible multiple\nexpressions summarizing the same content as subjective bias and examine the\nrole of this bias in the context of abstractive summarization. In this paper a\nlightweight and effective method to extract the feature embeddings of\nsubjective styles is proposed. Results of summarization models trained on\nstyle-clustered datasets show that there are certain types of styles that lead\nto better convergence, abstraction and generalization. The reproducible code\nand generated summaries are available online.",
          "link": "http://arxiv.org/abs/2106.10084",
          "publishedOn": "2021-06-21T02:07:37.133Z",
          "wordCount": 565,
          "title": "Subjective Bias in Abstractive Summarization. (arXiv:2106.10084v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rego_R/0/1/0/all/0/1\">Rosana C. B. Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Ver&#xf4;nica M. L. Silva</a>",
          "description": "Predicting gender by the name is not a simple task. In many applications,\nespecially in the natural language processing (NLP) field, this task may be\nnecessary, mainly when considering foreign names. Some machine learning\nalgorithms can satisfactorily perform the prediction. In this paper, we\nexamined and implemented feedforward and recurrent deep neural network models,\nsuch as MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first\nname. A dataset of Brazilian names is used to train and evaluate the models. We\nanalyzed the accuracy, recall, precision, and confusion matrix to measure the\nmodels' performances. The results indicate that the gender prediction can be\nperformed from the feature extraction strategy looking at the names as a set of\nstrings. Some models accurately predict the gender in more than 90% of the\ncases. The recurrent models overcome the feedforward models in this binary\nclassification problem.",
          "link": "http://arxiv.org/abs/2106.10156",
          "publishedOn": "2021-06-21T02:07:37.091Z",
          "wordCount": 589,
          "title": "Predicting gender of Brazilian names using deep learning. (arXiv:2106.10156v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vivek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "Code-mixing is a frequent communication style among multilingual speakers\nwhere they mix words and phrases from two different languages in the same\nutterance of text or speech. Identifying and filtering code-mixed text is a\nchallenging task due to its co-existence with monolingual and noisy text. Over\nthe years, several code-mixing metrics have been extensively used to identify\nand validate code-mixed text quality. This paper demonstrates several inherent\nlimitations of code-mixing metrics with examples from the already existing\ndatasets that are popularly used across various experiments.",
          "link": "http://arxiv.org/abs/2106.10123",
          "publishedOn": "2021-06-21T02:07:37.059Z",
          "wordCount": 520,
          "title": "Challenges and Limitations with the Metrics Measuring the Complexity of Code-Mixed Text. (arXiv:2106.10123v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>",
          "description": "Generating synthesised singing voice with models trained on speech data has\nmany advantages due to the models' flexibility and controllability. However,\nsince the information about the temporal relationship between segments and\nbeats are lacking in speech training data, the synthesised singing may sound\noff-beat at times. Therefore, the availability of the information on the\ntemporal relationship between speech segments and music beats is crucial. The\ncurrent study investigated the segment-beat synchronisation in singing data,\nwith hypotheses formed based on the linguistics theories of P-centre and\nsonority hierarchy. A Mandarin corpus and an English corpus of professional\nsinging data were manually annotated and analysed. The results showed that the\npresence of musical beats was more dependent on segment duration than sonority.\nHowever, the sonority hierarchy and the P-centre theory were highly related to\nthe location of beats. Mandarin and English demonstrated cross-linguistic\nvariations despite exhibiting common patterns.",
          "link": "http://arxiv.org/abs/2106.10045",
          "publishedOn": "2021-06-21T02:07:37.051Z",
          "wordCount": 597,
          "title": "Synchronising speech segments with musical beats in Mandarin and English singing. (arXiv:2106.10045v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong-Son Nguyen</a>",
          "description": "We aim to create an unprecedented attempt to build an end-to-end Question\nAnswering (QA) over Knowledge Graphs (KGs), which can construct SPARQL queries\nfrom natural language questions and generate a verbalized answer to its\nqueries. Hence, we introduce SPBERT, a Transformer-based language model\npre-trained on massive SPARQL query logs. By incorporating masked language\nmodelling objective and word structural objective, SPBERT can learn\ngeneral-purpose representations in both natural language and SPARQL query\nlanguage and make the most of the sequential order of words that are crucial\nfor structured language like SPARQL. In this paper, we investigate how SPBERT\nand encoder-decoder architecture can be adapted for Knowledge-based QA corpora.\nWe conduct exhaustive experiments on two auxiliary tasks, including SPARQL\nQuery Construction and Answer Verbalization Generation. Results show that\nSPBERT obtains promising performance and achieves state-of-the-art results on\nseveral of these tasks.",
          "link": "http://arxiv.org/abs/2106.09997",
          "publishedOn": "2021-06-21T02:07:37.037Z",
          "wordCount": 580,
          "title": "SPBERT: Pre-training BERT on SPARQL Queries for End-to-end Question Answering over Knowledge Graphs. (arXiv:2106.09997v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terblanche_M/0/1/0/all/0/1\">Michelle Terblanche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>",
          "description": "Sentiment analysis as a sub-field of natural language processing has received\nincreased attention in the past decade enabling organisations to more\neffectively manage their reputation through online media monitoring. Many\ndrivers impact reputation, however, this thesis focuses only the aspect of\nfinancial performance and explores the gap with regards to financial sentiment\nanalysis in a South African context. Results showed that pre-trained sentiment\nanalysers are least effective for this task and that traditional lexicon-based\nand machine learning approaches are best suited to predict financial sentiment\nof news articles. The evaluated methods produced accuracies of 84\\%-94\\%. The\npredicted sentiments correlated quite well with share price and highlighted the\npotential use of sentiment as an indicator of financial performance. A main\ncontribution of the study was updating an existing sentiment dictionary for\nfinancial sentiment analysis. Model generalisation was less acceptable due to\nthe limited amount of training data used. Future work includes expanding the\ndata set to improve general usability and contribute to an open-source\nfinancial sentiment analyser for South African data.",
          "link": "http://arxiv.org/abs/2106.10004",
          "publishedOn": "2021-06-21T02:07:37.028Z",
          "wordCount": 609,
          "title": "Towards Financial Sentiment Analysis in a South African Landscape. (arXiv:2106.10004v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1\">Georgi V. Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1\">Danko D. Georgiev</a>",
          "description": "Human creativity generates novel ideas to solve real-world problems. This\nthereby grants us the power to transform the surrounding world and extend our\nhuman attributes beyond what is currently possible. Creative ideas are not just\nnew and unexpected, but are also successful in providing solutions that are\nuseful, efficient and valuable. Thus, creativity optimizes the use of available\nresources and increases wealth. The origin of human creativity, however, is\npoorly understood, and semantic measures that could predict the success of\ngenerated ideas are currently unknown. Here, we analyze a dataset of design\nproblem-solving conversations in real-world settings by using 49 semantic\nmeasures based on WordNet 3.1 and demonstrate that a divergence of semantic\nsimilarity, an increased information content, and a decreased polysemy predict\nthe success of generated ideas. The first feedback from clients also enhances\ninformation content and leads to a divergence of successful ideas in creative\nproblem solving. These results advance cognitive science by identifying\nreal-world processes in human problem solving that are relevant to the success\nof produced solutions and provide tools for real-time monitoring of problem\nsolving, student training and skill acquisition. A selected subset of\ninformation content (IC S\\'anchez-Batet) and semantic similarity\n(Lin/S\\'anchez-Batet) measures, which are both statistically powerful and\ncomputationally fast, could support the development of technologies for\ncomputer-assisted enhancements of human creativity or for the implementation of\ncreativity in machines endowed with general artificial intelligence.",
          "link": "http://arxiv.org/abs/2106.10131",
          "publishedOn": "2021-06-21T02:07:37.005Z",
          "wordCount": 677,
          "title": "Enhancing user creativity: Semantic measures for idea generation. (arXiv:2106.10131v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingbing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zelong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1\">Haining An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>",
          "description": "One of the key problems in multi-label text classification is how to take\nadvantage of the correlation among labels. However, it is very challenging to\ndirectly model the correlations among labels in a complex and unknown label\nspace. In this paper, we propose a Label Mask multi-label text classification\nmodel (LM-MTC), which is inspired by the idea of cloze questions of language\nmodel. LM-MTC is able to capture implicit relationships among labels through\nthe powerful ability of pre-train language models. On the basis, we assign a\ndifferent token to each potential label, and randomly mask the token with a\ncertain probability to build a label based Masked Language Model (MLM). We\ntrain the MTC and MLM together, further improving the generalization ability of\nthe model. A large number of experiments on multiple datasets demonstrate the\neffectiveness of our method.",
          "link": "http://arxiv.org/abs/2106.10076",
          "publishedOn": "2021-06-21T02:07:36.996Z",
          "wordCount": 579,
          "title": "Label Mask for Multi-Label Text Classification. (arXiv:2106.10076v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seonwoo_Y/0/1/0/all/0/1\">Yeon Seonwoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Ji-Hoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>",
          "description": "In multi-hop QA, answering complex questions entails iterative document\nretrieval for finding the missing entity of the question. The main steps of\nthis process are sub-question detection, document retrieval for the\nsub-question, and generation of a new query for the final document retrieval.\nHowever, building a dataset that contains complex questions with sub-questions\nand their corresponding documents requires costly human annotation. To address\nthe issue, we propose a new method for weakly supervised multi-hop retriever\npre-training without human efforts. Our method includes 1) a pre-training task\nfor generating vector representations of complex questions, 2) a scalable data\ngeneration method that produces the nested structure of question and\nsub-question as weak supervision for pre-training, and 3) a pre-training model\nstructure based on dense encoders. We conduct experiments to compare the\nperformance of our pre-trained retriever with several state-of-the-art models\non end-to-end multi-hop QA as well as document retrieval. The experimental\nresults show that our pre-trained retriever is effective and also robust on\nlimited data and computational resources.",
          "link": "http://arxiv.org/abs/2106.09983",
          "publishedOn": "2021-06-21T02:07:36.944Z",
          "wordCount": 599,
          "title": "Weakly Supervised Pre-Training for Multi-Hop Retriever. (arXiv:2106.09983v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makino_K/0/1/0/all/0/1\">Kohei Makino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1\">Makoto Miwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_Y/0/1/0/all/0/1\">Yutaka Sasaki</a>",
          "description": "In this paper, we propose a novel edge-editing approach to extract relation\ninformation from a document. We treat the relations in a document as a relation\ngraph among entities in this approach. The relation graph is iteratively\nconstructed by editing edges of an initial graph, which might be a graph\nextracted by another system or an empty graph. The way to edit edges is to\nclassify them in a close-first manner using the document and\ntemporally-constructed graph information; each edge is represented with a\ndocument context information by a pretrained transformer model and a graph\ncontext information by a graph convolutional neural network model. We evaluate\nour approach on the task to extract material synthesis procedures from\nmaterials science texts. The experimental results show the effectiveness of our\napproach in editing the graphs initialized by our in-house rule-based system\nand empty graphs.",
          "link": "http://arxiv.org/abs/2106.09900",
          "publishedOn": "2021-06-21T02:07:36.926Z",
          "wordCount": 595,
          "title": "A Neural Edge-Editing Approach for Document-Level Relation Graph Extraction. (arXiv:2106.09900v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1\">Jordan T. Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Surbhi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Akshay Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1\">Dipendra Misra</a>",
          "description": "Noise contrastive learning is a popular technique for unsupervised\nrepresentation learning. In this approach, a representation is obtained via\nreduction to supervised learning, where given a notion of semantic similarity,\nthe learner tries to distinguish a similar (positive) example from a collection\nof random (negative) examples. The success of modern contrastive learning\npipelines relies on many parameters such as the choice of data augmentation,\nthe number of negative examples, and the batch size; however, there is limited\nunderstanding as to how these parameters interact and affect downstream\nperformance. We focus on disambiguating the role of one of these parameters:\nthe number of negative examples. Theoretically, we show the existence of a\ncollision-coverage trade-off suggesting that the optimal number of negative\nexamples should scale with the number of underlying concepts in the data.\nEmpirically, we scrutinize the role of the number of negatives in both NLP and\nvision tasks. In the NLP task, we find that the results broadly agree with our\ntheory, while our vision experiments are murkier with performance sometimes\neven being insensitive to the number of negatives. We discuss plausible\nexplanations for this behavior and suggest future directions to better align\ntheory and practice.",
          "link": "http://arxiv.org/abs/2106.09943",
          "publishedOn": "2021-06-21T02:07:36.915Z",
          "wordCount": 639,
          "title": "Investigating the Role of Negatives in Contrastive Representation Learning. (arXiv:2106.09943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurajada_S/0/1/0/all/0/1\">Sairam Gurajada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qiuhao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neelam_S/0/1/0/all/0/1\">Sumit Neelam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popa_L/0/1/0/all/0/1\">Lucian Popa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1\">Prithviraj Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_A/0/1/0/all/0/1\">Alexander Gray</a>",
          "description": "Entity linking (EL), the task of disambiguating mentions in text by linking\nthem to entities in a knowledge graph, is crucial for text understanding,\nquestion answering or conversational systems. Entity linking on short text\n(e.g., single sentence or question) poses particular challenges due to limited\ncontext. While prior approaches use either heuristics or black-box neural\nmethods, here we propose LNN-EL, a neuro-symbolic approach that combines the\nadvantages of using interpretable rules based on first-order logic with the\nperformance of neural learning. Even though constrained to using rules, LNN-EL\nperforms competitively against SotA black-box neural approaches, with the added\nbenefits of extensibility and transferability. In particular, we show that we\ncan easily blend existing rule templates given by a human expert, with multiple\ntypes of features (priors, BERT encodings, box embeddings, etc), and even\nscores resulting from previous EL methods, thus improving on such methods. For\ninstance, on the LC-QuAD-1.0 dataset, we show more than $4$\\% increase in F1\nscore over previous SotA. Finally, we show that the inductive bias offered by\nusing logic results in learned rules that transfer well across datasets, even\nwithout fine tuning, while maintaining high accuracy.",
          "link": "http://arxiv.org/abs/2106.09795",
          "publishedOn": "2021-06-21T02:07:36.895Z",
          "wordCount": 643,
          "title": "LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking. (arXiv:2106.09795v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hengyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_R/0/1/0/all/0/1\">Rui Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Joint extraction of entities and relations from unstructured texts is a\ncrucial task in information extraction. Recent methods achieve considerable\nperformance but still suffer from some inherent limitations, such as redundancy\nof relation prediction, poor generalization of span-based extraction and\ninefficiency. In this paper, we decompose this task into three subtasks,\nRelation Judgement, Entity Extraction and Subject-object Alignment from a novel\nperspective and then propose a joint relational triple extraction framework\nbased on Potential Relation and Global Correspondence (PRGC). Specifically, we\ndesign a component to predict potential relations, which constrains the\nfollowing entity extraction to the predicted relation subset rather than all\nrelations; then a relation-specific sequence tagging component is applied to\nhandle the overlapping problem between subjects and objects; finally, a global\ncorrespondence component is designed to align the subject and object into a\ntriple with low-complexity. Extensive experiments show that PRGC achieves\nstate-of-the-art performance on public benchmarks with higher efficiency and\ndelivers consistent performance gain on complex scenarios of overlapping\ntriples.",
          "link": "http://arxiv.org/abs/2106.09895",
          "publishedOn": "2021-06-21T02:07:36.885Z",
          "wordCount": 617,
          "title": "PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction. (arXiv:2106.09895v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haisong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>",
          "description": "Quotations are crucial for successful explanations and persuasions in\ninterpersonal communications. However, finding what to quote in a conversation\nis challenging for both humans and machines. This work studies automatic\nquotation generation in an online conversation and explores how language\nconsistency affects whether a quotation fits the given context. Here, we\ncapture the contextual consistency of a quotation in terms of latent topics,\ninteractions with the dialogue history, and coherence to the query turn's\nexisting content. Further, an encoder-decoder neural framework is employed to\ncontinue the context with a quotation via language generation. Experiment\nresults on two large-scale datasets in English and Chinese demonstrate that our\nquotation generation model outperforms the state-of-the-art models. Further\nanalysis shows that topic, interaction, and query consistency are all helpful\nto learn how to quote in online conversations.",
          "link": "http://arxiv.org/abs/2106.09896",
          "publishedOn": "2021-06-21T02:07:36.876Z",
          "wordCount": 584,
          "title": "Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations. (arXiv:2106.09896v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jingli Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yongchareon_S/0/1/0/all/0/1\">Sira Yongchareon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Quan Bai</a>",
          "description": "Public concern detection provides potential guidance to the authorities for\ncrisis management before or during a pandemic outbreak. Detecting people's\nconcerns and attention from online social media platforms has been widely\nacknowledged as an effective approach to relieve public panic and prevent a\nsocial crisis. However, detecting concerns in time from massive information in\nsocial media turns out to be a big challenge, especially when sufficient\nmanually labeled data is in the absence of public health emergencies, e.g.,\nCOVID-19. In this paper, we propose a novel end-to-end deep learning model to\nidentify people's concerns and the corresponding relations based on Graph\nConvolutional Network and Bi-directional Long Short Term Memory integrated with\nConcern Graph. Except for the sequential features from BERT embeddings, the\nregional features of tweets can be extracted by the Concern Graph module, which\nnot only benefits the concern detection but also enables our model to be high\nnoise-tolerant. Thus, our model can address the issue of insufficient manually\nlabeled data. We conduct extensive experiments to evaluate the proposed model\nby using both manually labeled tweets and automatically labeled tweets. The\nexperimental results show that our model can outperform the state-of-art models\non real-world datasets.",
          "link": "http://arxiv.org/abs/2106.09929",
          "publishedOn": "2021-06-21T02:07:36.859Z",
          "wordCount": 682,
          "title": "Graph-based Joint Pandemic Concern and Relation Extraction on Twitter. (arXiv:2106.09929v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boucher_N/0/1/0/all/0/1\">Nicholas Boucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1\">Ilia Shumailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1\">Ross Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1\">Nicolas Papernot</a>",
          "description": "Several years of research have shown that machine-learning systems are\nvulnerable to adversarial examples, both in theory and in practice. Until now,\nsuch attacks have primarily targeted visual models, exploiting the gap between\nhuman and machine perception. Although text-based models have also been\nattacked with adversarial examples, such attacks struggled to preserve semantic\nmeaning and indistinguishability. In this paper, we explore a large class of\nadversarial examples that can be used to attack text-based models in a\nblack-box setting without making any human-perceptible visual modification to\ninputs. We use encoding-specific perturbations that are imperceptible to the\nhuman eye to manipulate the outputs of a wide range of Natural Language\nProcessing (NLP) systems from neural machine-translation pipelines to web\nsearch engines. We find that with a single imperceptible encoding injection --\nrepresenting one invisible character, homoglyph, reordering, or deletion -- an\nattacker can significantly reduce the performance of vulnerable models, and\nwith three injections most models can be functionally broken. Our attacks work\nagainst currently-deployed commercial systems, including those produced by\nMicrosoft and Google, in addition to open source models published by Facebook\nand IBM. This novel series of attacks presents a significant threat to many\nlanguage processing systems: an attacker can affect systems in a targeted\nmanner without any assumptions about the underlying model. We conclude that\ntext-based NLP systems require careful input sanitization, just like\nconventional applications, and that given such systems are now being deployed\nrapidly at scale, the urgent attention of architects and operators is required.",
          "link": "http://arxiv.org/abs/2106.09898",
          "publishedOn": "2021-06-21T02:07:36.850Z",
          "wordCount": 683,
          "title": "Bad Characters: Imperceptible NLP Attacks. (arXiv:2106.09898v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1\">Edward Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Huaishao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1\">Taroon Bharti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1\">Arun Sacheti</a>",
          "description": "In this paper, we present GEM as a General Evaluation benchmark for\nMultimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,\nXGLUE and XTREME that mainly focus on natural language tasks, GEM is a\nlarge-scale vision-language benchmark, which consists of GEM-I for\nimage-language tasks and GEM-V for video-language tasks. Comparing with\nexisting multimodal datasets such as MSCOCO and Flicker30K for image-language\ntasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the\nlargest vision-language dataset covering image-language tasks and\nvideo-language tasks at the same time, but also labeled in multiple languages.\nWe also provide two baseline models for this benchmark. We will release the\ndataset, code and baseline models, aiming to advance the development of\nmultilingual multimodal research.",
          "link": "http://arxiv.org/abs/2106.09889",
          "publishedOn": "2021-06-21T02:07:36.841Z",
          "wordCount": 581,
          "title": "GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turcan_E/0/1/0/all/0/1\">Elsbeth Turcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anubhai_R/0/1/0/all/0/1\">Rishita Anubhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_K/0/1/0/all/0/1\">Kasturi Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Onaizan_Y/0/1/0/all/0/1\">Yaser Al-Onaizan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>",
          "description": "Detecting what emotions are expressed in text is a well-studied problem in\nnatural language processing. However, research on finer grained emotion\nanalysis such as what causes an emotion is still in its infancy. We present\nsolutions that tackle both emotion recognition and emotion cause detection in a\njoint fashion. Considering that common-sense knowledge plays an important role\nin understanding implicitly expressed emotions and the reasons for those\nemotions, we propose novel methods that combine common-sense knowledge via\nadapted knowledge models with multi-task learning to perform joint emotion\nclassification and emotion cause tagging. We show performance improvement on\nboth tasks when including common-sense reasoning and a multitask framework. We\nprovide a thorough analysis to gain insights into model performance.",
          "link": "http://arxiv.org/abs/2106.09790",
          "publishedOn": "2021-06-21T02:07:36.699Z",
          "wordCount": 569,
          "title": "Multi-Task Learning and Adapted Knowledge Models for Emotion-Cause Extraction. (arXiv:2106.09790v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09760",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kwangyoun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sridhar_P/0/1/0/all/0/1\">Prashant Sridhar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1\">Kyu J. Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "Automatic speech recognition (ASR) models make fewer errors when more\nsurrounding speech information is presented as context. Unfortunately,\nacquiring a larger future context leads to higher latency. There exists an\ninevitable trade-off between speed and accuracy. Naively, to fit different\nlatency requirements, people have to store multiple models and pick the best\none under the constraints. Instead, a more desirable approach is to have a\nsingle model that can dynamically adjust its latency based on different\nconstraints, which we refer to as Multi-mode ASR. A Multi-mode ASR model can\nfulfill various latency requirements during inference -- when a larger latency\nbecomes acceptable, the model can process longer future context to achieve\nhigher accuracy and when a latency budget is not flexible, the model can be\nless dependent on future context but still achieve reliable accuracy. In\npursuit of Multi-mode ASR, we propose Stochastic Future Context, a simple\ntraining procedure that samples one streaming configuration in each iteration.\nThrough extensive experiments on AISHELL-1 and LibriSpeech datasets, we show\nthat a Multi-mode ASR model rivals, if not surpasses, a set of competitive\nstreaming baselines trained with different latency budgets.",
          "link": "http://arxiv.org/abs/2106.09760",
          "publishedOn": "2021-06-21T02:07:36.655Z",
          "wordCount": 639,
          "title": "Multi-mode Transformer Transducer with Stochastic Future Context. (arXiv:2106.09760v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mustafizur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_D/0/1/0/all/0/1\">Dinesh Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_D/0/1/0/all/0/1\">Dhiraj Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1\">Mucahid Kutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>",
          "description": "Building a benchmark dataset for hate speech detection presents several\nchallenges. Firstly, because hate speech is relatively rare -- e.g., less than\n3\\% of Twitter posts are hateful \\citep{founta2018large} -- random sampling of\ntweets to annotate is inefficient in capturing hate speech. A common practice\nis to only annotate tweets containing known ``hate words'', but this risks\nyielding a biased benchmark that only partially captures the real-world\nphenomenon of interest. A second challenge is that definitions of hate speech\ntend to be highly variable and subjective. Annotators having diverse prior\nnotions of hate speech may not only disagree with one another but also struggle\nto conform to specified labeling guidelines. Our key insight is that the rarity\nand subjectivity of hate speech are akin to that of relevance in information\nretrieval (IR). This connection suggests that well-established methodologies\nfor creating IR test collections might also be usefully applied to create\nbetter benchmark datasets for hate speech detection. Firstly, to intelligently\nand efficiently select which tweets to annotate, we apply established IR\ntechniques of {\\em pooling} and {\\em active learning}. Secondly, to improve\nboth consistency and value of annotations, we apply {\\em task decomposition}\n\\cite{Zhang-sigir14} and {\\em annotator rationale} \\cite{mcdonnell16-hcomp}\ntechniques. Using the above techniques, we create and share a new benchmark\ndataset\\footnote{We will release the dataset upon publication.} for hate speech\ndetection with broader coverage than prior datasets. We also show a dramatic\ndrop in accuracy of existing detection models when tested on these broader\nforms of hate. Collected annotator rationales not only provide documented\nsupport for labeling decisions but also create exciting future work\nopportunities for dual-supervision and/or explanation generation in modeling.",
          "link": "http://arxiv.org/abs/2106.09775",
          "publishedOn": "2021-06-21T02:07:36.643Z",
          "wordCount": 726,
          "title": "An Information Retrieval Approach to Building Datasets for Hate Speech Detection. (arXiv:2106.09775v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Devansh Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>",
          "description": "Tables are widely used in various kinds of documents to present information\nconcisely. Understanding tables is a challenging problem that requires an\nunderstanding of language and table structure, along with numerical and logical\nreasoning. In this paper, we present our systems to solve Task 9 of\nSemEval-2021: Statement Verification and Evidence Finding with Tables\n(SEM-TAB-FACTS). The task consists of two subtasks: (A) Given a table and a\nstatement, predicting whether the table supports the statement and (B)\nPredicting which cells in the table provide evidence for/against the statement.\nWe fine-tune TAPAS (a model which extends BERT's architecture to capture\ntabular structure) for both the subtasks as it has shown state-of-the-art\nperformance in various table understanding tasks. In subtask A, we evaluate how\ntransfer learning and standardizing tables to have a single header row improves\nTAPAS' performance. In subtask B, we evaluate how different fine-tuning\nstrategies can improve TAPAS' performance. Our systems achieve an F1 score of\n67.34 in subtask A three-way classification, 72.89 in subtask A two-way\nclassification, and 62.95 in subtask B.",
          "link": "http://arxiv.org/abs/2106.00248",
          "publishedOn": "2021-06-18T02:06:34.994Z",
          "wordCount": 648,
          "title": "Volta at SemEval-2021 Task 9: Statement Verification and Evidence Finding with Tables using TAPAS and Transfer Learning. (arXiv:2106.00248v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>",
          "description": "In this work, we focus on two crucial components in the cross-domain\ntext-to-SQL semantic parsing task: schema linking and value filling. To\nencourage the model to learn better encoding ability, we propose a column\nselection auxiliary task to empower the encoder with the relevance matching\ncapability by using explicit learning targets. Furthermore, we propose two\nvalue filling methods to build the bridge from the existing zero-shot semantic\nparsers to real-world applications, considering most of the existing parsers\nignore the values filling in the synthesized SQL. With experiments on Spider,\nour proposed framework improves over the baselines on the execution accuracy\nand exact set match accuracy when database contents are unavailable, and\ndetailed analysis sheds light on future work.",
          "link": "http://arxiv.org/abs/2106.09588",
          "publishedOn": "2021-06-18T02:06:34.929Z",
          "wordCount": 549,
          "title": "End-to-End Cross-Domain Text-to-SQL Semantic Parsing with Auxiliary Task. (arXiv:2106.09588v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thulke_D/0/1/0/all/0/1\">David Thulke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstenberger_A/0/1/0/all/0/1\">Alexander Gerstenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khoa Viet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>",
          "description": "As the vocabulary size of modern word-based language models becomes ever\nlarger, many sampling-based training criteria are proposed and investigated.\nThe essence of these sampling methods is that the softmax-related traversal\nover the entire vocabulary can be simplified, giving speedups compared to the\nbaseline. A problem we notice about the current landscape of such sampling\nmethods is the lack of a systematic comparison and some myths about preferring\none over another. In this work, we consider Monte Carlo sampling, importance\nsampling, a novel method we call compensated partial summation, and noise\ncontrastive estimation. Linking back to the three traditional criteria, namely\nmean squared error, binary cross-entropy, and cross-entropy, we derive the\ntheoretical solutions to the training problems. Contrary to some common belief,\nwe show that all these sampling methods can perform equally well, as long as we\ncorrect for the intended class posterior probabilities. Experimental results in\nlanguage modeling and automatic speech recognition on Switchboard and\nLibriSpeech support our claim, with all sampling-based methods showing similar\nperplexities and word error rates while giving the expected speedups.",
          "link": "http://arxiv.org/abs/2104.10507",
          "publishedOn": "2021-06-18T02:06:34.905Z",
          "wordCount": 651,
          "title": "On Sampling-Based Training Criteria for Neural Language Modeling. (arXiv:2104.10507v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.01212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1\">Tamali Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1\">Rudra Murthy V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>",
          "description": "In this paper, we identify an interesting kind of error in the output of\nUnsupervised Neural Machine Translation (UNMT) systems like\n\\textit{Undreamt}(footnote). We refer to this error type as \\textit{Scrambled\nTranslation problem}. We observe that UNMT models which use \\textit{word\nshuffle} noise (as in case of Undreamt) can generate correct words, but fail to\nstitch them together to form phrases. As a result, words of the translated\nsentence look \\textit{scrambled}, resulting in decreased BLEU. We hypothesise\nthat the reason behind \\textit{scrambled translation problem} is 'shuffling\nnoise' which is introduced in every input sentence as a denoising strategy. To\ntest our hypothesis, we experiment by retraining UNMT models with a simple\n\\textit{retraining} strategy. We stop the training of the Denoising UNMT model\nafter a pre-decided number of iterations and resume the training for the\nremaining iterations -- which number is also pre-decided -- using original\nsentence as input without adding any noise. Our proposed solution achieves\nsignificant performance improvement UNMT models that train conventionally. We\ndemonstrate these performance gains on four language pairs, \\textit{viz.},\nEnglish-French, English-German, English-Spanish, Hindi-Punjabi. Our qualitative\nand quantitative analysis shows that the retraining strategy helps achieve\nbetter alignment as observed by attention heatmap and better phrasal\ntranslation, leading to statistically significant improvement in BLEU scores.",
          "link": "http://arxiv.org/abs/1911.01212",
          "publishedOn": "2021-06-18T02:06:34.897Z",
          "wordCount": 682,
          "title": "Scrambled Translation Problem: A Problem of Denoising UNMT. (arXiv:1911.01212v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Amish Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1\">Sourav Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1\">Arnhav Datar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1\">Juned Kadiwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1\">Hrithwik Shalu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Jimson Mathew</a>",
          "description": "Reliable detection of the prodromal stages of Alzheimer's disease (AD)\nremains difficult even today because, unlike other neurocognitive impairments,\nthere is no definitive diagnosis of AD in vivo. In this context, existing\nresearch has shown that patients often develop language impairment even in mild\nAD conditions. We propose a multimodal deep learning method that utilizes\nspeech and the corresponding transcript simultaneously to detect AD. For audio\nsignals, the proposed audio-based network, a convolutional neural network (CNN)\nbased model, predicts the diagnosis for multiple speech segments, which are\ncombined for the final prediction. Similarly, we use contextual embedding\nextracted from BERT concatenated with a CNN-generated embedding for classifying\nthe transcript. The individual predictions of the two models are then combined\nto make the final classification. We also perform experiments to analyze the\nmodel performance when Automated Speech Recognition (ASR) system generated\ntranscripts are used instead of manual transcription in the text-based model.\nThe proposed method achieves 85.3% 10-fold cross-validation accuracy when\ntrained and evaluated on the Dementiabank Pitt corpus.",
          "link": "http://arxiv.org/abs/2012.00096",
          "publishedOn": "2021-06-18T02:06:34.875Z",
          "wordCount": 635,
          "title": "Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.08782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bruyne_L/0/1/0/all/0/1\">Luna De Bruyne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1\">Pepa Atanasova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>",
          "description": "Emotion lexica are commonly used resources to combat data poverty in\nautomatic emotion detection. However, vocabulary coverage issues, differences\nin construction method and discrepancies in emotion framework and\nrepresentation result in a heterogeneous landscape of emotion detection\nresources, calling for a unified approach to utilising them. To combat this, we\npresent an extended emotion lexicon of 30,273 unique entries, which is a result\nof merging eight existing emotion lexica by means of a multi-view variational\nautoencoder (VAE). We showed that a VAE is a valid approach for combining\nlexica with different label spaces into a joint emotion label space with a\nchosen number of dimensions, and that these dimensions are still interpretable.\nWe tested the utility of the unified VAE lexicon by employing the lexicon\nvalues as features in an emotion detection model. We found that the VAE lexicon\noutperformed individual lexica, but contrary to our expectations, it did not\noutperform a naive concatenation of lexica, although it did contribute to the\nnaive concatenation when added as an extra lexicon. Furthermore, using lexicon\ninformation as additional features next to state-of-the-art language models\nusually resulted in a better performance than when no lexicon information was\nused.",
          "link": "http://arxiv.org/abs/1911.08782",
          "publishedOn": "2021-06-18T02:06:34.869Z",
          "wordCount": 651,
          "title": "Joint Emotion Label Space Modelling for Affect Lexica. (arXiv:1911.08782v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiangpeng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Michael C. Lucic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazzai_H/0/1/0/all/0/1\">Hakim Ghazzai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massoud_Y/0/1/0/all/0/1\">Yehia Massoud</a>",
          "description": "Currently, the world is in the midst of a severe global pandemic, which has\naffected all aspects of people's lives. As a result, there is a deluge of\nCOVID-related digital media articles published in the United States, due to the\ndisparate effects of the pandemic. This large volume of information is\ndifficult to consume by the audience in a reasonable amount of time. In this\npaper, we develop a Natural Language Processing (NLP) pipeline that is capable\nof automatically distilling various digital articles into manageable pieces of\ninformation, while also modelling the progression topics discussed over time in\norder to aid readers in rapidly gaining holistic perspectives on pressing\nissues (i.e., the COVID-19 pandemic) from a diverse array of sources. We\nachieve these goals by first collecting a large corpus of COVID-related\narticles during the onset of the pandemic. After, we apply unsupervised and\nsemi-supervised learning procedures to summarize articles, then cluster them\nbased on their similarities using the community detection methods. Next, we\nidentify the topic of each cluster of articles using the BART algorithm.\nFinally, we provide a detailed digital media analysis based on the NLP-pipeline\noutputs and show how the conversation surrounding COVID-19 evolved over time.",
          "link": "http://arxiv.org/abs/2106.09572",
          "publishedOn": "2021-06-18T02:06:34.833Z",
          "wordCount": 692,
          "title": "Topic Modeling and Progression of American Digital News Media During the Onset of the COVID-19 Pandemic. (arXiv:2106.09572v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nadkarni_R/0/1/0/all/0/1\">Rahul Nadkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>",
          "description": "Biomedical knowledge graphs (KGs) hold rich information on entities such as\ndiseases, drugs, and genes. Predicting missing links in these graphs can boost\nmany important applications, such as drug design and repurposing. Recent work\nhas shown that general-domain language models (LMs) can serve as \"soft\" KGs,\nand that they can be fine-tuned for the task of KG completion. In this work, we\nstudy scientific LMs for KG completion, exploring whether we can tap into their\nlatent knowledge to enhance biomedical link prediction. We evaluate several\ndomain-specific LMs, fine-tuning them on datasets centered on drugs and\ndiseases that we represent as KGs and enrich with textual entity descriptions.\nWe integrate the LM-based models with KG embedding models, using a router\nmethod that learns to assign each input example to either type of model and\nprovides a substantial boost in performance. Finally, we demonstrate the\nadvantage of LM models in the inductive setting with novel scientific entities.\nOur datasets and code are made publicly available.",
          "link": "http://arxiv.org/abs/2106.09700",
          "publishedOn": "2021-06-18T02:06:34.807Z",
          "wordCount": 610,
          "title": "Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. (arXiv:2106.09700v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1\">Usman Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khushi_M/0/1/0/all/0/1\">Matloob Khushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1\">Adam G. Dunn</a>",
          "description": "Vaccines are an important public health measure, but vaccine hesitancy and\nrefusal can create clusters of low vaccine coverage and reduce the\neffectiveness of vaccination programs. Social media provides an opportunity to\nestimate emerging risks to vaccine acceptance by including geographical\nlocation and detailing vaccine-related concerns. Methods for classifying social\nmedia posts, such as vaccine-related tweets, use language models (LMs) trained\non general domain text. However, challenges to measuring vaccine sentiment at\nscale arise from the absence of tonal stress and gestural cues and may not\nalways have additional information about the user, e.g., past tweets or social\nconnections. Another challenge in LMs is the lack of commonsense knowledge that\nare apparent in users metadata, i.e., emoticons, positive and negative words\netc. In this study, to classify vaccine sentiment tweets with limited\ninformation, we present a novel end-to-end framework consisting of\ninterconnected components that use domain-specific LM trained on\nvaccine-related tweets and models commonsense knowledge into a bidirectional\ngated recurrent network (CK-BiGRU) with context-aware attention. We further\nleverage syntactical, user metadata and sentiment information to capture the\nsentiment of a tweet. We experimented using two popular vaccine-related Twitter\ndatasets and demonstrate that our proposed approach outperforms\nstate-of-the-art models in identifying pro-vaccine, anti-vaccine and neutral\ntweets.",
          "link": "http://arxiv.org/abs/2106.09589",
          "publishedOn": "2021-06-18T02:06:34.767Z",
          "wordCount": 662,
          "title": "Classifying vaccine sentiment tweets by modelling domain-specific representation and commonsense knowledge into context-aware attentive GRU. (arXiv:2106.09589v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark O. Riedl</a>",
          "description": "We provide a dataset that enables the creation of learning agents that can\nbuild knowledge graph-based world models of interactive narratives. Interactive\nnarratives -- or text-adventure games -- are partially observable environments\nstructured as long puzzles or quests in which an agent perceives and interacts\nwith the world purely through textual natural language. Each individual game\ntypically contains hundreds of locations, characters, and objects -- each with\ntheir own unique descriptions -- providing an opportunity to study the problem\nof giving language-based agents the structured memory necessary to operate in\nsuch worlds. Our dataset provides 24198 mappings between rich natural language\nobservations and: (1) knowledge graphs that reflect the world state in the form\nof a map; (2) natural language actions that are guaranteed to cause a change in\nthat particular world state. The training data is collected across 27 games in\nmultiple genres and contains a further 7836 heldout instances over 9 additional\ngames in the test set. We further provide baseline models using rules-based,\nquestion-answering, and sequence learning approaches in addition to an analysis\nof the data and corresponding learning tasks.",
          "link": "http://arxiv.org/abs/2106.09578",
          "publishedOn": "2021-06-18T02:06:34.682Z",
          "wordCount": 620,
          "title": "Modeling Worlds in Text. (arXiv:2106.09578v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Lingyong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>",
          "description": "Open relation extraction aims to cluster relation instances referring to the\nsame underlying relation, which is a critical step for general relation\nextraction. Current OpenRE models are commonly trained on the datasets\ngenerated from distant supervision, which often results in instability and\nmakes the model easily collapsed. In this paper, we revisit the procedure of\nOpenRE from a causal view. By formulating OpenRE using a structural causal\nmodel, we identify that the above-mentioned problems stem from the spurious\ncorrelations from entities and context to the relation type. To address this\nissue, we conduct \\emph{Element Intervention}, which intervenes on the context\nand entities respectively to obtain the underlying causal effects of them. We\nalso provide two specific implementations of the interventions based on entity\nranking and context contrasting. Experimental results on unsupervised relation\nextraction datasets show that our methods outperform previous state-of-the-art\nmethods and are robust across different datasets.",
          "link": "http://arxiv.org/abs/2106.09558",
          "publishedOn": "2021-06-18T02:06:34.675Z",
          "wordCount": 582,
          "title": "Element Intervention for Open Relation Extraction. (arXiv:2106.09558v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.01072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>",
          "description": "This paper introduces the task of factual error correction: performing edits\nto a claim so that the generated rewrite is better supported by evidence. This\nextends the well-studied task of fact verification by providing a mechanism to\ncorrect written texts that are refuted or only partially supported by evidence.\nWe demonstrate that it is feasible to train factual error correction systems\nfrom existing fact checking datasets which only contain labeled claims\naccompanied by evidence, but not the correction. We achieve this by employing a\ntwo-stage distant supervision approach that incorporates evidence into masked\nclaims when generating corrections. Our approach, based on the T5 transformer\nand using retrieved evidence, achieved better results than existing work which\nused a pointer copy network and gold evidence, producing accurate factual error\ncorrections for 5x more instances in human evaluation and a .125 increase in\nSARI score. The evaluation is conducted on a dataset of 65,000 instances based\non a recent fact verification shared task and we release it to enable further\nwork on the task.",
          "link": "http://arxiv.org/abs/2106.01072",
          "publishedOn": "2021-06-18T02:06:34.668Z",
          "wordCount": 640,
          "title": "Evidence-based Factual Error Correction. (arXiv:2106.01072v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03416",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ng_E/0/1/0/all/0/1\">Edwin G. Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>",
          "description": "We combine recent advancements in end-to-end speech recognition to\nnon-autoregressive automatic speech recognition. We push the limits of\nnon-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,\nFisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC\non giant Conformer neural network architectures with SpecAugment and wav2vec2\npre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,\n5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without\na language model.",
          "link": "http://arxiv.org/abs/2104.03416",
          "publishedOn": "2021-06-18T02:06:34.597Z",
          "wordCount": 553,
          "title": "Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>",
          "description": "Multi-head attention plays a crucial role in the recent success of\nTransformer models, which leads to consistent performance improvements over\nconventional attention in various applications. The popular belief is that this\neffectiveness stems from the ability of jointly attending multiple positions.\nIn this paper, we first demonstrate that jointly attending multiple positions\nis not a unique feature of multi-head attention, as multi-layer single-head\nattention also attends multiple positions and is more effective. Then, we\nsuggest the main advantage of the multi-head attention is the training\nstability, since it has less number of layers than the single-head attention,\nwhen attending the same number of positions. For example, 24-layer 16-head\nTransformer (BERT-large) and 384-layer single-head Transformer has the same\ntotal attention head number and roughly the same model size, while the\nmulti-head one is significantly shallower. Meanwhile, we show that, with recent\nadvances in deep learning, we can successfully stabilize the training of the\n384-layer Transformer. As the training difficulty is no longer a bottleneck,\nsubstantially deeper single-head Transformer achieves consistent performance\nimprovements without tuning hyper-parameters.",
          "link": "http://arxiv.org/abs/2106.09650",
          "publishedOn": "2021-06-18T02:06:34.554Z",
          "wordCount": 614,
          "title": "Multi-head or Single-head? An Empirical Comparison for Transformer Training. (arXiv:2106.09650v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinnuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1\">Ioannis Konstas</a>",
          "description": "We present AGGGEN (pronounced 'again'), a data-to-text model which\nre-introduces two explicit sentence planning stages into neural data-to-text\nsystems: input ordering and input aggregation. In contrast to previous work\nusing sentence planning, our model is still end-to-end: AGGGEN performs\nsentence planning at the same time as generating text by learning latent\nalignments (via semantic facts) between input representation and target text.\nExperiments on the WebNLG and E2E challenge data show that by using fact-based\nalignments our approach is more interpretable, expressive, robust to noise, and\neasier to control, while retaining the advantages of end-to-end systems in\nterms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen.",
          "link": "http://arxiv.org/abs/2106.05580",
          "publishedOn": "2021-06-18T02:06:34.420Z",
          "wordCount": 580,
          "title": "AGGGEN: Ordering and Aggregating while Generating. (arXiv:2106.05580v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "Short textual descriptions of entities provide summaries of their key\nattributes and have been shown to be useful sources of background knowledge for\ntasks such as entity linking and question answering. However, generating entity\ndescriptions, especially for new and long-tail entities, can be challenging\nsince relevant information is often scattered across multiple sources with\nvaried content and style. We introduce DESCGEN: given mentions spread over\nmultiple documents, the goal is to generate an entity summary description.\nDESCGEN consists of 37K entity descriptions from Wikipedia and Fandom, each\npaired with nine evidence documents on average. The documents were collected\nusing a combination of entity linking and hyperlinks to the Wikipedia and\nFandom entity pages, which together provide high-quality distant supervision.\nThe resulting summaries are more abstractive than those found in existing\ndatasets and provide a better proxy for the challenge of describing new and\nemerging entities. We also propose a two-stage extract-then-generate baseline\nand show that there exists a large gap (19.9% in ROUGE-L) between\nstate-of-the-art models and human performance, suggesting that the data will\nsupport significant future work.",
          "link": "http://arxiv.org/abs/2106.05365",
          "publishedOn": "2021-06-18T02:06:34.412Z",
          "wordCount": 636,
          "title": "DESCGEN: A Distantly Supervised Dataset for Generating Abstractive Entity Descriptions. (arXiv:2106.05365v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muralidaran_V/0/1/0/all/0/1\">Vigneshwaran Muralidaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_N/0/1/0/all/0/1\">Navya Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suryawanshi_S/0/1/0/all/0/1\">Shardul Suryawanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherly_E/0/1/0/all/0/1\">Elizabeth Sherly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCrae_J/0/1/0/all/0/1\">John P. McCrae</a>",
          "description": "This paper describes the development of a multilingual, manually annotated\ndataset for three under-resourced Dravidian languages generated from social\nmedia comments. The dataset was annotated for sentiment analysis and offensive\nlanguage identification for a total of more than 60,000 YouTube comments. The\ndataset consists of around 44,000 comments in Tamil-English, around 7,000\ncomments in Kannada-English, and around 20,000 comments in Malayalam-English.\nThe data was manually annotated by volunteer annotators and has a high\ninter-annotator agreement in Krippendorff's alpha. The dataset contains all\ntypes of code-mixing phenomena since it comprises user-generated content from a\nmultilingual country. We also present baseline experiments to establish\nbenchmarks on the dataset using machine learning methods. The dataset is\navailable on Github\n(https://github.com/bharathichezhiyan/DravidianCodeMix-Dataset) and Zenodo\n(https://zenodo.org/record/4750858\\#.YJtw0SYo\\_0M).",
          "link": "http://arxiv.org/abs/2106.09460",
          "publishedOn": "2021-06-18T02:06:34.385Z",
          "wordCount": 579,
          "title": "DravidianCodeMix: Sentiment Analysis and Offensive Language Identification Dataset for Dravidian Languages in Code-Mixed Text. (arXiv:2106.09460v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Edward J. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1\">Phillip Wallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shean Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>",
          "description": "The dominant paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, conventional fine-tuning, which\nretrains all model parameters, becomes less feasible. Using GPT-3 175B as an\nexample, deploying many independent instances of fine-tuned models, each with\n175B parameters, is extremely expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. For GPT-3,\nLoRA can reduce the number of trainable parameters by 10,000 times and the\ncomputation hardware requirement by 3 times compared to full fine-tuning. LoRA\nperforms on-par or better than fine-tuning in model quality on both GPT-3 and\nGPT-2, despite having fewer trainable parameters, a higher training throughput,\nand no additional inference latency. We also provide an empirical investigation\ninto rank-deficiency in language model adaptations, which sheds light on the\nefficacy of LoRA. We release our implementation in GPT-2 at\nhttps://github.com/microsoft/LoRA .",
          "link": "http://arxiv.org/abs/2106.09685",
          "publishedOn": "2021-06-18T02:06:34.376Z",
          "wordCount": 623,
          "title": "LoRA: Low-Rank Adaptation of Large Language Models. (arXiv:2106.09685v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09545",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wenninger_M/0/1/0/all/0/1\">Marc Wenninger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_J/0/1/0/all/0/1\">Jochen Schmidt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gudenberg_A/0/1/0/all/0/1\">Alexander Wolff von Gudenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>",
          "description": "Stuttering is a complex speech disorder identified by repeti-tions,\nprolongations of sounds, syllables or words and blockswhile speaking. Specific\nstuttering behaviour differs strongly,thus needing personalized therapy.\nTherapy sessions requirea high level of concentration by the therapist. We\nintroduceSTAN, a system to aid speech therapists in stuttering therapysessions.\nSuch an automated feedback system can lower thecognitive load on the therapist\nand thereby enable a more con-sistent therapy as well as allowing analysis of\nstuttering overthe span of multiple therapy sessions.",
          "link": "http://arxiv.org/abs/2106.09545",
          "publishedOn": "2021-06-18T02:06:34.356Z",
          "wordCount": 539,
          "title": "STAN: A stuttering therapy analysis helper. (arXiv:2106.09545v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haoyun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1\">Evgeny Kharlamov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Entity alignment, aiming to identify equivalent entities across different\nknowledge graphs (KGs), is a fundamental problem for constructing large-scale\nKGs. Over the course of its development, supervision has been considered\nnecessary for accurate alignments. Inspired by the recent progress of\nself-supervised learning, we explore the extent to which we can get rid of\nsupervision for entity alignment. Existing supervised methods for this task\nfocus on pulling each pair of positive (labeled) entities close to each other.\nHowever, our analysis suggests that the learning of entity alignment can\nactually benefit more from pushing sampled (unlabeled) negatives far away than\npulling positive aligned pairs close. We present SelfKG by leveraging this\ndiscovery to design a contrastive learning strategy across two KGs. Extensive\nexperiments on benchmark datasets demonstrate that SelfKG without supervision\ncan match or achieve comparable results with state-of-the-art supervised\nbaselines. The performance of SelfKG demonstrates self-supervised learning\noffers great potential for entity alignment in KGs.",
          "link": "http://arxiv.org/abs/2106.09395",
          "publishedOn": "2021-06-18T02:06:34.349Z",
          "wordCount": 595,
          "title": "A Self-supervised Method for Entity Alignment. (arXiv:2106.09395v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1\">Wuwei Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>",
          "description": "Monolingual word alignment is important for studying fine-grained editing\noperations (i.e., deletion, addition, and substitution) in text-to-text\ngeneration tasks, such as paraphrase generation, text simplification,\nneutralizing biased language, etc. In this paper, we present a novel neural\nsemi-Markov CRF alignment model, which unifies word and phrase alignments\nthrough variable-length spans. We also create a new benchmark with human\nannotations that cover four different text genres to evaluate monolingual word\nalignment models in more realistic settings. Experimental results show that our\nproposed model outperforms all previous approaches for monolingual word\nalignment as well as a competitive QA-based baseline, which was previously only\napplied to bilingual data. Our model demonstrates good generalizability to\nthree out-of-domain datasets and shows great utility in two downstream\napplications: automatic text simplification and sentence pair classification\ntasks.",
          "link": "http://arxiv.org/abs/2106.02569",
          "publishedOn": "2021-06-18T02:06:34.342Z",
          "wordCount": 580,
          "title": "Neural semi-Markov CRF for Monolingual Word Alignment. (arXiv:2106.02569v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.03405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xingyi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kunlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1\">Bin Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Taifeng Wang</a>",
          "description": "The ability of semantic reasoning over the sentence pair is essential for\nmany natural language understanding tasks, e.g., natural language inference and\nmachine reading comprehension. A recent significant improvement in these tasks\ncomes from BERT. As reported, the next sentence prediction (NSP) in BERT, which\nlearns the contextual relationship between two sentences, is of great\nsignificance for downstream problems with sentence-pair input. Despite the\neffectiveness of NSP, we suggest that NSP still lacks the essential signal to\ndistinguish between entailment and shallow correlation. To remedy this, we\npropose to augment the NSP task to a 3-class categorization task, which\nincludes a category for previous sentence prediction (PSP). The involvement of\nPSP encourages the model to focus on the informative semantics to determine the\nsentence order, thereby improves the ability of semantic understanding. This\nsimple modification yields remarkable improvement against vanilla BERT. To\nfurther incorporate the document-level information, the scope of NSP and PSP is\nexpanded into a broader range, i.e., NSP and PSP also include close but\nnonsuccessive sentences, the noise of which is mitigated by the label-smoothing\ntechnique. Both qualitative and quantitative experimental results demonstrate\nthe effectiveness of the proposed method. Our method consistently improves the\nperformance on the NLI and MRC benchmarks, including the challenging HANS\ndataset \\cite{hans}, suggesting that the document-level task is still promising\nfor the pre-training.",
          "link": "http://arxiv.org/abs/1909.03405",
          "publishedOn": "2021-06-18T02:06:34.328Z",
          "wordCount": 712,
          "title": "Symmetric Regularization based BERT for Pair-wise Semantic Reasoning. (arXiv:1909.03405v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ashim Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>",
          "description": "In this work, we introduce X-FACT: the largest publicly available\nmultilingual dataset for factual verification of naturally existing real-world\nclaims. The dataset contains short statements in 25 languages and is labeled\nfor veracity by expert fact-checkers. The dataset includes a multilingual\nevaluation benchmark that measures both out-of-domain generalization, and\nzero-shot capabilities of the multilingual models. Using state-of-the-art\nmultilingual transformer-based models, we develop several automated\nfact-checking models that, along with textual claims, make use of additional\nmetadata and evidence from news stories retrieved using a search engine.\nEmpirically, our best model attains an F-score of around 40%, suggesting that\nour dataset is a challenging benchmark for evaluation of multilingual\nfact-checking models.",
          "link": "http://arxiv.org/abs/2106.09248",
          "publishedOn": "2021-06-18T02:06:34.303Z",
          "wordCount": 551,
          "title": "X-FACT: A New Benchmark Dataset for Multilingual Fact Checking. (arXiv:2106.09248v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark O. Riedl</a>",
          "description": "World models improve a learning agent's ability to efficiently operate in\ninteractive and situated environments. This work focuses on the task of\nbuilding world models of text-based game environments. Text-based games, or\ninteractive narratives, are reinforcement learning environments in which agents\nperceive and interact with the world using textual natural language. These\nenvironments contain long, multi-step puzzles or quests woven through a world\nthat is filled with hundreds of characters, locations, and objects. Our world\nmodel learns to simultaneously: (1) predict changes in the world caused by an\nagent's actions when representing the world as a knowledge graph; and (2)\ngenerate the set of contextually relevant natural language actions required to\noperate in the world. We frame this task as a Set of Sequences generation\nproblem by exploiting the inherent structure of knowledge graphs and actions\nand introduce both a transformer-based multi-task architecture and a loss\nfunction to train it. A zero-shot ablation study on never-before-seen textual\nworlds shows that our methodology significantly outperforms existing textual\nworld modeling techniques as well as the importance of each of our\ncontributions.",
          "link": "http://arxiv.org/abs/2106.09608",
          "publishedOn": "2021-06-18T02:06:34.281Z",
          "wordCount": 619,
          "title": "Learning Knowledge Graph-based World Models of Textual Environments. (arXiv:2106.09608v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>",
          "description": "Most prior work on task-oriented dialogue systems are restricted to limited\ncoverage of domain APIs. However, users oftentimes have requests that are out\nof the scope of these APIs. This work focuses on responding to these\nbeyond-API-coverage user turns by incorporating external, unstructured\nknowledge sources. Our approach works in a pipelined manner with\nknowledge-seeking turn detection, knowledge selection, and response generation\nin sequence. We introduce novel data augmentation methods for the first two\nsteps and demonstrate that the use of information extracted from dialogue\ncontext improves the knowledge selection and end-to-end performances. Through\nexperiments, we achieve state-of-the-art performance for both automatic and\nhuman evaluation metrics on the DSTC9 Track 1 benchmark dataset, validating the\neffectiveness of our contributions.",
          "link": "http://arxiv.org/abs/2106.09174",
          "publishedOn": "2021-06-18T02:06:34.274Z",
          "wordCount": 579,
          "title": "Can I Be of Further Assistance? Using Unstructured Knowledge Access to Improve Task-oriented Conversational Modeling. (arXiv:2106.09174v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1911.10088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1\">Paul Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbonell_J/0/1/0/all/0/1\">Jaime Carbonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "To acquire a new skill, humans learn better and faster if a tutor, based on\ntheir current knowledge level, informs them of how much attention they should\npay to particular content or practice problems. Similarly, a machine learning\nmodel could potentially be trained better with a scorer that \"adapts\" to its\ncurrent learning state and estimates the importance of each training data\ninstance. Training such an adaptive scorer efficiently is a challenging\nproblem; in order to precisely quantify the effect of a data instance at a\ngiven time during the training, it is typically necessary to first complete the\nentire training process. To efficiently optimize data usage, we propose a\nreinforcement learning approach called Differentiable Data Selection (DDS). In\nDDS, we formulate a scorer network as a learnable function of the training\ndata, which can be efficiently updated along with the main model being trained.\nSpecifically, DDS updates the scorer with an intuitive reward signal: it should\nup-weigh the data that has a similar gradient with a dev set upon which we\nwould finally like to perform well. Without significant computing overhead, DDS\ndelivers strong and consistent improvements over several strong baselines on\ntwo very different tasks of machine translation and image classification.",
          "link": "http://arxiv.org/abs/1911.10088",
          "publishedOn": "2021-06-18T02:06:34.266Z",
          "wordCount": 688,
          "title": "Optimizing Data Usage via Differentiable Rewards. (arXiv:1911.10088v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1\">Ravi Shankar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1\">Kartik Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasiwasia_N/0/1/0/all/0/1\">Nikhil Rasiwasia</a>",
          "description": "In this paper, we present SANTA, a scalable framework to automatically\nnormalize E-commerce attribute values (e.g. \"Win 10 Pro\") to a fixed set of\npre-defined canonical values (e.g. \"Windows 10\"). Earlier works on attribute\nnormalization focused on fuzzy string matching (also referred as syntactic\nmatching in this paper). In this work, we first perform an extensive study of\nnine syntactic matching algorithms and establish that 'cosine' similarity leads\nto best results, showing 2.7% improvement over commonly used Jaccard index.\nNext, we argue that string similarity alone is not sufficient for attribute\nnormalization as many surface forms require going beyond syntactic matching\n(e.g. \"720p\" and \"HD\" are synonyms). While semantic techniques like\nunsupervised embeddings (e.g. word2vec/fastText) have shown good results in\nword similarity tasks, we observed that they perform poorly to distinguish\nbetween close canonical forms, as these close forms often occur in similar\ncontexts. We propose to learn token embeddings using a twin network with\ntriplet loss. We propose an embedding learning task leveraging raw attribute\nvalues and product titles to learn these embeddings in a self-supervised\nfashion. We show that providing supervision using our proposed task improves\nover both syntactic and unsupervised embeddings based techniques for attribute\nnormalization. Experiments on a real-world attribute normalization dataset of\n50 attributes show that the embeddings trained using our proposed approach\nobtain 2.3% improvement over best string matching and 19.3% improvement over\nbest unsupervised embeddings.",
          "link": "http://arxiv.org/abs/2106.09493",
          "publishedOn": "2021-06-18T02:06:34.258Z",
          "wordCount": 678,
          "title": "Scalable Approach for Normalizing E-commerce Text Attributes (SANTA). (arXiv:2106.09493v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Olano_D/0/1/0/all/0/1\">Diego Garcia-Olano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1\">Yasumasa Onoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1\">Ioana Baldini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1\">Joydeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1\">Kush R. Varshney</a>",
          "description": "Pre-trained language models induce dense entity representations that offer\nstrong performance on entity-centric NLP tasks, but such representations are\nnot immediately interpretable. This can be a barrier to model uptake in\nimportant domains such as biomedicine. There has been recent work on general\ninterpretable representation learning (Onoe and Durrett, 2020), but these\ndomain-agnostic representations do not readily transfer to the important domain\nof biomedicine. In this paper, we create a new entity type system and training\nset from a large corpus of biomedical texts by mapping entities to concepts in\na medical ontology, and from these to Wikipedia pages whose categories are our\ntypes. From this mapping we derive Biomedical Interpretable Entity\nRepresentations(BIERs), in which dimensions correspond to fine-grained entity\ntypes, and values are predicted probabilities that a given entity is of the\ncorresponding type. We propose a novel method that exploits BIER's final sparse\nand intermediate dense representations to facilitate model and entity type\ndebugging. We show that BIERs achieve strong performance in biomedical tasks\nincluding named entity disambiguation and entity label classification, and we\nprovide error analysis to highlight the utility of their interpretability,\nparticularly in low-supervision settings. Finally, we provide our induced 68K\nbiomedical type system, the corresponding 37 million triples of derived data\nused to train BIER models and our best performing model.",
          "link": "http://arxiv.org/abs/2106.09502",
          "publishedOn": "2021-06-18T02:06:34.249Z",
          "wordCount": 658,
          "title": "Biomedical Interpretable Entity Representations. (arXiv:2106.09502v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09449",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>",
          "description": "Natural language inference (NLI) is formulated as a unified framework for\nsolving various NLP problems such as relation extraction, question answering,\nsummarization, etc. It has been studied intensively in the past few years\nthanks to the availability of large-scale labeled datasets. However, most\nexisting studies focus on merely sentence-level inference, which limits the\nscope of NLI's application in downstream NLP problems. This work presents\nDocNLI -- a newly-constructed large-scale dataset for document-level NLI.\nDocNLI is transformed from a broad range of NLP problems and covers multiple\ngenres of text. The premises always stay in the document granularity, whereas\nthe hypotheses vary in length from single sentences to passages with hundreds\nof words. Additionally, DocNLI has pretty limited artifacts which unfortunately\nwidely exist in some popular sentence-level NLI datasets. Our experiments\ndemonstrate that, even without fine-tuning, a model pretrained on DocNLI shows\npromising performance on popular sentence-level benchmarks, and generalizes\nwell to out-of-domain NLP tasks that rely on inference at document granularity.\nTask-specific fine-tuning can bring further improvements. Data, code, and\npretrained models can be found at https://github.com/salesforce/DocNLI.",
          "link": "http://arxiv.org/abs/2106.09449",
          "publishedOn": "2021-06-18T02:06:34.229Z",
          "wordCount": 614,
          "title": "DocNLI: A Large-scale Dataset for Document-level Natural Language Inference. (arXiv:2106.09449v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>",
          "description": "Distant supervision tackles the data bottleneck in NER by automatically\ngenerating training instances via dictionary matching. Unfortunately, the\nlearning of DS-NER is severely dictionary-biased, which suffers from spurious\ncorrelations and therefore undermines the effectiveness and the robustness of\nthe learned models. In this paper, we fundamentally explain the dictionary bias\nvia a Structural Causal Model (SCM), categorize the bias into intra-dictionary\nand inter-dictionary biases, and identify their causes. Based on the SCM, we\nlearn de-biased DS-NER via causal interventions. For intra-dictionary bias, we\nconduct backdoor adjustment to remove the spurious correlations introduced by\nthe dictionary confounder. For inter-dictionary bias, we propose a causal\ninvariance regularizer which will make DS-NER models more robust to the\nperturbation of dictionaries. Experiments on four datasets and three DS-NER\nmodels show that our method can significantly improve the performance of\nDS-NER.",
          "link": "http://arxiv.org/abs/2106.09233",
          "publishedOn": "2021-06-18T02:06:34.218Z",
          "wordCount": 574,
          "title": "De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention. (arXiv:2106.09233v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09488",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elibol_O/0/1/0/all/0/1\">Oguz Elibol</a>",
          "description": "There is a recent trend in machine learning to increase model quality by\ngrowing models to sizes previously thought to be unreasonable. Recent work has\nshown that autoregressive generative models with cross-entropy objective\nfunctions exhibit smooth power-law relationships, or scaling laws, that predict\nmodel quality from model size, training set size, and the available compute\nbudget. These scaling laws allow one to choose nearly optimal hyper-parameters\ngiven constraints on available training data, model parameter count, or\ntraining computation budget. In this paper, we demonstrate that acoustic models\ntrained with an auto-predictive coding loss behave as if they are subject to\nsimilar scaling laws. We extend previous work to jointly predict loss due to\nmodel size, to training set size, and to the inherent \"irreducible loss\" of the\ntask. We find that the scaling laws accurately match model performance over two\norders of magnitude in both model size and training set size, and make\npredictions about the limits of model performance.",
          "link": "http://arxiv.org/abs/2106.09488",
          "publishedOn": "2021-06-18T02:06:34.210Z",
          "wordCount": 606,
          "title": "Scaling Laws for Acoustic Models. (arXiv:2106.09488v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan Manuel P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giudici_J/0/1/0/all/0/1\">Juan Carlos Giudici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_F/0/1/0/all/0/1\">Franco Luque</a>",
          "description": "Extracting opinions from texts has gathered a lot of interest in the last\nyears, as we are experiencing an unprecedented volume of user-generated content\nin social networks and other places. A problem that social researchers find in\nusing opinion mining tools is that they are usually behind commercial APIs and\nunavailable for other languages than English. To address these issues, we\npresent pysentimiento, a multilingual Python toolkit for Sentiment Analysis and\nother Social NLP tasks. This open-source library brings state-of-the-art models\nfor Spanish and English in a black-box fashion, allowing researchers to easily\naccess these techniques.",
          "link": "http://arxiv.org/abs/2106.09462",
          "publishedOn": "2021-06-18T02:06:34.202Z",
          "wordCount": 547,
          "title": "pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks. (arXiv:2106.09462v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pelecanos_J/0/1/0/all/0/1\">Jason Pelecanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>",
          "description": "Many neural network speaker recognition systems model each speaker using a\nfixed-dimensional embedding vector. These embeddings are generally compared\nusing either linear or 2nd-order scoring and, until recently, do not handle\nutterance-specific uncertainty. In this work we propose scoring these\nrepresentations in a way that can capture uncertainty, enroll/test asymmetry\nand additional non-linear information. This is achieved by incorporating a\n2nd-stage neural network (known as a decision network) as part of an end-to-end\ntraining regimen. In particular, we propose the concept of decision residual\nnetworks which involves the use of a compact decision network to leverage\ncosine scores and to model the residual signal that's needed. Additionally, we\npresent a modification to the generalized end-to-end softmax loss function to\ntarget the separation of same/different speaker scores. We observed significant\nperformance gains for the two techniques.",
          "link": "http://arxiv.org/abs/2104.01989",
          "publishedOn": "2021-06-18T02:06:34.194Z",
          "wordCount": 610,
          "title": "Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker Recognition. (arXiv:2104.01989v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07071",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ray_S/0/1/0/all/0/1\">Swayambhu Nath Ray</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Minhua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghahremani_P/0/1/0/all/0/1\">Pegah Ghahremani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bilgi_R/0/1/0/all/0/1\">Raghavendra Bilgi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rao_M/0/1/0/all/0/1\">Milind Rao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arsikere_H/0/1/0/all/0/1\">Harish Arsikere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>",
          "description": "Comprehending the overall intent of an utterance helps a listener recognize\nthe individual words spoken. Inspired by this fact, we perform a novel study of\nthe impact of explicitly incorporating intent representations as additional\ninformation to improve a recurrent neural network-transducer (RNN-T) based\nautomatic speech recognition (ASR) system. An audio-to-intent (A2I) model\nencodes the intent of the utterance in the form of embeddings or posteriors,\nand these are used as auxiliary inputs for RNN-T training and inference.\nExperimenting with a 50k-hour far-field English speech corpus, this study shows\nthat when running the system in non-streaming mode, where intent representation\nis extracted from the entire utterance and then used to bias streaming RNN-T\nsearch from the start, it provides a 5.56% relative word error rate reduction\n(WERR). On the other hand, a streaming system using per-frame intent posteriors\nas extra inputs for the RNN-T ASR system yields a 3.33% relative WERR. A\nfurther detailed analysis of the streaming system indicates that our proposed\nmethod brings especially good gain on media-playing related intents (e.g. 9.12%\nrelative WERR on PlayMusicIntent).",
          "link": "http://arxiv.org/abs/2105.07071",
          "publishedOn": "2021-06-18T02:06:34.173Z",
          "wordCount": 661,
          "title": "Listen with Intent: Improving Speech Recognition with Audio-to-Intent Front-End. (arXiv:2105.07071v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glushko_A/0/1/0/all/0/1\">Aleksandr Glushko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_W/0/1/0/all/0/1\">Wilfried Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1\">Albert Zeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>",
          "description": "Attention-based encoder-decoder (AED) models learn an implicit internal\nlanguage model (ILM) from the training transcriptions. The integration with an\nexternal LM trained on much more unpaired text usually leads to better\nperformance. A Bayesian interpretation as in the hybrid autoregressive\ntransducer (HAT) suggests dividing by the prior of the discriminative acoustic\nmodel, which corresponds to this implicit LM, similarly as in the hybrid hidden\nMarkov model approach. The implicit LM cannot be calculated efficiently in\ngeneral and it is yet unclear what are the best methods to estimate it. In this\nwork, we compare different approaches from the literature and propose several\nnovel methods to estimate the ILM directly from the AED model. Our proposed\nmethods outperform all previous approaches. We also investigate other methods\nto suppress the ILM mainly by decreasing the capacity of the AED model,\nlimiting the label context, and also by training the AED model together with a\npre-existing LM.",
          "link": "http://arxiv.org/abs/2104.05544",
          "publishedOn": "2021-06-18T02:06:34.166Z",
          "wordCount": 636,
          "title": "Investigating Methods to Improve Language Model Integration for Attention-based Encoder-Decoder ASR Models. (arXiv:2104.05544v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1\">Letitia Parcalabescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1\">Iacer Calixto</a>",
          "description": "We investigate the reasoning ability of pretrained vision and language (V&L)\nmodels in two tasks that require multimodal integration: (1) discriminating a\ncorrect image-sentence pair from an incorrect one, and (2) counting entities in\nan image. We evaluate three pretrained V&L models on these tasks: ViLBERT,\nViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results\nshow that models solve task (1) very well, as expected, since all models are\npretrained on task (1). However, none of the pretrained V&L models is able to\nadequately solve task (2), our counting probe, and they cannot generalise to\nout-of-distribution quantities. We propose a number of explanations for these\nfindings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of\ncatastrophic forgetting on task (1). Concerning our results on the counting\nprobe, we find evidence that all models are impacted by dataset bias, and also\nfail to individuate entities in the visual input. While a selling point of\npretrained V&L models is their ability to solve complex tasks, our findings\nsuggest that understanding their reasoning and grounding capabilities requires\nmore targeted investigations on specific phenomena.",
          "link": "http://arxiv.org/abs/2012.12352",
          "publishedOn": "2021-06-18T02:06:34.158Z",
          "wordCount": 725,
          "title": "Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks. (arXiv:2012.12352v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huidan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhicheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Jing Yuan</a>",
          "description": "Denoising is the essential step for distant supervision based named entity\nrecognition. Previous denoising methods are mostly based on instance-level\nconfidence statistics, which ignore the variety of the underlying noise\ndistribution on different datasets and entity types. This makes them difficult\nto be adapted to high noise rate settings. In this paper, we propose\nHypergeometric Learning (HGL), a denoising algorithm for distantly supervised\nNER that takes both noise distribution and instance-level confidence into\nconsideration. Specifically, during neural network training, we naturally model\nthe noise samples in each batch following a hypergeometric distribution\nparameterized by the noise-rate. Then each instance in the batch is regarded as\neither correct or noisy one according to its label confidence derived from\nprevious training step, as well as the noise distribution in this sampled\nbatch. Experiments show that HGL can effectively denoise the weakly-labeled\ndata retrieved from distant supervision, and therefore results in significant\nimprovements on the trained models.",
          "link": "http://arxiv.org/abs/2106.09234",
          "publishedOn": "2021-06-18T02:06:34.151Z",
          "wordCount": 602,
          "title": "Denoising Distantly Supervised Named Entity Recognition via a Hypergeometric Probabilistic Model. (arXiv:2106.09234v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09216",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jaesong Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_J/0/1/0/all/0/1\">Jingu Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "Deploying an end-to-end automatic speech recognition (ASR) model on\nmobile/embedded devices is a challenging task, since the device computational\npower and energy consumption requirements are dynamically changed in practice.\nTo overcome the issue, we present a training and pruning method for ASR based\non the connectionist temporal classification (CTC) which allows reduction of\nmodel depth at run-time without any extra fine-tuning. To achieve the goal, we\nadopt two regularization methods, intermediate CTC and stochastic depth, to\ntrain a model whose performance does not degrade much after pruning. We present\nan in-depth analysis of layer behaviors using singular vector canonical\ncorrelation analysis (SVCCA), and efficient strategies for finding layers which\nare safe to prune. Using the proposed method, we show that a Transformer-CTC\nmodel can be pruned in various depth on demand, improving real-time factor from\n0.005 to 0.002 on GPU, while each pruned sub-model maintains the accuracy of\nindividually trained model of the same depth.",
          "link": "http://arxiv.org/abs/2106.09216",
          "publishedOn": "2021-06-18T02:06:34.143Z",
          "wordCount": 600,
          "title": "Layer Pruning on Demand with Intermediate CTC. (arXiv:2106.09216v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2010.10811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Puhai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xianling Mao</a>",
          "description": "Scalability for handling unknown slot values is a important problem in\ndialogue state tracking (DST). As far as we know, previous scalable DST\napproaches generally rely on either the candidate generation from slot tagging\noutput or the span extraction in dialogue context. However, the candidate\ngeneration based DST often suffers from error propagation due to its pipelined\ntwo-stage process; meanwhile span extraction based DST has the risk of\ngenerating invalid spans in the lack of semantic constraints between start and\nend position pointers. To tackle the above drawbacks, in this paper, we propose\na novel scalable dialogue state tracking method based on slot tagging\nnavigation, which implements an end-to-end single-step pointer to locate and\nextract slot value quickly and accurately by the joint learning of slot tagging\nand slot value position prediction in the dialogue context, especially for\nunknown slot values. Extensive experiments over several benchmark datasets show\nthat the proposed model performs better than state-of-the-art baselines\ngreatly.",
          "link": "http://arxiv.org/abs/2010.10811",
          "publishedOn": "2021-06-18T02:06:34.135Z",
          "wordCount": 621,
          "title": "STN4DST: A Scalable Dialogue State Tracking based on Slot Tagging Navigation. (arXiv:2010.10811v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1\">Jerret Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belgodere_B/0/1/0/all/0/1\">Brian Belgodere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1\">Vijil Chenthamarakshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1\">Inkit Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1\">Youssef Mroueh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>",
          "description": "Predicting chemical properties from the structure of a molecule is of great\nimportance in many applications including drug discovery and material design.\nMachine learning based molecular property prediction holds the promise of\nenabling accurate predictions at much less complexity, when compared to, for\nexample Density Functional Theory (DFT) calculations. Features extracted from\nmolecular graphs, using graph neural nets in a supervised manner, have emerged\nas strong baselines for such tasks. However, the vast chemical space together\nwith the limited availability of labels makes supervised learning challenging,\ncalling for learning a general-purpose molecular representation. Recently,\npre-trained transformer-based language models (PTLMs) on large unlabeled corpus\nhave produced state-of-the-art results in many downstream natural language\nprocessing tasks. Inspired by this development, here we present molecular\nembeddings obtained by training an efficient transformer encoder model,\nreferred to as MoLFormer. This model was employed with a linear attention\nmechanism and highly paralleized training on 1D SMILES sequences of 1.1 billion\nunlabeled molecules from the PubChem and ZINC datasets. Experiments show that\nthe learned molecular representation performs competitively, when compared to\nexisting graph-based and fingerprint-based supervised learning baselines, on\nthe challenging tasks of predicting properties of QM8 and QM9 molecules.\nFurther task-specific fine-tuning of the MoLFormerr representation improves\nperformance on several of those property prediction benchmarks. These results\nprovide encouraging evidence that large-scale molecular language models can\ncapture sufficient structural information to be able to accurately predict\nquantum chemical properties and beyond.",
          "link": "http://arxiv.org/abs/2106.09553",
          "publishedOn": "2021-06-18T02:06:34.115Z",
          "wordCount": 688,
          "title": "Do Large Scale Molecular Language Representations Capture Important Structural Information?. (arXiv:2106.09553v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09532",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shenoy_A/0/1/0/all/0/1\">Ashish Shenoy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>",
          "description": "Automatic Speech Recognition (ASR) robustness toward slot entities are\ncritical in e-commerce voice assistants that involve monetary transactions and\npurchases. Along with effective domain adaptation, it is intuitive that cross\nutterance contextual cues play an important role in disambiguating domain\nspecific content words from speech. In this paper, we investigate various\ntechniques to improve contextualization, content word robustness and domain\nadaptation of a Transformer-XL neural language model (NLM) to rescore ASR\nN-best hypotheses. To improve contextualization, we utilize turn level dialogue\nacts along with cross utterance context carry over. Additionally, to adapt our\ndomain-general NLM towards e-commerce on-the-fly, we use embeddings derived\nfrom a finetuned masked LM on in-domain data. Finally, to improve robustness\ntowards in-domain content words, we propose a multi-task model that can jointly\nperform content word detection and language modeling tasks. Compared to a\nnon-contextual LSTM LM baseline, our best performing NLM rescorer results in a\ncontent WER reduction of 19.2% on e-commerce audio test set and a slot labeling\nF1 improvement of 6.4%.",
          "link": "http://arxiv.org/abs/2106.09532",
          "publishedOn": "2021-06-18T02:06:34.106Z",
          "wordCount": 633,
          "title": "ASR Adaptation for E-commerce Chatbots using Cross-Utterance Context and Multi-Task Language Modeling. (arXiv:2106.09532v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yaojie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jialong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Annan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Meng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoyi Chen</a>",
          "description": "Event extraction is challenging due to the complex structure of event records\nand the semantic gap between text and event. Traditional methods usually\nextract event records by decomposing the complex structure prediction task into\nmultiple subtasks. In this paper, we propose Text2Event, a\nsequence-to-structure generation paradigm that can directly extract events from\nthe text in an end-to-end manner. Specifically, we design a\nsequence-to-structure network for unified event extraction, a constrained\ndecoding algorithm for event knowledge injection during inference, and a\ncurriculum learning algorithm for efficient model learning. Experimental\nresults show that, by uniformly modeling all tasks in a single model and\nuniversally predicting different labels, our method can achieve competitive\nperformance using only record-level annotations in both supervised learning and\ntransfer learning settings.",
          "link": "http://arxiv.org/abs/2106.09232",
          "publishedOn": "2021-06-18T02:06:34.093Z",
          "wordCount": 570,
          "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction. (arXiv:2106.09232v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xueqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chi Wang</a>",
          "description": "The performance of fine-tuning pre-trained language models largely depends on\nthe hyperparameter configuration. In this paper, we investigate the performance\nof modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained\nlanguage models. First, we study and report three HPO algorithms' performances\non fine-tuning two state-of-the-art language models on the GLUE dataset. We\nfind that using the same time budget, HPO often fails to outperform grid search\ndue to two reasons: insufficient time budget and overfitting. We propose two\ngeneral strategies and an experimental procedure to systematically troubleshoot\nHPO's failure cases. By applying the procedure, we observe that HPO can succeed\nwith more appropriate settings in the search space and time budget; however, in\ncertain cases overfitting remains. Finally, we make suggestions for future\nwork. Our implementation can be found in\nhttps://github.com/microsoft/FLAML/tree/main/flaml/nlp/.",
          "link": "http://arxiv.org/abs/2106.09204",
          "publishedOn": "2021-06-18T02:06:33.971Z",
          "wordCount": 571,
          "title": "An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models. (arXiv:2106.09204v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.03974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1\">Silvia Terragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>",
          "description": "Topic models extract groups of words from documents, whose interpretation as\na topic hopefully allows for a better understanding of the data. However, the\nresulting word groups are often not coherent, making them harder to interpret.\nRecently, neural topic models have shown improvements in overall coherence.\nConcurrently, contextual embeddings have advanced the state of the art of\nneural models in general. In this paper, we combine contextualized\nrepresentations with neural topic models. We find that our approach produces\nmore meaningful and coherent topics than traditional bag-of-words topic models\nand recent neural models. Our results indicate that future improvements in\nlanguage models will translate into better topic models.",
          "link": "http://arxiv.org/abs/2004.03974",
          "publishedOn": "2021-06-18T02:06:33.963Z",
          "wordCount": 581,
          "title": "Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence. (arXiv:2004.03974v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Machacek_D/0/1/0/all/0/1\">Dominik Mach&#xe1;&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilinec_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; &#x17d;ilinec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>",
          "description": "Interpreters facilitate multi-lingual meetings but the affordable set of\nlanguages is often smaller than what is needed. Automatic simultaneous speech\ntranslation can extend the set of provided languages. We investigate if such an\nautomatic system should rather follow the original speaker, or an interpreter\nto achieve better translation quality at the cost of increased delay.\n\nTo answer the question, we release Europarl Simultaneous Interpreting Corpus\n(ESIC), 10 hours of recordings and transcripts of European Parliament speeches\nin English, with simultaneous interpreting into Czech and German. We evaluate\nquality and latency of speaker-based and interpreter-based spoken translation\nsystems from English to Czech. We study the differences in implicit\nsimplification and summarization of the human interpreter compared to a machine\ntranslation system trained to shorten the output to some extent. Finally, we\nperform human evaluation to measure information loss of each of these\napproaches.",
          "link": "http://arxiv.org/abs/2106.09343",
          "publishedOn": "2021-06-18T02:06:33.954Z",
          "wordCount": 581,
          "title": "Lost in Interpreting: Speech Translation from Source or Interpreter?. (arXiv:2106.09343v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Boxi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Lingyong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Meng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>",
          "description": "Previous literatures show that pre-trained masked language models (MLMs) such\nas BERT can achieve competitive factual knowledge extraction performance on\nsome datasets, indicating that MLMs can potentially be a reliable knowledge\nsource. In this paper, we conduct a rigorous study to explore the underlying\npredicting mechanisms of MLMs over different extraction paradigms. By\ninvestigating the behaviors of MLMs, we find that previous decent performance\nmainly owes to the biased prompts which overfit dataset artifacts. Furthermore,\nincorporating illustrative cases and external contexts improve knowledge\nprediction mainly due to entity type guidance and golden answer leakage. Our\nfindings shed light on the underlying predicting mechanisms of MLMs, and\nstrongly question the previous conclusion that current MLMs can potentially\nserve as reliable factual knowledge bases.",
          "link": "http://arxiv.org/abs/2106.09231",
          "publishedOn": "2021-06-18T02:06:33.945Z",
          "wordCount": 574,
          "title": "Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases. (arXiv:2106.09231v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>",
          "description": "Multimodal image-language transformers have achieved impressive results on a\nvariety of tasks that rely on fine-tuning (e.g., visual question answering and\nimage retrieval). We are interested in shedding light on the quality of their\npretrained representations -- in particular, if these models can distinguish\ndifferent types of verbs or if they rely solely on nouns in a given sentence.\nTo do so, we collect a dataset of image-sentence pairs (in English) consisting\nof 421 verbs that are either visual or commonly found in the pretraining data\n(i.e., the Conceptual Captions dataset). We use this dataset to evaluate\npretrained image-language transformers and find that they fail more in\nsituations that require verb understanding compared to other parts of speech.\nWe also investigate what category of verbs are particularly challenging.",
          "link": "http://arxiv.org/abs/2106.09141",
          "publishedOn": "2021-06-18T02:06:33.929Z",
          "wordCount": 560,
          "title": "Probing Image-Language Transformers for Verb Understanding. (arXiv:2106.09141v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09063",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chau_E/0/1/0/all/0/1\">Ethan C. Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Contextualized word representations from pretrained multilingual language\nmodels have become the de facto standard for addressing natural language tasks\nin many different languages, but the success of this approach is far from\nuniversal. For languages rarely or never seen by these models, directly using\nsuch models often results in suboptimal representation or use of data,\nmotivating additional model adaptations to achieve reasonably strong\nperformance. In this work, we study the performance, extensibility, and\ninteraction of two such adaptations for this low-resource setting: vocabulary\naugmentation and script transliteration. Our evaluations on a set of three\ntasks in nine diverse low-resource languages yield a mixed result, upholding\nthe viability of these approaches while raising new questions around how to\noptimally adapt multilingual models to low-resource settings.",
          "link": "http://arxiv.org/abs/2106.09063",
          "publishedOn": "2021-06-18T02:06:33.703Z",
          "wordCount": 552,
          "title": "Specializing Multilingual Language Models: An Empirical Study. (arXiv:2106.09063v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pappadopulo_D/0/1/0/all/0/1\">Duccio Pappadopulo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_L/0/1/0/all/0/1\">Lisa Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farina_M/0/1/0/all/0/1\">Marco Farina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irsoy_O/0/1/0/all/0/1\">Ozan &#x130;rsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Many modern messaging systems allow fast and synchronous textual\ncommunication among many users. The resulting sequence of messages hides a more\ncomplicated structure in which independent sub-conversations are interwoven\nwith one another. This poses a challenge for any task aiming to understand the\ncontent of the chat logs or gather information from them. The ability to\ndisentangle these conversations is then tantamount to the success of many\ndownstream tasks such as summarization and question answering. Structured\ninformation accompanying the text such as user turn, user mentions, timestamps,\nis used as a cue by the participants themselves who need to follow the\nconversation and has been shown to be important for disentanglement. DAG-LSTMs,\na generalization of Tree-LSTMs that can handle directed acyclic dependencies,\nare a natural way to incorporate such information and its non-sequential\nnature. In this paper, we apply DAG-LSTMs to the conversation disentanglement\ntask. We perform our experiments on the Ubuntu IRC dataset. We show that the\nnovel model we propose achieves state of the art status on the task of\nrecovering reply-to relations and it is competitive on other disentanglement\nmetrics.",
          "link": "http://arxiv.org/abs/2106.09024",
          "publishedOn": "2021-06-18T02:06:33.655Z",
          "wordCount": 625,
          "title": "Disentangling Online Chats with DAG-Structured LSTMs. (arXiv:2106.09024v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mille_S/0/1/0/all/0/1\">Simon Mille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh D. Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahamood_S/0/1/0/all/0/1\">Saad Mahamood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Beltrachini_L/0/1/0/all/0/1\">Laura Perez-Beltrachini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miltenburg_E/0/1/0/all/0/1\">Emiel van Miltenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>",
          "description": "Machine learning approaches applied to NLP are often evaluated by summarizing\ntheir performance in a single number, for example accuracy. Since most test\nsets are constructed as an i.i.d. sample from the overall data, this approach\noverly simplifies the complexity of language and encourages overfitting to the\nhead of the data distribution. As such, rare language phenomena or text about\nunderrepresented groups are not equally included in the evaluation. To\nencourage more in-depth model analyses, researchers have proposed the use of\nmultiple test sets, also called challenge sets, that assess specific\ncapabilities of a model. In this paper, we develop a framework based on this\nidea which is able to generate controlled perturbations and identify subsets in\ntext-to-scalar, text-to-text, or data-to-text settings. By applying this\nframework to the GEM generation benchmark, we propose an evaluation suite made\nof 80 challenge sets, demonstrate the kinds of analyses that it enables and\nshed light onto the limits of current generation models.",
          "link": "http://arxiv.org/abs/2106.09069",
          "publishedOn": "2021-06-18T02:06:33.642Z",
          "wordCount": 609,
          "title": "Automatic Construction of Evaluation Suites for Natural Language Generation Datasets. (arXiv:2106.09069v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1\">Richard Antonello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1\">Javier Turek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1\">Vy Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander Huth</a>",
          "description": "How related are the representations learned by neural language models,\ntranslation models, and language tagging tasks? We answer this question by\nadapting an encoder-decoder transfer learning method from computer vision to\ninvestigate the structure among 100 different feature spaces extracted from\nhidden representations of various networks trained on language tasks. This\nmethod reveals a low-dimensional structure where language models and\ntranslation models smoothly interpolate between word embeddings, syntactic and\nsemantic tasks, and future word embeddings. We call this low-dimensional\nstructure a language representation embedding because it encodes the\nrelationships between representations needed to process language for a variety\nof NLP tasks. We find that this representation embedding can predict how well\neach individual feature space maps to human brain responses to natural language\nstimuli recorded using fMRI. Additionally, we find that the principal dimension\nof this structure can be used to create a metric which highlights the brain's\nnatural language processing hierarchy. This suggests that the embedding\ncaptures some part of the brain's natural language representation structure.",
          "link": "http://arxiv.org/abs/2106.05426",
          "publishedOn": "2021-06-17T15:44:16.262Z",
          "wordCount": 628,
          "title": "Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "We investigate the training of sparse layers that use different parameters\nfor different inputs based on hashing in large Transformer models.\nSpecifically, we modify the feedforward layer to hash to different sets of\nweights depending on the current token, over all tokens in the sequence. We\nshow that this procedure either outperforms or is competitive with\nlearning-to-route mixture-of-expert methods such as Switch Transformers and\nBASE Layers, while requiring no routing parameters or extra terms in the\nobjective function such as a load balancing loss, and no sophisticated\nassignment algorithm. We study the performance of different hashing techniques,\nhash sizes and input features, and show that balanced and random hashes focused\non the most local features work best, compared to either learning clusters or\nusing longer-range context. We show our approach works well both on large\nlanguage modeling and dialogue tasks, and on downstream fine-tuning tasks.",
          "link": "http://arxiv.org/abs/2106.04426",
          "publishedOn": "2021-06-17T01:58:44.889Z",
          "wordCount": 587,
          "title": "Hash Layers For Large Sparse Models. (arXiv:2106.04426v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1\">Gullal S. Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1\">Sherzod Hakimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1\">Eric M&#xfc;ller-Budack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>",
          "description": "Opinion and sentiment analysis is a vital task to characterize subjective\ninformation in social media posts. In this paper, we present a comprehensive\nexperimental evaluation and comparison with six state-of-the-art methods, from\nwhich we have re-implemented one of them. In addition, we investigate different\ntextual and visual feature embeddings that cover different aspects of the\ncontent, as well as the recently introduced multimodal CLIP embeddings.\nExperimental results are presented for two different publicly available\nbenchmark datasets of tweets and corresponding images. In contrast to the\nevaluation methodology of previous work, we introduce a reproducible and fair\nevaluation scheme to make results comparable. Finally, we conduct an error\nanalysis to outline the limitations of the methods and possibilities for the\nfuture work.",
          "link": "http://arxiv.org/abs/2106.08829",
          "publishedOn": "2021-06-17T01:58:43.289Z",
          "wordCount": 583,
          "title": "A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment Analysis Methods. (arXiv:2106.08829v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>",
          "description": "This paper introduces the task of factual error correction: performing edits\nto a claim so that the generated rewrite is better supported by evidence. This\nextends the well-studied task of fact verification by providing a mechanism to\ncorrect written texts that are refuted or only partially supported by evidence.\nWe demonstrate that it is feasible to train factual error correction systems\nfrom existing fact checking datasets which only contain labeled claims\naccompanied by evidence, but not the correction. We achieve this by employing a\ntwo-stage distant supervision approach that incorporates evidence into masked\nclaims when generating corrections. Our approach, based on the T5 transformer\nand using retrieved evidence, achieved better results than existing work which\nused a pointer copy network and gold evidence, producing accurate factual error\ncorrections for 5x more instances in human evaluation and a .125 increase in\nSARI score. The evaluation is conducted on a dataset of 65,000 instances based\non a recent fact verification shared task and we release it to enable further\nwork on the task.",
          "link": "http://arxiv.org/abs/2012.15788",
          "publishedOn": "2021-06-17T01:58:42.859Z",
          "wordCount": 620,
          "title": "Evidence-based Factual Error Correction. (arXiv:2012.15788v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hipson_W/0/1/0/all/0/1\">Will E. Hipson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>",
          "description": "Emotion dynamics is a framework for measuring how an individual's emotions\nchange over time. It is a powerful tool for understanding how we behave and\ninteract with the world. In this paper, we introduce a framework to track\nemotion dynamics through one's utterances. Specifically we introduce a number\nof utterance emotion dynamics (UED) metrics inspired by work in Psychology. We\nuse this approach to trace emotional arcs of movie characters. We analyze\nthousands of such character arcs to test hypotheses that inform our broader\nunderstanding of stories. Notably, we show that there is a tendency for\ncharacters to use increasingly more negative words and become increasingly\nemotionally discordant with each other until about 90 percent of the narrative\nlength. UED also has applications in behavior studies, social sciences, and\npublic health.",
          "link": "http://arxiv.org/abs/2103.01345",
          "publishedOn": "2021-06-17T01:58:42.811Z",
          "wordCount": 589,
          "title": "Emotion Dynamics in Movie Dialogues. (arXiv:2103.01345v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boes_W/0/1/0/all/0/1\">Wim Boes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rompaey_R/0/1/0/all/0/1\">Robbe Van Rompaey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verwimp_L/0/1/0/all/0/1\">Lyan Verwimp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelemans_J/0/1/0/all/0/1\">Joris Pelemans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wambacq_P/0/1/0/all/0/1\">Patrick Wambacq</a>",
          "description": "We inspect the long-term learning ability of Long Short-Term Memory language\nmodels (LSTM LMs) by evaluating a contextual extension based on the Continuous\nBag-of-Words (CBOW) model for both sentence- and discourse-level LSTM LMs and\nby analyzing its performance. We evaluate on text and speech. Sentence-level\nmodels using the long-term contextual module perform comparably to vanilla\ndiscourse-level LSTM LMs. On the other hand, the extension does not provide\ngains for discourse-level models. These findings indicate that discourse-level\nLSTM LMs already rely on contextual information to perform long-term learning.",
          "link": "http://arxiv.org/abs/2106.08927",
          "publishedOn": "2021-06-17T01:58:42.805Z",
          "wordCount": 540,
          "title": "On the long-term learning ability of LSTM LMs. (arXiv:2106.08927v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaijing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fengxiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tieyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yusen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Ming Zhao</a>",
          "description": "For the task of conversation emotion recognition, recent works focus on\nspeaker relationship modeling but ignore the role of utterance's emotional\ntendency.In this paper, we propose a new expression paradigm of sentence-level\nemotion orientation vector to model the potential correlation of emotions\nbetween sentence vectors. Based on it, we design an emotion recognition model,\nwhich extracts the sentence-level emotion orientation vectors from the language\nmodel and jointly learns from the dialogue sentiment analysis model and\nextracted sentence-level emotion orientation vectors to identify the speaker's\nemotional orientation during the conversation. We conduct experiments on two\nbenchmark datasets and compare them with the five baseline models.The\nexperimental results show that our model has better performance on all data\nsets.",
          "link": "http://arxiv.org/abs/2106.08785",
          "publishedOn": "2021-06-17T01:58:42.405Z",
          "wordCount": 555,
          "title": "SEOVER: Sentence-level Emotion Orientation Vector based Conversation Emotion Recognition Model. (arXiv:2106.08785v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Illa_M/0/1/0/all/0/1\">Marc Illa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halpern_B/0/1/0/all/0/1\">Bence Mark Halpern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_R/0/1/0/all/0/1\">Rob van Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moro_Velazquez_L/0/1/0/all/0/1\">Laureano Moro-Velazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharenborg_O/0/1/0/all/0/1\">Odette Scharenborg</a>",
          "description": "In this paper, we propose a new approach to pathological speech synthesis.\nInstead of using healthy speech as a source, we customise an existing\npathological speech sample to a new speaker's voice characteristics. This\napproach alleviates the evaluation problem one normally has when converting\ntypical speech to pathological speech, as in our approach, the voice conversion\n(VC) model does not need to be optimised for speech degradation but only for\nthe speaker change. This change in the optimisation ensures that any\ndegradation found in naturalness is due to the conversion process and not due\nto the model exaggerating characteristics of a speech pathology. To show a\nproof of concept of this method, we convert dysarthric speech using the\nUASpeech database and an autoencoder-based VC technique. Subjective evaluation\nresults show reasonable naturalness for high intelligibility dysarthric\nspeakers, though lower intelligibility seems to introduce a marginal\ndegradation in naturalness scores for mid and low intelligibility speakers\ncompared to ground truth. Conversion of speaker characteristics for low and\nhigh intelligibility speakers is successful, but not for mid. Whether the\ndifferences in the results for the different intelligibility levels is due to\nthe intelligibility levels or due to the speakers needs to be further\ninvestigated.",
          "link": "http://arxiv.org/abs/2106.08427",
          "publishedOn": "2021-06-17T01:58:42.393Z",
          "wordCount": 650,
          "title": "Pathological voice adaptation with autoencoder-based voice conversion. (arXiv:2106.08427v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>",
          "description": "Dialogue policy optimisation via reinforcement learning requires a large\nnumber of training interactions, which makes learning with real users time\nconsuming and expensive. Many set-ups therefore rely on a user simulator\ninstead of humans. These user simulators have their own problems. While\nhand-coded, rule-based user simulators have been shown to be sufficient in\nsmall, simple domains, for complex domains the number of rules quickly becomes\nintractable. State-of-the-art data-driven user simulators, on the other hand,\nare still domain-dependent. This means that adaptation to each new domain\nrequires redesigning and retraining. In this work, we propose a\ndomain-independent transformer-based user simulator (TUS). The structure of our\nTUS is not tied to a specific domain, enabling domain generalisation and\nlearning of cross-domain user behaviour from data. We compare TUS with the\nstate of the art using automatic as well as human evaluations. TUS can compete\nwith rule-based user simulators on pre-defined domains and is able to\ngeneralise to unseen domains in a zero-shot fashion.",
          "link": "http://arxiv.org/abs/2106.08838",
          "publishedOn": "2021-06-17T01:58:42.340Z",
          "wordCount": 597,
          "title": "Domain-independent User Simulation with Transformers for Task-oriented Dialogue Systems. (arXiv:2106.08838v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1\">Marco Baroni</a>",
          "description": "A lively research field has recently emerged that uses experimental methods\nto probe the linguistic behavior of modern deep networks. While work in this\ntradition often reports intriguing results about the grammatical skills of deep\nnets, it is not clear what their implications for linguistic theorizing should\nbe. As a consequence, linguistically-oriented deep net analysis has had very\nlittle impact on linguistics at large. In this chapter, I suggest that deep\nnetworks should be treated as theories making explicit predictions about the\nacceptability of linguistic utterances. I argue that, if we overcome some\nobstacles standing in the way of seriously pursuing this idea, we will gain a\npowerful new theoretical tool, complementary to mainstream algebraic\napproaches.",
          "link": "http://arxiv.org/abs/2106.08694",
          "publishedOn": "2021-06-17T01:58:42.035Z",
          "wordCount": 559,
          "title": "On the proper role of linguistically-oriented deep net analysis in linguistic theorizing. (arXiv:2106.08694v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "Word segmentation and part-of-speech tagging are two critical preliminary\nsteps for downstream tasks in Vietnamese natural language processing. In\nreality, people tend to consider also the phrase boundary when performing word\nsegmentation and part of speech tagging rather than solely process word by word\nfrom left to right. In this paper, we implement this idea to improve word\nsegmentation and part of speech tagging the Vietnamese language by employing a\nsimplified constituency parser. Our neural model for joint word segmentation\nand part-of-speech tagging has the architecture of the syllable-based CRF\nconstituency parser. To reduce the complexity of parsing, we replace all\nconstituent labels with a single label indicating for phrases. This model can\nbe augmented with predicted word boundary and part-of-speech tags by other\ntools. Because Vietnamese and Chinese have some similar linguistic phenomena,\nwe evaluated the proposed model and its augmented versions on three Vietnamese\nbenchmark datasets and six Chinese benchmark datasets. Our experimental results\nshow that the proposed model achieves higher performances than previous works\nfor both languages.",
          "link": "http://arxiv.org/abs/2102.12136",
          "publishedOn": "2021-06-17T01:58:41.744Z",
          "wordCount": 662,
          "title": "Augmenting Part-of-speech Tagging with Syntactic Information for Vietnamese and Chinese. (arXiv:2102.12136v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weizhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kewen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiusheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>",
          "description": "In this paper, we propose BANG, a new pretraining model to Bridge the gap\nbetween Autoregressive (AR) and Non-autoregressive (NAR) Generation. AR and NAR\ngeneration can be uniformly regarded as to what extent previous tokens can be\nattended, and BANG bridges AR and NAR generation by designing a novel model\nstructure for large-scale pretraining. The pretrained BANG model can\nsimultaneously support AR, NAR and semi-NAR generation to meet different\nrequirements. Experiments on question generation (SQuAD 1.1), summarization\n(XSum) and dialogue generation (PersonaChat) show that BANG improves NAR and\nsemi-NAR performance significantly as well as attaining comparable performance\nwith strong AR pretrained models. Compared with the semi-NAR strong baselines,\nBANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of\nSQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute\nimprovements of 10.73, 6.39 and 5.90 in the overall scores of SQuAD, XSUM and\nPersonaChat respectively compared with the strong NAR baselines.",
          "link": "http://arxiv.org/abs/2012.15525",
          "publishedOn": "2021-06-17T01:58:41.703Z",
          "wordCount": 644,
          "title": "BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining. (arXiv:2012.15525v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>",
          "description": "Proposal of large-scale datasets has facilitated research on deep neural\nmodels for news summarization. Deep learning can also be potentially useful for\nspoken dialogue summarization, which can benefit a range of real-life scenarios\nincluding customer service management and medication tracking. To this end, we\npropose DialogSum, a large-scale labeled dialogue summarization dataset. We\nconduct empirical analysis on DialogSum using state-of-the-art neural\nsummarizers. Experimental results show unique challenges in dialogue\nsummarization, such as spoken terms, special discourse structures, coreferences\nand ellipsis, pragmatics and social common sense, which require specific\nrepresentation learning technologies to better deal with.",
          "link": "http://arxiv.org/abs/2105.06762",
          "publishedOn": "2021-06-17T01:58:41.617Z",
          "wordCount": 571,
          "title": "DialogSum: A Real-Life Scenario Dialogue Summarization Dataset. (arXiv:2105.06762v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Haipeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuxue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakari_R/0/1/0/all/0/1\">Rufai Yusuf Zakari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owusu_J/0/1/0/all/0/1\">Jim Wilson Owusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Ke Qin</a>",
          "description": "Knowledge graph embedding has been an active research topic for knowledge\nbase completion (KGC), with progressive improvement from the initial TransE,\nTransH, RotatE et al to the current state-of-the-art QuatE. However, QuatE\nignores the multi-faceted nature of the entity and the complexity of the\nrelation, only using rigorous operation on quaternion space to capture the\ninteraction between entitiy pair and relation, leaving opportunities for better\nknowledge representation which will finally help KGC. In this paper, we propose\na novel model, QuatDE, with a dynamic mapping strategy to explicitly capture\nthe variety of relational patterns and separate different semantic information\nof the entity, using transition vectors to adjust the point position of the\nentity embedding vectors in the quaternion space via Hamilton product,\nenhancing the feature interaction capability between elements of the triplet.\nExperiment results show QuatDE achieves state-of-the-art performance on three\nwell-established knowledge graph completion benchmarks. In particular, the MR\nevaluation has relatively increased by 26% on WN18 and 15% on WN18RR, which\nproves the generalization of QuatDE.",
          "link": "http://arxiv.org/abs/2105.09002",
          "publishedOn": "2021-06-17T01:58:41.610Z",
          "wordCount": 627,
          "title": "QuatDE: Dynamic Quaternion Embedding for Knowledge Graph Completion. (arXiv:2105.09002v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08459",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mussakhojayeva_S/0/1/0/all/0/1\">Saida Mussakhojayeva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Janaliyeva_A/0/1/0/all/0/1\">Aigerim Janaliyeva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mirzakhmetov_A/0/1/0/all/0/1\">Almas Mirzakhmetov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khassanov_Y/0/1/0/all/0/1\">Yerbolat Khassanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>",
          "description": "This paper introduces a high-quality open-source speech synthesis dataset for\nKazakh, a low-resource language spoken by over 13 million people worldwide. The\ndataset consists of about 93 hours of transcribed audio recordings spoken by\ntwo professional speakers (female and male). It is the first publicly available\nlarge-scale dataset developed to promote Kazakh text-to-speech (TTS)\napplications in both academia and industry. In this paper, we share our\nexperience by describing the dataset development procedures and faced\nchallenges, and discuss important future directions. To demonstrate the\nreliability of our dataset, we built baseline end-to-end TTS models and\nevaluated them using the subjective mean opinion score (MOS) measure.\nEvaluation results show that the best TTS models trained on our dataset achieve\nMOS above 4 for both speakers, which makes them applicable for practical use.\nThe dataset, training recipe, and pretrained TTS models are freely available.",
          "link": "http://arxiv.org/abs/2104.08459",
          "publishedOn": "2021-06-17T01:58:41.604Z",
          "wordCount": 619,
          "title": "KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset. (arXiv:2104.08459v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1\">Krishna Pillutla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1\">John Thickstun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "As major progress is made in open-ended text generation, measuring how close\nmachine-generated text is to human language remains a critical open problem. We\npropose Mauve, a comparison measure for open-ended text generation, which\ndirectly compares a generation model's distribution to that of human-written\ntext. Mauve measures the mean area under a divergence curve for the two\ndistributions, exploring the trade-off between two types of errors: those\narising from parts of the human distribution that the model distribution\napproximates well, and those it does not. Mauve extends a family of information\ndivergence metrics, introducing a tractable approximation based on computing\nthe KL divergence in a quantized embedding space. This yields an efficient\nimplementation that scales up to modern text generation models. Through an\nextensive empirical study on three open-ended generation tasks, we find that\nMauve identifies known properties of generated text, scales naturally with\nmodel size, and correlates with human judgments, with fewer restrictions than\nexisting distributional evaluation metrics.",
          "link": "http://arxiv.org/abs/2102.01454",
          "publishedOn": "2021-06-17T01:58:41.552Z",
          "wordCount": 625,
          "title": "An Information Divergence Measure Between Neural Text and Human Text. (arXiv:2102.01454v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1\">Damir Koren&#x10d;i&#x107;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ristov_S/0/1/0/all/0/1\">Strahil Ristov</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Repar_J/0/1/0/all/0/1\">Jelena Repar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a> (2) ((1) Rudjer Bo&#x161;kovi&#x107; Institute, Croatia, (2) University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)",
          "description": "Topic models are widely used unsupervised models of text capable of learning\ntopics - weighted lists of words and documents - from large collections of text\ndocuments. When topic models are used for discovery of topics in text\ncollections, a question that arises naturally is how well the model-induced\ntopics correspond to topics of interest to the analyst. In this paper we\nrevisit and extend a so far neglected approach to topic model evaluation based\non measuring topic coverage - computationally matching model topics with a set\nof reference topics that models are expected to uncover. The approach is well\nsuited for analyzing models' performance in topic discovery and for large-scale\nanalysis of both topic models and measures of model quality. We propose new\nmeasures of coverage and evaluate, in a series of experiments, different types\nof topic models on two distinct text domains for which interest for topic\ndiscovery exists. The experiments include evaluation of model quality, analysis\nof coverage of distinct topic categories, and the analysis of the relationship\nbetween coverage and other methods of topic model evaluation. The contributions\nof the paper include new measures of coverage, insights into both topic models\nand other methods of model evaluation, and the datasets and code for\nfacilitating future research of both topic coverage and other approaches to\ntopic model evaluation.",
          "link": "http://arxiv.org/abs/2012.06274",
          "publishedOn": "2021-06-17T01:58:41.545Z",
          "wordCount": 741,
          "title": "A Topic Coverage Approach to Evaluation of Topic Models. (arXiv:2012.06274v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerz_E/0/1/0/all/0/1\">Elma Kerz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>",
          "description": "In recent years, automated approaches to assessing linguistic complexity in\nsecond language (L2) writing have made significant progress in gauging learner\nperformance, predicting human ratings of the quality of learner productions,\nand benchmarking L2 development. In contrast, there is comparatively little\nwork in the area of speaking, particularly with respect to fully automated\napproaches to assessing L2 spontaneous speech. While the importance of a\nwell-performing ASR system is widely recognized, little research has been\nconducted to investigate the impact of its performance on subsequent automatic\ntext analysis. In this paper, we focus on this issue and examine the impact of\nusing a state-of-the-art ASR system for subsequent automatic analysis of\nlinguistic complexity in spontaneously produced L2 speech. A set of 30 selected\nmeasures were considered, falling into four categories: syntactic, lexical,\nn-gram frequency, and information-theoretic measures. The agreement between the\nscores for these measures obtained on the basis of ASR-generated vs. manual\ntranscriptions was determined through correlation analysis. A more differential\neffect of ASR performance on specific types of complexity measures when\ncontrolling for task type effects is also presented.",
          "link": "http://arxiv.org/abs/2104.08529",
          "publishedOn": "2021-06-17T01:58:41.539Z",
          "wordCount": 655,
          "title": "The Impact of ASR on the Automatic Analysis of Linguistic Complexity and Sophistication in Spontaneous L2 Speech. (arXiv:2104.08529v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huihan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Pre-trained language models have been successful on text classification\ntasks, but are prone to learning spurious correlations from biased datasets,\nand are thus vulnerable when making inferences in a new domain. Prior works\nreveal such spurious patterns via post-hoc explanation algorithms which compute\nthe importance of input features. Further, the model is regularized to align\nthe importance scores with human knowledge, so that the unintended model\nbehaviors are eliminated. However, such a regularization technique lacks\nflexibility and coverage, since only importance scores towards a pre-defined\nlist of features are adjusted, while more complex human knowledge such as\nfeature interaction and pattern generalization can hardly be incorporated. In\nthis work, we propose to refine a learned language model for a target domain by\ncollecting human-provided compositional explanations regarding observed biases.\nBy parsing these explanations into executable logic rules, the human-specified\nrefinement advice from a small set of explanations can be generalized to more\ntraining examples. We additionally introduce a regularization term allowing\nadjustments for both importance and interaction of features to better rectify\nmodel behavior. We demonstrate the effectiveness of the proposed approach on\ntwo text classification tasks by showing improved performance in target domain\nas well as improved model fairness after refinement.",
          "link": "http://arxiv.org/abs/2103.10415",
          "publishedOn": "2021-06-17T01:58:41.531Z",
          "wordCount": 663,
          "title": "Refining Language Models with Compositional Explanations. (arXiv:2103.10415v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rio_M/0/1/0/all/0/1\">Miguel Del Rio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delworth_N/0/1/0/all/0/1\">Natalie Delworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westerman_R/0/1/0/all/0/1\">Ryan Westerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Michelle Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1\">Nishchal Bhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palakapilly_J/0/1/0/all/0/1\">Joseph Palakapilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNamara_Q/0/1/0/all/0/1\">Quinten McNamara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Joshua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelasko_P/0/1/0/all/0/1\">Piotr Zelasko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jette_M/0/1/0/all/0/1\">Miguel Jette</a>",
          "description": "Commonly used speech corpora inadequately challenge academic and commercial\nASR systems. In particular, speech corpora lack metadata needed for detailed\nanalysis and WER measurement. In response, we present Earnings-21, a 39-hour\ncorpus of earnings calls containing entity-dense speech from nine different\nfinancial sectors. This corpus is intended to benchmark ASR systems in the wild\nwith special attention towards named entity recognition. We benchmark four\ncommercial ASR models, two internal models built with open-source tools, and an\nopen-source LibriSpeech model and discuss their differences in performance on\nEarnings-21. Using our recently released fstalign tool, we provide a candid\nanalysis of each model's recognition capabilities under different partitions.\nOur analysis finds that ASR accuracy for certain NER categories is poor,\npresenting a significant impediment to transcript comprehension and usage.\nEarnings-21 bridges academic and commercial ASR system evaluation and enables\nfurther research on entity modeling and WER on real world audio.",
          "link": "http://arxiv.org/abs/2104.11348",
          "publishedOn": "2021-06-17T01:58:41.524Z",
          "wordCount": 700,
          "title": "Earnings-21: A Practical Benchmark for ASR in the Wild. (arXiv:2104.11348v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.09764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xianghong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoli Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "Variational autoencoders have been widely applied for natural language\ngeneration, however, there are two long-standing problems: information\nunder-representation and posterior collapse. The former arises from the fact\nthat only the last hidden state from the encoder is transformed to the latent\nspace, which is insufficient to summarize data. The latter comes as a result of\nthe imbalanced scale between the reconstruction loss and the KL divergence in\nthe objective function. To tackle these issues, in this paper we propose the\ndiscrete variational attention model with categorical distribution over the\nattention mechanism owing to the discrete nature in languages. Our approach is\ncombined with an auto-regressive prior to capture the sequential dependency\nfrom observations, which can enhance the latent space for language generation.\nMoreover, thanks to the property of discreteness, the training of our proposed\napproach does not suffer from posterior collapse. Furthermore, we carefully\nanalyze the superiority of discrete latent space over the continuous space with\nthe common Gaussian distribution. Extensive experiments on language generation\ndemonstrate superior advantages of our proposed approach in comparison with the\nstate-of-the-art counterparts.",
          "link": "http://arxiv.org/abs/2004.09764",
          "publishedOn": "2021-06-17T01:58:41.517Z",
          "wordCount": 665,
          "title": "Discrete Variational Attention Models for Language Generation. (arXiv:2004.09764v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asgari_E/0/1/0/all/0/1\">Ehsaneddin Asgari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1\">Philipp Dufter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ringlstetter_C/0/1/0/all/0/1\">Christopher Ringlstetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "Annotation projection is an important area in NLP that can greatly contribute\nto creating language resources for low-resource languages. Word alignment plays\na key role in this setting. However, most of the existing word alignment\nmethods are designed for a high resource setting in machine translation where\nmillions of parallel sentences are available. This amount reduces to a few\nthousands of sentences when dealing with low-resource languages failing the\nexisting established IBM models. In this paper, we propose subword\nsampling-based alignment of text units. This method's hypothesis is that the\naggregation of different granularities of text for certain language pairs can\nhelp word-level alignment. For certain languages for which gold-standard\nalignments exist, we propose an iterative Bayesian optimization framework to\noptimize selecting possible subwords from the space of possible subword\nrepresentations of the source and target sentences. We show that the subword\nsampling method consistently outperforms word-level alignment on six language\npairs: English-German, English-French, English-Romanian, English-Persian,\nEnglish-Hindi, and English-Inuktitut. In addition, we show that the\nhyperparameters learned for certain language pairs can be applied to other\nlanguages at no supervision and consistently improve the alignment results. We\nobserve that using $5K$ parallel sentences together with our proposed subword\nsampling approach, we obtain similar F1 scores to the use of $100K$'s of\nparallel sentences in existing word-level fast-align/eflomal alignment methods.",
          "link": "http://arxiv.org/abs/2012.11657",
          "publishedOn": "2021-06-17T01:58:41.489Z",
          "wordCount": 676,
          "title": "Subword Sampling for Low Resource Word Alignment. (arXiv:2012.11657v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00804",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sundararaman_M/0/1/0/all/0/1\">Mukuntha Narayanan Sundararaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vepa_J/0/1/0/all/0/1\">Jithendra Vepa</a>",
          "description": "Recent years have witnessed significant improvement in ASR systems to\nrecognize spoken utterances. However, it is still a challenging task for noisy\nand out-of-domain data, where substitution and deletion errors are prevalent in\nthe transcribed text. These errors significantly degrade the performance of\ndownstream tasks. In this work, we propose a BERT-style language model,\nreferred to as PhonemeBERT, that learns a joint language model with phoneme\nsequence and ASR transcript to learn phonetic-aware representations that are\nrobust to ASR errors. We show that PhonemeBERT can be used on downstream tasks\nusing phoneme sequences as additional features, and also in low-resource setup\nwhere we only have ASR-transcripts for the downstream tasks with no phoneme\ninformation available. We evaluate our approach extensively by generating noisy\ndata for three benchmark datasets - Stanford Sentiment Treebank, TREC and ATIS\nfor sentiment, question and intent classification tasks respectively. The\nresults of the proposed approach beats the state-of-the-art baselines\ncomprehensively on each dataset.",
          "link": "http://arxiv.org/abs/2102.00804",
          "publishedOn": "2021-06-17T01:58:41.472Z",
          "wordCount": 619,
          "title": "Phoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript. (arXiv:2102.00804v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>",
          "description": "Publicly available, large pretrained LanguageModels (LMs) generate text with\nremarkable quality, but only sequentially from left to right. As a result, they\nare not immediately applicable to generation tasks that break the\nunidirectional assumption, such as paraphrasing or text-infilling,\nnecessitating task-specific supervision.\n\nIn this paper, we present Reflective Decoding, a novel unsupervised algorithm\nthat allows for direct application of unidirectional LMs to non-sequential\ntasks. Our 2-step approach requires no supervision or even parallel corpora,\nonly two off-the-shelf pretrained LMs in opposite directions: forward and\nbackward. First, in the contextualization step, we use LMs to generate\nensembles of past and future contexts which collectively capture the input\n(e.g. the source sentence for paraphrasing). Second, in the reflection step, we\ncondition on these \"context ensembles\", generating outputs that are compatible\nwith them. Comprehensive empirical results demonstrate that Reflective Decoding\noutperforms strong unsupervised baselines on both paraphrasing and abductive\ntext infilling, significantly narrowing the gap between unsupervised and\nsupervised methods. Reflective Decoding surpasses multiple supervised baselines\non various metrics including human evaluation.",
          "link": "http://arxiv.org/abs/2010.08566",
          "publishedOn": "2021-06-17T01:58:41.430Z",
          "wordCount": 640,
          "title": "Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models. (arXiv:2010.08566v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ates_T/0/1/0/all/0/1\">Tayfun Ates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atesoglu_M/0/1/0/all/0/1\">Muhammed Samil Atesoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yigit_C/0/1/0/all/0/1\">Cagatay Yigit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1\">Ilker Kesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobas_M/0/1/0/all/0/1\">Mert Kobas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksun_T/0/1/0/all/0/1\">Tilbe Goksun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>",
          "description": "Humans are able to perceive, understand and reason about physical events.\nDeveloping models with similar physical understanding capabilities is a\nlong-standing goal of artificial intelligence. As a step towards this goal, in\nthis work, we introduce CRAFT, a new visual question answering dataset that\nrequires causal reasoning about physical forces and object interactions. It\ncontains 58K video and question pairs that are generated from 10K videos from\n20 different virtual environments, containing various objects in motion that\ninteract with each other and the scene. Two question categories from CRAFT\ninclude previously studied descriptive and counterfactual questions. Besides,\ninspired by the theories of force dynamics in cognitive linguistics, we\nintroduce new question categories that involve understanding the interactions\nof objects through the notions of cause, enable, and prevent. Our results\ndemonstrate that even though these tasks seem to be simple and intuitive for\nhumans, the evaluated baseline models, including existing state-of-the-art\nmethods, do not yet deal with the challenges posed in our benchmark dataset.",
          "link": "http://arxiv.org/abs/2012.04293",
          "publishedOn": "2021-06-17T01:58:41.390Z",
          "wordCount": 659,
          "title": "CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions. (arXiv:2012.04293v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.09991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fausti_F/0/1/0/all/0/1\">Fabrizio De Fausti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugliese_F/0/1/0/all/0/1\">Francesco Pugliese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zardetto_D/0/1/0/all/0/1\">Diego Zardetto</a>",
          "description": "In recent years, the interest in Big Data sources has been steadily growing\nwithin the Official Statistic community. The Italian National Institute of\nStatistics (Istat) is currently carrying out several Big Data pilot studies.\nOne of these studies, the ICT Big Data pilot, aims at exploiting massive\namounts of textual data automatically scraped from the websites of Italian\nenterprises in order to predict a set of target variables (e.g. e-commerce)\nthat are routinely observed by the traditional ICT Survey. In this paper, we\nshow that Deep Learning techniques can successfully address this problem.\nEssentially, we tackle a text classification task: an algorithm must learn to\ninfer whether an Italian enterprise performs e-commerce from the textual\ncontent of its website. To reach this goal, we developed a sophisticated\nprocessing pipeline and evaluated its performance through extensive\nexperiments. Our pipeline uses Convolutional Neural Networks and relies on Word\nEmbeddings to encode raw texts into grayscale images (i.e. normalized numeric\nmatrices). Web-scraped texts are huge and have very low signal to noise ratio:\nto overcome these issues, we adopted a framework known as False Positive\nReduction, which has seldom (if ever) been applied before to text\nclassification tasks. Several original contributions enable our processing\npipeline to reach good classification results. Empirical evidence shows that\nour proposal outperforms all the alternative Machine Learning solutions already\ntested in Istat for the same task.",
          "link": "http://arxiv.org/abs/1910.09991",
          "publishedOn": "2021-06-17T01:58:41.379Z",
          "wordCount": 714,
          "title": "Towards Automated Website Classification by Deep Learning. (arXiv:1910.09991v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>",
          "description": "Video-grounded dialogue systems aim to integrate video understanding and\ndialogue understanding to generate responses that are relevant to both the\ndialogue and video context. Most existing approaches employ deep learning\nmodels and have achieved remarkable performance, given the relatively small\ndatasets available. However, the results are partly accomplished by exploiting\nbiases in the datasets rather than developing multimodal reasoning, resulting\nin limited generalization. In this paper, we propose a novel approach of\nCompositional Counterfactual Contrastive Learning ($C^3$) to develop\ncontrastive training between factual and counterfactual samples in\nvideo-grounded dialogues. Specifically, we design factual/counterfactual\nsampling based on the temporal steps in videos and tokens in dialogues and\npropose contrastive loss functions that exploit object-level or action-level\nvariance. Different from prior approaches, we focus on contrastive hidden state\nrepresentations among compositional output tokens to optimize the\nrepresentation space in a generation setting. We achieved promising performance\ngains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the\nbenefits of our approach in grounding video and dialogue context.",
          "link": "http://arxiv.org/abs/2106.08914",
          "publishedOn": "2021-06-17T01:58:41.308Z",
          "wordCount": 608,
          "title": "$C^3$: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagaraja_V/0/1/0/all/0/1\">Varun Nagaraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Ganesh Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>",
          "description": "On-device speech recognition requires training models of different sizes for\ndeploying on devices with various computational budgets. When building such\ndifferent models, we can benefit from training them jointly to take advantage\nof the knowledge shared between them. Joint training is also efficient since it\nreduces the redundancy in the training procedure's data handling operations. We\npropose a method for collaboratively training acoustic encoders of different\nsizes for speech recognition. We use a sequence transducer setup where\ndifferent acoustic encoders share a common predictor and joiner modules. The\nacoustic encoders are also trained using co-distillation through an auxiliary\ntask for frame level chenone prediction, along with the transducer loss. We\nperform experiments using the LibriSpeech corpus and demonstrate that the\ncollaboratively trained acoustic encoders can provide up to a 11% relative\nimprovement in the word error rate on both the test partitions.",
          "link": "http://arxiv.org/abs/2106.08960",
          "publishedOn": "2021-06-17T01:58:41.287Z",
          "wordCount": 576,
          "title": "Collaborative Training of Acoustic Encoders for Speech Recognition. (arXiv:2106.08960v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Yang Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianping Shen</a>",
          "description": "Recently developed large pre-trained language models, e.g., BERT, have\nachieved remarkable performance in many downstream natural language processing\napplications. These pre-trained language models often contain hundreds of\nmillions of parameters and suffer from high computation and latency in\nreal-world applications. It is desirable to reduce the computation overhead of\nthe models for fast training and inference while keeping the model performance\nin downstream applications. Several lines of work utilize knowledge\ndistillation to compress the teacher model to a smaller student model. However,\nthey usually discard the teacher's knowledge when in inference. Differently, in\nthis paper, we propose RefBERT to leverage the knowledge learned from the\nteacher, i.e., facilitating the pre-computed BERT representation on the\nreference sample and compressing BERT into a smaller student model. To\nguarantee our proposal, we provide theoretical justification on the loss\nfunction and the usage of reference samples. Significantly, the theoretical\nresult shows that including the pre-computed teacher's representations on the\nreference samples indeed increases the mutual information in learning the\nstudent model. Finally, we conduct the empirical evaluation and show that our\nRefBERT can beat the vanilla TinyBERT over 8.1\\% and achieves more than 94\\% of\nthe performance of $\\BERTBASE$ on the GLUE benchmark. Meanwhile, RefBERT is\n7.4x smaller and 9.5x faster on inference than BERT$_{\\rm BASE}$.",
          "link": "http://arxiv.org/abs/2106.08898",
          "publishedOn": "2021-06-17T01:58:41.281Z",
          "wordCount": 652,
          "title": "RefBERT: Compressing BERT by Referencing to Pre-computed Representations. (arXiv:2106.08898v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Samridhi Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKenna_J/0/1/0/all/0/1\">Joseph P. McKenna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>",
          "description": "End-to-end (E2E) spoken language understanding (SLU) systems predict\nutterance semantics directly from speech using a single model. Previous work in\nthis area has focused on targeted tasks in fixed domains, where the output\nsemantic structure is assumed a priori and the input speech is of limited\ncomplexity. In this work we present our approach to developing an E2E model for\ngeneralized SLU in commercial voice assistants (VAs). We propose a fully\ndifferentiable, transformer-based, hierarchical system that can be pretrained\nat both the ASR and NLU levels. This is then fine-tuned on both transcription\nand semantic classification losses to handle a diverse set of intent and\nargument combinations. This leads to an SLU system that achieves significant\nimprovements over baselines on a complex internal generalized VA dataset with a\n43% improvement in accuracy, while still meeting the 99% accuracy benchmark on\nthe popular Fluent Speech Commands dataset. We further evaluate our model on a\nhard test set, exclusively containing slot arguments unseen in training, and\ndemonstrate a nearly 20% improvement, showing the efficacy of our approach in\ntruly demanding VA scenarios.",
          "link": "http://arxiv.org/abs/2106.09009",
          "publishedOn": "2021-06-17T01:58:41.271Z",
          "wordCount": 632,
          "title": "End-to-End Spoken Language Understanding for Generalized Voice Assistants. (arXiv:2106.09009v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>",
          "description": "Weak supervision has shown promising results in many natural language\nprocessing tasks, such as Named Entity Recognition (NER). Existing work mainly\nfocuses on learning deep NER models only with weak supervision, i.e., without\nany human annotation, and shows that by merely using weakly labeled data, one\ncan achieve good performance, though still underperforms fully supervised NER\nwith manually/strongly labeled data. In this paper, we consider a more\npractical scenario, where we have both a small amount of strongly labeled data\nand a large amount of weakly labeled data. Unfortunately, we observe that\nweakly labeled data does not necessarily improve, or even deteriorate the model\nperformance (due to the extensive noise in the weak labels) when we train deep\nNER models over a simple or weighted combination of the strongly labeled and\nweakly labeled data. To address this issue, we propose a new multi-stage\ncomputational framework -- NEEDLE with three essential ingredients: (1) weak\nlabel completion, (2) noise-aware loss function, and (3) final fine-tuning over\nthe strongly labeled data. Through experiments on E-commerce query NER and\nBiomedical NER, we demonstrate that NEEDLE can effectively suppress the noise\nof the weak labels and outperforms existing methods. In particular, we achieve\nnew SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,\nBC5CDR-disease 90.69, NCBI-disease 92.28.",
          "link": "http://arxiv.org/abs/2106.08977",
          "publishedOn": "2021-06-17T01:58:41.262Z",
          "wordCount": 669,
          "title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jialong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Meng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yaojie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weijian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>",
          "description": "Current event-centric knowledge graphs highly rely on explicit connectives to\nmine relations between events. Unfortunately, due to the sparsity of\nconnectives, these methods severely undermine the coverage of EventKGs. The\nlack of high-quality labelled corpora further exacerbates that problem. In this\npaper, we propose a knowledge projection paradigm for event relation\nextraction: projecting discourse knowledge to narratives by exploiting the\ncommonalities between them. Specifically, we propose Multi-tier Knowledge\nProjection Network (MKPNet), which can leverage multi-tier discourse knowledge\neffectively for event relation extraction. In this way, the labelled data\nrequirement is significantly reduced, and implicit event relations can be\neffectively extracted. Intrinsic experimental results show that MKPNet achieves\nthe new state-of-the-art performance, and extrinsic experimental results verify\nthe value of the extracted event relations.",
          "link": "http://arxiv.org/abs/2106.08629",
          "publishedOn": "2021-06-17T01:58:41.255Z",
          "wordCount": 567,
          "title": "From Discourse to Narrative: Knowledge Projection for Event Relation Extraction. (arXiv:2106.08629v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1\">Tristan Karch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1\">Laetitia Teodorescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1\">Cl&#xe9;ment Moulin-Frier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Language is an interface to the outside world. In order for embodied agents\nto use it, language must be grounded in other, sensorimotor modalities. While\nthere is an extended literature studying how machines can learn grounded\nlanguage, the topic of how to learn spatio-temporal linguistic concepts is\nstill largely uncharted. To make progress in this direction, we here introduce\na novel spatio-temporal language grounding task where the goal is to learn the\nmeaning of spatio-temporal descriptions of behavioral traces of an embodied\nagent. This is achieved by training a truth function that predicts if a\ndescription matches a given history of observations. The descriptions involve\ntime-extended predicates in past and present tense as well as spatio-temporal\nreferences to objects in the scene. To study the role of architectural biases\nin this task, we train several models including multimodal Transformer\narchitectures; the latter implement different attention computations between\nwords and objects across space and time. We test models on two classes of\ngeneralization: 1) generalization to randomly held-out sentences; 2)\ngeneralization to grammar primitives. We observe that maintaining object\nidentity in the attention computation of our Transformers is instrumental to\nachieving good performance on generalization overall, and that summarizing\nobject traces in a single token has little influence on performance. We then\ndiscuss how this opens new perspectives for language-guided autonomous embodied\nagents. We also release our code under open-source license as well as\npretrained models and datasets to encourage the wider community to build upon\nand extend our work in the future.",
          "link": "http://arxiv.org/abs/2106.08858",
          "publishedOn": "2021-06-17T01:58:41.228Z",
          "wordCount": 687,
          "title": "Grounding Spatio-Temporal Language with Transformers. (arXiv:2106.08858v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuefeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiechmann_D/0/1/0/all/0/1\">Daniel Wiechmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerz_E/0/1/0/all/0/1\">Elma Kerz</a>",
          "description": "In this paper, we combined linguistic complexity and (dis)fluency features\nwith pretrained language models for the task of Alzheimer's disease detection\nof the 2021 ADReSSo (Alzheimer's Dementia Recognition through Spontaneous\nSpeech) challenge. An accuracy of 83.1% was achieved on the test set, which\namounts to an improvement of 4.23% over the baseline model. Our best-performing\nmodel that integrated component models using a stacking ensemble technique\nperformed equally well on cross-validation and test data, indicating that it is\nrobust against overfitting.",
          "link": "http://arxiv.org/abs/2106.08689",
          "publishedOn": "2021-06-17T01:58:41.215Z",
          "wordCount": 528,
          "title": "Alzheimer's Disease Detection from Spontaneous Speech through Combining Linguistic Complexity and (Dis)Fluency Features with Pretrained Language Models. (arXiv:2106.08689v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Ting Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chongxuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>",
          "description": "Dialogue State Tracking (DST), which is the process of inferring user goals\nby estimating belief states given the dialogue history, plays a critical role\nin task-oriented dialogue systems. A coreference phenomenon observed in\nmulti-turn conversations is not addressed by existing DST models, leading to\nsub-optimal performances. In this paper, we propose Coreference Dialogue State\nTracker (CDST) that explicitly models the coreference feature. In particular,\nat each turn, the proposed model jointly predicts the coreferred domain-slot\npair and extracts the coreference values from the dialogue context.\nExperimental results on MultiWOZ 2.1 dataset show that the proposed model\nachieves the state-of-the-art joint goal accuracy of 56.47%.",
          "link": "http://arxiv.org/abs/2106.08723",
          "publishedOn": "2021-06-17T01:58:41.207Z",
          "wordCount": 532,
          "title": "Coreference Augmentation for Multi-Domain Task-Oriented Dialogue State Tracking. (arXiv:2106.08723v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olaleye_K/0/1/0/all/0/1\">Kayode Olaleye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>",
          "description": "Visually grounded speech models learn from images paired with spoken\ncaptions. By tagging images with soft text labels using a trained visual\nclassifier with a fixed vocabulary, previous work has shown that it is possible\nto train a model that can detect whether a particular text keyword occurs in\nspeech utterances or not. Here we investigate whether visually grounded speech\nmodels can also do keyword localisation: predicting where, within an utterance,\na given textual keyword occurs without any explicit text-based or alignment\nsupervision. We specifically consider whether incorporating attention into a\nconvolutional model is beneficial for localisation. Although absolute\nlocalisation performance with visually supervised models is still modest\n(compared to using unordered bag-of-word text labels for supervision), we show\nthat attention provides a large gain in performance over previous visually\ngrounded models. As in many other speech-image studies, we find that many of\nthe incorrect localisations are due to semantic confusions, e.g. locating the\nword 'backstroke' for the query keyword 'swimming'.",
          "link": "http://arxiv.org/abs/2106.08859",
          "publishedOn": "2021-06-17T01:58:41.200Z",
          "wordCount": 598,
          "title": "Attention-Based Keyword Localisation in Speech using Visual Grounding. (arXiv:2106.08859v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kiegeland_S/0/1/0/all/0/1\">Samuel Kiegeland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>",
          "description": "Policy gradient algorithms have found wide adoption in NLP, but have recently\nbecome subject to criticism, doubting their suitability for NMT. Choshen et al.\n(2020) identify multiple weaknesses and suspect that their success is\ndetermined by the shape of output distributions rather than the reward. In this\npaper, we revisit these claims and study them under a wider range of\nconfigurations. Our experiments on in-domain and cross-domain adaptation reveal\nthe importance of exploration and reward scaling, and provide empirical\ncounter-evidence to these claims.",
          "link": "http://arxiv.org/abs/2106.08942",
          "publishedOn": "2021-06-17T01:58:41.183Z",
          "wordCount": 524,
          "title": "Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation. (arXiv:2106.08942v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08649",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gabrys_A/0/1/0/all/0/1\">Adam Gabry&#x15b;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiao_Y/0/1/0/all/0/1\">Yunlong Jiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klimkov_V/0/1/0/all/0/1\">Viacheslav Klimkov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korzekwa_D/0/1/0/all/0/1\">Daniel Korzekwa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barra_Chicote_R/0/1/0/all/0/1\">Roberto Barra-Chicote</a>",
          "description": "This paper proposes a general enhancement to the Normalizing Flows (NF) used\nin neural vocoding. As a case study, we improve expressive speech vocoding with\na revamped Parallel Wavenet (PW). Specifically, we propose to extend the affine\ntransformation of PW to the more expressive invertible non-affine function. The\ngreater expressiveness of the improved PW leads to better-perceived signal\nquality and naturalness in the waveform reconstruction and text-to-speech (TTS)\ntasks. We evaluate the model across different speaking styles on a\nmulti-speaker, multi-lingual dataset. In the waveform reconstruction task, the\nproposed model closes the naturalness and signal quality gap from the original\nPW to recordings by $10\\%$, and from other state-of-the-art neural vocoding\nsystems by more than $60\\%$. We also demonstrate improvements in objective\nmetrics on the evaluation test set with L2 Spectral Distance and Cross-Entropy\nreduced by $3\\%$ and $6\\unicode{x2030}$ comparing to the affine PW.\nFurthermore, we extend the probability density distillation procedure proposed\nby the original PW paper, so that it works with any non-affine invertible and\ndifferentiable function.",
          "link": "http://arxiv.org/abs/2106.08649",
          "publishedOn": "2021-06-17T01:58:41.166Z",
          "wordCount": 624,
          "title": "Improving the expressiveness of neural vocoding with non-affine Normalizing Flows. (arXiv:2106.08649v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fu-Ming Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1\">Austin Huang</a>",
          "description": "Reducing computation cost, inference latency, and memory footprint of neural\nnetworks are frequently cited as research motivations for pruning and sparsity.\nHowever, operationalizing those benefits and understanding the end-to-end\neffect of algorithm design and regularization on the runtime execution is not\noften examined in depth.\n\nHere we apply structured and unstructured pruning to attention weights of\ntransformer blocks of the BERT language model, while also expanding block\nsparse representation (BSR) operations in the TVM compiler. Integration of BSR\noperations enables the TVM runtime execution to leverage structured pattern\nsparsity induced by model regularization.\n\nThis integrated view of pruning algorithms enables us to study relationships\nbetween modeling decisions and their direct impact on sparsity-enhanced\nexecution. Our main findings are: 1) we validate that performance benefits of\nstructured sparsity block regularization must be enabled by the BSR\naugmentations to TVM, with 4x speedup relative to vanilla PyTorch and 2.2x\nspeedup relative to standard TVM compilation (without expanded BSR support). 2)\nfor BERT attention weights, the end-to-end optimal block sparsity shape in this\nCPU inference context is not a square block (as in \\cite{gray2017gpu}) but\nrather a linear 32x1 block 3) the relationship between performance and block\nsize / shape is is suggestive of how model regularization parameters interact\nwith task scheduler optimizations resulting in the observed end-to-end\nperformance.",
          "link": "http://arxiv.org/abs/2106.08846",
          "publishedOn": "2021-06-17T01:58:41.071Z",
          "wordCount": 657,
          "title": "Algorithm to Compilation Codesign: An Integrated View of Neural Network Sparsity. (arXiv:2106.08846v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merkx_D/0/1/0/all/0/1\">Danny Merkx</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_S/0/1/0/all/0/1\">Stefan L. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernestus_M/0/1/0/all/0/1\">Mirjam Ernestus</a>",
          "description": "This study addresses the question whether visually grounded speech\nrecognition (VGS) models learn to capture sentence semantics without access to\nany prior linguistic knowledge. We produce synthetic and natural spoken\nversions of a well known semantic textual similarity database and show that our\nVGS model produces embeddings that correlate well with human semantic\nsimilarity judgements. Our results show that a model trained on a small\nimage-caption database outperforms two models trained on much larger databases,\nindicating that database size is not all that matters. We also investigate the\nimportance of having multiple captions per image and find that this is indeed\nhelpful even if the total number of images is lower, suggesting that\nparaphrasing is a valuable learning signal. While the general trend in the\nfield is to create ever larger datasets to train models on, our findings\nindicate other characteristics of the database can just as important important.",
          "link": "http://arxiv.org/abs/2106.08648",
          "publishedOn": "2021-06-17T01:58:41.037Z",
          "wordCount": 597,
          "title": "Semantic sentence similarity: size does not always matter. (arXiv:2106.08648v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sanjay Singh</a>",
          "description": "With language models being deployed increasingly in the real world, it is\nessential to address the issue of the fairness of their outputs. The word\nembedding representations of these language models often implicitly draw\nunwanted associations that form a social bias within the model. The nature of\ngendered languages like Hindi, poses an additional problem to the\nquantification and mitigation of bias, owing to the change in the form of the\nwords in the sentence, based on the gender of the subject. Additionally, there\nis sparse work done in the realm of measuring and debiasing systems for Indic\nlanguages. In our work, we attempt to evaluate and quantify the gender bias\nwithin a Hindi-English machine translation system. We implement a modified\nversion of the existing TGBI metric based on the grammatical considerations for\nHindi. We also compare and contrast the resulting bias measurements across\nmultiple metrics for pre-trained embeddings and the ones learned by our machine\ntranslation model.",
          "link": "http://arxiv.org/abs/2106.08680",
          "publishedOn": "2021-06-17T01:58:41.028Z",
          "wordCount": 587,
          "title": "Evaluating Gender Bias in Hindi-English Machine Translation. (arXiv:2106.08680v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1\">Rui Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zonghan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "While synthetic bilingual corpora have demonstrated their effectiveness in\nlow-resource neural machine translation (NMT), adding more synthetic data often\ndeteriorates translation performance. In this work, we propose alternated\ntraining with synthetic and authentic data for NMT. The basic idea is to\nalternate synthetic and authentic corpora iteratively during training. Compared\nwith previous work, we introduce authentic data as guidance to prevent the\ntraining of NMT models from being disturbed by noisy synthetic data.\nExperiments on Chinese-English and German-English translation tasks show that\nour approach improves the performance over several strong baselines. We\nvisualize the BLEU landscape to further investigate the role of authentic and\nsynthetic data during alternated training. From the visualization, we find that\nauthentic data helps to direct the NMT model parameters towards points with\nhigher BLEU scores and leads to consistent translation performance improvement.",
          "link": "http://arxiv.org/abs/2106.08582",
          "publishedOn": "2021-06-17T01:58:41.007Z",
          "wordCount": 574,
          "title": "Alternated Training with Synthetic and Authentic Data for Neural Machine Translation. (arXiv:2106.08582v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Ke Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>",
          "description": "Summarizing conversations via neural approaches has been gaining research\ntraction lately, yet it is still challenging to obtain practical solutions.\nExamples of such challenges include unstructured information exchange in\ndialogues, informal interactions between speakers, and dynamic role changes of\nspeakers as the dialogue evolves. Many of such challenges result in complex\ncoreference links. Therefore, in this work, we investigate different approaches\nto explicitly incorporate coreference information in neural abstractive\ndialogue summarization models to tackle the aforementioned challenges.\nExperimental results show that the proposed approaches achieve state-of-the-art\nperformance, implying it is useful to utilize coreference information in\ndialogue summarization. Evaluation results on factual correctness suggest such\ncoreference-aware models are better at tracing the information flow among\ninterlocutors and associating accurate status/actions with the corresponding\ninterlocutors and person mentions.",
          "link": "http://arxiv.org/abs/2106.08556",
          "publishedOn": "2021-06-17T01:58:41.001Z",
          "wordCount": 552,
          "title": "Coreference-Aware Dialogue Summarization. (arXiv:2106.08556v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhiyuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Knowledge Graph (KG) alignment aims at finding equivalent entities and\nrelations (i.e., mappings) between two KGs. The existing approaches utilize\neither reasoning-based or semantic embedding-based techniques, but few studies\nexplore their combination. In this demonstration, we present PRASEMap, an\nunsupervised KG alignment system that iteratively computes the Mappings with\nboth Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.\nPRASEMap can support various embedding-based KG alignment approaches as the SE\nmodule, and enables easy human computer interaction that additionally provides\nan option for users to feed the mapping annotations back to the system for\nbetter results. The demonstration showcases these features via a stand-alone\nWeb application with user friendly interfaces.",
          "link": "http://arxiv.org/abs/2106.08801",
          "publishedOn": "2021-06-17T01:58:40.968Z",
          "wordCount": 552,
          "title": "PRASEMap: A Probabilistic Reasoning and Semantic Embedding based Knowledge Graph Alignment System. (arXiv:2106.08801v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+OConnor_J/0/1/0/all/0/1\">Joe O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>",
          "description": "Transformer-based language models benefit from conditioning on contexts of\nhundreds to thousands of previous tokens. What aspects of these contexts\ncontribute to accurate model prediction? We describe a series of experiments\nthat measure usable information by selectively ablating lexical and structural\ninformation in transformer language models trained on English Wikipedia. In\nboth mid- and long-range contexts, we find that several extremely destructive\ncontext manipulations -- including shuffling word order within sentences and\ndeleting all words other than nouns -- remove less than 15% of the usable\ninformation. Our results suggest that long contexts, but not their detailed\nsyntactic and propositional content, are important for the low perplexity of\ncurrent transformer language models.",
          "link": "http://arxiv.org/abs/2106.08367",
          "publishedOn": "2021-06-17T01:58:40.892Z",
          "wordCount": 545,
          "title": "What Context Features Can Transformer Language Models Use?. (arXiv:2106.08367v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1\">Harsh Jhamtani</a>",
          "description": "Humans often refer to personal narratives, life experiences, and events to\nmake a conversation more engaging and rich. While persona-grounded dialog\nmodels are able to generate responses that follow a given persona, they often\nmiss out on stating detailed experiences or events related to a persona, often\nleaving conversations shallow and dull. In this work, we equip dialog models\nwith 'background stories' related to a persona by leveraging fictional\nnarratives from existing story datasets (e.g. ROCStories). Since current dialog\ndatasets do not contain such narratives as responses, we perform an\nunsupervised adaptation of a retrieved story for generating a dialog response\nusing a gradient-based rewriting technique. Our proposed method encourages the\ngenerated response to be fluent (i.e., highly likely) with the dialog history,\nminimally different from the retrieved story to preserve event ordering and\nconsistent with the original persona. We demonstrate that our method can\ngenerate responses that are more diverse, and are rated more engaging and\nhuman-like by human evaluators, compared to outputs from existing dialog\nmodels.",
          "link": "http://arxiv.org/abs/2106.08364",
          "publishedOn": "2021-06-17T01:58:40.885Z",
          "wordCount": 603,
          "title": "Unsupervised Enrichment of Persona-grounded Dialog with Background Stories. (arXiv:2106.08364v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>",
          "description": "Entity embeddings, which represent different aspects of each entity with a\nsingle vector like word embeddings, are a key component of neural entity\nlinking models. Existing entity embeddings are learned from canonical Wikipedia\narticles and local contexts surrounding target entities. Such entity embeddings\nare effective, but too distinctive for linking models to learn contextual\ncommonality. We propose a simple yet effective method, FGS2EE, to inject\nfine-grained semantic information into entity embeddings to reduce the\ndistinctiveness and facilitate the learning of contextual commonality. FGS2EE\nfirst uses the embeddings of semantic type words to generate semantic\nembeddings, and then combines them with existing entity embeddings through\nlinear aggregation. Extensive experiments show the effectiveness of such\nembeddings. Based on our entity embeddings, we achieved new sate-of-the-art\nperformance on entity linking.",
          "link": "http://arxiv.org/abs/2106.08495",
          "publishedOn": "2021-06-17T01:58:40.825Z",
          "wordCount": 576,
          "title": "Improving Entity Linking through Semantic Reinforced Entity Embeddings. (arXiv:2106.08495v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xianghong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoli Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "Variational autoencoders (VAEs) have been widely applied for text modeling.\nIn practice, however, they are troubled by two challenges: information\nunderrepresentation and posterior collapse. The former arises as only the last\nhidden state of LSTM encoder is transformed into the latent space, which is\ngenerally insufficient to summarize the data. The latter is a long-standing\nproblem during the training of VAEs as the optimization is trapped to a\ndisastrous local optimum. In this paper, we propose Discrete Auto-regressive\nVariational Attention Model (DAVAM) to address the challenges. Specifically, we\nintroduce an auto-regressive variational attention approach to enrich the\nlatent space by effectively capturing the semantic dependency from the input.\nWe further design discrete latent space for the variational attention and\nmathematically show that our model is free from posterior collapse. Extensive\nexperiments on language modeling tasks demonstrate the superiority of DAVAM\nagainst several VAE counterparts.",
          "link": "http://arxiv.org/abs/2106.08571",
          "publishedOn": "2021-06-17T01:58:40.807Z",
          "wordCount": 579,
          "title": "Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2106.08571v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_J/0/1/0/all/0/1\">Junayed Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1\">Fahim Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnob_R/0/1/0/all/0/1\">Raihan Islam Arnob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_K/0/1/0/all/0/1\">Kevin Moran</a>",
          "description": "Automated source code summarization is a popular software engineering\nresearch topic wherein machine translation models are employed to \"translate\"\ncode snippets into relevant natural language descriptions. Most evaluations of\nsuch models are conducted using automatic reference-based metrics. However,\ngiven the relatively large semantic gap between programming languages and\nnatural language, we argue that this line of research would benefit from a\nqualitative investigation into the various error modes of current\nstate-of-the-art models. Therefore, in this work, we perform both a\nquantitative and qualitative comparison of three recently proposed source code\nsummarization models. In our quantitative evaluation, we compare the models\nbased on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics,\nand in our qualitative evaluation, we perform a manual open-coding of the most\ncommon errors committed by the models when compared to ground truth captions.\nOur investigation reveals new insights into the relationship between\nmetric-based performance and model prediction errors grounded in an empirically\nderived error taxonomy that can be used to drive future research efforts",
          "link": "http://arxiv.org/abs/2106.08415",
          "publishedOn": "2021-06-17T01:58:40.800Z",
          "wordCount": 651,
          "title": "Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors. (arXiv:2106.08415v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>",
          "description": "Document-level relation extraction (DocRE) aims at extracting the semantic\nrelations among entity pairs in a document. In DocRE, a subset of the sentences\nin a document, called the evidence sentences, might be sufficient for\npredicting the relation between a specific entity pair. To make better use of\nthe evidence sentences, in this paper, we propose a three-stage\nevidence-enhanced DocRE framework consisting of joint relation and evidence\nextraction, evidence-centered relation extraction (RE), and fusion of\nextraction results. We first jointly train an RE model with a simple and\nmemory-efficient evidence extraction model. Then, we construct pseudo documents\nbased on the extracted evidence sentences and run the RE model again. Finally,\nwe fuse the extraction results of the first two stages using a blending layer\nand make a final prediction. Extensive experiments show that our proposed\nframework achieves state-of-the-art performance on the DocRED dataset,\noutperforming the second-best method by 0.76/0.82 Ign F1/F1. In particular, our\nmethod significantly improves the performance on inter-sentence relations by\n1.23 Inter F1.",
          "link": "http://arxiv.org/abs/2106.08657",
          "publishedOn": "2021-06-17T01:58:40.790Z",
          "wordCount": 587,
          "title": "Eider: Evidence-enhanced Document-level Relation Extraction. (arXiv:2106.08657v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>",
          "description": "Inspired by recent work in meta-learning and generative teaching networks, we\npropose a framework called Generative Conversational Networks, in which\nconversational agents learn to generate their own labelled training data (given\nsome seed data) and then train themselves from that data to perform a given\ntask. We use reinforcement learning to optimize the data generation process\nwhere the reward signal is the agent's performance on the task. The task can be\nany language-related task, from intent detection to full task-oriented\nconversations. In this work, we show that our approach is able to generalise\nfrom seed data and performs well in limited data and limited computation\nsettings, with significant gains for intent detection and slot tagging across\nmultiple datasets: ATIS, TOD, SNIPS, and Restaurants8k. We show an average\nimprovement of 35% in intent detection and 21% in slot tagging over a baseline\nmodel trained from the seed data. We also conduct an analysis of the novelty of\nthe generated data and provide generated examples for intent detection, slot\ntagging, and non-goal oriented conversations.",
          "link": "http://arxiv.org/abs/2106.08484",
          "publishedOn": "2021-06-17T01:58:40.780Z",
          "wordCount": 599,
          "title": "Generative Conversational Networks. (arXiv:2106.08484v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zandie_R/0/1/0/all/0/1\">Rohola Zandie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoor_M/0/1/0/all/0/1\">Mohammad H. Mahoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madsen_J/0/1/0/all/0/1\">Julia Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emamian_E/0/1/0/all/0/1\">Eshrat S. Emamian</a>",
          "description": "This paper introduces RyanSpeech, a new speech corpus for research on\nautomated text-to-speech (TTS) systems. Publicly available TTS corpora are\noften noisy, recorded with multiple speakers, or lack quality male speech data.\nIn order to meet the need for a high quality, publicly available male speech\ncorpus within the field of speech recognition, we have designed and created\nRyanSpeech which contains textual materials from real-world conversational\nsettings. These materials contain over 10 hours of a professional male voice\nactor's speech recorded at 44.1 kHz. This corpus's design and pipeline make\nRyanSpeech ideal for developing TTS systems in real-world applications. To\nprovide a baseline for future research, protocols, and benchmarks, we trained 4\nstate-of-the-art speech models and a vocoder on RyanSpeech. The results show\n3.36 in mean opinion scores (MOS) in our best model. We have made both the\ncorpus and trained models for public use.",
          "link": "http://arxiv.org/abs/2106.08468",
          "publishedOn": "2021-06-17T01:58:40.772Z",
          "wordCount": 570,
          "title": "RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis. (arXiv:2106.08468v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Li-Ming Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Haowen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1\">Albert Y.S. Lam</a>",
          "description": "Out-of-scope intent detection is of practical importance in task-oriented\ndialogue systems. Since the distribution of outlier utterances is arbitrary and\nunknown in the training stage, existing methods commonly rely on strong\nassumptions on data distribution such as mixture of Gaussians to make\ninference, resulting in either complex multi-step training procedures or\nhand-crafted rules such as confidence threshold selection for outlier\ndetection. In this paper, we propose a simple yet effective method to train an\nout-of-scope intent classifier in a fully end-to-end manner by simulating the\ntest scenario in training, which requires no assumption on data distribution\nand no additional post-processing or threshold setting. Specifically, we\nconstruct a set of pseudo outliers in the training stage, by generating\nsynthetic outliers using inliner features via self-supervision and sampling\nout-of-scope sentences from easily available open-domain datasets. The pseudo\noutliers are used to train a discriminative classifier that can be directly\napplied to and generalize well on the test task. We evaluate our method\nextensively on four benchmark dialogue datasets and observe significant\nimprovements over state-of-the-art approaches. Our code has been released at\nhttps://github.com/liam0949/DCLOOS.",
          "link": "http://arxiv.org/abs/2106.08616",
          "publishedOn": "2021-06-17T01:58:40.754Z",
          "wordCount": 618,
          "title": "Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training. (arXiv:2106.08616v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_B/0/1/0/all/0/1\">Badr M. Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaitova_I/0/1/0/all/0/1\">Iuliia Zaitova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobius_B/0/1/0/all/0/1\">Bernd M&#xf6;bius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>",
          "description": "Several variants of deep neural networks have been successfully employed for\nbuilding parametric models that project variable-duration spoken word segments\nonto fixed-size vector representations, or acoustic word embeddings (AWEs).\nHowever, it remains unclear to what degree we can rely on the distance in the\nemerging AWE space as an estimate of word-form similarity. In this paper, we\nask: does the distance in the acoustic embedding space correlate with\nphonological dissimilarity? To answer this question, we empirically investigate\nthe performance of supervised approaches for AWEs with different neural\narchitectures and learning objectives. We train AWE models in controlled\nsettings for two languages (German and Czech) and evaluate the embeddings on\ntwo tasks: word discrimination and phonological similarity. Our experiments\nshow that (1) the distance in the embedding space in the best cases only\nmoderately correlates with phonological distance, and (2) improving the\nperformance on the word discrimination task does not necessarily yield models\nthat better reflect word phonological similarity. Our findings highlight the\nnecessity to rethink the current intrinsic evaluations for AWEs.",
          "link": "http://arxiv.org/abs/2106.08686",
          "publishedOn": "2021-06-17T01:58:40.737Z",
          "wordCount": 619,
          "title": "Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study. (arXiv:2106.08686v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "In spoken conversational question answering (SCQA), the answer to the\ncorresponding question is generated by retrieving and then analyzing a fixed\nspoken document, including multi-part conversations. Most SCQA systems have\nconsidered only retrieving information from ordered utterances. However, the\nsequential order of dialogue is important to build a robust spoken\nconversational question answering system, and the changes of utterances order\nmay severely result in low-quality and incoherent corpora. To this end, we\nintroduce a self-supervised learning approach, including incoherence\ndiscrimination, insertion detection, and question prediction, to explicitly\ncapture the coreference resolution and dialogue coherence among spoken\ndocuments. Specifically, we design a joint learning framework where the\nauxiliary self-supervised tasks can enable the pre-trained SCQA systems towards\nmore coherent and meaningful spoken dialogue learning. We also utilize the\nproposed self-supervised learning tasks to capture intra-sentence coherence.\nExperimental results demonstrate that our proposed method provides more\ncoherent, meaningful, and appropriate responses, yielding superior performance\ngains compared to the original pre-trained language models. Our method achieves\nstate-of-the-art results on the Spoken-CoQA dataset.",
          "link": "http://arxiv.org/abs/2106.02182",
          "publishedOn": "2021-06-16T01:21:10.832Z",
          "wordCount": 630,
          "title": "Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09106",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zuoyun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>",
          "description": "Subword units are commonly used for end-to-end automatic speech recognition\n(ASR), while a fully acoustic-oriented subword modeling approach is somewhat\nmissing. We propose an acoustic data-driven subword modeling (ADSM) approach\nthat adapts the advantages of several text-based and acoustic-based subword\nmethods into one pipeline. With a fully acoustic-oriented label design and\nlearning process, ADSM produces acoustic-structured subword units and\nacoustic-matched target sequence for further ASR training. The obtained ADSM\nlabels are evaluated with different end-to-end ASR approaches including CTC,\nRNN-Transducer and attention models. Experiments on the LibriSpeech corpus show\nthat ADSM clearly outperforms both byte pair encoding (BPE) and\npronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis\nshows that ADSM achieves acoustically more logical word segmentation and more\nbalanced sequence length, and thus, is suitable for both time-synchronous and\nlabel-synchronous models. We also briefly describe how to apply acoustic-based\nsubword regularization and unseen text segmentation using ADSM.",
          "link": "http://arxiv.org/abs/2104.09106",
          "publishedOn": "2021-06-16T01:21:08.065Z",
          "wordCount": 609,
          "title": "Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition. (arXiv:2104.09106v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>",
          "description": "This paper proposes a pre-training objective based on question answering (QA)\nfor learning general-purpose contextual representations, motivated by the\nintuition that the representation of a phrase in a passage should encode all\nquestions that the phrase can answer in context. We accomplish this goal by\ntraining a bi-encoder QA model, which independently encodes passages and\nquestions, to match the predictions of a more accurate cross-encoder model on\n80 million synthesized QA pairs. By encoding QA-relevant information, the\nbi-encoder's token-level representations are useful for non-QA downstream tasks\nwithout extensive (or in some cases, any) fine-tuning. We show large\nimprovements over both RoBERTa-large and previous state-of-the-art results on\nzero-shot and few-shot paraphrase detection on four datasets, few-shot named\nentity recognition on two datasets, and zero-shot sentiment analysis on three\ndatasets.",
          "link": "http://arxiv.org/abs/2106.08190",
          "publishedOn": "2021-06-16T01:21:08.021Z",
          "wordCount": 552,
          "title": "Question Answering Infused Pre-training of General-Purpose Contextualized Representations. (arXiv:2106.08190v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Taya_Y/0/1/0/all/0/1\">Yuki Taya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_L/0/1/0/all/0/1\">Lis Kanashiro Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_I/0/1/0/all/0/1\">Ichiro Kobayashi</a>",
          "description": "We propose an ensemble model for predicting the lexical complexity of words\nand multiword expressions (MWEs). The model receives as input a sentence with a\ntarget word or MWEand outputs its complexity score. Given that a key challenge\nwith this task is the limited size of annotated data, our model relies on\npretrained contextual representations from different state-of-the-art\ntransformer-based language models (i.e., BERT and RoBERTa), and on a variety of\ntraining methods for further enhancing model generalization and\nrobustness:multi-step fine-tuning and multi-task learning, and adversarial\ntraining. Additionally, we propose to enrich contextual representations by\nadding hand-crafted features during training. Our model achieved competitive\nresults and ranked among the top-10 systems in both sub-tasks.",
          "link": "http://arxiv.org/abs/2105.05535",
          "publishedOn": "2021-06-16T01:21:08.014Z",
          "wordCount": 587,
          "title": "OCHADAI-KYOTO at SemEval-2021 Task 1: Enhancing Model Generalization and Robustness for Lexical Complexity Prediction. (arXiv:2105.05535v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "Fine-tuning pre-trained cross-lingual language models can transfer\ntask-specific supervision from one language to the others. In this work, we\npropose to improve cross-lingual fine-tuning with consistency regularization.\nSpecifically, we use example consistency regularization to penalize the\nprediction sensitivity to four types of data augmentations, i.e., subword\nsampling, Gaussian noise, code-switch substitution, and machine translation. In\naddition, we employ model consistency to regularize the models trained with two\naugmented versions of the same training set. Experimental results on the XTREME\nbenchmark show that our method significantly improves cross-lingual fine-tuning\nacross various tasks, including text classification, question answering, and\nsequence labeling.",
          "link": "http://arxiv.org/abs/2106.08226",
          "publishedOn": "2021-06-16T01:21:08.008Z",
          "wordCount": 533,
          "title": "Consistency Regularization for Cross-Lingual Fine-Tuning. (arXiv:2106.08226v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isonuma_M/0/1/0/all/0/1\">Masaru Isonuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_J/0/1/0/all/0/1\">Junichiro Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakata_I/0/1/0/all/0/1\">Ichiro Sakata</a>",
          "description": "This paper presents a novel unsupervised abstractive summarization method for\nopinionated texts. While the basic variational autoencoder-based models assume\na unimodal Gaussian prior for the latent code of sentences, we alternate it\nwith a recursive Gaussian mixture, where each mixture component corresponds to\nthe latent code of a topic sentence and is mixed by a tree-structured topic\ndistribution. By decoding each Gaussian component, we generate sentences with\ntree-structured topic guidance, where the root sentence conveys generic\ncontent, and the leaf sentences describe specific topics. Experimental results\ndemonstrate that the generated topic sentences are appropriate as a summary of\nopinionated texts, which are more informative and cover more input contents\nthan those generated by the recent unsupervised summarization model\n(Bra\\v{z}inskas et al., 2020). Furthermore, we demonstrate that the variance of\nlatent Gaussians represents the granularity of sentences, analogous to Gaussian\nword embedding (Vilnis and McCallum, 2015).",
          "link": "http://arxiv.org/abs/2106.08007",
          "publishedOn": "2021-06-16T01:21:07.994Z",
          "wordCount": 592,
          "title": "Unsupervised Abstractive Opinion Summarization by Generating Sentences with Tree-Structured Topic Guidance. (arXiv:2106.08007v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1\">Dipendra Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schapire_R/0/1/0/all/0/1\">Robert Schapire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1\">Miro Dud&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafto_P/0/1/0/all/0/1\">Patrick Shafto</a>",
          "description": "We present a novel interactive learning protocol that enables training\nrequest-fulfilling agents by verbally describing their activities. Unlike\nimitation learning (IL), our protocol allows the teaching agent to provide\nfeedback in a language that is most appropriate for them. Compared with reward\nin reinforcement learning (RL), the description feedback is richer and allows\nfor improved sample complexity. We develop a probabilistic framework and an\nalgorithm that practically implements our protocol. Empirical results in two\nchallenging request-fulfilling problems demonstrate the strengths of our\napproach: compared with RL baselines, it is more sample-efficient; compared\nwith IL baselines, it achieves competitive success rates without requiring the\nteaching agent to be able to demonstrate the desired behavior using the\nlearning agent's actions. Apart from empirical evaluation, we also provide\ntheoretical guarantees for our algorithm under certain assumptions about the\nteacher and the environment.",
          "link": "http://arxiv.org/abs/2102.07024",
          "publishedOn": "2021-06-16T01:21:07.976Z",
          "wordCount": 606,
          "title": "Interactive Learning from Activity Description. (arXiv:2102.07024v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.",
          "link": "http://arxiv.org/abs/2010.11066",
          "publishedOn": "2021-06-16T01:21:07.968Z",
          "wordCount": 616,
          "title": "Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_V/0/1/0/all/0/1\">Venkatesh Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>",
          "description": "Speaker identification in the household scenario (e.g., for smart speakers)\nis typically based on only a few enrollment utterances but a much larger set of\nunlabeled data, suggesting semisupervised learning to improve speaker profiles.\nWe propose a graph-based semi-supervised learning approach for speaker\nidentification in the household scenario, to leverage the unlabeled speech\nsamples. In contrast to most of the works in speaker recognition that focus on\nspeaker-discriminative embeddings, this work focuses on speaker label inference\n(scoring). Given a pre-trained embedding extractor, graph-based learning allows\nus to integrate information about both labeled and unlabeled utterances.\nConsidering each utterance as a graph node, we represent pairwise utterance\nsimilarity scores as edge weights. Graphs are constructed per household, and\nspeaker identities are propagated to unlabeled nodes to optimize a global\nconsistency criterion. We show in experiments on the VoxCeleb dataset that this\napproach makes effective use of unlabeled data and improves speaker\nidentification accuracy compared to two state-of-the-art scoring methods as\nwell as their semi-supervised variants based on pseudo-labels.",
          "link": "http://arxiv.org/abs/2106.08207",
          "publishedOn": "2021-06-16T01:21:07.961Z",
          "wordCount": 599,
          "title": "Graph-based Label Propagation for Semi-Supervised Speaker Identification. (arXiv:2106.08207v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2008.03945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Sijie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>",
          "description": "BERT has been used for solving commonsense tasks such as CommonsenseQA. While\nprior research has found that BERT does contain commonsense information to some\nextent, there has been work showing that pre-trained models can rely on\nspurious associations (e.g., data bias) rather than key cues in solving\nsentiment classification and other problems. We quantitatively investigate the\npresence of structural commonsense cues in BERT when solving commonsense tasks,\nand the importance of such cues for the model prediction. Using two different\nmeasures, we find that BERT does use relevant knowledge for solving the task,\nand the presence of commonsense knowledge is positively correlated to the model\naccuracy.",
          "link": "http://arxiv.org/abs/2008.03945",
          "publishedOn": "2021-06-16T01:21:07.944Z",
          "wordCount": 570,
          "title": "On Commonsense Cues in BERT for Solving Commonsense Tasks. (arXiv:2008.03945v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>",
          "description": "We present two supervised (pre-)training methods to incorporate gloss\ndefinitions from lexical resources into neural language models (LMs). The\ntraining improves our models' performance for Word Sense Disambiguation (WSD)\nbut also benefits general language understanding tasks while adding almost no\nparameters. We evaluate our techniques with seven different neural LMs and find\nthat XLNet is more suitable for WSD than BERT. Our best-performing methods\nexceeds state-of-the-art WSD techniques on the SemCor 3.0 dataset by 0.5% F1\nand increase BERT's performance on the GLUE benchmark by 1.1% on average.",
          "link": "http://arxiv.org/abs/2106.07967",
          "publishedOn": "2021-06-16T01:21:07.936Z",
          "wordCount": 519,
          "title": "Incorporating Word Sense Disambiguation in Neural Language Models. (arXiv:2106.07967v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vivek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "Multilingualism refers to the high degree of proficiency in two or more\nlanguages in the written and oral communication modes. It often results in\nlanguage mixing, a.k.a. code-mixing, when a multilingual speaker switches\nbetween multiple languages in a single utterance of a text or speech. This\npaper discusses the current state of the NLP research, limitations, and\nforeseeable pitfalls in addressing five real-world applications for social good\ncrisis management, healthcare, political campaigning, fake news, and hate\nspeech for multilingual societies. We also propose futuristic datasets, models,\nand tools that can significantly advance the current research in multilingual\nNLP applications for the societal good. As a representative example, we\nconsider English-Hindi code-mixing but draw similar inferences for other\nlanguage pairs",
          "link": "http://arxiv.org/abs/2106.07823",
          "publishedOn": "2021-06-16T01:21:07.925Z",
          "wordCount": 547,
          "title": "Challenges and Considerations with Code-Mixed NLP for Multilingual Societies. (arXiv:2106.07823v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongsheng Wang</a>",
          "description": "Semantic representation and inference is essential for Natural Language\nProcessing (NLP). The state of the art for semantic representation and\ninference is deep learning, and particularly Recurrent Neural Networks (RNNs),\nConvolutional Neural Networks (CNNs), and transformer Self-Attention models.\nThis thesis investigates the use of deep learning for novel semantic\nrepresentation and inference, and makes contributions in the following three\nareas: creating training data, improving semantic representations and extending\ninference learning. In terms of creating training data, we contribute the\nlargest publicly available dataset of real-life factual claims for the purpose\nof automatic claim verification (MultiFC), and we present a novel inference\nmodel composed of multi-scale CNNs with different kernel sizes that learn from\nexternal sources to infer fact checking labels. In terms of improving semantic\nrepresentations, we contribute a novel model that captures non-compositional\nsemantic indicators. By definition, the meaning of a non-compositional phrase\ncannot be inferred from the individual meanings of its composing words (e.g.,\nhot dog). Motivated by this, we operationalize the compositionality of a phrase\ncontextually by enriching the phrase representation with external word\nembeddings and knowledge graphs. Finally, in terms of inference learning, we\npropose a series of novel deep learning architectures that improve inference by\nusing syntactic dependencies, by ensembling role guided attention heads,\nincorporating gating layers, and concatenating multiple heads in novel and\neffective ways. This thesis consists of seven publications (five published and\ntwo under review).",
          "link": "http://arxiv.org/abs/2106.08117",
          "publishedOn": "2021-06-16T01:21:07.918Z",
          "wordCount": 663,
          "title": "Semantic Representation and Inference for NLP. (arXiv:2106.08117v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_S/0/1/0/all/0/1\">Shohei Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshino_K/0/1/0/all/0/1\">Koichiro Yoshino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>",
          "description": "Human-assisting systems such as dialogue systems must take thoughtful,\nappropriate actions not only for clear and unambiguous user requests, but also\nfor ambiguous user requests, even if the users themselves are not aware of\ntheir potential requirements. To construct such a dialogue agent, we collected\na corpus and developed a model that classifies ambiguous user requests into\ncorresponding system actions. In order to collect a high-quality corpus, we\nasked workers to input antecedent user requests whose pre-defined actions could\nbe regarded as thoughtful. Although multiple actions could be identified as\nthoughtful for a single user request, annotating all combinations of user\nrequests and system actions is impractical. For this reason, we fully annotated\nonly the test data and left the annotation of the training data incomplete. In\norder to train the classification model on such training data, we applied the\npositive/unlabeled (PU) learning method, which assumes that only a part of the\ndata is labeled with positive examples. The experimental results show that the\nPU learning method achieved better performance than the general\npositive/negative (PN) learning method to classify thoughtful actions given an\nambiguous user request.",
          "link": "http://arxiv.org/abs/2106.07999",
          "publishedOn": "2021-06-16T01:21:07.911Z",
          "wordCount": 633,
          "title": "ARTA: Collection and Classification of Ambiguous Requests and Thoughtful Actions. (arXiv:2106.07999v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1\">Jay Mahadeokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiatong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chunyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuegen_C/0/1/0/all/0/1\">Christian Fuegen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>",
          "description": "As speech-enabled devices such as smartphones and smart speakers become\nincreasingly ubiquitous, there is growing interest in building automatic speech\nrecognition (ASR) systems that can run directly on-device; end-to-end (E2E)\nspeech recognition models such as recurrent neural network transducers and\ntheir variants have recently emerged as prime candidates for this task. Apart\nfrom being accurate and compact, such systems need to decode speech with low\nuser-perceived latency (UPL), producing words as soon as they are spoken. This\nwork examines the impact of various techniques - model architectures, training\ncriteria, decoding hyperparameters, and endpointer parameters - on UPL. Our\nanalyses suggest that measures of model size (parameters, input chunk sizes),\nor measures of computation (e.g., FLOPS, RTF) that reflect the model's ability\nto process input frames are not always strongly correlated with observed UPL.\nThus, conventional algorithmic latency measurements might be inadequate in\naccurately capturing latency observed when models are deployed on embedded\ndevices. Instead, we find that factors affecting token emission latency, and\nendpointing behavior have a larger impact on UPL. We achieve the best trade-off\nbetween latency and word error rate when performing ASR jointly with\nendpointing, while utilizing the recently proposed alignment regularization\nmechanism.",
          "link": "http://arxiv.org/abs/2104.02207",
          "publishedOn": "2021-06-16T01:21:07.898Z",
          "wordCount": 681,
          "title": "Dissecting User-Perceived Latency of On-Device E2E Speech Recognition. (arXiv:2104.02207v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1\">Chenze Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In recent years, Neural Machine Translation (NMT) has achieved notable\nresults in various translation tasks. However, the word-by-word generation\nmanner determined by the autoregressive mechanism leads to high translation\nlatency of the NMT and restricts its low-latency applications.\nNon-Autoregressive Neural Machine Translation (NAT) removes the autoregressive\nmechanism and achieves significant decoding speedup through generating target\nwords independently and simultaneously. Nevertheless, NAT still takes the\nword-level cross-entropy loss as the training objective, which is not optimal\nbecause the output of NAT cannot be properly evaluated due to the multimodality\nproblem. In this paper, we propose using sequence-level training objectives to\ntrain NAT models, which evaluate the NAT outputs as a whole and correlates well\nwith the real translation quality. Firstly, we propose training NAT models to\noptimize sequence-level evaluation metrics (e.g., BLEU) based on several novel\nreinforcement algorithms customized for NAT, which outperforms the conventional\nmethod by reducing the variance of gradient estimation. Secondly, we introduce\na novel training objective for NAT models, which aims to minimize the\nBag-of-Ngrams (BoN) difference between the model output and the reference\nsentence. The BoN training objective is differentiable and can be calculated\nefficiently without doing any approximations. Finally, we apply a three-stage\ntraining strategy to combine these two methods to train the NAT model. We\nvalidate our approach on four translation tasks (WMT14 En$\\leftrightarrow$De,\nWMT16 En$\\leftrightarrow$Ro), which shows that our approach largely outperforms\nNAT baselines and achieves remarkable performance on all translation tasks.",
          "link": "http://arxiv.org/abs/2106.08122",
          "publishedOn": "2021-06-16T01:21:07.859Z",
          "wordCount": 669,
          "title": "Sequence-Level Training for Non-Autoregressive Neural Machine Translation. (arXiv:2106.08122v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jisi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorila_C/0/1/0/all/0/1\">Catalin Zorila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1\">Rama Doddipatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barker_J/0/1/0/all/0/1\">Jon Barker</a>",
          "description": "In this paper, we introduce a novel semi-supervised learning framework for\nend-to-end speech separation. The proposed method first uses mixtures of\nunseparated sources and the mixture invariant training (MixIT) criterion to\ntrain a teacher model. The teacher model then estimates separated sources that\nare used to train a student model with standard permutation invariant training\n(PIT). The student model can be fine-tuned with supervised data, i.e., paired\nartificial mixtures and clean speech sources, and further improved via model\ndistillation. Experiments with single and multi channel mixtures show that the\nteacher-student training resolves the over-separation problem observed in the\noriginal MixIT method. Further, the semisupervised performance is comparable to\na fully-supervised separation system trained using ten times the amount of\nsupervised data.",
          "link": "http://arxiv.org/abs/2106.07843",
          "publishedOn": "2021-06-16T01:21:07.836Z",
          "wordCount": 560,
          "title": "Teacher-Student MixIT for Unsupervised and Semi-supervised Speech Separation. (arXiv:2106.07843v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Pre-trained text-to-text transformers such as BART have achieved impressive\nperformance across a range of NLP tasks. Recent study further shows that they\ncan learn to generalize to novel tasks, by including task descriptions as part\nof the source sequence and training the model with (source, target) examples.\nAt test time, these fine-tuned models can make inferences on new tasks using\nthe new task descriptions as part of the input. However, this approach has\npotential limitations, as the model learns to solve individual (source, target)\nexamples (i.e., at the instance level), instead of learning to solve tasks by\ntaking all examples within a task as a whole (i.e., at the task level). To this\nend, we introduce Hypter, a framework that improves text-to-text transformer's\ngeneralization ability to unseen tasks by training a hypernetwork to generate\ntask-specific, light-weight adapters from task descriptions. Experiments on\nZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves\nupon fine-tuning baselines. Notably, when using BART-Large as the main network,\nHypter brings 11.3% comparative improvement on ZEST dataset.",
          "link": "http://arxiv.org/abs/2101.00420",
          "publishedOn": "2021-06-16T01:21:07.822Z",
          "wordCount": 636,
          "title": "Learning to Generate Task-Specific Adapters from Task Description. (arXiv:2101.00420v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eyre_H/0/1/0/all/0/1\">Hannah Eyre</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Chapman_A/0/1/0/all/0/1\">Alec B Chapman</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Peterson_K/0/1/0/all/0/1\">Kelly S Peterson</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianlin Shi</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Alba_P/0/1/0/all/0/1\">Patrick R Alba</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1\">Makoto M Jones</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Box_T/0/1/0/all/0/1\">Tamara L Box</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+DuVall_S/0/1/0/all/0/1\">Scott L DuVall</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Patterson_O/0/1/0/all/0/1\">Olga V Patterson</a> (1 and 2) ((1) VA Salt Lake City Health Care System, (2) University of Utah, Salt Lake City, UT, USA, (3) Veterans Health Administration Office of Analytics and Performance Integration)",
          "description": "Despite impressive success of machine learning algorithms in clinical natural\nlanguage processing (cNLP), rule-based approaches still have a prominent role.\nIn this paper, we introduce medspaCy, an extensible, open-source cNLP library\nbased on spaCy framework that allows flexible integration of rule-based and\nmachine learning-based algorithms adapted to clinical text. MedspaCy includes a\nvariety of components that meet common cNLP needs such as context analysis and\nmapping to standard terminologies. By utilizing spaCy's clear and easy-to-use\nconventions, medspaCy enables development of custom pipelines that integrate\neasily with other spaCy-based modules. Our toolkit includes several core\ncomponents and facilitates rapid development of pipelines for clinical text.",
          "link": "http://arxiv.org/abs/2106.07799",
          "publishedOn": "2021-06-16T01:21:07.797Z",
          "wordCount": 616,
          "title": "Launching into clinical space with medspaCy: a new clinical text processing toolkit in Python. (arXiv:2106.07799v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slottje_A/0/1/0/all/0/1\">Andrew Slottje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wotherspoon_S/0/1/0/all/0/1\">Shannon Wotherspoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_W/0/1/0/all/0/1\">William Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snover_M/0/1/0/all/0/1\">Matthew Snover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimball_O/0/1/0/all/0/1\">Owen Kimball</a>",
          "description": "Modeling code-switched speech is an important problem in automatic speech\nrecognition (ASR). Labeled code-switched data are rare, so monolingual data are\noften used to model code-switched speech. These monolingual data may be more\nclosely matched to one of the languages in the code-switch pair. We show that\nsuch asymmetry can bias prediction toward the better-matched language and\ndegrade overall model performance. To address this issue, we propose a\nsemi-supervised approach for code-switched ASR. We consider the case of\nEnglish-Mandarin code-switching, and the problem of using monolingual data to\nbuild bilingual \"transcription models'' for annotation of unlabeled\ncode-switched data. We first build multiple transcription models so that their\nindividual predictions are variously biased toward either English or Mandarin.\nWe then combine these biased transcriptions using confidence-based selection.\nThis strategy generates a superior transcript for semi-supervised training, and\nobtains a 19% relative improvement compared to a semi-supervised system that\nrelies on a transcription model built with only the best-matched monolingual\ndata.",
          "link": "http://arxiv.org/abs/2106.07699",
          "publishedOn": "2021-06-16T01:21:07.767Z",
          "wordCount": 606,
          "title": "Using heterogeneity in semi-supervised transcription hypotheses to improve code-switched speech recognition. (arXiv:2106.07699v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1\">Sean Papay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>",
          "description": "In structured prediction, a major challenge for models is to represent the\ninterdependencies within their output structures. For the common case where\noutputs are structured as a sequence, linear-chain conditional random fields\n(CRFs) are a widely used model class which can learn local dependencies in\noutput sequences. However, the CRF's Markov assumption makes it impossible for\nthese models to capture nonlocal dependencies, and standard CRFs are unable to\nrespect nonlocal constraints of the data (such as global arity constraints on\noutput labels). We present a generalization of CRFs that can enforce a broad\nclass of constraints, including nonlocal ones, by specifying the space of\npossible output structures as a regular language $\\mathcal{L}$. The resulting\nregular-constrained CRF (RegCCRF) has the same formal properties as a standard\nCRF, but assigns zero probability to all label sequences not in $\\mathcal{L}$.\nNotably, RegCCRFs can incorporate their constraints during training, while\nrelated models only enforce constraints during decoding. We prove that\nconstrained training is never worse than constrained decoding, and show using\nsynthetic data that it can be substantially better in practice. Additionally,\nwe demonstrate a practical benefit on downstream tasks by incorporating a\nRegCCRF into a deep neural model for semantic role labeling, exceeding\nstate-of-the-art results on a standard dataset.",
          "link": "http://arxiv.org/abs/2106.07306",
          "publishedOn": "2021-06-16T01:21:07.743Z",
          "wordCount": 646,
          "title": "Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1\">Mahdi Hajiaghayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1\">Monir Hajiaghayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolin_M/0/1/0/all/0/1\">Mark Bolin</a>",
          "description": "In this paper, we present a multi-lingual sentence encoder that can be used\nin search engines as a query and document encoder. This embedding enables a\nsemantic similarity score between queries and documents that can be an\nimportant feature in document ranking and relevancy. To train such a customized\nsentence encoder, it is beneficial to leverage users search data in the form of\nquery-document clicked pairs however, we must avoid relying too much on search\nclick data as it is biased and does not cover many unseen cases. The search\ndata is heavily skewed towards short queries and for long queries is small and\noften noisy. The goal is to design a universal multi-lingual encoder that works\nfor all cases and covers both short and long queries. We select a number of\npublic NLI datasets in different languages and translation data and together\nwith user search data we train a language model using a multi-task approach. A\nchallenge is that these datasets are not homogeneous in terms of content, size\nand the balance ratio. While the public NLI datasets are usually two-sentence\nbased with the same portion of positive and negative pairs, the user search\ndata can contain multi-sentence documents and only positive pairs. We show how\nmulti-task training enables us to leverage all these datasets and exploit\nknowledge sharing across these tasks.",
          "link": "http://arxiv.org/abs/2106.07719",
          "publishedOn": "2021-06-16T01:21:07.736Z",
          "wordCount": 651,
          "title": "Unbiased Sentence Encoder For Large-Scale Multi-lingual Search Engines. (arXiv:2106.07719v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandsen_A/0/1/0/all/0/1\">Alex Brandsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambers_K/0/1/0/all/0/1\">Karsten Lambers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wansleeben_M/0/1/0/all/0/1\">Milco Wansleeben</a>",
          "description": "The amount of archaeological literature is growing rapidly. Until recently,\nthese data were only accessible through metadata search. We implemented a text\nretrieval engine for a large archaeological text collection ($\\sim 658$ Million\nwords). In archaeological IR, domain-specific entities such as locations, time\nperiods, and artefacts, play a central role. This motivated the development of\na named entity recognition (NER) model to annotate the full collection with\narchaeological named entities. In this paper, we present ArcheoBERTje, a BERT\nmodel pre-trained on Dutch archaeological texts. We compare the model's quality\nand output on a Named Entity Recognition task to a generic multilingual model\nand a generic Dutch model. We also investigate ensemble methods for combining\nmultiple BERT models, and combining the best BERT model with a domain thesaurus\nusing Conditional Random Fields (CRF). We find that ArcheoBERTje outperforms\nboth the multilingual and Dutch model significantly with a smaller standard\ndeviation between runs, reaching an average F1 score of 0.735. The model also\noutperforms ensemble methods combining the three models. Combining ArcheoBERTje\npredictions and explicit domain knowledge from the thesaurus did not increase\nthe F1 score. We quantitatively and qualitatively analyse the differences\nbetween the vocabulary and output of the BERT models on the full collection and\nprovide some valuable insights in the effect of fine-tuning for specific\ndomains. Our results indicate that for a highly specific text domain such as\narchaeology, further pre-training on domain-specific data increases the model's\nquality on NER by a much larger margin than shown for other domains in the\nliterature, and that domain-specific pre-training makes the addition of domain\nknowledge from a thesaurus unnecessary.",
          "link": "http://arxiv.org/abs/2106.07742",
          "publishedOn": "2021-06-16T01:21:07.661Z",
          "wordCount": 710,
          "title": "Can BERT Dig It? -- Named Entity Recognition for Information Retrieval in the Archaeology Domain. (arXiv:2106.07742v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00769",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Berg_A/0/1/0/all/0/1\">Axel Berg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OConnor_M/0/1/0/all/0/1\">Mark O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cruz_M/0/1/0/all/0/1\">Miguel Tairum Cruz</a>",
          "description": "The Transformer architecture has been successful across many domains,\nincluding natural language processing, computer vision and speech recognition.\nIn keyword spotting, self-attention has primarily been used on top of\nconvolutional or recurrent encoders. We investigate a range of ways to adapt\nthe Transformer architecture to keyword spotting and introduce the Keyword\nTransformer (KWT), a fully self-attentional architecture that exceeds\nstate-of-the-art performance across multiple tasks without any pre-training or\nadditional data. Surprisingly, this simple architecture outperforms more\ncomplex models that mix convolutional, recurrent and attentive layers. KWT can\nbe used as a drop-in replacement for these models, setting two new benchmark\nrecords on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on\nthe 12 and 35-command tasks respectively.",
          "link": "http://arxiv.org/abs/2104.00769",
          "publishedOn": "2021-06-16T01:21:07.623Z",
          "wordCount": 592,
          "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting. (arXiv:2104.00769v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlegel_V/0/1/0/all/0/1\">Viktor Schlegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batista_Navarro_R/0/1/0/all/0/1\">Riza Batista-Navarro</a>",
          "description": "Advances in NLP have yielded impressive results for the task of machine\nreading comprehension (MRC), with approaches having been reported to achieve\nperformance comparable to that of humans. In this paper, we investigate whether\nstate-of-the-art MRC models are able to correctly process Semantics Altering\nModifications (SAM): linguistically-motivated phenomena that alter the\nsemantics of a sentence while preserving most of its lexical surface form. We\npresent a method to automatically generate and align challenge sets featuring\noriginal and altered examples. We further propose a novel evaluation\nmethodology to correctly assess the capability of MRC systems to process these\nexamples independent of the data they were optimised on, by discounting for\neffects introduced by domain shift. In a large-scale empirical study, we apply\nthe methodology in order to evaluate extractive MRC models with regard to their\ncapability to correctly process SAM-enriched data. We comprehensively cover 12\ndifferent state-of-the-art neural architecture configurations and four training\ndatasets and find that -- despite their well-known remarkable performance --\noptimised models consistently struggle to correctly process semantically\naltered data.",
          "link": "http://arxiv.org/abs/2012.04056",
          "publishedOn": "2021-06-16T01:21:07.529Z",
          "wordCount": 642,
          "title": "Semantics Altering Modifications for Evaluating Comprehension in Machine Reading. (arXiv:2012.04056v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1\">Matthew Jagielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1\">Ariel Herbert-Voss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erlingsson_U/0/1/0/all/0/1\">Ulfar Erlingsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oprea_A/0/1/0/all/0/1\">Alina Oprea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>",
          "description": "It has become common to publish large (billion parameter) language models\nthat have been trained on private datasets. This paper demonstrates that in\nsuch settings, an adversary can perform a training data extraction attack to\nrecover individual training examples by querying the language model.\n\nWe demonstrate our attack on GPT-2, a language model trained on scrapes of\nthe public Internet, and are able to extract hundreds of verbatim text\nsequences from the model's training data. These extracted examples include\n(public) personally identifiable information (names, phone numbers, and email\naddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible\neven though each of the above sequences are included in just one document in\nthe training data.\n\nWe comprehensively evaluate our extraction attack to understand the factors\nthat contribute to its success. Worryingly, we find that larger models are more\nvulnerable than smaller models. We conclude by drawing lessons and discussing\npossible safeguards for training large language models.",
          "link": "http://arxiv.org/abs/2012.07805",
          "publishedOn": "2021-06-16T01:21:07.435Z",
          "wordCount": 643,
          "title": "Extracting Training Data from Large Language Models. (arXiv:2012.07805v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07583",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ujiie_S/0/1/0/all/0/1\">Shogo Ujiie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aramaki_E/0/1/0/all/0/1\">Eiji Aramaki</a>",
          "description": "We introduce BioCoM, a contrastive learning framework for biomedical entity\nlinking that uses only two resources: a small-sized dictionary and a large\nnumber of raw biomedical articles. Specifically, we build the training\ninstances from raw PubMed articles by dictionary matching and use them to train\na context-aware entity linking model with contrastive learning. We predict the\nnormalized biomedical entity at inference time through a nearest-neighbor\nsearch. Results found that BioCoM substantially outperforms state-of-the-art\nmodels, especially in low-resource settings, by effectively using the context\nof the entities.",
          "link": "http://arxiv.org/abs/2106.07583",
          "publishedOn": "2021-06-16T01:21:07.402Z",
          "wordCount": 526,
          "title": "Biomedical Entity Linking with Contrastive Context Matching. (arXiv:2106.07583v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wentao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jinhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Large-scale pre-trained models (PTMs) such as BERT and GPT have recently\nachieved great success and become a milestone in the field of artificial\nintelligence (AI). Owing to sophisticated pre-training objectives and huge\nmodel parameters, large-scale PTMs can effectively capture knowledge from\nmassive labeled and unlabeled data. By storing knowledge into huge parameters\nand fine-tuning on specific tasks, the rich knowledge implicitly encoded in\nhuge parameters can benefit a variety of downstream tasks, which has been\nextensively demonstrated via experimental verification and empirical analysis.\nIt is now the consensus of the AI community to adopt PTMs as backbone for\ndownstream tasks rather than learning models from scratch. In this paper, we\ntake a deep look into the history of pre-training, especially its special\nrelation with transfer learning and self-supervised learning, to reveal the\ncrucial position of PTMs in the AI development spectrum. Further, we\ncomprehensively review the latest breakthroughs of PTMs. These breakthroughs\nare driven by the surge of computational power and the increasing availability\nof data, towards four important directions: designing effective architectures,\nutilizing rich contexts, improving computational efficiency, and conducting\ninterpretation and theoretical analysis. Finally, we discuss a series of open\nproblems and research directions of PTMs, and hope our view can inspire and\nadvance the future study of PTMs.",
          "link": "http://arxiv.org/abs/2106.07139",
          "publishedOn": "2021-06-16T01:21:07.396Z",
          "wordCount": 690,
          "title": "Pre-Trained Models: Past, Present and Future. (arXiv:2106.07139v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.06723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yichen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "In the area of natural language processing, deep learning models are recently\nknown to be vulnerable to various types of adversarial perturbations, but\nrelatively few works are done on the defense side. Especially, there exists few\neffective defense method against the successful synonym substitution based\nattacks that preserve the syntactic structure and semantic information of the\noriginal text while fooling the deep learning models. We contribute in this\ndirection and propose a novel adversarial defense method called Synonym\nEncoding Method (SEM). Specifically, SEM inserts an encoder before the input\nlayer of the target model to map each cluster of synonyms to a unique encoding\nand trains the model to eliminate possible adversarial perturbations without\nmodifying the network architecture or adding extra data. Extensive experiments\ndemonstrate that SEM can effectively defend the current synonym substitution\nbased attacks and block the transferability of adversarial examples. SEM is\nalso easy and efficient to scale to large models and big datasets.",
          "link": "http://arxiv.org/abs/1909.06723",
          "publishedOn": "2021-06-16T01:21:07.353Z",
          "wordCount": 643,
          "title": "Natural Language Adversarial Defense through Synonym Encoding. (arXiv:1909.06723v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bojie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+huang_s/0/1/0/all/0/1\">shen huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Q/0/1/0/all/0/1\">Qi Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>",
          "description": "Encoder pre-training is promising in end-to-end Speech Translation (ST),\ngiven the fact that speech-to-translation data is scarce. But ST encoders are\nnot simple instances of Automatic Speech Recognition (ASR) or Machine\nTranslation (MT) encoders. For example, we find that ASR encoders lack the\nglobal context representation, which is necessary for translation, whereas MT\nencoders are not designed to deal with long but locally attentive acoustic\nsequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding\n(SATE) method for speech translation. Our encoder begins with processing the\nacoustic sequence as usual, but later behaves more like an MT encoder for a\nglobal representation of the input sequence. In this way, it is straightforward\nto incorporate the pre-trained models into the system. Also, we develop an\nadaptor module to alleviate the representation inconsistency between the\npre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge\ndistillation method to preserve the pre-training knowledge. Experimental\nresults on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method\nachieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we\nare the first to develop an end-to-end ST system that achieves comparable or\neven better BLEU performance than the cascaded ST counterpart when large-scale\nASR and MT data is available.",
          "link": "http://arxiv.org/abs/2105.05752",
          "publishedOn": "2021-06-16T01:21:07.328Z",
          "wordCount": 692,
          "title": "Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders. (arXiv:2105.05752v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forster_M/0/1/0/all/0/1\">Martina Forster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>",
          "description": "Beam search is a go-to strategy for decoding neural sequence models. The\nalgorithm can naturally be viewed as a subset optimization problem, albeit one\nwhere the corresponding set function does not reflect interactions between\ncandidates. Empirically, this leads to sets often exhibiting high overlap,\ne.g., strings may differ by only a single word. Yet in use-cases that call for\nmultiple solutions, a diverse or representative set is often desired. To\naddress this issue, we propose a reformulation of beam search, which we call\ndeterminantal beam search. Determinantal beam search has a natural relationship\nto determinantal point processes (DPPs), models over sets that inherently\nencode intra-set interactions. By posing iterations in beam search as a series\nof subdeterminant maximization problems, we can turn the algorithm into a\ndiverse subset selection process. In a case study, we use the string\nsubsequence kernel to explicitly encourage n-gram coverage in text generated\nfrom a sequence model. We observe that our algorithm offers competitive\nperformance against other diverse set generation strategies in the context of\nlanguage generation, while providing a more general approach to optimizing for\ndiversity.",
          "link": "http://arxiv.org/abs/2106.07400",
          "publishedOn": "2021-06-16T01:21:07.284Z",
          "wordCount": 619,
          "title": "Determinantal Beam Search. (arXiv:2106.07400v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stephenson_B/0/1/0/all/0/1\">Brooke Stephenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hueber_T/0/1/0/all/0/1\">Thomas Hueber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1\">Laurent Girin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>",
          "description": "The prosody of a spoken word is determined by its surrounding context. In\nincremental text-to-speech synthesis, where the synthesizer produces an output\nbefore it has access to the complete input, the full context is often unknown\nwhich can result in a loss of naturalness in the synthesized speech. In this\npaper, we investigate whether the use of predicted future text can attenuate\nthis loss. We compare several test conditions of next future word: (a) unknown\n(zero-word), (b) language model predicted, (c) randomly predicted and (d)\nground-truth. We measure the prosodic features (pitch, energy and duration) and\nfind that predicted text provides significant improvements over a zero-word\nlookahead, but only slight gains over random-word lookahead. We confirm these\nresults with a perceptive test.",
          "link": "http://arxiv.org/abs/2102.09914",
          "publishedOn": "2021-06-16T01:21:07.267Z",
          "wordCount": 595,
          "title": "Alternate Endings: Improving Prosody for Incremental Neural TTS with Predicted Future Text Input. (arXiv:2102.09914v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouraoui_Z/0/1/0/all/0/1\">Zied Bouraoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anke_L/0/1/0/all/0/1\">Luis Espinosa Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1\">Steven Schockaert</a>",
          "description": "One of the long-standing challenges in lexical semantics consists in learning\nrepresentations of words which reflect their semantic properties. The\nremarkable success of word embeddings for this purpose suggests that\nhigh-quality representations can be obtained by summarizing the sentence\ncontexts of word mentions. In this paper, we propose a method for learning word\nrepresentations that follows this basic strategy, but differs from standard\nword embeddings in two important ways. First, we take advantage of\ncontextualized language models (CLMs) rather than bags of word vectors to\nencode contexts. Second, rather than learning a word vector directly, we use a\ntopic model to partition the contexts in which words appear, and then learn\ndifferent topic-specific vectors for each word. Finally, we use a task-specific\nsupervision signal to make a soft selection of the resulting vectors. We show\nthat this simple strategy leads to high-quality word vectors, which are more\npredictive of semantic properties than word embeddings and existing CLM-based\nstrategies.",
          "link": "http://arxiv.org/abs/2106.07947",
          "publishedOn": "2021-06-16T01:21:07.233Z",
          "wordCount": 591,
          "title": "Deriving Word Vectors from Contextualized Language Models using Topic-Aware Mention Selection. (arXiv:2106.07947v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caucheteux_C/0/1/0/all/0/1\">Charlotte Caucheteux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gramfort_A/0/1/0/all/0/1\">Alexandre Gramfort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1\">Jean-Remi King</a>",
          "description": "The activations of language transformers like GPT-2 have been shown to\nlinearly map onto brain activity during speech comprehension. However, the\nnature of these activations remains largely unknown and presumably conflate\ndistinct linguistic classes. Here, we propose a taxonomy to factorize the\nhigh-dimensional activations of language models into four combinatorial\nclasses: lexical, compositional, syntactic, and semantic representations. We\nthen introduce a statistical method to decompose, through the lens of GPT-2's\nactivations, the brain activity of 345 subjects recorded with functional\nmagnetic resonance imaging (fMRI) during the listening of ~4.6 hours of\nnarrated text. The results highlight two findings. First, compositional\nrepresentations recruit a more widespread cortical network than lexical ones,\nand encompass the bilateral temporal, parietal and prefrontal cortices. Second,\ncontrary to previous claims, syntax and semantics are not associated with\nseparated modules, but, instead, appear to share a common and distributed\nneural substrate. Overall, this study introduces a versatile framework to\nisolate, in the brain activity, the distributed representations of linguistic\nconstructs.",
          "link": "http://arxiv.org/abs/2103.01620",
          "publishedOn": "2021-06-16T01:21:07.206Z",
          "wordCount": 631,
          "title": "Disentangling Syntax and Semantics in the Brain with Deep Networks. (arXiv:2103.01620v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.00453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wintner_S/0/1/0/all/0/1\">Shuly Wintner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "Despite impressive performance on many text classification tasks, deep neural\nnetworks tend to learn frequent superficial patterns that are specific to the\ntraining data and do not always generalize well. In this work, we observe this\nlimitation with respect to the task of native language identification. We find\nthat standard text classifiers which perform well on the test set end up\nlearning topical features which are confounds of the prediction task (e.g., if\nthe input text mentions Sweden, the classifier predicts that the author's\nnative language is Swedish). We propose a method that represents the latent\ntopical confounds and a model which \"unlearns\" confounding features by\npredicting both the label of the input text and the confound; but we train the\ntwo predictors adversarially in an alternating fashion to learn a text\nrepresentation that predicts the correct label but is less prone to using\ninformation about the confound. We show that this model generalizes better and\nlearns features that are indicative of the writing style rather than the\ncontent.",
          "link": "http://arxiv.org/abs/1909.00453",
          "publishedOn": "2021-06-16T01:21:07.198Z",
          "wordCount": 648,
          "title": "Topics to Avoid: Demoting Latent Confounds in Text Classification. (arXiv:1909.00453v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiesner_M/0/1/0/all/0/1\">Matthew Wiesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremerman_J/0/1/0/all/0/1\">Jacob Bremerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattoni_R/0/1/0/all/0/1\">Roldano Cattoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1\">Douglas W. Oard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>",
          "description": "We present the Multilingual TEDx corpus, built to support speech recognition\n(ASR) and speech translation (ST) research across many non-English source\nlanguages. The corpus is a collection of audio recordings from TEDx talks in 8\nsource languages. We segment transcripts into sentences and align them to the\nsource-language audio and target-language translations. The corpus is released\nalong with open-sourced code enabling extension to new talks and languages as\nthey become available. Our corpus creation methodology can be applied to more\nlanguages than previous work, and creates multi-way parallel evaluation sets.\nWe provide baselines in multiple ASR and ST settings, including multilingual\nmodels to improve translation performance for low-resource language pairs.",
          "link": "http://arxiv.org/abs/2102.01757",
          "publishedOn": "2021-06-16T01:21:07.191Z",
          "wordCount": 583,
          "title": "The Multilingual TEDx Corpus for Speech Recognition and Translation. (arXiv:2102.01757v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haitian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>",
          "description": "We present the Open Predicate Query Language (OPQL); a method for\nconstructing a virtual KB (VKB) trained entirely from text. Large Knowledge\nBases (KBs) are indispensable for a wide-range of industry applications such as\nquestion answering and recommendation. Typically, KBs encode world knowledge in\na structured, readily accessible form derived from laborious human annotation\nefforts. Unfortunately, while they are extremely high precision, KBs are\ninevitably highly incomplete and automated methods for enriching them are far\ntoo inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set\nof relation mentions in a way that naturally enables reasoning and can be\ntrained without any structured supervision. We demonstrate that OPQL\noutperforms prior VKB methods on two different KB reasoning tasks and,\nadditionally, can be used as an external memory integrated into a language\nmodel (OPQL-LM) leading to improvements on two open-domain question answering\ntasks.",
          "link": "http://arxiv.org/abs/2102.07043",
          "publishedOn": "2021-06-16T01:21:07.184Z",
          "wordCount": 625,
          "title": "Reasoning Over Virtual Knowledge Bases With Open Predicate Relations. (arXiv:2102.07043v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heitmeier_M/0/1/0/all/0/1\">Maria Heitmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yu-Ying Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1\">R. Harald Baayen</a>",
          "description": "This study addresses a series of methodological questions that arise when\nmodeling inflectional morphology with Linear Discriminative Learning. Taking\nthe semi-productive German noun system as example, we illustrate how decisions\nmade about the representation of form and meaning influence model performance.\nWe clarify that for modeling frequency effects in learning, it is essential to\nmake use of incremental learning rather than the endstate of learning. We also\ndiscuss how the model can be set up to approximate the learning of inflected\nwords in context. In addition, we illustrate how in this approach the wug task\ncan be modeled in considerable detail. In general, the model provides an\nexcellent memory for known words, but appropriately shows more limited\nperformance for unseen data, in line with the semi-productivity of German noun\ninflection and generalization performance of native German speakers.",
          "link": "http://arxiv.org/abs/2106.07936",
          "publishedOn": "2021-06-16T01:21:07.178Z",
          "wordCount": 572,
          "title": "Modeling morphology with Linear Discriminative Learning: considerations and design choices. (arXiv:2106.07936v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Soyoung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>",
          "description": "Data augmentation with mixup has shown to be effective on various computer\nvision tasks. Despite its great success, there has been a hurdle to apply mixup\nto NLP tasks since text consists of discrete tokens with variable length. In\nthis work, we propose SSMix, a novel mixup method where the operation is\nperformed on input text rather than on hidden vectors like previous approaches.\nSSMix synthesizes a sentence while preserving the locality of two original\ntexts by span-based mixing and keeping more tokens related to the prediction\nrelying on saliency information. With extensive experiments, we empirically\nvalidate that our method outperforms hidden-level mixup methods on a wide range\nof text classification benchmarks, including textual entailment, sentiment\nclassification, and question-type classification. Our code is available at\nhttps://github.com/clovaai/ssmix.",
          "link": "http://arxiv.org/abs/2106.08062",
          "publishedOn": "2021-06-16T01:21:07.158Z",
          "wordCount": 563,
          "title": "SSMix: Saliency-Based Span Mixup for Text Classification. (arXiv:2106.08062v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08126",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arabskyy_Y/0/1/0/all/0/1\">Yuriy Arabskyy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_A/0/1/0/all/0/1\">Aashish Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_S/0/1/0/all/0/1\">Subhadeep Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koller_O/0/1/0/all/0/1\">Oscar Koller</a>",
          "description": "This paper describes the winning approach in the public SwissText 2021\ncompetition on dialect recognition and translation of Swiss German speech to\nstandard German text. Swiss German refers to the multitude of Alemannic\ndialects spoken in the German-speaking parts of Switzerland. Swiss German\ndiffers significantly from standard German in pronunciation, word inventory and\ngrammar. It is mostly incomprehensible to native German speakers. Moreover, it\nlacks a standardized written script. To solve the challenging task, we propose\na hybrid automatic speech recognition system with a lexicon that incorporates\ntranslations, a 1st pass language model that deals with Swiss German\nparticularities, a transfer-learned acoustic model and a strong neural language\nmodel for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a blind\nconversational test set and outperforms the second best competitor by a 12%\nrelative margin.",
          "link": "http://arxiv.org/abs/2106.08126",
          "publishedOn": "2021-06-16T01:21:07.152Z",
          "wordCount": 598,
          "title": "Dialectal Speech Recognition and Translation of Swiss German Speech to Standard German Text: Microsoft's Submission to SwissText 2021. (arXiv:2106.08126v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2104.06104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1\">Albert Zeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merboldt_A/0/1/0/all/0/1\">Andr&#xe9; Merboldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>",
          "description": "With the advent of direct models in automatic speech recognition (ASR), the\nformerly prevalent frame-wise acoustic modeling based on hidden Markov models\n(HMM) diversified into a number of modeling architectures like encoder-decoder\nattention models, transducer models and segmental models (direct HMM). While\ntransducer models stay with a frame-level model definition, segmental models\nare defined on the level of label segments directly. While\n(soft-)attention-based models avoid explicit alignment, transducer and\nsegmental approach internally do model alignment, either by segment hypotheses\nor, more implicitly, by emitting so-called blank symbols. In this work, we\nprove that the widely used class of RNN-Transducer models and segmental models\n(direct HMM) are equivalent and therefore show equal modeling power. It is\nshown that blank probabilities translate into segment length probabilities and\nvice versa. In addition, we provide initial experiments investigating decoding\nand beam-pruning, comparing time-synchronous and label-/segment-synchronous\nsearch strategies and their properties using the same underlying model.",
          "link": "http://arxiv.org/abs/2104.06104",
          "publishedOn": "2021-06-16T01:21:07.137Z",
          "wordCount": 626,
          "title": "Equivalence of Segmental and Neural Transducer Modeling: A Proof of Concept. (arXiv:2104.06104v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ebadi_N/0/1/0/all/0/1\">Nima Ebadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1\">Peyman Najafirad</a>",
          "description": "The rapidly evolving literature of COVID-19 related articles makes it\nchallenging for NLP models to be effectively trained for information retrieval\nand extraction with the corresponding labeled data that follows the current\ndistribution of the pandemic. On the other hand, due to the uncertainty of the\nsituation, human experts' supervision would always be required to double check\nthe decision making of these models highlighting the importance of\ninterpretability. In the light of these challenges, this study proposes an\ninterpretable self-supervised multi-task learning model to jointly and\neffectively tackle the tasks of information retrieval (IR) and extraction (IE)\nduring the current emergency health crisis situation. Our results show that our\nmodel effectively leverage the multi-task and self-supervised learning to\nimprove generalization, data efficiency and robustness to the ongoing dataset\nshift problem. Our model outperforms baselines in IE and IR tasks, respectively\nby micro-f score of 0.08 (LCA-F score of 0.05), and MAP of 0.05 on average. In\nIE the zero- and few-shot learning performances are on average 0.32 and 0.19\nmicro-f score higher than those of the baselines.",
          "link": "http://arxiv.org/abs/2106.08252",
          "publishedOn": "2021-06-16T01:21:07.129Z",
          "wordCount": 650,
          "title": "Interpretable Self-supervised Multi-task Learning for COVID-19 Information Retrieval and Extraction. (arXiv:2106.08252v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1\">Runqiu Xiao</a>",
          "description": "Deep-Neural-Network (DNN) based speaker verification sys-tems use the angular\nsoftmax loss with margin penalties toenhance the intra-class compactness of\nspeaker embeddings,which achieved remarkable performance. In this paper, we\npro-pose a novel angular loss function called adaptive margin cir-cle loss for\nspeaker verification. The stage-based margin andchunk-based margin are applied\nto improve the angular discrim-ination of circle loss on the training set. The\nanalysis on gradi-ents shows that, compared with the previous angular loss\nlikeAdditive Margin Softmax(Am-Softmax), circle loss has flexi-ble optimization\nand definite convergence status. Experimentsare carried out on the Voxceleb and\nSITW. By applying adap-tive margin circle loss, our best system achieves\n1.31%EER onVoxceleb1 and 2.13% on SITW core-core.",
          "link": "http://arxiv.org/abs/2106.08004",
          "publishedOn": "2021-06-16T01:21:07.110Z",
          "wordCount": 541,
          "title": "Adaptive Margin Circle Loss for Speaker Verification. (arXiv:2106.08004v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>",
          "description": "Transformers have achieved great success in many artificial intelligence\nfields, such as natural language processing, computer vision, and audio\nprocessing. Therefore, it is natural to attract lots of interest from academic\nand industry researchers. Up to the present, a great variety of Transformer\nvariants (a.k.a. X-formers) have been proposed, however, a systematic and\ncomprehensive literature review on these Transformer variants is still missing.\nIn this survey, we provide a comprehensive review of various X-formers. We\nfirst briefly introduce the vanilla Transformer and then propose a new taxonomy\nof X-formers. Next, we introduce the various X-formers from three perspectives:\narchitectural modification, pre-training, and applications. Finally, we outline\nsome potential directions for future research.",
          "link": "http://arxiv.org/abs/2106.04554",
          "publishedOn": "2021-06-16T01:21:07.104Z",
          "wordCount": 554,
          "title": "A Survey of Transformers. (arXiv:2106.04554v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhaozhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Minghao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Transformer models have demonstrated superior performance in natural language\nprocessing. The dot product self-attention in Transformer allows us to model\ninteractions between words. However, this modeling comes with significant\ncomputational overhead. In this work, we revisit the memory-compute trade-off\nassociated with Transformer, particularly multi-head attention, and show a\nmemory-heavy but significantly more compute-efficient alternative to\nTransformer. Our proposal, denoted as PairConnect, a multilayer perceptron\n(MLP), models the pairwise interaction between words by explicit pairwise word\nembeddings. As a result, PairConnect substitutes self dot product with a simple\nembedding lookup. We show mathematically that despite being an MLP, our\ncompute-efficient PairConnect is strictly more expressive than Transformer. Our\nexperiment on language modeling tasks suggests that PairConnect could achieve\ncomparable results with Transformer while reducing the computational cost\nassociated with inference significantly.",
          "link": "http://arxiv.org/abs/2106.08235",
          "publishedOn": "2021-06-16T01:21:07.098Z",
          "wordCount": 557,
          "title": "PairConnect: A Compute-Efficient MLP Alternative to Attention. (arXiv:2106.08235v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grunewald_S/0/1/0/all/0/1\">Stefan Gr&#xfc;newald</a>",
          "description": "Modern graph-based syntactic dependency parsers operate by predicting, for\neach token within a sentence, a probability distribution over its possible\nsyntactic heads (i.e., all other tokens) and then extracting a maximum spanning\ntree from the resulting log-probabilities. Nowadays, virtually all such parsers\nutilize deep neural networks and may thus be susceptible to miscalibration (in\nparticular, overconfident predictions). In this paper, we prove that\ntemperature scaling, a popular technique for post-hoc calibration of neural\nnetworks, cannot change the output of the aforementioned procedure. We conclude\nthat other techniques are needed to tackle miscalibration in graph-based\ndependency parsers in a way that improves parsing accuracy.",
          "link": "http://arxiv.org/abs/2106.08159",
          "publishedOn": "2021-06-16T01:21:07.091Z",
          "wordCount": 536,
          "title": "Maximum Spanning Trees Are Invariant to Temperature Scaling in Graph-based Dependency Parsing. (arXiv:2106.08159v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kutuzov_A/0/1/0/all/0/1\">Andrey Kutuzov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pivovarova_L/0/1/0/all/0/1\">Lidia Pivovarova</a>",
          "description": "We present a manually annotated lexical semantic change dataset for Russian:\nRuShiftEval. Its novelty is ensured by a single set of target words annotated\nfor their diachronic semantic shifts across three time periods, while the\nprevious work either used only two time periods, or different sets of target\nwords. The paper describes the composition and annotation procedure for the\ndataset. In addition, it is shown how the ternary nature of RuShiftEval allows\nto trace specific diachronic trajectories: `changed at a particular time period\nand stable afterwards' or `was changing throughout all time periods'. Based on\nthe analysis of the submissions to the recent shared task on semantic change\ndetection for Russian, we argue that correctly identifying such trajectories\ncan be an interesting sub-task itself.",
          "link": "http://arxiv.org/abs/2106.08294",
          "publishedOn": "2021-06-16T01:21:07.085Z",
          "wordCount": 559,
          "title": "Three-part diachronic semantic change dataset for Russian. (arXiv:2106.08294v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1\">Xin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_H/0/1/0/all/0/1\">Hui Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1\">Hongying Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>",
          "description": "Artificial Intelligence (AI), along with the recent progress in biomedical\nlanguage understanding, is gradually changing medical practice. With the\ndevelopment of biomedical language understanding benchmarks, AI applications\nare widely used in the medical field. However, most benchmarks are limited to\nEnglish, which makes it challenging to replicate many of the successes in\nEnglish for other languages. To facilitate research in this direction, we\ncollect real-world biomedical data and present the first Chinese Biomedical\nLanguage Understanding Evaluation (CBLUE) benchmark: a collection of natural\nlanguage understanding tasks including named entity recognition, information\nextraction, clinical diagnosis normalization, single-sentence/sentence-pair\nclassification, and an associated online platform for model evaluation,\ncomparison, and analysis. To establish evaluation on these tasks, we report\nempirical results with the current 11 pre-trained Chinese models, and\nexperimental results show that state-of-the-art neural models perform by far\nworse than the human ceiling. Our benchmark is released at\n\\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us}.",
          "link": "http://arxiv.org/abs/2106.08087",
          "publishedOn": "2021-06-16T01:21:07.078Z",
          "wordCount": 624,
          "title": "CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balazy_K/0/1/0/all/0/1\">Klaudia Ba&#x142;azy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaei_M/0/1/0/all/0/1\">Mohammadreza Banaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1\">R&#xe9;mi Lebret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1\">Jacek Tabor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1\">Karl Aberer</a>",
          "description": "The adoption of Transformer-based models in natural language processing (NLP)\nhas led to great success using a massive number of parameters. However, due to\ndeployment constraints in edge devices, there has been a rising interest in the\ncompression of these models to improve their inference time and memory\nfootprint. This paper presents a novel loss objective to compress token\nembeddings in the Transformer-based models by leveraging an AutoEncoder\narchitecture. More specifically, we emphasize the importance of the direction\nof compressed embeddings with respect to original uncompressed embeddings. The\nproposed method is task-agnostic and does not require further language modeling\npre-training. Our method significantly outperforms the commonly used SVD-based\nmatrix-factorization approach in terms of initial language model Perplexity.\nMoreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several\ndownstream tasks from the GLUE benchmark, where we also outperform the baseline\nin most scenarios. Our code is public.",
          "link": "http://arxiv.org/abs/2106.08181",
          "publishedOn": "2021-06-16T01:21:07.059Z",
          "wordCount": 586,
          "title": "Direction is what you need: Improving Word Embedding Compression in Large Language Models. (arXiv:2106.08181v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Arun S. Maiya</a>",
          "description": "The vast majority of existing methods and systems for causal inference assume\nthat all variables under consideration are categorical or numerical (e.g.,\ngender, price, blood pressure, enrollment). In this paper, we present\nCausalNLP, a toolkit for inferring causality from observational data that\nincludes text in addition to traditional numerical and categorical variables.\nCausalNLP employs the use of meta-learners for treatment effect estimation and\nsupports using raw text and its linguistic properties as both a treatment and a\n\"controlled-for\" variable (e.g., confounder). The library is open-source and\navailable at: https://github.com/amaiya/causalnlp.",
          "link": "http://arxiv.org/abs/2106.08043",
          "publishedOn": "2021-06-16T01:21:07.053Z",
          "wordCount": 521,
          "title": "CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07759",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Manohar_V/0/1/0/all/0/1\">Vimal Manohar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zweig_G/0/1/0/all/0/1\">Geoffrey Zweig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>",
          "description": "In this paper, we introduce the Kaizen framework that uses a continuously\nimproving teacher to generate pseudo-labels for semi-supervised training. The\nproposed approach uses a teacher model which is updated as the exponential\nmoving average of the student model parameters. This can be seen as a\ncontinuous version of the iterative pseudo-labeling approach for\nsemi-supervised training. It is applicable for different training criteria, and\nin this paper we demonstrate it for frame-level hybrid hidden Markov model -\ndeep neural network (HMM-DNN) models and sequence-level connectionist temporal\nclassification (CTC) based models. The proposed approach shows more than 10%\nword error rate (WER) reduction over standard teacher-student training and more\nthan 50\\% relative WER reduction over 10 hour supervised baseline when using\nlarge scale realistic unsupervised public videos in UK English and Italian\nlanguages.",
          "link": "http://arxiv.org/abs/2106.07759",
          "publishedOn": "2021-06-16T01:21:07.045Z",
          "wordCount": 598,
          "title": "Kaizen: Continuously improving teacher using Exponential Moving Average for semi-supervised speech recognition. (arXiv:2106.07759v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1\">Ramon Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>",
          "description": "Speech-based image retrieval has been studied as a proxy for joint\nrepresentation learning, usually without emphasis on retrieval itself. As such,\nit is unclear how well speech-based retrieval can work in practice -- both in\nan absolute sense and versus alternative strategies that combine automatic\nspeech recognition (ASR) with strong text encoders. In this work, we\nextensively study and expand choices of encoder architectures, training\nmethodology (including unimodal and multimodal pretraining), and other factors.\nOur experiments cover different types of speech in three datasets: Flickr\nAudio, Places Audio, and Localized Narratives. Our best model configuration\nachieves large gains over state of the art, e.g., pushing recall-at-one from\n21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also\nshow our best speech-based models can match or exceed cascaded ASR-to-text\nencoding when speech is spontaneous, accented, or otherwise hard to\nautomatically transcribe.",
          "link": "http://arxiv.org/abs/2104.01894",
          "publishedOn": "2021-06-16T01:21:07.030Z",
          "wordCount": 631,
          "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadde_S/0/1/0/all/0/1\">Shoval Sadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1\">Aynat Rubinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portner_P/0/1/0/all/0/1\">Paul Portner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>",
          "description": "Modality is the linguistic ability to describe events with added information\nsuch as how desirable, plausible, or feasible they are. Modality is important\nfor many NLP downstream tasks such as the detection of hedging, uncertainty,\nspeculation, and more. Previous studies that address modality detection in NLP\noften restrict modal expressions to a closed syntactic class, and the modal\nsense labels are vastly different across different studies, lacking an accepted\nstandard. Furthermore, these senses are often analyzed independently of the\nevents that they modify. This work builds on the theoretical foundations of the\nGeorgetown Gradable Modal Expressions (GME) work by Rubinstein et al. (2013) to\npropose an event-based modality detection task where modal expressions can be\nwords of any syntactic class and sense labels are drawn from a comprehensive\ntaxonomy which harmonizes the modal concepts contributed by the different\nstudies. We present experiments on the GME corpus aiming to detect and classify\nfine-grained modal concepts and associate them with their modified events. We\nshow that detecting and classifying modal expressions is not only feasible, but\nalso improves the detection of modal events in their own right.",
          "link": "http://arxiv.org/abs/2106.08037",
          "publishedOn": "2021-06-16T01:21:07.012Z",
          "wordCount": 626,
          "title": "The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing. (arXiv:2106.08037v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We study transfer learning in the presence of spurious correlations. We\nexperimentally demonstrate that directly transferring the stable feature\nextractor learned on the source task may not eliminate these biases for the\ntarget task. However, we hypothesize that the unstable features in the source\ntask and those in the target task are directly related. By explicitly informing\nthe target classifier of the source task's unstable features, we can regularize\nthe biases in the target task. Specifically, we derive a representation that\nencodes the unstable features by contrasting different data environments in the\nsource task. On the target task, we cluster data from this representation, and\nachieve robustness by minimizing the worst-case risk across all clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask, outperforming the best baseline by 22.9% in absolute accuracy across 12\ntransfer settings. Our code is available at https://github.com/YujiaBao/Tofu.",
          "link": "http://arxiv.org/abs/2106.07847",
          "publishedOn": "2021-06-16T01:21:07.005Z",
          "wordCount": 601,
          "title": "Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Suraj Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brennan_J/0/1/0/all/0/1\">Joseph Brennan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1\">Jason R. C. Nurse</a>",
          "description": "We introduce StockBabble, a conversational agent designed to support\nunderstanding and engagement with the stock market. StockBabble's value and\nnovelty is in its ability to empower retail investors -- many of which may be\nnew to investing -- and supplement their informational needs using a\nuser-friendly agent. Users have the ability to query information on companies\nto retrieve a general and financial overview of a stock, including accessing\nthe latest news and trading recommendations. They can also request charts which\ncontain live prices and technical investment indicators, and add shares to a\npersonal portfolio to allow performance monitoring over time. To evaluate our\nagent's potential, we conducted a user study with 15 participants. In total,\n73% (11/15) of respondents said that they felt more confident in investing\nafter using StockBabble, and all 15 would consider recommending it to others.\nThese results are encouraging and suggest a wider appeal for such agents.\nMoreover, we believe this research can help to inform the design and\ndevelopment of future intelligent, financial personal assistants.",
          "link": "http://arxiv.org/abs/2106.08298",
          "publishedOn": "2021-06-16T01:21:06.989Z",
          "wordCount": 629,
          "title": "StockBabble: A Conversational Financial Agent to support Stock Market Investors. (arXiv:2106.08298v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuohao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flemotomos_N/0/1/0/all/0/1\">Nikolaos Flemotomos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_K/0/1/0/all/0/1\">Karan Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creed_T/0/1/0/all/0/1\">Torrey A. Creed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkins_D/0/1/0/all/0/1\">David C. Atkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>",
          "description": "Computational approaches for assessing the quality of conversation-based\npsychotherapy, such as Cognitive Behavioral Therapy (CBT) and Motivational\nInterviewing (MI), have been developed recently to support quality assurance\nand clinical training. However, due to the long session lengths and limited\nmodeling resources, computational methods largely rely on frequency-based\nlexical features or distribution of dialogue acts. In this work, we propose a\nhierarchical framework to automatically evaluate the quality of a CBT\ninteraction. We divide each psychotherapy session into conversation segments\nand input those into a BERT-based model to produce segment embeddings. We first\nfine-tune BERT for predicting segment-level (local) quality scores and then use\nsegment embeddings as lower-level input to a Bidirectional LSTM-based neural\nnetwork to predict session-level (global) quality estimates. In particular, the\nsegment-level quality scores are initialized with the session-level scores and\nwe model the global quality as a function of the local quality scores to\nachieve the accurate segment-level quality estimates. These estimated\nsegment-level scores benefit theBERT fine-tuning and in learning better segment\nembeddings. We evaluate the proposed framework on data drawn from real-world\nCBT clinical session recordings to predict multiple session-level behavior\ncodes. The results indicate that our approach leads to improved evaluation\naccuracy for most codes in both regression and classification tasks.",
          "link": "http://arxiv.org/abs/2106.07922",
          "publishedOn": "2021-06-16T01:21:06.932Z",
          "wordCount": 653,
          "title": "An Automated Quality Evaluation Framework of Psychotherapy Conversations with Local Quality Estimates. (arXiv:2106.07922v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nighojkar_A/0/1/0/all/0/1\">Animesh Nighojkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licato_J/0/1/0/all/0/1\">John Licato</a>",
          "description": "If two sentences have the same meaning, it should follow that they are\nequivalent in their inferential properties, i.e., each sentence should\ntextually entail the other. However, many paraphrase datasets currently in\nwidespread use rely on a sense of paraphrase based on word overlap and syntax.\nCan we teach them instead to identify paraphrases in a way that draws on the\ninferential properties of the sentences, and is not over-reliant on lexical and\nsyntactic similarities of a sentence pair? We apply the adversarial paradigm to\nthis question, and introduce a new adversarial method of dataset creation for\nparaphrase identification: the Adversarial Paraphrasing Task (APT), which asks\nparticipants to generate semantically equivalent (in the sense of mutually\nimplicative) but lexically and syntactically disparate paraphrases. These\nsentence pairs can then be used both to test paraphrase identification models\n(which get barely random accuracy) and then improve their performance. To\naccelerate dataset generation, we explore automation of APT using T5, and show\nthat the resulting dataset also improves accuracy. We discuss implications for\nparaphrase detection and release our dataset in the hope of making paraphrase\ndetection models better able to detect sentence-level meaning equivalence.",
          "link": "http://arxiv.org/abs/2106.07691",
          "publishedOn": "2021-06-16T01:21:06.889Z",
          "wordCount": 616,
          "title": "Improving Paraphrase Detection with the Adversarial Paraphrasing Task. (arXiv:2106.07691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07890",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bradley_T/0/1/0/all/0/1\">Tai-Danae Bradley</a>, <a href=\"http://arxiv.org/find/math/1/au:+Terilla_J/0/1/0/all/0/1\">John Terilla</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vlassopoulos_Y/0/1/0/all/0/1\">Yiannis Vlassopoulos</a>",
          "description": "Given a piece of text, the ability to generate a coherent extension of it\nimplies some sophistication, including a knowledge of grammar and semantics. In\nthis paper, we propose a mathematical framework for passing from probability\ndistributions on extensions of given texts to an enriched category containing\nsemantic information. Roughly speaking, we model probability distributions on\ntexts as a category enriched over the unit interval. Objects of this category\nare expressions in language and hom objects are conditional probabilities that\none expression is an extension of another. This category is syntactical: it\ndescribes what goes with what. We then pass to the enriched category of unit\ninterval-valued copresheaves on this syntactical category to find semantic\ninformation.",
          "link": "http://arxiv.org/abs/2106.07890",
          "publishedOn": "2021-06-16T01:21:06.883Z",
          "wordCount": 551,
          "title": "An enriched category theory of language: from syntax to semantics. (arXiv:2106.07890v1 [math.CT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_R/0/1/0/all/0/1\">Rupak Vignesh Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_B/0/1/0/all/0/1\">Brian King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Grant P. Strimel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>",
          "description": "We propose a simple yet effective method to compress an RNN-Transducer\n(RNN-T) through the well-known knowledge distillation paradigm. We show that\nthe transducer's encoder outputs naturally have a high entropy and contain rich\ninformation about acoustically similar word-piece confusions. This rich\ninformation is suppressed when combined with the lower entropy decoder outputs\nto produce the joint network logits. Consequently, we introduce an auxiliary\nloss to distill the encoder logits from a teacher transducer's encoder, and\nexplore training strategies where this encoder distillation works effectively.\nWe find that tandem training of teacher and student encoders with an inplace\nencoder distillation outperforms the use of a pre-trained and static teacher\ntransducer. We also report an interesting phenomenon we refer to as implicit\ndistillation, that occurs when the teacher and student encoders share the same\ndecoder. Our experiments show 5.37-8.4% relative word error rate reductions\n(WERR) on in-house test sets, and 5.05-6.18% relative WERRs on LibriSpeech test\nsets.",
          "link": "http://arxiv.org/abs/2106.07734",
          "publishedOn": "2021-06-16T01:21:06.826Z",
          "wordCount": 606,
          "title": "CoDERT: Distilling Encoder Representations with Co-learning for Transducer-based Speech Recognition. (arXiv:2106.07734v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a> (Member, IEEE), <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a> (Fellow, IEEE)",
          "description": "Generating personalized responses is one of the major challenges in natural\nhuman-robot interaction. Current researches in this field mainly focus on\ngenerating responses consistent with the robot's pre-assigned persona, while\nignoring the user's persona. Such responses may be inappropriate or even\noffensive, which may lead to the bad user experience. Therefore, we propose a\nbilateral personalized dialogue generation (BPDG) method with dynamic\npersona-aware fusion via multi-task transfer learning to generate responses\nconsistent with both personas. The proposed method aims to accomplish three\nlearning tasks: 1) an encoder is trained with dialogue utterances added with\ncorresponded personalized attributes and relative position (language model\ntask), 2) a dynamic persona-aware fusion module predicts the persona presence\nto adaptively fuse the contextual and bilateral personas encodings (persona\nprediction task) and 3) a decoder generates natural, fluent and personalized\nresponses (dialogue generation task). To make the generated responses more\npersonalized and bilateral persona-consistent, the Conditional Mutual\nInformation Maximum (CMIM) criterion is adopted to select the final response\nfrom the generated candidates. The experimental results show that the proposed\nmethod outperforms several state-of-the-art methods in terms of both automatic\nand manual evaluations.",
          "link": "http://arxiv.org/abs/2106.07857",
          "publishedOn": "2021-06-16T01:21:06.797Z",
          "wordCount": 623,
          "title": "Bilateral Personalized Dialogue Generation with Dynamic Persona-Aware Fusion. (arXiv:2106.07857v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shanbo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Multilingual Neural Machine Translation (MNMT) has aroused widespread\ninterest due to its efficiency. An exciting advantage of MNMT models is that\nthey could also translate between unsupervised (zero-shot) language directions.\nLanguage tag (LT) strategies are often adopted to indicate the translation\ndirections in MNMT. In this paper, we demonstrate that the LTs are not only\nindicators for translation directions but also crucial to zero-shot translation\nqualities. Unfortunately, previous work tends to ignore the importance of LT\nstrategies. We demonstrate that a proper LT strategy could enhance the\nconsistency of semantic representations and alleviate the off-target issue in\nzero-shot directions. Experimental results show that by ignoring the source\nlanguage tag (SLT) and adding the target language tag (TLT) to the encoder, the\nzero-shot translations could achieve a +8 BLEU score difference over other LT\nstrategies in IWSLT17, Europarl, TED talks translation tasks.",
          "link": "http://arxiv.org/abs/2106.07930",
          "publishedOn": "2021-06-16T01:21:06.788Z",
          "wordCount": 578,
          "title": "Language Tags Matter for Zero-Shot Neural Machine Translation. (arXiv:2106.07930v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1\">Minae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1\">Siddharth Karamcheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuellar_M/0/1/0/all/0/1\">Mariano-Florentino Cuellar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>",
          "description": "Successful negotiators must learn how to balance optimizing for self-interest\nand cooperation. Yet current artificial negotiation agents often heavily depend\non the quality of the static datasets they were trained on, limiting their\ncapacity to fashion an adaptive response balancing self-interest and\ncooperation. For this reason, we find that these agents can achieve either high\nutility or cooperation, but not both. To address this, we introduce a targeted\ndata acquisition framework where we guide the exploration of a reinforcement\nlearning agent using annotations from an expert oracle. The guided exploration\nincentivizes the learning agent to go beyond its static dataset and develop new\nnegotiation strategies. We show that this enables our agents to obtain\nhigher-reward and more Pareto-optimal solutions when negotiating with both\nsimulated and human partners compared to standard supervised learning and\nreinforcement learning methods. This trend additionally holds when comparing\nagents using our targeted data acquisition framework to variants of agents\ntrained with a mix of supervised learning and reinforcement learning, or to\nagents using tailored reward functions that explicitly optimize for utility and\nPareto-optimality.",
          "link": "http://arxiv.org/abs/2106.07728",
          "publishedOn": "2021-06-16T01:21:06.781Z",
          "wordCount": 615,
          "title": "Targeted Data Acquisition for Evolving Negotiation Agents. (arXiv:2106.07728v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>",
          "description": "Automatic readability assessment (ARA) is the task of evaluating the level of\nease or difficulty of text documents for a target audience. For researchers,\none of the many open problems in the field is to make such models trained for\nthe task show efficacy even for low-resource languages. In this study, we\npropose an alternative way of utilizing the information-rich embeddings of BERT\nmodels through a joint-learning method combined with handcrafted linguistic\nfeatures for readability assessment. Results show that the proposed method\noutperforms classical approaches in readability assessment using English and\nFilipino datasets, and obtaining as high as 12.4% increase in F1 performance.\nWe also show that the knowledge encoded in BERT embeddings can be used as a\nsubstitute feature set for low-resource languages like Filipino with limited\nsemantic and syntactic NLP tools to explicitly extract feature values for the\ntask.",
          "link": "http://arxiv.org/abs/2106.07935",
          "publishedOn": "2021-06-16T01:21:06.744Z",
          "wordCount": 558,
          "title": "Knowledge-Rich BERT Embeddings for Readability Assessment. (arXiv:2106.07935v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bowen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>",
          "description": "Maximum likelihood estimation (MLE) is the predominant algorithm for training\ntext generation models. This paradigm relies on direct supervision examples,\nwhich is not applicable to many applications, such as generating adversarial\nattacks or generating prompts to control language models. Reinforcement\nlearning (RL) on the other hand offers a more flexible solution by allowing\nusers to plug in arbitrary task metrics as reward. Yet previous RL algorithms\nfor text generation, such as policy gradient (on-policy RL) and Q-learning\n(off-policy RL), are often notoriously inefficient or unstable to train due to\nthe large sequence space and the sparse reward received only at the end of\nsequences. In this paper, we introduce a new RL formulation for text generation\nfrom the soft Q-learning perspective. It further enables us to draw from the\nlatest RL advances, such as path consistency learning, to combine the best of\non-/off-policy updates, and learn effectively from sparse reward. We apply the\napproach to a wide range of tasks, including learning from noisy/negative\nexamples, adversarial attacks, and prompt generation. Experiments show our\napproach consistently outperforms both task-specialized algorithms and the\nprevious RL methods. On standard supervised tasks where MLE prevails, our\napproach also achieves competitive performance and stability by training text\ngeneration from scratch.",
          "link": "http://arxiv.org/abs/2106.07704",
          "publishedOn": "2021-06-16T01:21:06.738Z",
          "wordCount": 641,
          "title": "Text Generation with Efficient (Soft) Q-Learning. (arXiv:2106.07704v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>",
          "description": "This work explores constituency parsing on automatically recognized\ntranscripts of conversational speech. The neural parser is based on a sentence\nencoder that leverages word vectors contextualized with prosodic features,\njointly learning prosodic feature extraction with parsing. We assess the\nutility of the prosody in parsing on imperfect transcripts, i.e. transcripts\nwith automatic speech recognition (ASR) errors, by applying the parser in an\nN-best reranking framework. In experiments on Switchboard, we obtain 13-15% of\nthe oracle N-best gain relative to parsing the 1-best ASR output, with\ninsignificant impact on word recognition error rate. Prosody provides a\nsignificant part of the gain, and analyses suggest that it leads to more\ngrammatical utterances via recovering function words.",
          "link": "http://arxiv.org/abs/2106.07794",
          "publishedOn": "2021-06-16T01:21:06.702Z",
          "wordCount": 544,
          "title": "Assessing the Use of Prosody in Constituency Parsing of Imperfect Transcripts. (arXiv:2106.07794v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiarun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veen_E/0/1/0/all/0/1\">Elke M van Veen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peek_N/0/1/0/all/0/1\">Niels Peek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renehan_A/0/1/0/all/0/1\">Andrew G Renehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>",
          "description": "To interpret the genetic profile present in a patient sample, it is necessary\nto know which mutations have important roles in the development of the\ncorresponding cancer type. Named entity recognition is a core step in the text\nmining pipeline which facilitates mining valuable cancer information from the\nscientific literature. However, due to the scarcity of related datasets,\nprevious NER attempts in this domain either suffer from low performance when\ndeep learning based models are deployed, or they apply feature based machine\nlearning models or rule based models to tackle this problem, which requires\nintensive efforts from domain experts, and limit the model generalization\ncapability. In this paper, we propose EPICURE, an ensemble pre trained model\nequipped with a conditional random field pattern layer and a span prediction\npattern layer to extract cancer mutations from text. We also adopt a data\naugmentation strategy to expand our training set from multiple datasets.\nExperimental results on three benchmark datasets show competitive results\ncompared to the baseline models.",
          "link": "http://arxiv.org/abs/2106.07722",
          "publishedOn": "2021-06-16T01:21:06.615Z",
          "wordCount": 602,
          "title": "EPICURE Ensemble Pretrained Models for Extracting Cancer Mutations from Literature. (arXiv:2106.07722v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chak-Fai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keith_F/0/1/0/all/0/1\">Francis Keith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_W/0/1/0/all/0/1\">William Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snover_M/0/1/0/all/0/1\">Matthew Snover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimball_O/0/1/0/all/0/1\">Owen Kimball</a>",
          "description": "Sequence-to-sequence (seq2seq) models are competitive with hybrid models for\nautomatic speech recognition (ASR) tasks when large amounts of training data\nare available. However, data sparsity and domain adaptation are more\nproblematic for seq2seq models than their hybrid counterparts. We examine\ncorpora of five languages from the IARPA MATERIAL program where the transcribed\ndata is conversational telephone speech (CTS) and evaluation data is broadcast\nnews (BN). We show that there is a sizable initial gap in such a data condition\nbetween hybrid and seq2seq models, and the hybrid model is able to further\nimprove through the use of additional language model (LM) data. We use an\nadditional set of untranscribed data primarily in the BN domain for\nsemisupervised training. In semisupervised training, a seed model trained on\ntranscribed data generates hypothesized transcripts for unlabeled\ndomain-matched data for further training. By using a hybrid model with an\nexpanded language model for pseudotranscription, we are able to improve our\nseq2seq model from an average word error rate (WER) of 66.7% across all five\nlanguages to 29.0% WER. While this puts the seq2seq model at a competitive\noperating point, hybrid models are still able to use additional LM data to\nmaintain an advantage.",
          "link": "http://arxiv.org/abs/2106.07716",
          "publishedOn": "2021-06-16T01:21:06.609Z",
          "wordCount": 650,
          "title": "Overcoming Domain Mismatch in Low Resource Sequence-to-Sequence ASR Models using Hybrid Generated Pseudotranscripts. (arXiv:2106.07716v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>",
          "description": "Contrastive learning has been applied successfully to learn vector\nrepresentations of text. Previous research demonstrated that learning\nhigh-quality representations benefits from batch-wise contrastive loss with a\nlarge number of negatives. In practice, the technique of in-batch negative is\nused, where for each example in a batch, other batch examples' positives will\nbe taken as its negatives, avoiding encoding extra negatives. This, however,\nstill conditions each example's loss on all batch examples and requires fitting\nthe entire large batch into GPU memory. This paper introduces a gradient\ncaching technique that decouples backpropagation between contrastive loss and\nthe encoder, removing encoder backward pass data dependency along the batch\ndimension. As a result, gradients can be computed for one subset of the batch\nat a time, leading to almost constant memory usage.",
          "link": "http://arxiv.org/abs/2101.06983",
          "publishedOn": "2021-06-16T00:27:38.461Z",
          "wordCount": 593,
          "title": "Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup. (arXiv:2101.06983v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "While deep and large pre-trained models are the state-of-the-art for various\nnatural language processing tasks, their huge size poses significant challenges\nfor practical uses in resource constrained settings. Recent works in knowledge\ndistillation propose task-agnostic as well as task-specific methods to compress\nthese models, with task-specific ones often yielding higher compression rate.\nIn this work, we develop a new task-agnostic distillation framework\nXtremeDistilTransformers that leverages the advantage of task-specific methods\nfor learning a small universal model that can be applied to arbitrary tasks and\nlanguages. To this end, we study the transferability of several source tasks,\naugmentation resources and model architecture for distillation. We evaluate our\nmodel performance on multiple tasks, including the General Language\nUnderstanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and\na massive multi-lingual NER dataset with 41 languages. We release three\ndistilled task-agnostic checkpoints with 13MM, 22MM and 33MM parameters\nobtaining SOTA performance in several tasks.",
          "link": "http://arxiv.org/abs/2106.04563",
          "publishedOn": "2021-06-15T22:41:24.981Z",
          "wordCount": 604,
          "title": "XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mina Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1\">Chris Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyabor_A/0/1/0/all/0/1\">Alexander Iyabor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "We release a new benchmark for lexical substitution, the task of finding\nappropriate substitutes for a target word in a context. To assist humans with\nwriting, lexical substitution systems can suggest words that humans cannot\neasily think of. However, existing benchmarks depend on human recall as the\nonly source of data, and therefore lack coverage of the substitutes that would\nbe most helpful to humans. Furthermore, annotators often provide substitutes of\nlow quality, which are not actually appropriate in the given context. We\ncollect higher-coverage and higher-quality data by framing lexical substitution\nas a classification problem, guided by the intuition that it is easier for\nhumans to judge the appropriateness of candidate substitutes than conjure them\nfrom memory. To this end, we use a context-free thesaurus to produce candidates\nand rely on human judgement to determine contextual appropriateness. Compared\nto the previous largest benchmark, our Swords benchmark has 4.1x more\nsubstitutes per target word for the same level of quality, and its substitutes\nare 1.5x more appropriate (based on human judgement) for the same number of\nsubstitutes.",
          "link": "http://arxiv.org/abs/2106.04102",
          "publishedOn": "2021-06-15T22:41:24.969Z",
          "wordCount": 638,
          "title": "Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality. (arXiv:2106.04102v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damonte_M/0/1/0/all/0/1\">Marco Damonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monti_E/0/1/0/all/0/1\">Emilio Monti</a>",
          "description": "Semantic parsers map natural language utterances to meaning representations.\nThe lack of a single standard for meaning representations led to the creation\nof a plethora of semantic parsing datasets. To unify different datasets and\ntrain a single model for them, we investigate the use of Multi-Task Learning\n(MTL) architectures. We experiment with five datasets (Geoquery, NLMaps, TOP,\nOvernight, AMR). We find that an MTL architecture that shares the entire\nnetwork across datasets yields competitive or better parsing accuracies than\nthe single-task baselines, while reducing the total number of parameters by\n68%. We further provide evidence that MTL has also better compositional\ngeneralization than single-task models. We also present a comparison of task\nsampling methods and propose a competitive alternative to widespread\nproportional sampling strategies.",
          "link": "http://arxiv.org/abs/2106.04476",
          "publishedOn": "2021-06-15T22:41:24.890Z",
          "wordCount": 592,
          "title": "One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets. (arXiv:2106.04476v2 [cs.CL] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2103.12906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Sheshera Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>",
          "description": "Query by Example is a well-known information retrieval task in which a\ndocument is chosen by the user as the search query and the goal is to retrieve\nrelevant documents from a large collection. However, a document often covers\nmultiple aspects of a topic. To address this scenario we introduce the task of\nfaceted Query by Example in which users can also specify a finer grained aspect\nin addition to the input query document. We focus on the application of this\ntask in scientific literature search. We envision models which are able to\nretrieve scientific papers analogous to a query scientific paper along\nspecifically chosen rhetorical structure elements as one solution to this\nproblem. In this work, the rhetorical structure elements, which we refer to as\nfacets, indicate backgrounds, methods, or results of a scientific paper. We\nintroduce and describe an expert annotated test collection to evaluate models\ntrained to perform this task. Our test collection consists of a diverse set of\n50 query documents, drawn from computational linguistics and machine learning\nvenues. We carefully followed the annotation guideline used by TREC for depth-k\npooling (k = 100 or 250) and the resulting data collection consists of graded\nrelevance scores with high annotation agreement. The data is freely available\nfor research purposes.",
          "link": "http://arxiv.org/abs/2103.12906",
          "publishedOn": "2021-06-21T02:07:36.688Z",
          "wordCount": 700,
          "title": "CSFCube -- A Test Collection of Computer Science Research Articles for Faceted Query by Example. (arXiv:2103.12906v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiancan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1\">Jianxun Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "Representation learning on user-item graph for recommendation has evolved\nfrom using single ID or interaction history to exploiting higher-order\nneighbors. This leads to the success of graph convolution networks (GCNs) for\nrecommendation such as PinSage and LightGCN. Despite effectiveness, we argue\nthat they suffer from two limitations: (1) high-degree nodes exert larger\nimpact on the representation learning, deteriorating the recommendations of\nlow-degree (long-tail) items; and (2) representations are vulnerable to noisy\ninteractions, as the neighborhood aggregation scheme further enlarges the\nimpact of observed edges.\n\nIn this work, we explore self-supervised learning on user-item graph, so as\nto improve the accuracy and robustness of GCNs for recommendation. The idea is\nto supplement the classical supervised task of recommendation with an auxiliary\nself-supervised task, which reinforces node representation learning via\nself-discrimination. Specifically, we generate multiple views of a node,\nmaximizing the agreement between different views of the same node compared to\nthat of other nodes. We devise three operators to generate the views -- node\ndropout, edge dropout, and random walk -- that change the graph structure in\ndifferent manners. We term this new learning paradigm as\n\\textit{Self-supervised Graph Learning} (SGL), implementing it on the\nstate-of-the-art model LightGCN. Through theoretical analyses, we find that SGL\nhas the ability of automatically mining hard negatives. Empirical studies on\nthree benchmark datasets demonstrate the effectiveness of SGL, which improves\nthe recommendation accuracy, especially on long-tail items, and the robustness\nagainst interaction noises. Our implementations are available at\n\\url{https://github.com/wujcan/SGL}.",
          "link": "http://arxiv.org/abs/2010.10783",
          "publishedOn": "2021-06-21T02:07:36.677Z",
          "wordCount": 740,
          "title": "Self-supervised Graph Learning for Recommendation. (arXiv:2010.10783v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Pre-training and fine-tuning have achieved remarkable success in many\ndownstream natural language processing (NLP) tasks. Recently, pre-training\nmethods tailored for information retrieval (IR) have also been explored, and\nthe latest success is the PROP method which has reached new SOTA on a variety\nof ad-hoc retrieval benchmarks. The basic idea of PROP is to construct the\n\\textit{representative words prediction} (ROP) task for pre-training inspired\nby the query likelihood model. Despite its exciting performance, the\neffectiveness of PROP might be bounded by the classical unigram language model\nadopted in the ROP task construction process. To tackle this problem, we\npropose a bootstrapped pre-training method (namely B-PROP) based on BERT for\nad-hoc retrieval. The key idea is to use the powerful contextual language model\nBERT to replace the classical unigram language model for the ROP task\nconstruction, and re-train BERT itself towards the tailored objective for IR.\nSpecifically, we introduce a novel contrastive method, inspired by the\ndivergence-from-randomness idea, to leverage BERT's self-attention mechanism to\nsample representative words from the document. By further fine-tuning on\ndownstream ad-hoc retrieval tasks, our method achieves significant improvements\nover baselines without pre-training or with other pre-training methods, and\nfurther pushes forward the SOTA on a variety of ad-hoc retrieval tasks.",
          "link": "http://arxiv.org/abs/2104.09791",
          "publishedOn": "2021-06-21T02:07:36.665Z",
          "wordCount": 696,
          "title": "B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval. (arXiv:2104.09791v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yi-Ling Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yu-Che Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng-Te Li</a>",
          "description": "Financial technology (FinTech) has drawn much attention among investors and\ncompanies. While conventional stock analysis in FinTech targets at predicting\nstock prices, less effort is made for profitable stock recommendation. Besides,\nin existing approaches on modeling time series of stock prices, the\nrelationships among stocks and sectors (i.e., categories of stocks) are either\nneglected or pre-defined. Ignoring stock relationships will miss the\ninformation shared between stocks while using pre-defined relationships cannot\ndepict the latent interactions or influence of stock prices between stocks. In\nthis work, we aim at recommending the top-K profitable stocks in terms of\nreturn ratio using time series of stock prices and sector information. We\npropose a novel deep learning-based model, Financial Graph Attention Networks\n(FinGAT), to tackle the task under the setting that no pre-defined\nrelationships between stocks are given. The idea of FinGAT is three-fold.\nFirst, we devise a hierarchical learning component to learn short-term and\nlong-term sequential patterns from stock time series. Second, a fully-connected\ngraph between stocks and a fully-connected graph between sectors are\nconstructed, along with graph attention networks, to learn the latent\ninteractions among stocks and sectors. Third, a multi-task objective is devised\nto jointly recommend the profitable stocks and predict the stock movement.\nExperiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit\nremarkable recommendation performance of our FinGAT, comparing to\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.10159",
          "publishedOn": "2021-06-21T02:07:36.627Z",
          "wordCount": 695,
          "title": "FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks. (arXiv:2106.10159v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eugene Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1\">David D. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1\">Ophir Frieder</a>",
          "description": "Technology-assisted review (TAR) refers to human-in-the-loop active learning\nworkflows for finding relevant documents in large collections. These workflows\noften must meet a target for the proportion of relevant documents found (i.e.\nrecall) while also holding down costs. A variety of heuristic stopping rules\nhave been suggested for striking this tradeoff in particular settings, but none\nhave been tested against a range of recall targets and tasks. We propose two\nnew heuristic stopping rules, Quant and QuantCI based on model-based estimation\ntechniques from survey research. We compare them against a range of proposed\nheuristics and find they are accurate at hitting a range of recall targets\nwhile substantially reducing review costs.",
          "link": "http://arxiv.org/abs/2106.09871",
          "publishedOn": "2021-06-21T02:07:36.436Z",
          "wordCount": 549,
          "title": "Heuristic Stopping Rules For Technology-Assisted Review. (arXiv:2106.09871v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eugene Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1\">David D. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1\">Ophir Frieder</a>",
          "description": "Technology-assisted review (TAR) refers to human-in-the-loop machine learning\nworkflows for document review in legal discovery and other high recall review\ntasks. Attorneys and legal technologists have debated whether review should be\na single iterative process (one-phase TAR workflows) or whether model training\nand review should be separate (two-phase TAR workflows), with implications for\nthe choice of active learning algorithm. The relative cost of manual labeling\nfor different purposes (training vs. review) and of different documents\n(positive vs. negative examples) is a key and neglected factor in this debate.\nUsing a novel cost dynamics analysis, we show analytically and empirically that\nthese relative costs strongly impact whether a one-phase or two-phase workflow\nminimizes cost. We also show how category prevalence, classification task\ndifficulty, and collection size impact the optimal choice not only of workflow\ntype, but of active learning method and stopping point.",
          "link": "http://arxiv.org/abs/2106.09866",
          "publishedOn": "2021-06-21T02:07:36.386Z",
          "wordCount": 585,
          "title": "On Minimizing Cost in Legal Document Review Workflows. (arXiv:2106.09866v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1\">Pablo S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>",
          "description": "Point-of-Interest recommendation is an increasing research and developing\narea within the widely adopted technologies known as Recommender Systems. Among\nthem, those that exploit information coming from Location-Based Social Networks\n(LBSNs) are very popular nowadays and could work with different information\nsources, which pose several challenges and research questions to the community\nas a whole. We present a systematic review focused on the research done in the\nlast 10 years about this topic. We discuss and categorize the algorithms and\nevaluation methodologies used in these works and point out the opportunities\nand challenges that remain open in the field. More specifically, we report the\nleading recommendation techniques and information sources that have been\nexploited more often (such as the geographical signal and deep learning\napproaches) while we also alert about the lack of reproducibility in the field\nthat may hinder real performance improvements.",
          "link": "http://arxiv.org/abs/2106.10069",
          "publishedOn": "2021-06-21T02:07:36.339Z",
          "wordCount": 586,
          "title": "Point-of-Interest Recommender Systems: A Survey from an Experimental Perspective. (arXiv:2106.10069v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mustafizur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_D/0/1/0/all/0/1\">Dinesh Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_D/0/1/0/all/0/1\">Dhiraj Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1\">Mucahid Kutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>",
          "description": "Building a benchmark dataset for hate speech detection presents several\nchallenges. Firstly, because hate speech is relatively rare -- e.g., less than\n3\\% of Twitter posts are hateful \\citep{founta2018large} -- random sampling of\ntweets to annotate is inefficient in capturing hate speech. A common practice\nis to only annotate tweets containing known ``hate words'', but this risks\nyielding a biased benchmark that only partially captures the real-world\nphenomenon of interest. A second challenge is that definitions of hate speech\ntend to be highly variable and subjective. Annotators having diverse prior\nnotions of hate speech may not only disagree with one another but also struggle\nto conform to specified labeling guidelines. Our key insight is that the rarity\nand subjectivity of hate speech are akin to that of relevance in information\nretrieval (IR). This connection suggests that well-established methodologies\nfor creating IR test collections might also be usefully applied to create\nbetter benchmark datasets for hate speech detection. Firstly, to intelligently\nand efficiently select which tweets to annotate, we apply established IR\ntechniques of {\\em pooling} and {\\em active learning}. Secondly, to improve\nboth consistency and value of annotations, we apply {\\em task decomposition}\n\\cite{Zhang-sigir14} and {\\em annotator rationale} \\cite{mcdonnell16-hcomp}\ntechniques. Using the above techniques, we create and share a new benchmark\ndataset\\footnote{We will release the dataset upon publication.} for hate speech\ndetection with broader coverage than prior datasets. We also show a dramatic\ndrop in accuracy of existing detection models when tested on these broader\nforms of hate. Collected annotator rationales not only provide documented\nsupport for labeling decisions but also create exciting future work\nopportunities for dual-supervision and/or explanation generation in modeling.",
          "link": "http://arxiv.org/abs/2106.09775",
          "publishedOn": "2021-06-21T02:07:36.316Z",
          "wordCount": 726,
          "title": "An Information Retrieval Approach to Building Datasets for Hate Speech Detection. (arXiv:2106.09775v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1\">Qingtao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jia Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jun Lei</a>",
          "description": "Click-through rate (CTR) prediction plays an important role in online\nadvertising and recommender systems. In practice, the training of CTR models\ndepends on click data which is intrinsically biased towards higher positions\nsince higher position has higher CTR by nature. Existing methods such as actual\nposition training with fixed position inference and inverse propensity weighted\ntraining with no position inference alleviate the bias problem to some extend.\nHowever, the different treatment of position information between training and\ninference will inevitably lead to inconsistency and sub-optimal online\nperformance. Meanwhile, the basic assumption of these methods, i.e., the click\nprobability is the product of examination probability and relevance\nprobability, is oversimplified and insufficient to model the rich interaction\nbetween position and other information. In this paper, we propose a Deep\nPosition-wise Interaction Network (DPIN) to efficiently combine all candidate\nitems and positions for estimating CTR at each position, achieving consistency\nbetween offline and online as well as modeling the deep non-linear interaction\namong position, user, context and item under the limit of serving performance.\nFollowing our new treatment to the position bias in CTR prediction, we propose\na new evaluation metrics named PAUC (position-wise AUC) that is suitable for\nmeasuring the ranking quality at a given position. Through extensive\nexperiments on a real world dataset, we show empirically that our method is\nboth effective and efficient in solving position bias problem. We have also\ndeployed our method in production and observed statistically significant\nimprovement over a highly optimized baseline in a rigorous A/B test.",
          "link": "http://arxiv.org/abs/2106.05482",
          "publishedOn": "2021-06-18T02:06:33.753Z",
          "wordCount": 710,
          "title": "Deep Position-wise Interaction Network for CTR Prediction. (arXiv:2106.05482v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Juno Kim</a>",
          "description": "Social media creates crucial mass changes, as popular posts and opinions cast\na significant influence on users' decisions and thought processes. For example,\nthe recent Reddit uprising inspired by r/wallstreetbets which had remarkable\neconomic impact was started with a series of posts on the thread. The\nprediction of posts that may have a notable impact will allow for the\npreparation of possible following trends. This study aims to develop a machine\nlearning model capable of accurately predicting the popularity of a Reddit\npost. Specifically, the model is predicting the number of upvotes a post will\nreceive based on its textual content. I experimented with three different\nmodels: a baseline linear regression model, a random forest regression model,\nand a neural network. I collected Reddit post data from an online data set and\nanalyzed the model's performance when trained on a single subreddit and a\ncollection of subreddits. The results showed that the neural network model\nperformed the best when the loss of the models were compared. With the use of a\nmachine learning model to predict social trends through the reaction users have\nto post, a better picture of the near future can be envisioned.",
          "link": "http://arxiv.org/abs/2106.07380",
          "publishedOn": "2021-06-18T02:06:33.744Z",
          "wordCount": 645,
          "title": "Predicting the Popularity of Reddit Posts with AI. (arXiv:2106.07380v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wenige_L/0/1/0/all/0/1\">Lisa Wenige</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stadler_C/0/1/0/all/0/1\">Claus Stadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1\">Michael Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figura_R/0/1/0/all/0/1\">Richard Figura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauter_R/0/1/0/all/0/1\">Robert Sauter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_C/0/1/0/all/0/1\">Christopher W. Frank</a>",
          "description": "This paper presents a framework for assessing data and metadata quality\nwithin Open Data portals. Although a few benchmark frameworks already exist for\nthis purpose, they are not yet detailed enough in both breadth and depth to\nmake valid statements about the actual discoverability and accessibility of\npublicly available data collections. To address this research gap, we have\ndesigned a quality framework that is able to evaluate data quality in Open Data\nportals on dedicated and fine-grained dimensions, such as interoperability,\nfindability, uniqueness or completeness. Additionally, we propose quality\nmeasures that allow for valid assessments regarding cross-portal findability\nand uniqueness of dataset descriptions. We have validated our novel quality\nframework for the German Open Data landscape and found out that metadata often\nstill lacks meaningful descriptions and is not yet extensively connected to the\nSemantic Web.",
          "link": "http://arxiv.org/abs/2106.09590",
          "publishedOn": "2021-06-18T02:06:33.730Z",
          "wordCount": 605,
          "title": "Open Data and the Status Quo -- A Fine-Grained Evaluation Framework for Open Data Quality and an Analysis of Open Data portals in Germany. (arXiv:2106.09590v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haoyun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1\">Evgeny Kharlamov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Entity alignment, aiming to identify equivalent entities across different\nknowledge graphs (KGs), is a fundamental problem for constructing large-scale\nKGs. Over the course of its development, supervision has been considered\nnecessary for accurate alignments. Inspired by the recent progress of\nself-supervised learning, we explore the extent to which we can get rid of\nsupervision for entity alignment. Existing supervised methods for this task\nfocus on pulling each pair of positive (labeled) entities close to each other.\nHowever, our analysis suggests that the learning of entity alignment can\nactually benefit more from pushing sampled (unlabeled) negatives far away than\npulling positive aligned pairs close. We present SelfKG by leveraging this\ndiscovery to design a contrastive learning strategy across two KGs. Extensive\nexperiments on benchmark datasets demonstrate that SelfKG without supervision\ncan match or achieve comparable results with state-of-the-art supervised\nbaselines. The performance of SelfKG demonstrates self-supervised learning\noffers great potential for entity alignment in KGs.",
          "link": "http://arxiv.org/abs/2106.09395",
          "publishedOn": "2021-06-18T02:06:33.689Z",
          "wordCount": 595,
          "title": "A Self-supervised Method for Entity Alignment. (arXiv:2106.09395v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lingwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_X/0/1/0/all/0/1\">Xiaoyong Huai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhiqi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>",
          "description": "Session-based recommendation aims to predict user the next action based on\nhistorical behaviors in an anonymous session. For better recommendations, it is\nvital to capture user preferences as well as their dynamics. Besides, user\npreferences evolve over time dynamically and each preference has its own\nevolving track. However, most previous works neglect the evolving trend of\npreferences and can be easily disturbed by the effect of preference drifting.\nIn this paper, we propose a novel Preference Evolution Networks for\nsession-based Recommendation (PEN4Rec) to model preference evolving process by\na two-stage retrieval from historical contexts. Specifically, the first-stage\nprocess integrates relevant behaviors according to recent items. Then, the\nsecond-stage process models the preference evolving trajectory over time\ndynamically and infer rich preferences. The process can strengthen the effect\nof relevant sequential behaviors during the preference evolution and weaken the\ndisturbance from preference drifting. Extensive experiments on three public\ndatasets demonstrate the effectiveness and superiority of the proposed model.",
          "link": "http://arxiv.org/abs/2106.09306",
          "publishedOn": "2021-06-18T02:06:33.617Z",
          "wordCount": 600,
          "title": "PEN4Rec: Preference Evolution Networks for Session-based Recommendation. (arXiv:2106.09306v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tierney_G/0/1/0/all/0/1\">Graham Tierney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bail_C/0/1/0/all/0/1\">Christopher Bail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volfovsky_A/0/1/0/all/0/1\">Alexander Volfovsky</a>",
          "description": "Analysis of short text, such as social media posts, is extremely difficult\nbecause it relies on observing many document-level word co-occurrence pairs.\nBeyond topic distributions, a common downstream task of the modeling is\ngrouping the authors of these documents for subsequent analyses. Traditional\nmodels estimate the document groupings and identify user clusters with an\nindependent procedure. We propose a novel model that expands on the Latent\nDirichlet Allocation by modeling strong dependence among the words in the same\ndocument, with user-level topic distributions. We also simultaneously cluster\nusers, removing the need for post-hoc cluster estimation and improving topic\nestimation by shrinking noisy user-level topic distributions towards typical\nvalues. Our method performs as well as -- or better -- than traditional\napproaches to problems arising in short text, and we demonstrate its usefulness\non a dataset of tweets from United States Senators, recovering both meaningful\ntopics and clusters that reflect partisan ideology.",
          "link": "http://arxiv.org/abs/2106.09533",
          "publishedOn": "2021-06-18T02:06:33.599Z",
          "wordCount": 590,
          "title": "Author Clustering and Topic Estimation for Short Texts. (arXiv:2106.09533v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lv_F/0/1/0/all/0/1\">Fuyu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengxue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tonglei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1\">Taiwei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">Hong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_W/0/1/0/all/0/1\">Wilfred Ng</a>",
          "description": "Deep learning-based sequential recommender systems have recently attracted\nincreasing attention from both academia and industry. Most of industrial\nEmbedding-Based Retrieval (EBR) system for recommendation share the similar\nideas with sequential recommenders. Among them, how to comprehensively capture\nsequential user interest is a fundamental problem. However, most existing\nsequential recommendation models take as input clicked or purchased behavior\nsequences from user-item interactions. This leads to incomprehensive user\nrepresentation and sub-optimal model performance, since they ignore the\ncomplete user behavior exposure data, i.e., items impressed yet unclicked by\nusers. In this work, we attempt to incorporate and model those unclicked item\nsequences using a new learning approach in order to explore better sequential\nrecommendation technique. An efficient triplet metric learning algorithm is\nproposed to appropriately learn the representation of unclicked items. Our\nmethod can be simply integrated with existing sequential recommendation models\nby a confidence fusion network and further gain better user representation. The\noffline experimental results based on real-world E-commerce data demonstrate\nthe effectiveness and verify the importance of unclicked items in sequential\nrecommendation. Moreover we deploy our new model (named XDM) into EBR of\nrecommender system at Taobao, outperforming the deployed previous generation\nSDM.",
          "link": "http://arxiv.org/abs/2010.12837",
          "publishedOn": "2021-06-18T02:06:33.583Z",
          "wordCount": 686,
          "title": "XDM: Improving Sequential Deep Matching with Unclicked User Behaviors for Recommender System. (arXiv:2010.12837v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">Rosie Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Ching-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Sravana Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_A/0/1/0/all/0/1\">Ann Clifton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlgren_J/0/1/0/all/0/1\">Jussi Karlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_H/0/1/0/all/0/1\">Helia Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappu_A/0/1/0/all/0/1\">Aasish Pappu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazari_Z/0/1/0/all/0/1\">Zahra Nazari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semerci_O/0/1/0/all/0/1\">Oguz Semerci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchard_H/0/1/0/all/0/1\">Hugues Bouchard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carterette_B/0/1/0/all/0/1\">Ben Carterette</a>",
          "description": "Podcasts are spoken documents across a wide-range of genres and styles, with\ngrowing listenership across the world, and a rapidly lowering barrier to entry\nfor both listeners and creators. The great strides in search and recommendation\nin research and industry have yet to see impact in the podcast space, where\nrecommendations are still largely driven by word of mouth. In this perspective\npaper, we highlight the many differences between podcasts and other media, and\ndiscuss our perspective on challenges and future research directions in the\ndomain of podcast information access.",
          "link": "http://arxiv.org/abs/2106.09227",
          "publishedOn": "2021-06-18T02:06:33.569Z",
          "wordCount": 545,
          "title": "Current Challenges and Future Directions in Podcast Information Access. (arXiv:2106.09227v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_F/0/1/0/all/0/1\">Fuyu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1\">Taiwei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>",
          "description": "Nowadays, the product search service of e-commerce platforms has become a\nvital shopping channel in people's life. The retrieval phase of products\ndetermines the search system's quality and gradually attracts researchers'\nattention. Retrieving the most relevant products from a large-scale corpus\nwhile preserving personalized user characteristics remains an open question.\nRecent approaches in this domain have mainly focused on embedding-based\nretrieval (EBR) systems. However, after a long period of practice on Taobao, we\nfind that the performance of the EBR system is dramatically degraded due to\nits: (1) low relevance with a given query and (2) discrepancy between the\ntraining and inference phases. Therefore, we propose a novel and practical\nembedding-based product retrieval model, named Multi-Grained Deep Semantic\nProduct Retrieval (MGDSPR). Specifically, we first identify the inconsistency\nbetween the training and inference stages, and then use the softmax\ncross-entropy loss as the training objective, which achieves better performance\nand faster convergence. Two efficient methods are further proposed to improve\nretrieval relevance, including smoothing noisy training data and generating\nrelevance-improving hard negative samples without requiring extra knowledge and\ntraining procedures. We evaluate MGDSPR on Taobao Product Search with\nsignificant metrics gains observed in offline experiments and online A/B tests.\nMGDSPR has been successfully deployed to the existing multi-channel retrieval\nsystem in Taobao Search. We also introduce the online deployment scheme and\nshare practical lessons of our retrieval system to contribute to the community.",
          "link": "http://arxiv.org/abs/2106.09297",
          "publishedOn": "2021-06-18T02:06:33.557Z",
          "wordCount": 672,
          "title": "Embedding-based Product Retrieval in Taobao Search. (arXiv:2106.09297v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ardah_K/0/1/0/all/0/1\">Khaled Ardah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haardt_M/0/1/0/all/0/1\">Martin Haardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matter_F/0/1/0/all/0/1\">Frederic Matter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesavento_M/0/1/0/all/0/1\">Marius Pesavento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfetsch_M/0/1/0/all/0/1\">Marc E. Pfetsch</a>",
          "description": "This paper addresses sparse signal reconstruction under various types of\nstructural side constraints with applications in multi-antenna systems. Side\nconstraints may result from prior information on the measurement system and the\nsparse signal structure. They may involve the structure of the sensing matrix,\nthe structure of the non-zero support values, the temporal structure of the\nsparse representationvector, and the nonlinear measurement structure. First, we\ndemonstrate how a priori information in form of structural side constraints\ninfluence recovery guarantees (null space properties) using L1-minimization.\nFurthermore, for constant modulus signals, signals with row-, block- and\nrank-sparsity, as well as non-circular signals, we illustrate how structural\nprior information can be used to devise efficient algorithms with improved\nrecovery performance and reduced computational complexity. Finally, we address\nthe measurement system design for linear and nonlinear measurements of sparse\nsignals. Moreover, we discuss the linear mixing matrix design based on\ncoherence minimization. Then we extend our focus to nonlinear measurement\nsystems where we design parallel optimization algorithms to efficiently compute\nstationary points in the sparse phase retrieval problem with and without\ndictionary learning.",
          "link": "http://arxiv.org/abs/2106.09375",
          "publishedOn": "2021-06-18T02:06:33.545Z",
          "wordCount": 610,
          "title": "Recovery under Side Constraints. (arXiv:2106.09375v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hansi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>",
          "description": "Modern E-commerce websites contain heterogeneous sources of information, such\nas numerical ratings, textual reviews and images. These information can be\nutilized to assist recommendation. Through textual reviews, a user explicitly\nexpress her affinity towards the item. Previous researchers found that by using\nthe information extracted from these reviews, we can better profile the users'\nexplicit preferences as well as the item features, leading to the improvement\nof recommendation performance. However, most of the previous algorithms were\nonly utilizing the review information for explicit-feedback problem i.e. rating\nprediction, and when it comes to implicit-feedback ranking problem such as\ntop-N recommendation, the usage of review information has not been fully\nexplored. Seeing this gap, in this work, we investigate the effectiveness of\ntextual review information for top-N recommendation under E-commerce settings.\nWe adapt several SOTA review-based rating prediction models for top-N\nrecommendation tasks and compare them to existing top-N recommendation models\nfrom both performance and efficiency. We find that models utilizing only review\ninformation can not achieve better performances than vanilla implicit-feedback\nmatrix factorization method. When utilizing review information as a regularizer\nor auxiliary information, the performance of implicit-feedback matrix\nfactorization method can be further improved. However, the optimal model\nstructure to utilize textual reviews for E-commerce top-N recommendation is yet\nto be determined.",
          "link": "http://arxiv.org/abs/2106.09665",
          "publishedOn": "2021-06-18T02:06:33.522Z",
          "wordCount": 648,
          "title": "Understanding the Effectiveness of Reviews in E-commerce Top-N Recommendation. (arXiv:2106.09665v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruoming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>",
          "description": "Recently, linear regression models, such as EASE and SLIM, have shown to\noften produce rather competitive results against more sophisticated deep\nlearning models. On the other side, the (weighted) matrix factorization\napproaches have been popular choices for recommendation in the past and widely\nadopted in the industry. In this work, we aim to theoretically understand the\nrelationship between these two approaches, which are the cornerstones of\nmodel-based recommendations. Through the derivation and analysis of the\nclosed-form solutions for two basic regression and matrix factorization\napproaches, we found these two approaches are indeed inherently related but\nalso diverge in how they \"scale-down\" the singular values of the original\nuser-item interaction matrix. This analysis also helps resolve the questions\nrelated to the regularization parameter range and model complexities. We\nfurther introduce a new learning algorithm in searching (hyper)parameters for\nthe closed-form solution and utilize it to discover the nearby models of the\nexisting solutions. The experimental results demonstrate that the basic models\nand their closed-form solutions are indeed quite competitive against the\nstate-of-the-art models, thus, confirming the validity of studying the basic\nmodels. The effectiveness of exploring the nearby models are also\nexperimentally validated.",
          "link": "http://arxiv.org/abs/2105.12937",
          "publishedOn": "2021-06-17T01:58:40.720Z",
          "wordCount": 663,
          "title": "Towards a Better Understanding of Linear Models for Recommendation. (arXiv:2105.12937v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1\">Damir Koren&#x10d;i&#x107;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ristov_S/0/1/0/all/0/1\">Strahil Ristov</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Repar_J/0/1/0/all/0/1\">Jelena Repar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a> (2) ((1) Rudjer Bo&#x161;kovi&#x107; Institute, Croatia, (2) University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)",
          "description": "Topic models are widely used unsupervised models of text capable of learning\ntopics - weighted lists of words and documents - from large collections of text\ndocuments. When topic models are used for discovery of topics in text\ncollections, a question that arises naturally is how well the model-induced\ntopics correspond to topics of interest to the analyst. In this paper we\nrevisit and extend a so far neglected approach to topic model evaluation based\non measuring topic coverage - computationally matching model topics with a set\nof reference topics that models are expected to uncover. The approach is well\nsuited for analyzing models' performance in topic discovery and for large-scale\nanalysis of both topic models and measures of model quality. We propose new\nmeasures of coverage and evaluate, in a series of experiments, different types\nof topic models on two distinct text domains for which interest for topic\ndiscovery exists. The experiments include evaluation of model quality, analysis\nof coverage of distinct topic categories, and the analysis of the relationship\nbetween coverage and other methods of topic model evaluation. The contributions\nof the paper include new measures of coverage, insights into both topic models\nand other methods of model evaluation, and the datasets and code for\nfacilitating future research of both topic coverage and other approaches to\ntopic model evaluation.",
          "link": "http://arxiv.org/abs/2012.06274",
          "publishedOn": "2021-06-17T01:58:40.701Z",
          "wordCount": 741,
          "title": "A Topic Coverage Approach to Evaluation of Topic Models. (arXiv:2012.06274v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mryglod_O/0/1/0/all/0/1\">O. Mryglod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazarovets_S/0/1/0/all/0/1\">S. Nazarovets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozmenko_S/0/1/0/all/0/1\">S. Kozmenko</a>",
          "description": "Our study is one of the first examples of multidimensional and longitudinal\ndisciplinary analysis at the national level based on Crossref data. We present\na large-scale quantitative analysis of Ukrainian economics. This study is not\nyet another example of research aimed at ranking of local journals, authors or\ninstitutions, but rather exploring general tendencies that can be compared to\nother countries or regions. We study different aspects of Ukrainian economics\noutput. In particular, the collaborative nature, geographic landscape and some\npeculiarities of citation statistics are investigated. We have found that\nUkrainian economics is characterized by a comparably small share of co-authored\npublications, however, it demonstrates the tendency towards more collaborative\noutput. Based on our analysis, we discuss specific and universal features of\nUkrainian economic research. The importance of supporting various initiatives\naimed at enriching open scholarly metadata is considered. A comprehensive and\nhigh-quality meta description of publications is probably the shortest path to\na better understanding of national trends, especially for non-English speaking\ncountries. The results of our analysis can be used to better understand\nUkrainian economic research and support research policy decisions.",
          "link": "http://arxiv.org/abs/2106.08701",
          "publishedOn": "2021-06-17T01:58:40.608Z",
          "wordCount": 647,
          "title": "Universal and specific features of Ukrainian economic research: publication analysis based on Crossref data. (arXiv:2106.08701v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dusart_A/0/1/0/all/0/1\">Alexis Dusart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinel_Sauvagnat_K/0/1/0/all/0/1\">Karen Pinel-Sauvagnat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubert_G/0/1/0/all/0/1\">Gilles Hubert</a>",
          "description": "The development of deep neural networks and the emergence of pre-trained\nlanguage models such as BERT allow to increase performance on many NLP tasks.\nHowever, these models do not meet the same popularity for tweet summarization,\nwhich can probably be explained by the lack of existing collections for\ntraining and evaluation. Our contribution in this paper is twofold : (1) we\nintroduce a large dataset for Twitter event summarization, and (2) we propose a\nneural model to automatically summarize huge tweet streams. This extractive\nmodel combines in an original way pre-trained language models and vocabulary\nfrequency-based representations to predict tweet salience. An additional\nadvantage of the model is that it automatically adapts the size of the output\nsummary according to the input tweet stream. We conducted experiments using two\ndifferent Twitter collections, and promising results are observed in comparison\nwith state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2106.08770",
          "publishedOn": "2021-06-17T01:58:40.264Z",
          "wordCount": 563,
          "title": "TSSuBERT: Tweet Stream Summarization Using BERT. (arXiv:2106.08770v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>",
          "description": "Personalized news recommendation is an important technique to help users find\ntheir interested news information and alleviate their information overload. It\nhas been extensively studied over decades and has achieved notable success in\nimproving users' news reading experience. However, there are still many\nunsolved problems and challenges that need to be further studied. To help\nresearchers master the advances in personalized news recommendation over the\npast years, in this paper we present a comprehensive overview of personalized\nnews recommendation. Instead of following the conventional taxonomy of news\nrecommendation methods, in this paper we propose a novel perspective to\nunderstand personalized news recommendation based on its core problems and the\nassociated techniques and challenges. We first review the techniques for\ntackling each core problem in a personalized news recommender system and the\nchallenges they face. Next, we introduce the public datasets and evaluation\nmetrics used for personalized news recommendation. We then discuss the key\npoints on improving the responsibility of personalized news recommender\nsystems. Finally, we raise several research directions that are worth\ninvestigating in future. This paper can provide up-to-date and comprehensive\nviews to help readers understand the personalized news recommendation field. We\nhope this paper can facilitate research on personalized news recommendation and\nas well as related fields in natural language processing and data mining.",
          "link": "http://arxiv.org/abs/2106.08934",
          "publishedOn": "2021-06-17T01:58:40.220Z",
          "wordCount": 631,
          "title": "Personalized News Recommendation: A Survey. (arXiv:2106.08934v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhiyi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Tables are widely used with various structures to organize and present data.\nRecent attempts on table understanding mainly focus on relational tables, yet\noverlook to other common table structures. In this paper, we propose TUTA, a\nunified pre-training architecture for understanding generally structured\ntables. Noticing that understanding a table requires spatial, hierarchical, and\nsemantic information, we enhance transformers with three novel structure-aware\nmechanisms. First, we devise a unified tree-based structure, called a\nbi-dimensional coordinate tree, to describe both the spatial and hierarchical\ninformation of generally structured tables. Upon this, we propose tree-based\nattention and position embedding to better capture the spatial and hierarchical\ninformation. Moreover, we devise three progressive pre-training objectives to\nenable representations at the token, cell, and table levels. We pre-train TUTA\non a wide range of unlabeled web and spreadsheet tables and fine-tune it on two\ncritical tasks in the field of table structure understanding: cell type\nclassification and table type classification. Experiments show that TUTA is\nhighly effective, achieving state-of-the-art on five widely-studied datasets.",
          "link": "http://arxiv.org/abs/2010.12537",
          "publishedOn": "2021-06-17T01:58:40.200Z",
          "wordCount": 645,
          "title": "TUTA: Tree-based Transformers for Generally Structured Table Pre-training. (arXiv:2010.12537v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">SeongKu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Junyoung Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_W/0/1/0/all/0/1\">Wonbin Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hwanjo Yu</a>",
          "description": "Recommender Systems (RS) have employed knowledge distillation which is a\nmodel compression technique training a compact student model with the knowledge\ntransferred from a pre-trained large teacher model. Recent work has shown that\ntransferring knowledge from the teacher's intermediate layer significantly\nimproves the recommendation quality of the student. However, they transfer the\nknowledge of individual representation point-wise and thus have a limitation in\nthat primary information of RS lies in the relations in the representation\nspace. This paper proposes a new topology distillation approach that guides the\nstudent by transferring the topological structure built upon the relations in\nthe teacher space. We first observe that simply making the student learn the\nwhole topological structure is not always effective and even degrades the\nstudent's performance. We demonstrate that because the capacity of the student\nis highly limited compared to that of the teacher, learning the whole\ntopological structure is daunting for the student. To address this issue, we\npropose a novel method named Hierarchical Topology Distillation (HTD) which\ndistills the topology hierarchically to cope with the large capacity gap. Our\nextensive experiments on real-world datasets show that the proposed method\nsignificantly outperforms the state-of-the-art competitors. We also provide\nin-depth analyses to ascertain the benefit of distilling the topology for RS.",
          "link": "http://arxiv.org/abs/2106.08700",
          "publishedOn": "2021-06-17T01:58:40.114Z",
          "wordCount": 642,
          "title": "Topology Distillation for Recommender System. (arXiv:2106.08700v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruoyuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1\">Chirag Shah</a>",
          "description": "With the emerging needs of creating fairness-aware solutions for search and\nrecommendation systems, a daunting challenge exists of evaluating such\nsolutions. While many of the traditional information retrieval (IR) metrics can\ncapture the relevance, diversity and novelty for the utility with respect to\nusers, they are not suitable for inferring whether the presented results are\nfair from the perspective of responsible information exposure. On the other\nhand, various fairness metrics have been proposed but they do not account for\nthe user utility or do not measure it adequately. To address this problem, we\npropose a new metric called Fairness-Aware IR (FAIR). By unifying standard IR\nmetrics and fairness measures into an integrated metric, this metric offers a\nnew perspective for evaluating fairness-aware ranking results. Based on this\nmetric, we developed an effective ranking algorithm that jointly optimized user\nutility and fairness. The experimental results showed that our FAIR metric\ncould highlight results with good user utility and fair information exposure.\nWe showed how FAIR related to existing metrics and demonstrated the\neffectiveness of our FAIR-based algorithm. We believe our work opens up a new\ndirection of pursuing a computationally feasible metric for evaluating and\nimplementing the fairness-aware IR systems.",
          "link": "http://arxiv.org/abs/2106.08527",
          "publishedOn": "2021-06-17T01:58:40.089Z",
          "wordCount": 628,
          "title": "FAIR: Fairness-Aware Information Retrieval Evaluation. (arXiv:2106.08527v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sidiropoulos_G/0/1/0/all/0/1\">Georgios Sidiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1\">Nikos Voskarides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1\">Svitlana Vakulenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>",
          "description": "We analyse the performance of passage retrieval models in the presence of\ncomplex (multi-hop) questions to provide a better understanding of how\nretrieval systems behave when multiple hops of reasoning are needed. In simple\nopen-domain question answering (QA), dense passage retrieval has become one of\nthe standard approaches for retrieving the relevant passages to infer an\nanswer. Recently, dense passage retrieval also achieved state-of-the-art\nresults in multi-hop QA, where aggregating information from multiple documents\nand reasoning over them is required. However, so far, the dense retrieval\nmodels are not evaluated properly concerning the multi-hop nature of the\nproblem: models are typically evaluated by the end result of the retrieval\npipeline, which leaves unclear where their success lies. In this work, we\nprovide an in-depth evaluation of such models not only unveiling the reasons\nbehind their success but also their limitations. Moreover, we introduce a\nhybrid (lexical and dense) retrieval approach that is highly competitive with\nthe state-of-the-art dense retrieval model, while requiring substantially less\ncomputational resources. Furthermore, we also perform qualitative analysis to\nbetter understand the challenges behind passage retrieval for multi-hop QA.",
          "link": "http://arxiv.org/abs/2106.08433",
          "publishedOn": "2021-06-17T01:58:40.067Z",
          "wordCount": 607,
          "title": "Analysing Dense Passage Retrieval for Multi-hop Question Answering. (arXiv:2106.08433v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pappas_D/0/1/0/all/0/1\">Dimitris Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>",
          "description": "Question answering (QA) systems for large document collections typically use\npipelines that (i) retrieve possibly relevant documents, (ii) re-rank them,\n(iii) rank paragraphs or other snippets of the top-ranked documents, and (iv)\nselect spans of the top-ranked snippets as exact answers. Pipelines are\nconceptually simple, but errors propagate from one component to the next,\nwithout later components being able to revise earlier decisions. We present an\narchitecture for joint document and snippet ranking, the two middle stages,\nwhich leverages the intuition that relevant documents have good snippets and\ngood snippets come from relevant documents. The architecture is general and can\nbe used with any neural text relevance ranker. We experiment with two main\ninstantiations of the architecture, based on POSIT-DRMM (PDRMM) and a\nBERT-based ranker. Experiments on biomedical data from BIOASQ show that our\njoint models vastly outperform the pipelines in snippet retrieval, the main\ngoal for QA, with fewer trainable parameters, also remaining competitive in\ndocument retrieval. Furthermore, our joint PDRMM-based model is competitive\nwith BERT-based models, despite using orders of magnitude fewer parameters.\nThese claims are also supported by human evaluation on two test batches of\nBIOASQ. To test our key findings on another dataset, we modified the Natural\nQuestions dataset so that it can also be used for document and snippet\nretrieval. Our joint PDRMM-based model again outperforms the corresponding\npipeline in snippet retrieval on the modified Natural Questions dataset, even\nthough it performs worse than the pipeline in document retrieval. We make our\ncode and the modified Natural Questions dataset publicly available.",
          "link": "http://arxiv.org/abs/2106.08908",
          "publishedOn": "2021-06-17T01:58:40.029Z",
          "wordCount": 713,
          "title": "A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections. (arXiv:2106.08908v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pande_H/0/1/0/all/0/1\">Harshit Pande</a>",
          "description": "Click-through rate (CTR) prediction models are common in many online\napplications such as digital advertising and recommender systems. Field-Aware\nFactorization Machine (FFM) and Field-weighted Factorization Machine (FwFM) are\nstate-of-the-art among the shallow models for CTR prediction. Recently, many\ndeep learning-based models have also been proposed. Among deeper models,\nDeepFM, xDeepFM, AutoInt+, and FiBiNet are state-of-the-art models. The deeper\nmodels combine a core architectural component, which learns explicit feature\ninteractions, with a deep neural network (DNN) component. We propose a novel\nshallow Field-Embedded Factorization Machine (FEFM) and its deep counterpart\nDeep Field-Embedded Factorization Machine (DeepFEFM). FEFM learns symmetric\nmatrix embeddings for each field pair along with the usual single vector\nembeddings for each feature. FEFM has significantly lower model complexity than\nFFM and roughly the same complexity as FwFM. FEFM also has insightful\nmathematical properties about important fields and field interactions. DeepFEFM\ncombines the FEFM interaction vectors learned by the FEFM component with a DNN\nand is thus able to learn higher order interactions. We conducted comprehensive\nexperiments over a wide range of hyperparameters on two large publicly\navailable real-world datasets. When comparing test AUC and log loss, the\nresults show that FEFM and DeepFEFM outperform the existing state-of-the-art\nshallow and deep models for CTR prediction tasks. We have made the code of FEFM\nand DeepFEFM available in the DeepCTR library\n(https://github.com/shenweichen/DeepCTR).",
          "link": "http://arxiv.org/abs/2009.09931",
          "publishedOn": "2021-06-16T01:21:04.771Z",
          "wordCount": 675,
          "title": "Field-Embedded Factorization Machines for Click-through rate prediction. (arXiv:2009.09931v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.",
          "link": "http://arxiv.org/abs/2010.11066",
          "publishedOn": "2021-06-16T01:21:04.362Z",
          "wordCount": 616,
          "title": "Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ebadi_N/0/1/0/all/0/1\">Nima Ebadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1\">Peyman Najafirad</a>",
          "description": "The rapidly evolving literature of COVID-19 related articles makes it\nchallenging for NLP models to be effectively trained for information retrieval\nand extraction with the corresponding labeled data that follows the current\ndistribution of the pandemic. On the other hand, due to the uncertainty of the\nsituation, human experts' supervision would always be required to double check\nthe decision making of these models highlighting the importance of\ninterpretability. In the light of these challenges, this study proposes an\ninterpretable self-supervised multi-task learning model to jointly and\neffectively tackle the tasks of information retrieval (IR) and extraction (IE)\nduring the current emergency health crisis situation. Our results show that our\nmodel effectively leverage the multi-task and self-supervised learning to\nimprove generalization, data efficiency and robustness to the ongoing dataset\nshift problem. Our model outperforms baselines in IE and IR tasks, respectively\nby micro-f score of 0.08 (LCA-F score of 0.05), and MAP of 0.05 on average. In\nIE the zero- and few-shot learning performances are on average 0.32 and 0.19\nmicro-f score higher than those of the baselines.",
          "link": "http://arxiv.org/abs/2106.08252",
          "publishedOn": "2021-06-16T01:21:04.341Z",
          "wordCount": 650,
          "title": "Interpretable Self-supervised Multi-task Learning for COVID-19 Information Retrieval and Extraction. (arXiv:2106.08252v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1\">Ramon Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>",
          "description": "Speech-based image retrieval has been studied as a proxy for joint\nrepresentation learning, usually without emphasis on retrieval itself. As such,\nit is unclear how well speech-based retrieval can work in practice -- both in\nan absolute sense and versus alternative strategies that combine automatic\nspeech recognition (ASR) with strong text encoders. In this work, we\nextensively study and expand choices of encoder architectures, training\nmethodology (including unimodal and multimodal pretraining), and other factors.\nOur experiments cover different types of speech in three datasets: Flickr\nAudio, Places Audio, and Localized Narratives. Our best model configuration\nachieves large gains over state of the art, e.g., pushing recall-at-one from\n21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also\nshow our best speech-based models can match or exceed cascaded ASR-to-text\nencoding when speech is spontaneous, accented, or otherwise hard to\nautomatically transcribe.",
          "link": "http://arxiv.org/abs/2104.01894",
          "publishedOn": "2021-06-16T01:21:04.317Z",
          "wordCount": 631,
          "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milton_A/0/1/0/all/0/1\">Ashlee Milton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_G/0/1/0/all/0/1\">Garrett Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pera_M/0/1/0/all/0/1\">Maria Soledad Pera</a>",
          "description": "Research in the area of search engines for children remains in its infancy.\nSeminal works have studied how children use mainstream search engines, as well\nas how to design and evaluate custom search engines explicitly for children.\nThese works, however, tend to take a one-size-fits-all view, treating children\nas a unit. Nevertheless, even at the same age, children are known to possess\nand exhibit different capabilities. These differences affect how children\naccess and use search engines. To better serve children, in this vision paper,\nwe spotlight accessibility and discuss why current research on children and\nsearch engines does not, but should, focus on this significant matter.",
          "link": "http://arxiv.org/abs/2106.07813",
          "publishedOn": "2021-06-16T01:21:04.306Z",
          "wordCount": 567,
          "title": "To Infinity and Beyond! Accessibility is the Future for Kids' Search Engines. (arXiv:2106.07813v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alivanistos_D/0/1/0/all/0/1\">Dimitrios Alivanistos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1\">Max Berrendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1\">Michael Cochez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1\">Mikhail Galkin</a>",
          "description": "Multi-hop logical reasoning is an established problem in the field of\nrepresentation learning on knowledge graphs (KGs). It subsumes both one-hop\nlink prediction as well as other more complex types of logical queries.\nExisting algorithms operate only on classical, triple-based graphs, whereas\nmodern KGs often employ a hyper-relational modeling paradigm. In this paradigm,\ntyped edges may have several key-value pairs known as qualifiers that provide\nfine-grained context for facts. In queries, this context modifies the meaning\nof relations, and usually reduces the answer set. Hyper-relational queries are\noften observed in real-world KG applications, and existing approaches for\napproximate query answering cannot make use of qualifier pairs. In this work,\nwe bridge this gap and extend the multi-hop reasoning problem to\nhyper-relational KGs allowing to tackle this new type of complex queries.\nBuilding upon recent advancements in Graph Neural Networks and query embedding\ntechniques, we study how to embed and answer hyper-relational conjunctive\nqueries. Besides that, we propose a method to answer such queries and\ndemonstrate in our experiments that qualifiers improve query answering on a\ndiverse set of query patterns.",
          "link": "http://arxiv.org/abs/2106.08166",
          "publishedOn": "2021-06-16T01:21:04.296Z",
          "wordCount": 611,
          "title": "Query Embedding on Hyper-relational Knowledge Graphs. (arXiv:2106.08166v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shuo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lining Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yifan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoqiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1\">Kun Gai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kuang-chih Lee</a>",
          "description": "Modern online advertising systems inevitably rely on personalization methods,\nsuch as click-through rate (CTR) prediction. Recent progress in CTR prediction\nenjoys the rich representation capabilities of deep learning and achieves great\nsuccess in large-scale industrial applications. However, these methods can\nsuffer from lack of exploration. Another line of prior work addresses the\nexploration-exploitation trade-off problem with contextual bandit methods,\nwhich are recently less studied in the industry due to the difficulty in\nextending their flexibility with deep models. In this paper, we propose a novel\nDeep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on\nGaussian processes, which can provide predictive uncertainty estimations while\nmaintaining the flexibility of deep neural networks. DUAL can be easily\nimplemented on existing models and deployed in real-time systems with minimal\nextra computational overhead. By linking the predictive uncertainty estimation\nability of DUAL to well-known bandit algorithms, we further present DUAL-based\nAd-ranking strategies to boost up long-term utilities such as the social\nwelfare in advertising systems. Experimental results on several public datasets\ndemonstrate the effectiveness of our methods. Remarkably, an online A/B test\ndeployed in the Alibaba display advertising platform shows an 8.2% social\nwelfare improvement and an 8.0% revenue lift.",
          "link": "http://arxiv.org/abs/2012.02298",
          "publishedOn": "2021-06-16T01:21:04.267Z",
          "wordCount": 675,
          "title": "Exploration in Online Advertising Systems with Deep Uncertainty-Aware Learning. (arXiv:2012.02298v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tseytlin_B/0/1/0/all/0/1\">Boris Tseytlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1\">Ilya Makarov</a>",
          "description": "We approach the problem of hotel recognition with deep metric learning. We\noverview the existing approaches and propose a modification to Contrastive loss\ncalled Contrastive-Triplet loss. We construct a robust pipeline for\nbenchmarking metric learning models and perform experiments on Hotels-50K and\nCUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval\non Hotels-50k. We open-source our code.",
          "link": "http://arxiv.org/abs/2106.08042",
          "publishedOn": "2021-06-16T01:21:03.979Z",
          "wordCount": 491,
          "title": "Hotel Recognition via Latent Image Embedding. (arXiv:2106.08042v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Volske_M/0/1/0/all/0/1\">Michael V&#xf6;lske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondarenko_A/0/1/0/all/0/1\">Alexander Bondarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1\">Maik Fr&#xf6;be</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1\">Matthias Hagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>",
          "description": "Recently, neural networks have been successfully employed to improve upon\nstate-of-the-art performance in ad-hoc retrieval tasks via machine-learned\nranking functions. While neural retrieval models grow in complexity and impact,\nlittle is understood about their correspondence with well-studied IR\nprinciples. Recent work on interpretability in machine learning has provided\ntools and techniques to understand neural models in general, yet there has been\nlittle progress towards explaining ranking models.\n\nWe investigate whether one can explain the behavior of neural ranking models\nin terms of their congruence with well understood principles of document\nranking by using established theories from axiomatic IR. Axiomatic analysis of\ninformation retrieval models has formalized a set of constraints on ranking\ndecisions that reasonable retrieval models should fulfill. We operationalize\nthis axiomatic thinking to reproduce rankings based on combinations of\nelementary constraints. This allows us to investigate to what extent the\nranking decisions of neural rankers can be explained in terms of retrieval\naxioms, and which axioms apply in which situations. Our experimental study\nconsiders a comprehensive set of axioms over several representative neural\nrankers. While the existing axioms can already explain the particularly\nconfident ranking decisions rather well, future work should extend the axiom\nset to also cover the other still \"unexplainable\" neural IR rank decisions.",
          "link": "http://arxiv.org/abs/2106.08019",
          "publishedOn": "2021-06-16T01:21:03.957Z",
          "wordCount": 649,
          "title": "Towards Axiomatic Explanations for Neural Ranking Models. (arXiv:2106.08019v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fajie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaxi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>",
          "description": "Making accurate recommendations for cold-start users has been a longstanding\nand critical challenge for recommender systems (RS). Cross-domain\nrecommendations (CDR) offer a solution to tackle such a cold-start problem when\nthere is no sufficient data for the users who have rarely used the system. An\neffective approach in CDR is to leverage the knowledge (e.g., user\nrepresentations) learned from a related but different domain and transfer it to\nthe target domain. Fine-tuning works as an effective transfer learning\ntechnique for this objective, which adapts the parameters of a pre-trained\nmodel from the source domain to the target domain. However, current methods are\nmainly based on the global fine-tuning strategy: the decision of which layers\nof the pre-trained model to freeze or fine-tune is taken for all users in the\ntarget domain. In this paper, we argue that users in RS are personalized and\nshould have their own fine-tuning policies for better preference transfer\nlearning. As such, we propose a novel User-specific Adaptive Fine-tuning method\n(UAF), selecting which layers of the pre-trained network to fine-tune, on a\nper-user basis. Specifically, we devise a policy network with three alternative\nstrategies to automatically decide which layers to be fine-tuned and which\nlayers to have their parameters frozen for each user. Extensive experiments\nshow that the proposed UAF exhibits significantly better and more robust\nperformance for user cold-start recommendation.",
          "link": "http://arxiv.org/abs/2106.07864",
          "publishedOn": "2021-06-16T01:21:03.932Z",
          "wordCount": 648,
          "title": "User-specific Adaptive Fine-tuning for Cross-domain Recommendations. (arXiv:2106.07864v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peito_J/0/1/0/all/0/1\">Joel Peito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qiwei Han</a>",
          "description": "In contrast to many other domains, recommender systems in health services may\nbenefit particularly from the incorporation of health domain knowledge, as it\nhelps to provide meaningful and personalised recommendations catering to the\nindividual's health needs. With recent advances in representation learning\nenabling the hierarchical embedding of health knowledge into the hyperbolic\nPoincare space, this work proposes a content-based recommender system for\npatient-doctor matchmaking in primary care based on patients' health profiles,\nenriched by pre-trained Poincare embeddings of the ICD-9 codes through transfer\nlearning. The proposed model outperforms its conventional counterpart in terms\nof recommendation accuracy and has several important business implications for\nimproving the patient-doctor relationship.",
          "link": "http://arxiv.org/abs/2106.07720",
          "publishedOn": "2021-06-16T01:21:03.919Z",
          "wordCount": 552,
          "title": "Incorporating Domain Knowledge into Health Recommender Systems using Hyperbolic Embeddings. (arXiv:2106.07720v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beelen_T/0/1/0/all/0/1\">T. Beelen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velner_E/0/1/0/all/0/1\">E. Velner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordelman_R/0/1/0/all/0/1\">R. Ordelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_K/0/1/0/all/0/1\">K.P. Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evers_V/0/1/0/all/0/1\">V. Evers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huibers_T/0/1/0/all/0/1\">T. Huibers</a>",
          "description": "In this paper, we identify challenges in children's current information\nretrieval process, and propose conversational robots as an opportunity to ease\nthis process in a responsible way. Tools children currently use in this\nprocess, such as search engines on a computer or voice agents, do not always\nmeet their specific needs. The conversational robot we propose maintains\ncontext, asks clarifying questions, and gives suggestions in order to better\nmeet children's needs. Since children are often too trusting of robots, we\npropose to have the robot measure, monitor and adapt to the trust the child has\nin the robot. This way, we hope to induce a critical attitude with the children\nduring their information retrieval process.",
          "link": "http://arxiv.org/abs/2106.07931",
          "publishedOn": "2021-06-16T01:21:03.897Z",
          "wordCount": 570,
          "title": "Does your robot know? Enhancing children's information retrieval through spoken conversation with responsible robots. (arXiv:2106.07931v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07742",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandsen_A/0/1/0/all/0/1\">Alex Brandsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambers_K/0/1/0/all/0/1\">Karsten Lambers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wansleeben_M/0/1/0/all/0/1\">Milco Wansleeben</a>",
          "description": "The amount of archaeological literature is growing rapidly. Until recently,\nthese data were only accessible through metadata search. We implemented a text\nretrieval engine for a large archaeological text collection ($\\sim 658$ Million\nwords). In archaeological IR, domain-specific entities such as locations, time\nperiods, and artefacts, play a central role. This motivated the development of\na named entity recognition (NER) model to annotate the full collection with\narchaeological named entities. In this paper, we present ArcheoBERTje, a BERT\nmodel pre-trained on Dutch archaeological texts. We compare the model's quality\nand output on a Named Entity Recognition task to a generic multilingual model\nand a generic Dutch model. We also investigate ensemble methods for combining\nmultiple BERT models, and combining the best BERT model with a domain thesaurus\nusing Conditional Random Fields (CRF). We find that ArcheoBERTje outperforms\nboth the multilingual and Dutch model significantly with a smaller standard\ndeviation between runs, reaching an average F1 score of 0.735. The model also\noutperforms ensemble methods combining the three models. Combining ArcheoBERTje\npredictions and explicit domain knowledge from the thesaurus did not increase\nthe F1 score. We quantitatively and qualitatively analyse the differences\nbetween the vocabulary and output of the BERT models on the full collection and\nprovide some valuable insights in the effect of fine-tuning for specific\ndomains. Our results indicate that for a highly specific text domain such as\narchaeology, further pre-training on domain-specific data increases the model's\nquality on NER by a much larger margin than shown for other domains in the\nliterature, and that domain-specific pre-training makes the addition of domain\nknowledge from a thesaurus unnecessary.",
          "link": "http://arxiv.org/abs/2106.07742",
          "publishedOn": "2021-06-16T01:21:03.869Z",
          "wordCount": 710,
          "title": "Can BERT Dig It? -- Named Entity Recognition for Information Retrieval in the Archaeology Domain. (arXiv:2106.07742v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Emery_J/0/1/0/all/0/1\">Jules Azad Emery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latapy_M/0/1/0/all/0/1\">Matthieu Latapy</a>",
          "description": "Despite the fact that it is publicly available, collecting and processing the\nfull bitcoin blockchain data is not trivial. Its mere size, history, and other\nfeatures indeed raise quite specific challenges, that we address in this paper.\nThe strengths of our approach are the following: it relies on very basic and\nstandard tools, which makes the procedure reliable and easily reproducible; it\nis a purely lossless procedure ensuring that we catch and preserve all existing\ndata; it provides additional indexing that makes it easy to further process the\nwhole data and select appropriate subsets of it. We present our procedure in\ndetails and illustrate its added value on large-scale use cases, like address\nclustering. We provide an implementation online, as well as the obtained\ndataset.",
          "link": "http://arxiv.org/abs/2106.08072",
          "publishedOn": "2021-06-16T01:21:03.828Z",
          "wordCount": 559,
          "title": "Full Bitcoin Blockchain Data Made Easy. (arXiv:2106.08072v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>",
          "description": "Contrastive learning has been applied successfully to learn vector\nrepresentations of text. Previous research demonstrated that learning\nhigh-quality representations benefits from batch-wise contrastive loss with a\nlarge number of negatives. In practice, the technique of in-batch negative is\nused, where for each example in a batch, other batch examples' positives will\nbe taken as its negatives, avoiding encoding extra negatives. This, however,\nstill conditions each example's loss on all batch examples and requires fitting\nthe entire large batch into GPU memory. This paper introduces a gradient\ncaching technique that decouples backpropagation between contrastive loss and\nthe encoder, removing encoder backward pass data dependency along the batch\ndimension. As a result, gradients can be computed for one subset of the batch\nat a time, leading to almost constant memory usage.",
          "link": "http://arxiv.org/abs/2101.06983",
          "publishedOn": "2021-06-16T00:27:38.497Z",
          "wordCount": 593,
          "title": "Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup. (arXiv:2101.06983v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1\">Geand Trindade Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1\">Moises Rocha dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1\">Andre Carlos Ponce de Leon Ferreira de Carvalho</a>",
          "description": "With the popularity of Machine Learning (ML) solutions, algorithms and data\nhave been released faster than the capacity of processing them. In this\ncontext, the problem of Algorithm Recommendation (AR) is receiving a\nsignificant deal of attention recently. This problem has been addressed in the\nliterature as a learning task, often as a Meta-Learning problem where the aim\nis to recommend the best alternative for a specific dataset. For such, datasets\nencoded by meta-features are explored by ML algorithms that try to learn the\nmapping between meta-representations and the best technique to be used. One of\nthe challenges for the successful use of ML is to define which features are the\nmost valuable for a specific dataset since several meta-features can be used,\nwhich increases the meta-feature dimension. This paper presents an empirical\nanalysis of Feature Selection and Feature Extraction in the meta-level for the\nAR problem. The present study was focused on three criteria: predictive\nperformance, dimensionality reduction, and pipeline runtime. As we verified,\napplying Dimensionality Reduction (DR) methods did not improve predictive\nperformances in general. However, DR solutions reduced about 80% of the\nmeta-features, obtaining pretty much the same performance as the original setup\nbut with lower runtimes. The only exception was PCA, which presented about the\nsame runtime as the original meta-features. Experimental results also showed\nthat various datasets have many non-informative meta-features and that it is\npossible to obtain high predictive performance using around 20% of the original\nmeta-features. Therefore, due to their natural trend for high dimensionality,\nDR methods should be used for Meta-Feature Selection and Meta-Feature\nExtraction.",
          "link": "http://arxiv.org/abs/2106.03954",
          "publishedOn": "2021-06-15T22:41:24.942Z",
          "wordCount": 718,
          "title": "Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Liyi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Junqi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhenzhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhizhuang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Lvyin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoqiang Zhu</a>",
          "description": "Advertising expenditures have become the major source of revenue for\ne-commerce platforms. Providing good advertising experiences for advertisers by\nreducing their costs of trial and error in discovering the optimal advertising\nstrategies is crucial for the long-term prosperity of online advertising. To\nachieve this goal, the advertising platform needs to identify the advertiser's\noptimization objectives, and then recommend the corresponding strategies to\nfulfill the objectives. In this work, we first deploy a prototype of strategy\nrecommender system on Taobao display advertising platform, which indeed\nincreases the advertisers' performance and the platform's revenue, indicating\nthe effectiveness of strategy recommendation for online advertising. We further\naugment this prototype system by explicitly learning the advertisers'\npreferences over various advertising performance indicators and then\noptimization objectives through their adoptions of different recommending\nadvertising strategies. We use contextual bandit algorithms to efficiently\nlearn the advertisers' preferences and maximize the recommendation adoption,\nsimultaneously. Simulation experiments based on Taobao online bidding data show\nthat the designed algorithms can effectively optimize the strategy adoption\nrate of advertisers.",
          "link": "http://arxiv.org/abs/2105.14188",
          "publishedOn": "2021-06-15T22:41:24.921Z",
          "wordCount": 675,
          "title": "We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v3 [cs.IR] UPDATED)"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.09889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1\">Edward Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Huaishao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1\">Taroon Bharti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1\">Arun Sacheti</a>",
          "description": "In this paper, we present GEM as a General Evaluation benchmark for\nMultimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,\nXGLUE and XTREME that mainly focus on natural language tasks, GEM is a\nlarge-scale vision-language benchmark, which consists of GEM-I for\nimage-language tasks and GEM-V for video-language tasks. Comparing with\nexisting multimodal datasets such as MSCOCO and Flicker30K for image-language\ntasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the\nlargest vision-language dataset covering image-language tasks and\nvideo-language tasks at the same time, but also labeled in multiple languages.\nWe also provide two baseline models for this benchmark. We will release the\ndataset, code and baseline models, aiming to advance the development of\nmultilingual multimodal research.",
          "link": "http://arxiv.org/abs/2106.09889",
          "publishedOn": "2021-06-21T02:07:36.399Z",
          "wordCount": 581,
          "title": "GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Graf_M/0/1/0/all/0/1\">Max Graf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opara_H/0/1/0/all/0/1\">Harold Chijioke Opara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1\">Mathieu Barthet</a>",
          "description": "Computer-generated visualisations can accompany recorded or live music to\ncreate novel audiovisual experiences for audiences. We present a system to\nstreamline the creation of audio-driven visualisations based on audio feature\nextraction and mapping interfaces. Its architecture is based on three modular\nsoftware components: backend (audio plugin), frontend (3D game-like\nenvironment), and middleware (visual mapping interface). We conducted a user\nevaluation comprising two stages. Results from the first stage (34\nparticipants) indicate that music visualisations generated with the system were\nsignificantly better at complementing the music than a baseline visualisation.\nNine participants took part in the second stage involving interactive tasks.\nOverall, the system yielded a Creativity Support Index above average (68.1) and\na System Usability Scale index (58.6) suggesting that ease of use can be\nimproved. Thematic analysis revealed that participants enjoyed the system's\nsynchronicity and expressive capabilities, but found technical problems and\ndifficulties understanding the audio feature terminology.",
          "link": "http://arxiv.org/abs/2106.10134",
          "publishedOn": "2021-06-21T02:07:36.368Z",
          "wordCount": 587,
          "title": "An Audio-Driven System For Real-Time Music Visualisation. (arXiv:2106.10134v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geleta_M/0/1/0/all/0/1\">Margarita Geleta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punti_C/0/1/0/all/0/1\">Cristina Punti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_J/0/1/0/all/0/1\">Jordi Pons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canton_C/0/1/0/all/0/1\">Cristian Canton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Giro-i-Nieto</a>",
          "description": "Steganography comprises the mechanics of hiding data in a host media that may\nbe publicly available. While previous works focused on unimodal setups (e.g.,\nhiding images in images, or hiding audio in audio), PixInWav targets the\nmultimodal case of hiding images in audio. To this end, we propose a novel\nresidual architecture operating on top of short-time discrete cosine transform\n(STDCT) audio spectrograms. Among our results, we find that the residual audio\nsteganography setup we propose allows independent encoding of the hidden image\nfrom the host audio without compromising quality. Accordingly, while previous\nworks require both host and hidden signals to hide a signal, PixInWav can\nencode images offline -- which can be later hidden, in a residual fashion, into\nany audio signal. Finally, we test our scheme in a lab setting to transmit\nimages over airwaves from a loudspeaker to a microphone verifying our\ntheoretical insights and obtaining promising results.",
          "link": "http://arxiv.org/abs/2106.09814",
          "publishedOn": "2021-06-21T02:07:36.277Z",
          "wordCount": 604,
          "title": "PixInWav: Residual Steganography for Hiding Pixels in Audio. (arXiv:2106.09814v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chenye Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feiyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_M/0/1/0/all/0/1\">Ming Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>",
          "description": "Recently, there has been an increasing interest in neural speech synthesis.\nWhile the deep neural network achieves the state-of-the-art result in\ntext-to-speech (TTS) tasks, how to generate a more emotional and more\nexpressive speech is becoming a new challenge to researchers due to the\nscarcity of high-quality emotion speech dataset and the lack of advanced\nemotional TTS model. In this paper, we first briefly introduce and publicly\nrelease a Mandarin emotion speech dataset including 9,724 samples with audio\nfiles and its emotion human-labeled annotation. After that, we propose a simple\nbut efficient architecture for emotional speech synthesis called EMSpeech.\nUnlike those models which need additional reference audio as input, our model\ncould predict emotion labels just from the input text and generate more\nexpressive speech conditioned on the emotion embedding. In the experiment\nphase, we first validate the effectiveness of our dataset by an emotion\nclassification task. Then we train our model on the proposed dataset and\nconduct a series of subjective evaluations. Finally, by showing a comparable\nperformance in the emotional speech synthesis task, we successfully demonstrate\nthe ability of the proposed model.",
          "link": "http://arxiv.org/abs/2106.09317",
          "publishedOn": "2021-06-18T02:06:33.097Z",
          "wordCount": 644,
          "title": "EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model. (arXiv:2106.09317v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_A/0/1/0/all/0/1\">Anjana Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barmaki_R/0/1/0/all/0/1\">Roghayeh Barmaki</a>",
          "description": "Autism spectrum disorder (ASD) is a developmental disorder that influences\nthe communication and social behavior of a person in a way that those in the\nspectrum have difficulty in perceiving other people's facial expressions, as\nwell as presenting and communicating emotions and affect via their own faces\nand bodies. Some efforts have been made to predict and improve children with\nASD's affect states in play therapy, a common method to improve children's\nsocial skills via play and games. However, many previous works only used\npre-trained models on benchmark emotion datasets and failed to consider the\ndistinction in emotion between typically developing children and children with\nautism. In this paper, we present an open-source two-stage multi-modal approach\nleveraging acoustic and visual cues to predict three main affect states of\nchildren with ASD's affect states (positive, negative, and neutral) in\nreal-world play therapy scenarios, and achieved an overall accuracy of 72:40%.\nThis work presents a novel way to combine human expertise and machine\nintelligence for ASD affect recognition by proposing a two-stage schema.",
          "link": "http://arxiv.org/abs/2106.09199",
          "publishedOn": "2021-06-18T02:06:33.053Z",
          "wordCount": 636,
          "title": "A Two-stage Multi-modal Affect Analysis Framework for Children with Autism Spectrum Disorder. (arXiv:2106.09199v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murray_Browne_T/0/1/0/all/0/1\">Tim Murray-Browne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tigas_P/0/1/0/all/0/1\">Panagiotis Tigas</a>",
          "description": "In many contexts, creating mappings for gestural interactions can form part\nof an artistic process. Creators seeking a mapping that is expressive, novel,\nand affords them a sense of authorship may not know how to program it up in a\nsignal processing patch. Tools like Wekinator and MIMIC allow creators to use\nsupervised machine learning to learn mappings from example input/output\npairings. However, a creator may know a good mapping when they encounter it yet\nstart with little sense of what the inputs or outputs should be. We call this\nan open-ended mapping process. Addressing this need, we introduce the latent\nmapping, which leverages the latent space of an unsupervised machine learning\nalgorithm such as a Variational Autoencoder trained on a corpus of unlabelled\ngestural data from the creator. We illustrate it with Sonified Body, a system\nmapping full-body movement to sound which we explore in a residency with three\ndancers.",
          "link": "http://arxiv.org/abs/2106.08867",
          "publishedOn": "2021-06-17T01:58:40.177Z",
          "wordCount": 629,
          "title": "Latent Mappings: Generating Open-Ended Expressive Mappings Using Variational Autoencoders. (arXiv:2106.08867v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08936",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1\">Luka Murn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blasi_S/0/1/0/all/0/1\">Saverio Blasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smeaton_A/0/1/0/all/0/1\">Alan F. Smeaton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>",
          "description": "The versatility of recent machine learning approaches makes them ideal for\nimprovement of next generation video compression solutions. Unfortunately,\nthese approaches typically bring significant increases in computational\ncomplexity and are difficult to interpret into explainable models, affecting\ntheir potential for implementation within practical video coding applications.\nThis paper introduces a novel explainable neural network-based inter-prediction\nscheme, to improve the interpolation of reference samples needed for fractional\nprecision motion compensation. The approach requires a single neural network to\nbe trained from which a full quarter-pixel interpolation filter set is derived,\nas the network is easily interpretable due to its linear structure. A novel\ntraining framework enables each network branch to resemble a specific\nfractional shift. This practical solution makes it very efficient to use\nalongside conventional video coding schemes. When implemented in the context of\nthe state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and\n2.25% BD-rate savings can be achieved on average for lower resolution sequences\nunder the random access, low-delay B and low-delay P configurations,\nrespectively, while the complexity of the learned interpolation schemes is\nsignificantly reduced compared to the interpolation with full CNNs.",
          "link": "http://arxiv.org/abs/2106.08936",
          "publishedOn": "2021-06-17T01:58:40.151Z",
          "wordCount": 664,
          "title": "Improved CNN-based Learning of Interpolation Filters for Low-Complexity Inter Prediction in Video Coding. (arXiv:2106.08936v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">C.-H. Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1\">Mohit Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Y.-C. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Quan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1\">Tomoaki Yoshinaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1\">Tomokazu Murakami</a>",
          "description": "Physical processes, camera movement, and unpredictable environmental\nconditions like the presence of dust can induce noise and artifacts in video\nfeeds. We observe that popular unsupervised MOT methods are dependent on\nnoise-free inputs. We show that the addition of a small amount of artificial\nrandom noise causes a sharp degradation in model performance on benchmark\nmetrics. We resolve this problem by introducing a robust unsupervised\nmulti-object tracking (MOT) model: AttU-Net. The proposed single-head attention\nmodel helps limit the negative impact of noise by learning visual\nrepresentations at different segment scales. AttU-Net shows better unsupervised\nMOT tracking performance over variational inference-based state-of-the-art\nbaselines. We evaluate our method in the MNIST-MOT and the Atari game video\nbenchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''\nwhich consists of moving Japanese characters and ``Fashion-MNIST MOT'' to\nvalidate the effectiveness of the MOT models.",
          "link": "http://arxiv.org/abs/2105.10005",
          "publishedOn": "2021-06-16T01:21:04.389Z",
          "wordCount": 644,
          "title": "Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mingfu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shichang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yushu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqiang Liu</a>",
          "description": "Deep neural networks (DNN) have achieved remarkable performance in various\nfields. However, training a DNN model from scratch requires a lot of computing\nresources and training data. It is difficult for most individual users to\nobtain such computing resources and training data. Model copyright infringement\nis an emerging problem in recent years. For instance, pre-trained models may be\nstolen or abuse by illegal users without the authorization of the model owner.\nRecently, many works on protecting the intellectual property of DNN models have\nbeen proposed. In these works, embedding watermarks into DNN based on backdoor\nis one of the widely used methods. However, when the DNN model is stolen, the\nbackdoor-based watermark may face the risk of being detected and removed by an\nadversary. In this paper, we propose a scheme to detect and remove watermark in\ndeep neural networks via generative adversarial networks (GAN). We demonstrate\nthat the backdoor-based DNN watermarks are vulnerable to the proposed GAN-based\nwatermark removal attack. The proposed attack method includes two phases. In\nthe first phase, we use the GAN and few clean images to detect and reverse the\nwatermark in the DNN model. In the second phase, we fine-tune the watermarked\nDNN based on the reversed backdoor images. Experimental evaluations on the\nMNIST and CIFAR10 datasets demonstrate that, the proposed method can\neffectively remove about 98% of the watermark in DNN models, as the watermark\nretention rate reduces from 100% to less than 2% after applying the proposed\nattack. In the meantime, the proposed attack hardly affects the model's\nperformance. The test accuracy of the watermarked DNN on the MNIST and the\nCIFAR10 datasets drops by less than 1% and 3%, respectively.",
          "link": "http://arxiv.org/abs/2106.08104",
          "publishedOn": "2021-06-16T01:21:04.329Z",
          "wordCount": 724,
          "title": "Detect and remove watermark in deep neural networks via generative adversarial networks. (arXiv:2106.08104v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>",
          "description": "Legacy black-and-white photos are riddled with people's nostalgia and\nglorious memories of the past. To better relive the elapsed frozen moments, in\nthis paper, we present a deep exemplar-based image colorization approach named\nColor2Style to resurrect these grayscale image media by filling them with\nvibrant colors. Generally, for exemplar-based colorization, unsupervised and\nunpaired training are usually adopted, due to the difficulty of obtaining input\nand ground truth image pairs. To train an exemplar-based colorization model,\ncurrent algorithms usually strive to achieve two procedures: i) retrieving a\nlarge number of reference images with high similarity in advance, which is\ninevitably time-consuming and tedious; ii) designing complicated modules to\ntransfer the colors of the reference image to the grayscale image, by\ncalculating and leveraging the deep semantic correspondence between them (e.g.,\nnon-local operation). Contrary to the previous methods, we solve and simplify\nthe above two steps in one end-to-end learning procedure. First, we adopt a\nself-augmented self-reference training scheme, where the reference image is\ngenerated by graphical transformations from the original colorful one whereby\nthe training can be formulated in a paired manner. Second, instead of computing\ncomplex and inexplicable correspondence maps, our method exploits a simple yet\neffective deep feature modulation (DFM) module, which injects the color\nembeddings extracted from the reference image into the deep representations of\nthe input grayscale image. Such design is much more lightweight and\nintelligible, achieving appealing performance with real-time processing speed.\nMoreover, our model does not require multifarious loss functions and\nregularization terms like existing methods, but only two widely used loss\nfunctions. Codes and models will be available at\nhttps://github.com/zhaohengyuan1/Color2Style.",
          "link": "http://arxiv.org/abs/2106.08017",
          "publishedOn": "2021-06-16T01:21:04.242Z",
          "wordCount": 714,
          "title": "Color2Style: Real-Time Exemplar-Based Image Colorization with Self-Reference Learning and Deep Feature Modulation. (arXiv:2106.08017v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Ching-Chun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sisheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1\">Victor Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chang-Tsun Li</a>",
          "description": "Deep-learning\\textendash{centric} reversible steganography has emerged as a\npromising research paradigm. A direct way of applying deep learning to\nreversible steganography is to construct a pair of encoder and decoder, whose\nparameters are trained jointly, thereby learning the steganographic system as a\nwhole. This end-to-end framework, however, falls short of the reversibility\nrequirement because it is difficult for this kind of monolithic system, as a\nblack box, to create or duplicate intricate reversible mechanisms. In response\nto this issue, a recent approach is to carve up the steganographic system and\nwork on modules independently. In particular, neural networks are deployed in\nan analytics module to learn the data distribution, while an established\nmechanism is called upon to handle the remaining tasks. In this paper, we\ninvestigate the modular framework and deploy deep neural networks in a\nreversible steganographic scheme referred to as prediction-error modulation, in\nwhich an analytics module serves the purpose of pixel intensity prediction. The\nprimary focus of this study is on deep-learning\\textendash{based} context-aware\npixel intensity prediction. We address the unsolved issues reported in related\nliterature, including the impact of pixel initialisation on prediction accuracy\nand the influence of uncertainty propagation in dual-layer embedding.\nFurthermore, we establish a connection between context-aware pixel intensity\nprediction and low-level computer vision and analyse the performance of several\nadvanced neural networks.",
          "link": "http://arxiv.org/abs/2106.06924",
          "publishedOn": "2021-06-15T22:07:48.721Z",
          "wordCount": 658,
          "title": "Deep Learning for Reversible Steganography: Principles and Insights. (arXiv:2106.06924v1 [cs.MM])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.03135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1\">Janis Postels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengya Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spezialetti_R/0/1/0/all/0/1\">Riccardo Spezialetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "Recently normalizing flows (NFs) have demonstrated state-of-the-art\nperformance on modeling 3D point clouds while allowing sampling with arbitrary\nresolution at inference time. However, these flow-based models still require\nlong training times and large models for representing complicated geometries.\nThis work enhances their representational power by applying mixtures of NFs to\npoint clouds. We show that in this more general framework each component learns\nto specialize in a particular subregion of an object in a completely\nunsupervised fashion. By instantiating each mixture component with a\ncomparatively small NF we generate point clouds with improved details compared\nto single-flow-based models while using fewer parameters and considerably\nreducing the inference runtime. We further demonstrate that by adding data\naugmentation, individual mixture components can learn to specialize in a\nsemantically meaningful manner. We evaluate mixtures of NFs on generation,\nautoencoding and single-view reconstruction based on the ShapeNet dataset.",
          "link": "http://arxiv.org/abs/2106.03135",
          "publishedOn": "2021-06-21T02:07:40.727Z",
          "wordCount": 612,
          "title": "Go with the Flows: Mixtures of Normalizing Flows for Point Cloud Generation and Reconstruction. (arXiv:2106.03135v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jinka_S/0/1/0/all/0/1\">Sai Sagar Jinka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chacko_R/0/1/0/all/0/1\">Rohan Chacko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Astitva Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Avinash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1\">P.J. Narayanan</a>",
          "description": "3D human body reconstruction from monocular images is an interesting and\nill-posed problem in computer vision with wider applications in multiple\ndomains. In this paper, we propose SHARP, a novel end-to-end trainable network\nthat accurately recovers the detailed geometry and appearance of 3D people in\nloose clothing from a monocular image. We propose a sparse and efficient fusion\nof a parametric body prior with a non-parametric peeled depth map\nrepresentation of clothed models. The parametric body prior constraints our\nmodel in two ways: first, the network retains geometrically consistent body\nparts that are not occluded by clothing, and second, it provides a body shape\ncontext that improves prediction of the peeled depth maps. This enables SHARP\nto recover fine-grained 3D geometrical details with just L1 losses on the 2D\nmaps, given an input image. We evaluate SHARP on publicly available Cloth3D and\nTHuman datasets and report superior performance to state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2106.04778",
          "publishedOn": "2021-06-21T02:07:39.749Z",
          "wordCount": 606,
          "title": "SHARP: Shape-Aware Reconstruction of People In Loose Clothing. (arXiv:2106.04778v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhehan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinghao Ding</a>",
          "description": "Retinal artery/vein (A/V) classification is a critical technique for\ndiagnosing diabetes and cardiovascular diseases. Although deep learning based\nmethods achieve impressive results in A/V classification, their performances\nusually degrade severely when being directly applied to another database, due\nto the domain shift, e.g., caused by the variations in imaging protocols. In\nthis paper, we propose a novel vessel-mixing based consistency regularization\nframework, for cross-domain learning in retinal A/V classification. Specially,\nto alleviate the severe bias to source domain, based on the label smooth prior,\nthe model is regularized to give consistent predictions for unlabeled\ntarget-domain inputs that are under perturbation. This consistency\nregularization implicitly introduces a mechanism where the model and the\nperturbation is opponent to each other, where the model is pushed to be robust\nenough to cope with the perturbation. Thus, we investigate a more difficult\nopponent to further inspire the robustness of model, in the scenario of retinal\nA/V, called vessel-mixing perturbation. Specially, it effectively disturbs the\nfundus images especially the vessel structures by mixing two images regionally.\nWe conduct extensive experiments on cross-domain A/V classification using four\npublic datasets, which are collected by diverse institutions and imaging\ndevices. The results demonstrate that our method achieves the state-of-the-art\ncross-domain performance, which is also close to the upper bound obtained by\nfully supervised learning on target domain.",
          "link": "http://arxiv.org/abs/2103.09097",
          "publishedOn": "2021-06-21T02:07:39.678Z",
          "wordCount": 698,
          "title": "Consistent Posterior Distributions under Vessel-Mixing: A Regularization for Cross-Domain Retinal Artery/Vein Classification. (arXiv:2103.09097v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nikola Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guolei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "We define the concept of CompositeTasking as the fusion of multiple,\nspatially distributed tasks, for various aspects of image understanding.\nLearning to perform spatially distributed tasks is motivated by the frequent\navailability of only sparse labels across tasks, and the desire for a compact\nmulti-tasking network. To facilitate CompositeTasking, we introduce a novel\ntask conditioning model -- a single encoder-decoder network that performs\nmultiple, spatially varying tasks at once. The proposed network takes an image\nand a set of pixel-wise dense task requests as inputs, and performs the\nrequested prediction task for each pixel. Moreover, we also learn the\ncomposition of tasks that needs to be performed according to some\nCompositeTasking rules, which includes the decision of where to apply which\ntask. It not only offers us a compact network for multi-tasking, but also\nallows for task-editing. Another strength of the proposed method is\ndemonstrated by only having to supply sparse supervision per task. The obtained\nresults are on par with our baselines that use dense supervision and a\nmulti-headed multi-tasking design. The source code will be made publicly\navailable at www.github.com/nikola3794/composite-tasking.",
          "link": "http://arxiv.org/abs/2012.09030",
          "publishedOn": "2021-06-21T02:07:39.653Z",
          "wordCount": 655,
          "title": "CompositeTasking: Understanding Images by Spatial Composition of Tasks. (arXiv:2012.09030v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.01496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shuo Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>",
          "description": "Despite the great progress made by deep neural networks in the semantic\nsegmentation task, traditional neural-networkbased methods typically suffer\nfrom a shortage of large amounts of pixel-level annotations. Recent progress in\nfewshot semantic segmentation tackles the issue by only a few pixel-level\nannotated examples. However, these few-shot approaches cannot easily be applied\nto multi-way or weak annotation settings. In this paper, we advance the\nfew-shot segmentation paradigm towards a scenario where image-level annotations\nare available to help the training process of a few pixel-level annotations.\nOur key idea is to learn a better prototype representation of the class by\nfusing the knowledge from the image-level labeled data. Specifically, we\npropose a new framework, called PAIA, to learn the class prototype\nrepresentation in a metric space by integrating image-level annotations.\nFurthermore, by considering the uncertainty of pseudo-masks, a distilled soft\nmasked average pooling strategy is designed to handle distractions in\nimage-level annotations. Extensive empirical results on two datasets show\nsuperior performance of PAIA.",
          "link": "http://arxiv.org/abs/2007.01496",
          "publishedOn": "2021-06-21T02:07:39.582Z",
          "wordCount": 644,
          "title": "Few-Shot Semantic Segmentation Augmented with Image-Level Weak Annotations. (arXiv:2007.01496v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.01607",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1\">Tian Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "Pseudo-healthy synthesis is the task of creating a subject-specific `healthy'\nimage from a pathological one. Such images can be helpful in tasks such as\nanomaly detection and understanding changes induced by pathology and disease.\nIn this paper, we present a model that is encouraged to disentangle the\ninformation of pathology from what seems to be healthy. We disentangle what\nappears to be healthy and where disease is as a segmentation map, which are\nthen recombined by a network to reconstruct the input disease image. We train\nour models adversarially using either paired or unpaired settings, where we\npair disease images and maps when available. We quantitatively and\nsubjectively, with a human study, evaluate the quality of pseudo-healthy images\nusing several criteria. We show in a series of experiments, performed on ISLES,\nBraTS and Cam-CAN datasets, that our method is better than several baselines\nand methods from the literature. We also show that due to better training\nprocesses we could recover deformations, on surrounding tissue, caused by\ndisease. Our implementation is publicly available at\nhttps://github.com/xiat0616/pseudo-healthy-synthesis. This paper has been\naccepted by Medical Image Analysis:\nhttps://doi.org/10.1016/j.media.2020.101719.",
          "link": "http://arxiv.org/abs/2005.01607",
          "publishedOn": "2021-06-21T02:07:39.155Z",
          "wordCount": 674,
          "title": "Pseudo-healthy synthesis with pathology disentanglement and adversarial learning. (arXiv:2005.01607v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scribano_C/0/1/0/all/0/1\">Carmelo Scribano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapienza_D/0/1/0/all/0/1\">Davide Sapienza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchini_G/0/1/0/all/0/1\">Giorgia Franchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verucchi_M/0/1/0/all/0/1\">Micaela Verucchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertogna_M/0/1/0/all/0/1\">Marko Bertogna</a>",
          "description": "Combining Natural Language with Vision represents a unique and interesting\nchallenge in the domain of Artificial Intelligence. The AI City Challenge Track\n5 for Natural Language-Based Vehicle Retrieval focuses on the problem of\ncombining visual and textual information, applied to a smart-city use case. In\nthis paper, we present All You Can Embed (AYCE), a modular solution to\ncorrelate single-vehicle tracking sequences with natural language. The main\nbuilding blocks of the proposed architecture are (i) BERT to provide an\nembedding of the textual descriptions, (ii) a convolutional backbone along with\na Transformer model to embed the visual information. For the training of the\nretrieval model, a variation of the Triplet Margin Loss is proposed to learn a\ndistance measure between the visual and language embeddings. The code is\npublicly available at https://github.com/cscribano/AYCE_2021.",
          "link": "http://arxiv.org/abs/2106.10153",
          "publishedOn": "2021-06-21T02:07:38.907Z",
          "wordCount": 590,
          "title": "All You Can Embed: Natural Language based Vehicle Retrieval with Spatio-Temporal Transformers. (arXiv:2106.10153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1\">Andreas Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wightman_R/0/1/0/all/0/1\">Ross Wightman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1\">Jakob Uszkoreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>",
          "description": "Vision Transformers (ViT) have been shown to attain highly competitive\nperformance for a wide range of vision applications, such as image\nclassification, object detection and semantic image segmentation. In comparison\nto convolutional neural networks, the Vision Transformer's weaker inductive\nbias is generally found to cause an increased reliance on model regularization\nor data augmentation (``AugReg'' for short) when training on smaller training\ndatasets. We conduct a systematic empirical study in order to better understand\nthe interplay between the amount of training data, AugReg, model size and\ncompute budget. As one result of this study we find that the combination of\nincreased compute and AugReg can yield models with the same performance as\nmodels trained on an order of magnitude more training data: we train ViT models\nof various sizes on the public ImageNet-21k dataset which either match or\noutperform their counterparts trained on the larger, but not publicly available\nJFT-300M dataset.",
          "link": "http://arxiv.org/abs/2106.10270",
          "publishedOn": "2021-06-21T02:07:38.882Z",
          "wordCount": 649,
          "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. (arXiv:2106.10270v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Daisheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Recently, Vision Transformers (ViTs) have achieved impressive results on\nvarious vision tasks. Yet, their generalization ability under different\ndistribution shifts is rarely understood. In this work, we provide a\ncomprehensive study on the out-of-distribution generalization of ViTs. To\nsupport a systematic investigation, we first present a taxonomy of distribution\nshifts by categorizing them into five conceptual groups: corruption shift,\nbackground shift, texture shift, destruction shift, and style shift. Then we\nperform extensive evaluations of ViT variants under different groups of\ndistribution shifts and compare their generalization ability with CNNs. Several\nimportant observations are obtained: 1) ViTs generalize better than CNNs under\nmultiple distribution shifts. With the same or fewer parameters, ViTs are ahead\nof corresponding CNNs by more than 5% in top-1 accuracy under most distribution\nshifts. 2) Larger ViTs gradually narrow the in-distribution and\nout-of-distribution performance gap. To further improve the generalization of\nViTs, we design the Generalization-Enhanced ViTs by integrating adversarial\nlearning, information theory, and self-supervised learning. By investigating\nthree types of generalization-enhanced ViTs, we observe their\ngradient-sensitivity and design a smoother learning strategy to achieve a\nstable training process. With modified training schemes, we achieve\nimprovements on performance towards out-of-distribution data by 4% from vanilla\nViTs. We comprehensively compare three generalization-enhanced ViTs with their\ncorresponding CNNs, and observe that: 1) For the enhanced model, larger ViTs\nstill benefit more for the out-of-distribution generalization. 2)\ngeneralization-enhanced ViTs are more sensitive to the hyper-parameters than\ncorresponding CNNs. We hope our comprehensive study could shed light on the\ndesign of more generalizable learning architectures.",
          "link": "http://arxiv.org/abs/2106.07617",
          "publishedOn": "2021-06-21T02:07:38.810Z",
          "wordCount": 734,
          "title": "Delving Deep into the Generalization of Vision Transformers under Distribution Shifts. (arXiv:2106.07617v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.03369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Daqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Minghua Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Nearest neighbor search is to find the data points in the database such that\nthe distances from them to the query are the smallest, which is a fundamental\nproblem in various domains, such as computer vision, recommendation systems and\nmachine learning. Hashing is one of the most widely used methods for its\ncomputational and storage efficiency. With the development of deep learning,\ndeep hashing methods show more advantages than traditional methods. In this\npaper, we present a comprehensive survey of the deep hashing algorithms.\nSpecifically, we categorize deep supervised hashing methods into pairwise\nsimilarity preserving, multiwise similarity preserving, implicit similarity\npreserving, classification-oriented preserving as well as quantization\naccording to the manners of preserving the similarities. In addition, we also\nintroduce some other topics such as deep unsupervised hashing and multi-modal\ndeep hashing methods. Meanwhile, we also present some commonly used public\ndatasets and the scheme to measure the performance of deep hashing algorithms.\nFinally, we discussed some potential research directions in conclusion.",
          "link": "http://arxiv.org/abs/2003.03369",
          "publishedOn": "2021-06-21T02:07:38.792Z",
          "wordCount": 634,
          "title": "A Survey on Deep Hashing Methods. (arXiv:2003.03369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identifying\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-06-21T02:07:38.785Z",
          "wordCount": 617,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aksoy_A/0/1/0/all/0/1\">Ahmet Kerem Aksoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanbakhsh_M/0/1/0/all/0/1\">Mahdyar Ravanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreuziger_T/0/1/0/all/0/1\">Tristan Kreuziger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Begum Demir</a>",
          "description": "Collecting a large number of reliable training images annotated by multiple\nland-cover class labels in the framework of multi-label classification is\ntime-consuming and costly in remote sensing (RS). To address this problem,\npublicly available thematic products are often used for annotating RS images\nwith zero-labeling-cost. However, such an approach may result in constructing a\ntraining set with noisy multi-labels, distorting the learning process. To\naddress this problem, we propose a Consensual Collaborative Multi-Label\nLearning (CCML) method. The proposed CCML identifies, ranks and corrects\ntraining images with noisy multi-labels through four main modules: 1)\ndiscrepancy module; 2) group lasso module; 3) flipping module; and 4) swap\nmodule. The discrepancy module ensures that the two networks learn diverse\nfeatures, while obtaining the same predictions. The group lasso module detects\nthe potentially noisy labels by estimating the label uncertainty based on the\naggregation of two collaborative networks. The flipping module corrects the\nidentified noisy labels, whereas the swap module exchanges the ranking\ninformation between the two networks. The experimental results confirm the\nsuccess of the proposed CCML under high (synthetically added) multi-label noise\nrates. The code of the proposed method is publicly available at\nhttps://noisy-labels-in-rs.org",
          "link": "http://arxiv.org/abs/2105.05496",
          "publishedOn": "2021-06-21T02:07:38.778Z",
          "wordCount": 687,
          "title": "A Consensual Collaborative Learning Method for Remote Sensing Image Classification Under Noisy Multi-Labels. (arXiv:2105.05496v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1\">Wang Yifan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1\">Lukas Rahmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1\">Olga Sorkine-Hornung</a>",
          "description": "We present implicit displacement fields, a novel representation for detailed\n3D geometry. Inspired by a classic surface deformation technique, displacement\nmapping, our method represents a complex surface as a smooth base surface plus\na displacement along the base's normal directions, resulting in a\nfrequency-based shape decomposition, where the high frequency signal is\nconstrained geometrically by the low frequency signal. Importantly, this\ndisentanglement is unsupervised thanks to a tailored architectural design that\nhas an innate frequency hierarchy by construction. We explore implicit\ndisplacement field surface reconstruction and detail transfer and demonstrate\nsuperior representational power, training stability and generalizability.",
          "link": "http://arxiv.org/abs/2106.05187",
          "publishedOn": "2021-06-21T02:07:38.756Z",
          "wordCount": 567,
          "title": "Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qimeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>",
          "description": "Temporal action detection (TAD) aims to determine the semantic label and the\nboundaries of every action instance in an untrimmed video. It is a fundamental\ntask in video understanding and significant progress has been made in TAD.\nPrevious methods involve multiple stages or networks and hand-designed rules or\noperations, which fall short in efficiency and flexibility. Here, we construct\nan end-to-end framework for TAD upon Transformer, termed \\textit{TadTR}, which\nsimultaneously predicts all action instances as a set of labels and temporal\nlocations in parallel. TadTR is able to adaptively extract temporal context\ninformation needed for making action predictions, by selectively attending to a\nnumber of snippets in a video. It greatly simplifies the pipeline of TAD and\nruns much faster than previous detectors. Our method achieves state-of-the-art\nperformance on HACS Segments and THUMOS14 and competitive performance on\nActivityNet-1.3. Our code will be made available at\n\\url{https://github.com/xlliu7/TadTR}.",
          "link": "http://arxiv.org/abs/2106.10271",
          "publishedOn": "2021-06-21T02:07:38.741Z",
          "wordCount": 583,
          "title": "End-to-end Temporal Action Detection with Transformer. (arXiv:2106.10271v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_C/0/1/0/all/0/1\">Chintan Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1\">Antonios Liapis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1\">Georgios N. Yannakakis</a>",
          "description": "Representing games through their pixels offers a promising approach for\nbuilding general-purpose and versatile game models. While games are not merely\nimages, neural network models trained on game pixels often capture differences\nof the visual style of the image rather than the content of the game. As a\nresult, such models cannot generalize well even within similar games of the\nsame genre. In this paper we build on recent advances in contrastive learning\nand showcase its benefits for representation learning in games. Learning to\ncontrast images of games not only classifies games in a more efficient manner;\nit also yields models that separate games in a more meaningful fashion by\nignoring the visual style and focusing, instead, on their content. Our results\nin a large dataset of sports video games containing 100k images across 175\ngames and 10 game genres suggest that contrastive learning is better suited for\nlearning generalized game representations compared to conventional supervised\nlearning. The findings of this study bring us closer to universal visual\nencoders for games that can be reused across previously unseen games without\nrequiring retraining or fine-tuning.",
          "link": "http://arxiv.org/abs/2106.10060",
          "publishedOn": "2021-06-21T02:07:38.710Z",
          "wordCount": 625,
          "title": "Contrastive Learning of Generalized Game Representations. (arXiv:2106.10060v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1\">Natalia Neverova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanakoyeu_A/0/1/0/all/0/1\">Artsiom Sanakoyeu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1\">Patrick Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1\">David Novotny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>",
          "description": "We tackle the problem of learning the geometry of multiple categories of\ndeformable objects jointly. Recent work has shown that it is possible to learn\na unified dense pose predictor for several categories of related objects.\nHowever, training such models requires to initialize inter-category\ncorrespondences by hand. This is suboptimal and the resulting models fail to\nmaintain correct correspondences as individual categories are learned. In this\npaper, we show that improved correspondences can be learned automatically as a\nnatural byproduct of learning category-specific dense pose predictors. To do\nthis, we express correspondences between different categories and between\nimages and categories using a unified embedding. Then, we use the latter to\nenforce two constraints: symmetric inter-category cycle consistency and a new\nasymmetric image-to-category cycle consistency. Without any manual annotations\nfor the inter-category correspondences, we obtain state-of-the-art alignment\nresults, outperforming dedicated methods for matching 3D shapes. Moreover, the\nnew model is also better at the task of dense pose prediction than prior work.",
          "link": "http://arxiv.org/abs/2106.09758",
          "publishedOn": "2021-06-21T02:07:38.680Z",
          "wordCount": 610,
          "title": "Discovering Relationships between Object Categories via Universal Canonical Maps. (arXiv:2106.09758v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jiabao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>",
          "description": "Reconstruction of object or scene surfaces has tremendous applications in\ncomputer vision, computer graphics, and robotics. In this paper, we study a\nfundamental problem in this context about recovering a surface mesh from an\nimplicit field function whose zero-level set captures the underlying surface.\nTo achieve the goal, existing methods rely on traditional meshing algorithms;\nwhile promising, they suffer from loss of precision learned in the implicit\nsurface networks, due to the use of discrete space sampling in marching cubes.\nGiven that an MLP with activations of Rectified Linear Unit (ReLU) partitions\nits input space into a number of linear regions, we are motivated to connect\nthis local linearity with a same property owned by the desired result of\npolygon mesh. More specifically, we identify from the linear regions,\npartitioned by an MLP based implicit function, the analytic cells and analytic\nfaces that are associated with the function's zero-level isosurface. We prove\nthat under mild conditions, the identified analytic faces are guaranteed to\nconnect and form a closed, piecewise planar surface. Based on the theorem, we\npropose an algorithm of analytic marching, which marches among analytic cells\nto exactly recover the mesh captured by an implicit surface network. We also\nshow that our theory and algorithm are equally applicable to advanced MLPs with\nshortcut connections and max pooling. Given the parallel nature of analytic\nmarching, we contribute AnalyticMesh, a software package that supports\nefficient meshing of implicit surface networks via CUDA parallel computing, and\nmesh simplification for efficient downstream processing. We apply our method to\ndifferent settings of generative shape modeling using implicit surface\nnetworks. Extensive experiments demonstrate our advantages over existing\nmethods in terms of both meshing accuracy and efficiency.",
          "link": "http://arxiv.org/abs/2106.10031",
          "publishedOn": "2021-06-21T02:07:38.672Z",
          "wordCount": 740,
          "title": "Learning and Meshing from Deep Implicit Surface Networks Using an Efficient Implementation of Analytic Marching. (arXiv:2106.10031v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1\">Alireza Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halstead_M/0/1/0/all/0/1\">Michael Halstead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCool_C/0/1/0/all/0/1\">Chris McCool</a>",
          "description": "This paper explores the potential for performing temporal semantic\nsegmentation in the context of agricultural robotics without temporally\nlabelled data. We achieve this by proposing to generate virtual temporal\nsamples from labelled still images. This allows us, with no extra annotation\neffort, to generate virtually labelled temporal sequences. Normally, to train a\nrecurrent neural network (RNN), labelled samples from a video (temporal)\nsequence are required which is laborious and has stymied work in this\ndirection. By generating virtual temporal samples, we demonstrate that it is\npossible to train a lightweight RNN to perform semantic segmentation on two\nchallenging agricultural datasets. Our results show that by training a temporal\nsemantic segmenter using virtual samples we can increase the performance by an\nabsolute amount of 4.6 and 4.9 on sweet pepper and sugar beet datasets,\nrespectively. This indicates that our virtual data augmentation technique is\nable to accurately classify agricultural images temporally without the use of\ncomplicated synthetic data generation techniques nor with the overhead of\nlabelling large amounts of temporal sequences.",
          "link": "http://arxiv.org/abs/2106.10118",
          "publishedOn": "2021-06-21T02:07:38.655Z",
          "wordCount": 616,
          "title": "Virtual Temporal Samples for Recurrent Neural Networks: applied to semantic segmentation in agriculture. (arXiv:2106.10118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chengwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaohui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1\">Ruizhi Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "Novelty detection is the process of determining whether a query example\ndiffers from the learned training distribution. Previous methods attempt to\nlearn the representation of the normal samples via generative adversarial\nnetworks (GANs). However, they will suffer from instability training, mode\ndropping, and low discriminative ability. Recently, various pretext tasks (e.g.\nrotation prediction and clustering) have been proposed for self-supervised\nlearning in novelty detection. However, the learned latent features are still\nlow discriminative. We overcome such problems by introducing a novel\ndecoder-encoder framework. Firstly, a generative network (a.k.a. decoder)\nlearns the representation by mapping the initialized latent vector to an image.\nIn particular, this vector is initialized by considering the entire\ndistribution of training data to avoid the problem of mode-dropping. Secondly,\na contrastive network (a.k.a. encoder) aims to ``learn to compare'' through\nmutual information estimation, which directly helps the generative network to\nobtain a more discriminative representation by using a negative data\naugmentation strategy. Extensive experiments show that our model has\nsignificant superiority over cutting-edge novelty detectors and achieves new\nstate-of-the-art results on some novelty detection benchmarks, e.g. CIFAR10 and\nDCASE. Moreover, our model is more stable for training in a non-adversarial\nmanner, compared to other adversarial based novelty detection methods.",
          "link": "http://arxiv.org/abs/2106.09958",
          "publishedOn": "2021-06-21T02:07:38.648Z",
          "wordCount": 655,
          "title": "Novelty Detection via Contrastive Learning with Negative Data Augmentation. (arXiv:2106.09958v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Nanqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggioni_M/0/1/0/all/0/1\">Matteo Maggioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1\">Eduardo P&#xe9;rez-Pellitero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>",
          "description": "The breakthrough of contrastive learning (CL) has fueled the recent success\nof self-supervised learning (SSL) in high-level vision tasks on RGB images.\nHowever, CL is still ill-defined for low-level vision tasks, such as joint\ndemosaicking and denoising (JDD), in the RAW domain. To bridge this\nmethodological gap, we present a novel CL approach on RAW images, residual\ncontrastive learning (RCL), which aims to learn meaningful representations for\nJDD. Our work is built on the assumption that noise contained in each RAW image\nis signal-dependent, thus two crops from the same RAW image should have more\nsimilar noise distribution than two crops from different RAW images. We use\nresiduals as a discriminative feature and the earth mover's distance to measure\nthe distribution divergence for the contrastive loss. To evaluate the proposed\nCL strategy, we simulate a series of unsupervised JDD experiments with\nlarge-scale data corrupted by synthetic signal-dependent noise, where we set a\nnew benchmark for unsupervised JDD tasks with unknown (random) noise variance.\nOur empirical study not only validates that CL can be applied on distributions\n(c.f. features), but also exposes the lack of robustness of previous non-ML and\nSSL JDD methods when the statistics of the noise are unknown, thus providing\nsome further insight into signal-dependent noise problems.",
          "link": "http://arxiv.org/abs/2106.10070",
          "publishedOn": "2021-06-21T02:07:38.640Z",
          "wordCount": 653,
          "title": "Residual Contrastive Learning for Joint Demosaicking and Denoising. (arXiv:2106.10070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1\">Jiahong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1\">Adolf Pfefferbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharchuk_G/0/1/0/all/0/1\">Greg Zaharchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M Pohl</a>",
          "description": "Longitudinal MRIs are often used to capture the gradual deterioration of\nbrain structure and function caused by aging or neurological diseases.\nAnalyzing this data via machine learning generally requires a large number of\nground-truth labels, which are often missing or expensive to obtain. Reducing\nthe need for labels, we propose a self-supervised strategy for representation\nlearning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts\nin contrastive learning, LNE explicitly models the similarity between\ntrajectory vectors across different subjects. We do so by building a graph in\neach training iteration defining neighborhoods in the latent space so that the\nprogression direction of a subject follows the direction of its neighbors. This\nresults in a smooth trajectory field that captures the global morphological\nchange of the brain while maintaining the local continuity. We apply LNE to\nlongitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274\nhealthy subjects, and Alzheimer's Disease Neuroimaging Initiative (ADNI,\nN=632). The visualization of the smooth trajectory vector field and superior\nperformance on downstream tasks demonstrate the strength of the proposed method\nover existing self-supervised methods in extracting information associated with\nnormal aging and in revealing the impact of neurodegenerative disorders. The\ncode is available at\n\\url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding.git}.",
          "link": "http://arxiv.org/abs/2103.03840",
          "publishedOn": "2021-06-21T02:07:38.633Z",
          "wordCount": 692,
          "title": "Self-Supervised Longitudinal Neighbourhood Embedding. (arXiv:2103.03840v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1\">Veronika A. Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly\nused to visualize and quantify left atrial (LA) scars. The position and extent\nof scars provide important information of the pathophysiology and progression\nof atrial fibrillation (AF). Hence, LA scar segmentation and quantification\nfrom LGE MRI can be useful in computer-assisted diagnosis and treatment\nstratification of AF patients. Since manual delineation can be time-consuming\nand subject to intra- and inter-expert variability, automating this computing\nis highly desired, which nevertheless is still challenging and\nunder-researched.\n\nThis paper aims to provide a systematic review on computing methods for LA\ncavity, wall, scar and ablation gap segmentation and quantification from LGE\nMRI, and the related literature for AF studies. Specifically, we first\nsummarize AF-related imaging techniques, particularly LGE MRI. Then, we review\nthe methodologies of the four computing tasks in detail, and summarize the\nvalidation strategies applied in each task. Finally, the possible future\ndevelopments are outlined, with a brief survey on the potential clinical\napplications of the aforementioned methods. The review shows that the research\ninto this topic is still in early stages. Although several methods have been\nproposed, especially for LA segmentation, there is still large scope for\nfurther algorithmic developments due to performance issues related to the high\nvariability of enhancement appearance and differences in image acquisition.",
          "link": "http://arxiv.org/abs/2106.09862",
          "publishedOn": "2021-06-21T02:07:38.626Z",
          "wordCount": 669,
          "title": "Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review. (arXiv:2106.09862v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yoojin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Khamy_M/0/1/0/all/0/1\">Mostafa El-Khamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungwon Lee</a>",
          "description": "This paper proposes two novel knowledge transfer techniques for\nclass-incremental learning (CIL). First, we propose data-free generative replay\n(DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples\nfrom a generative model. In the conventional generative replay, the generative\nmodel is pre-trained for old data and shared in extra memory for later\nincremental learning. In our proposed DF-GR, we train a generative model from\nscratch without using any training data, based on the pre-trained\nclassification model from the past, so we curtail the cost of sharing\npre-trained generative models. Second, we introduce dual-teacher information\ndistillation (DT-ID) for knowledge distillation from two teachers to one\nstudent. In CIL, we use DT-ID to learn new classes incrementally based on the\npre-trained model for old classes and another model (pre-)trained on the new\ndata for new classes. We implemented the proposed schemes on top of one of the\nstate-of-the-art CIL methods and showed the performance improvement on\nCIFAR-100 and ImageNet datasets.",
          "link": "http://arxiv.org/abs/2106.09835",
          "publishedOn": "2021-06-21T02:07:38.617Z",
          "wordCount": 612,
          "title": "Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay. (arXiv:2106.09835v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Toering_M/0/1/0/all/0/1\">Martine Toering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatopoulos_I/0/1/0/all/0/1\">Ioannis Gatopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stol_M/0/1/0/all/0/1\">Maarten Stol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1\">Vincent Tao Hu</a>",
          "description": "Instance-level contrastive learning techniques, which rely on data\naugmentation and a contrastive loss function, have found great success in the\ndomain of visual representation learning. They are not suitable for exploiting\nthe rich dynamical structure of video however, as operations are done on many\naugmented instances. In this paper we propose \"Video Cross-Stream Prototypical\nContrasting\", a novel method which predicts consistent prototype assignments\nfrom both RGB and optical flow views, operating on sets of samples.\nSpecifically, we alternate the optimization process; while optimizing one of\nthe streams, all views are mapped to one set of stream prototype vectors. Each\nof the assignments is predicted with all views except the one matching the\nprediction, pushing representations closer to their assigned prototypes. As a\nresult, more efficient video embeddings with ingrained motion information are\nlearned, without the explicit need for optical flow computation during\ninference. We obtain state-of-the-art results on nearest neighbour video\nretrieval and action recognition, outperforming previous best by +3.2% on\nUCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and\n+15.1% on HMDB51 using the R(2+1)D backbone.",
          "link": "http://arxiv.org/abs/2106.10137",
          "publishedOn": "2021-06-21T02:07:38.609Z",
          "wordCount": 619,
          "title": "Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting. (arXiv:2106.10137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2001.08026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stucker_C/0/1/0/all/0/1\">Corinne Stucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>",
          "description": "We propose an embarrassingly simple but very effective scheme for\nhigh-quality dense stereo reconstruction: (i) generate an approximate\nreconstruction with your favourite stereo matcher; (ii) rewarp the input images\nwith that approximate model; (iii) with the initial reconstruction and the\nwarped images as input, train a deep network to enhance the reconstruction by\nregressing a residual correction; and (iv) if desired, iterate the refinement\nwith the new, improved reconstruction. The strategy to only learn the residual\ngreatly simplifies the learning problem. A standard Unet without bells and\nwhistles is enough to reconstruct even small surface details, like dormers and\nroof substructures in satellite images. We also investigate residual\nreconstruction with less information and find that even a single image is\nenough to greatly improve an approximate reconstruction. Our full model reduces\nthe mean absolute error of state-of-the-art stereo reconstruction systems by\n>50%, both in our target domain of satellite stereo and on stereo pairs from\nthe ETH3D benchmark.",
          "link": "http://arxiv.org/abs/2001.08026",
          "publishedOn": "2021-06-21T02:07:38.601Z",
          "wordCount": 640,
          "title": "ResDepth: Learned Residual Stereo Reconstruction. (arXiv:2001.08026v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aboutalebi_H/0/1/0/all/0/1\">Hossein Aboutalebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1\">Mohammad Javad Shafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karg_M/0/1/0/all/0/1\">Michelle Karg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharfenberger_C/0/1/0/all/0/1\">Christian Scharfenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "Despite the significant advances in deep learning over the past decade, a\nmajor challenge that limits the wide-spread adoption of deep learning has been\ntheir fragility to adversarial attacks. This sensitivity to making erroneous\npredictions in the presence of adversarially perturbed data makes deep neural\nnetworks difficult to adopt for certain real-world, mission-critical\napplications. While much of the research focus has revolved around adversarial\nexample creation and adversarial hardening, the area of performance measures\nfor assessing adversarial robustness is not well explored. Motivated by this,\nthis study presents the concept of residual error, a new performance measure\nfor not only assessing the adversarial robustness of a deep neural network at\nthe individual sample level, but also can be used to differentiate between\nadversarial and non-adversarial examples to facilitate for adversarial example\ndetection. Furthermore, we introduce a hybrid model for approximating the\nresidual error in a tractable manner. Experimental results using the case of\nimage classification demonstrates the effectiveness and efficacy of the\nproposed residual error metric for assessing several well-known deep neural\nnetwork architectures. These results thus illustrate that the proposed measure\ncould be a useful tool for not only assessing the robustness of deep neural\nnetworks used in mission-critical scenarios, but also in the design of\nadversarially robust models.",
          "link": "http://arxiv.org/abs/2106.10212",
          "publishedOn": "2021-06-21T02:07:38.577Z",
          "wordCount": 657,
          "title": "Residual Error: a New Performance Measure for Adversarial Robustness. (arXiv:2106.10212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lijin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1\">Yusuke Sugano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>",
          "description": "In this report, we describe the technical details of our submission to the\n2021 EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action\nRecognition. Leveraging multiple modalities has been proved to benefit the\nUnsupervised Domain Adaptation (UDA) task. In this work, we present Multi-Modal\nMutual Enhancement Module (M3EM), a deep module for jointly considering\ninformation from multiple modalities to find the most transferable\nrepresentations across domains. We achieve this by implementing two sub-modules\nfor enhancing each modality using the context of other modalities. The first\nsub-module exchanges information across modalities through the semantic space,\nwhile the second sub-module finds the most transferable spatial region based on\nthe consensus of all modalities.",
          "link": "http://arxiv.org/abs/2106.10026",
          "publishedOn": "2021-06-21T02:07:38.549Z",
          "wordCount": 555,
          "title": "EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021: Team M3EM Technical Report. (arXiv:2106.10026v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Armandpour_M/0/1/0/all/0/1\">Mohammadreza Armandpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghian_A/0/1/0/all/0/1\">Ali Sadeghian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "Despite the success of Generative Adversarial Networks (GANs), their training\nsuffers from several well-known problems, including mode collapse and\ndifficulties learning a disconnected set of manifolds. In this paper, we break\ndown the challenging task of learning complex high dimensional distributions,\nsupporting diverse data samples, to simpler sub-tasks. Our solution relies on\ndesigning a partitioner that breaks the space into smaller regions, each having\na simpler distribution, and training a different generator for each partition.\nThis is done in an unsupervised manner without requiring any labels.\n\nWe formulate two desired criteria for the space partitioner that aid the\ntraining of our mixture of generators: 1) to produce connected partitions and\n2) provide a proxy of distance between partitions and data samples, along with\na direction for reducing that distance. These criteria are developed to avoid\nproducing samples from places with non-existent data density, and also\nfacilitate training by providing additional direction to the generators. We\ndevelop theoretical constraints for a space partitioner to satisfy the above\ncriteria. Guided by our theoretical analysis, we design an effective neural\narchitecture for the space partitioner that empirically assures these\nconditions. Experimental results on various standard benchmarks show that the\nproposed unsupervised model outperforms several recent methods.",
          "link": "http://arxiv.org/abs/2104.00816",
          "publishedOn": "2021-06-21T02:07:38.540Z",
          "wordCount": 662,
          "title": "Partition-Guided GANs. (arXiv:2104.00816v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Baoming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Ke Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Bo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ban_C/0/1/0/all/0/1\">Chao Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobo Li</a>",
          "description": "Video affective understanding, which aims to predict the evoked expressions\nby the video content, is desired for video creation and recommendation. In the\nrecent EEV challenge, a dense affective understanding task is proposed and\nrequires frame-level affective prediction. In this paper, we propose a\nmulti-granularity network with modal attention (MGN-MA), which employs\nmulti-granularity features for better description of the target frame.\nSpecifically, the multi-granularity features could be divided into frame-level,\nclips-level and video-level features, which corresponds to visual-salient\ncontent, semantic-context and video theme information. Then the modal attention\nfusion module is designed to fuse the multi-granularity features and emphasize\nmore affection-relevant modals. Finally, the fused feature is fed into a\nMixtures Of Experts (MOE) classifier to predict the expressions. Further\nemploying model-ensemble post-processing, the proposed method achieves the\ncorrelation score of 0.02292 in the EEV challenge.",
          "link": "http://arxiv.org/abs/2106.09964",
          "publishedOn": "2021-06-21T02:07:38.523Z",
          "wordCount": 591,
          "title": "Multi-Granularity Network with Modal Attention for Dense Affective Understanding. (arXiv:2106.09964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiquan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>",
          "description": "GasHisSDB is a New Gastric Histopathology Subsize Image Database with a total\nof 245196 images. GasHisSDB is divided into 160*160 pixels sub-database,\n120*120 pixels sub-database and 80*80 pixels sub-database. GasHisSDB is made to\nrealize the function of valuating image classification. In order to prove that\nthe methods of different periods in the field of image classification have\ndiscrepancies on GasHisSDB, we select a variety of classifiers for evaluation.\nSeven classical machine learning classifiers, three CNN classifiers and a novel\ntransformer-based classifier are selected for testing on image classification\ntasks. GasHisSDB is available at the\nURL:https://github.com/NEUhwm/GasHisSDB.git.",
          "link": "http://arxiv.org/abs/2106.02473",
          "publishedOn": "2021-06-21T02:07:38.505Z",
          "wordCount": 592,
          "title": "A New Gastric Histopathology Subsize Image Database (GasHisSDB) for Classification Algorithm Test: from Linear Regression to Visual Transformer. (arXiv:2106.02473v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lina Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yawei Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xuemei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "The vulnerability of deep neural networks to adversarial examples, which are\ncrafted maliciously by modifying the inputs with imperceptible perturbations to\nmisled the network produce incorrect outputs, reveals the lack of robustness\nand poses security concerns. Previous works study the adversarial robustness of\nimage classifiers on image level and use all the pixel information in an image\nindiscriminately, lacking of exploration of regions with different semantic\nmeanings in the pixel space of an image. In this work, we fill this gap and\nexplore the pixel space of the adversarial image by proposing an algorithm to\nlooking for possible perturbations pixel by pixel in different regions of the\nsegmented image. The extensive experimental results on CIFAR-10 and ImageNet\nverify that searching for the modified pixel in only some pixels of an image\ncan successfully launch the one-pixel adversarial attacks without requiring all\nthe pixels of the entire image, and there exist multiple vulnerable points\nscattered in different regions of an image. We also demonstrate that the\nadversarial robustness of different regions on the image varies with the amount\nof semantic information contained.",
          "link": "http://arxiv.org/abs/2106.09872",
          "publishedOn": "2021-06-21T02:07:38.492Z",
          "wordCount": 646,
          "title": "Analyzing Adversarial Robustness of Deep Neural Networks in Pixel Space: a Semantic Perspective. (arXiv:2106.09872v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jinlong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Deep learning has demonstrated its power in image rectification by leveraging\nthe representation capacity of deep neural networks via supervised training\nbased on a large-scale synthetic dataset. However, the model may overfit the\nsynthetic images and generalize not well on real-world fisheye images due to\nthe limited universality of a specific distortion model and the lack of\nexplicitly modeling the distortion and rectification process. In this paper, we\npropose a novel self-supervised image rectification (SIR) method based on an\nimportant insight that the rectified results of distorted images of a same\nscene from different lens should be the same. Specifically, we devise a new\nnetwork architecture with a shared encoder and several prediction heads, each\nof which predicts the distortion parameter of a specific distortion model. We\nfurther leverage a differentiable warping module to generate the rectified\nimages and re-distorted images from the distortion parameters and exploit the\nintra- and inter-model consistency between them during training, thereby\nleading to a self-supervised learning scheme without the need for ground-truth\ndistortion parameters or normal images. Experiments on synthetic dataset and\nreal-world fisheye images demonstrate that our method achieves comparable or\neven better performance than the supervised baseline method and representative\nstate-of-the-art methods. Self-supervised learning also improves the\nuniversality of distortion models while keeping their self-consistency.",
          "link": "http://arxiv.org/abs/2011.14611",
          "publishedOn": "2021-06-21T02:07:38.474Z",
          "wordCount": 691,
          "title": "SIR: Self-supervised Image Rectification via Seeing the Same Scene from Multiple Different Lenses. (arXiv:2011.14611v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1\">Edward Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Huaishao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1\">Taroon Bharti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1\">Arun Sacheti</a>",
          "description": "In this paper, we present GEM as a General Evaluation benchmark for\nMultimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,\nXGLUE and XTREME that mainly focus on natural language tasks, GEM is a\nlarge-scale vision-language benchmark, which consists of GEM-I for\nimage-language tasks and GEM-V for video-language tasks. Comparing with\nexisting multimodal datasets such as MSCOCO and Flicker30K for image-language\ntasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the\nlargest vision-language dataset covering image-language tasks and\nvideo-language tasks at the same time, but also labeled in multiple languages.\nWe also provide two baseline models for this benchmark. We will release the\ndataset, code and baseline models, aiming to advance the development of\nmultilingual multimodal research.",
          "link": "http://arxiv.org/abs/2106.09889",
          "publishedOn": "2021-06-21T02:07:38.455Z",
          "wordCount": 581,
          "title": "GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pena_A/0/1/0/all/0/1\">Alejandro Pe&#xf1;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1\">Ignacio Serna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1\">Agata Lapedriza</a>",
          "description": "This work explores facial expression bias as a security vulnerability of face\nrecognition systems. Despite the great performance achieved by state-of-the-art\nface recognition systems, the algorithms are still sensitive to a large range\nof covariates. We present a comprehensive analysis of how facial expression\nbias impacts the performance of face recognition technologies. Our study\nanalyzes: i) facial expression biases in the most popular face recognition\ndatabases; and ii) the impact of facial expression in face recognition\nperformances. Our experimental framework includes two face detectors, three\nface recognition models, and three different databases. Our results demonstrate\na huge facial expression bias in the most widely used databases, as well as a\nrelated impact of face expression in the performance of state-of-the-art\nalgorithms. This work opens the door to new research lines focused on\nmitigating the observed vulnerability.",
          "link": "http://arxiv.org/abs/2011.08809",
          "publishedOn": "2021-06-21T02:07:38.440Z",
          "wordCount": 611,
          "title": "Facial Expressions as a Vulnerability in Face Recognition. (arXiv:2011.08809v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ke_R/0/1/0/all/0/1\">Ruimin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhiyong Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Meixin Zhu</a>, Hao (Frank) <a href=\"http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1\">Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinhai Wang</a>",
          "description": "Traffic near-crash events serve as critical data sources for various smart\ntransportation applications, such as being surrogate safety measures for\ntraffic safety research and corner case data for automated vehicle testing.\nHowever, there are several key challenges for near-crash detection. First,\nextracting near-crashes from original data sources requires significant\ncomputing, communication, and storage resources. Also, existing methods lack\nefficiency and transferability, which bottlenecks prospective large-scale\napplications. To this end, this paper leverages the power of edge computing to\naddress these challenges by processing the video streams from existing dashcams\nonboard in a real-time manner. We design a multi-thread system architecture\nthat operates on edge devices and model the bounding boxes generated by object\ndetection and tracking in linear complexity. The method is insensitive to\ncamera parameters and backward compatible with different vehicles. The edge\ncomputing system has been evaluated with recorded videos and real-world tests\non two cars and four buses for over ten thousand hours. It filters out\nirrelevant videos in real-time thereby saving labor cost, processing time,\nnetwork bandwidth, and data storage. It collects not only event videos but also\nother valuable data such as road user type, event location, time to collision,\nvehicle trajectory, vehicle speed, brake switch, and throttle. The experiments\ndemonstrate the promising performance of the system regarding efficiency,\naccuracy, reliability, and transferability. It is among the first efforts in\napplying edge computing for real-time traffic video analytics and is expected\nto benefit multiple sub-fields in smart transportation research and\napplications.",
          "link": "http://arxiv.org/abs/2008.00549",
          "publishedOn": "2021-06-21T02:07:38.432Z",
          "wordCount": 724,
          "title": "Edge Computing for Real-Time Near-Crash Detection for Smart Transportation Applications. (arXiv:2008.00549v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.01210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1\">Mihir Prabhudesai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1\">Hsiao-Yu Fish Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Syed Ashar Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sieb_M/0/1/0/all/0/1\">Maximilian Sieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1\">Adam W. Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>",
          "description": "We propose associating language utterances to 3D visual abstractions of the\nscene they describe. The 3D visual abstractions are encoded as 3-dimensional\nvisual feature maps. We infer these 3D visual scene feature maps from RGB\nimages of the scene via view prediction: when the generated 3D scene feature\nmap is neurally projected from a camera viewpoint, it should match the\ncorresponding RGB image. We present generative models that condition on the\ndependency tree of an utterance and generate a corresponding visual 3D feature\nmap as well as reason about its plausibility, and detector models that\ncondition on both the dependency tree of an utterance and a related image and\nlocalize the object referents in the 3D feature map inferred from the image.\nOur model outperforms models of language and vision that associate language\nwith 2D CNN activations or 2D images by a large margin in a variety of tasks,\nsuch as, classifying plausibility of utterances, detecting referential\nexpressions, and supplying rewards for trajectory optimization of object\nplacement policies from language instructions. We perform numerous ablations\nand show the improved performance of our detectors is due to its better\ngeneralization across camera viewpoints and lack of object interferences in the\ninferred 3D feature space, and the improved performance of our generators is\ndue to their ability to spatially reason about objects and their configurations\nin 3D when mapping from language to scenes.",
          "link": "http://arxiv.org/abs/1910.01210",
          "publishedOn": "2021-06-21T02:07:38.413Z",
          "wordCount": 735,
          "title": "Embodied Language Grounding with 3D Visual Feature Representations. (arXiv:1910.01210v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Binnan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yunxiang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Langechuan Liu</a>",
          "description": "Radars and cameras are mature, cost-effective, and robust sensors and have\nbeen widely used in the perception stack of mass-produced autonomous driving\nsystems. Due to their complementary properties, outputs from radar detection\n(radar pins) and camera perception (2D bounding boxes) are usually fused to\ngenerate the best perception results. The key to successful radar-camera fusion\nis the accurate data association. The challenges in the radar-camera\nassociation can be attributed to the complexity of driving scenes, the noisy\nand sparse nature of radar measurements, and the depth ambiguity from 2D\nbounding boxes. Traditional rule-based association methods are susceptible to\nperformance degradation in challenging scenarios and failure in corner cases.\nIn this study, we propose to address radar-camera association via deep\nrepresentation learning, to explore feature-level interaction and global\nreasoning. Additionally, we design a loss sampling mechanism and an innovative\nordinal loss to overcome the difficulty of imperfect labeling and to enforce\ncritical human-like reasoning. Despite being trained with noisy labels\ngenerated by a rule-based algorithm, our proposed method achieves a performance\nof 92.2% F1 score, which is 11.6% higher than the rule-based teacher. Moreover,\nthis data-driven method also lends itself to continuous improvement via corner\ncase mining.",
          "link": "http://arxiv.org/abs/2103.07825",
          "publishedOn": "2021-06-21T02:07:38.394Z",
          "wordCount": 690,
          "title": "Radar Camera Fusion via Representation Learning in Autonomous Driving. (arXiv:2103.07825v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhijian Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Huanshu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suo_C/0/1/0/all/0/1\">Chuanzhe Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>",
          "description": "3D Point cloud registration is still a very challenging topic due to the\ndifficulty in finding the rigid transformation between two point clouds with\npartial correspondences, and it's even harder in the absence of any initial\nestimation information. In this paper, we present an end-to-end deep-learning\nbased approach to resolve the point cloud registration problem. Firstly, the\nrevised LPD-Net is introduced to extract features and aggregate them with the\ngraph network. Secondly, the self-attention mechanism is utilized to enhance\nthe structure information in the point cloud and the cross-attention mechanism\nis designed to enhance the corresponding information between the two input\npoint clouds. Based on which, the virtual corresponding points can be generated\nby a soft pointer based method, and finally, the point cloud registration\nproblem can be solved by implementing the SVD method. Comparison results in\nModelNet40 dataset validate that the proposed approach reaches the\nstate-of-the-art in point cloud registration tasks and experiment resutls in\nKITTI dataset validate the effectiveness of the proposed approach in real\napplications.Our source code is available at\n\\url{https://github.com/qiaozhijian/VCR-Net.git}",
          "link": "http://arxiv.org/abs/2011.14579",
          "publishedOn": "2021-06-21T02:07:38.383Z",
          "wordCount": 656,
          "title": "End-to-End 3D Point Cloud Learning for Registration Task Using Virtual Correspondences. (arXiv:2011.14579v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jenner_E/0/1/0/all/0/1\">Erik Jenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1\">Maurice Weiler</a>",
          "description": "Recent work in equivariant deep learning bears strong similarities to\nphysics. Fields over a base space are fundamental entities in both subjects, as\nare equivariant maps between these fields. In deep learning, however, these\nmaps are usually defined by convolutions with a kernel, whereas they are\npartial differential operators (PDOs) in physics. Developing the theory of\nequivariant PDOs in the context of deep learning could bring these subjects\neven closer together and lead to a stronger flow of ideas. In this work, we\nderive a $G$-steerability constraint that completely characterizes when a PDO\nbetween feature vector fields is equivariant, for arbitrary symmetry groups\n$G$. We then fully solve this constraint for several important groups. We use\nour solutions as equivariant drop-in replacements for convolutional layers and\nbenchmark them in that role. Finally, we develop a framework for equivariant\nmaps based on Schwartz distributions that unifies classical convolutions and\ndifferential operators and gives insight about the relation between the two.",
          "link": "http://arxiv.org/abs/2106.10163",
          "publishedOn": "2021-06-21T02:07:38.376Z",
          "wordCount": 603,
          "title": "Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guangchun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Finding a suitable data representation for a specific task has been shown to\nbe crucial in many applications. The success of subspace clustering depends on\nthe assumption that the data can be separated into different subspaces.\nHowever, this simple assumption does not always hold since the raw data might\nnot be separable into subspaces. To recover the ``clustering-friendly''\nrepresentation and facilitate the subsequent clustering, we propose a graph\nfiltering approach by which a smooth representation is achieved. Specifically,\nit injects graph similarity into data features by applying a low-pass filter to\nextract useful data representations for clustering. Extensive experiments on\nimage and document clustering datasets demonstrate that our method improves\nupon state-of-the-art subspace clustering techniques. Especially, its\ncomparable performance with deep learning methods emphasizes the effectiveness\nof the simple graph filtering scheme for many real-world applications. An\nablation study shows that graph filtering can remove noise, preserve structure\nin the image, and increase the separability of classes.",
          "link": "http://arxiv.org/abs/2106.09874",
          "publishedOn": "2021-06-21T02:07:38.368Z",
          "wordCount": 606,
          "title": "Towards Clustering-friendly Representations: Subspace Clustering via Graph Filtering. (arXiv:2106.09874v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Biao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guocheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chingyao Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>",
          "description": "Pedestrian trajectory prediction in dynamic scenes remains a challenging and\ncritical problem in numerous applications, such as self-driving cars and\nsocially aware robots. Challenges concentrate on capturing pedestrians' motion\npatterns and social interactions, as well as handling the future uncertainties.\nRecent studies focus on modeling pedestrians' motion patterns with recurrent\nneural networks, capturing social interactions with pooling-based or\ngraph-based methods, and handling future uncertainties by using random Gaussian\nnoise as the latent variable. However, they do not integrate specific obstacle\navoidance experience (OAE) that may improve prediction performance. For\nexample, pedestrians' future trajectories are always influenced by others in\nfront. Here we propose GTPPO (Graph-based Trajectory Predictor with Pseudo\nOracle), an encoder-decoder-based method conditioned on pedestrians' future\nbehaviors. Pedestrians' motion patterns are encoded with a long short-term\nmemory unit, which introduces the temporal attention to highlight specific time\nsteps. Their interactions are captured by a graph-based attention mechanism,\nwhich draws OAE into the data-driven learning process of graph attention.\nFuture uncertainties are handled by generating multi-modal outputs with an\ninformative latent variable. Such a variable is generated by a novel pseudo\noracle predictor, which minimizes the knowledge gap between historical and\nground-truth trajectories. Finally, the GTPPO is evaluated on ETH, UCY and\nStanford Drone datasets, and the results demonstrate state-of-the-art\nperformance. Besides, the qualitative evaluations show successful cases of\nhandling sudden motion changes in the future. Such findings indicate that GTPPO\ncan peek into the future.",
          "link": "http://arxiv.org/abs/2002.00391",
          "publishedOn": "2021-06-21T02:07:38.351Z",
          "wordCount": 721,
          "title": "A Novel Graph based Trajectory Predictor with Pseudo Oracle. (arXiv:2002.00391v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianzhong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuaijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>",
          "description": "Multi-source unsupervised domain adaptation~(MSDA) aims at adapting models\ntrained on multiple labeled source domains to an unlabeled target domain. In\nthis paper, we propose a novel multi-source domain adaptation framework based\non collaborative learning for semantic segmentation. Firstly, a simple image\ntranslation method is introduced to align the pixel value distribution to\nreduce the gap between source domains and target domain to some extent. Then,\nto fully exploit the essential semantic information across source domains, we\npropose a collaborative learning method for domain adaptation without seeing\nany data from target domain. In addition, similar to the setting of\nunsupervised domain adaptation, unlabeled target domain data is leveraged to\nfurther improve the performance of domain adaptation. This is achieved by\nadditionally constraining the outputs of multiple adaptation models with pseudo\nlabels online generated by an ensembled model. Extensive experiments and\nablation studies are conducted on the widely-used domain adaptation benchmark\ndatasets in semantic segmentation. Our proposed method achieves 59.0\\% mIoU on\nthe validation set of Cityscapes by training on the labeled Synscapes and GTA5\ndatasets and unlabeled training set of Cityscapes. It significantly outperforms\nall previous state-of-the-arts single-source and multi-source unsupervised\ndomain adaptation methods.",
          "link": "http://arxiv.org/abs/2103.04717",
          "publishedOn": "2021-06-21T02:07:38.343Z",
          "wordCount": 670,
          "title": "Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation. (arXiv:2103.04717v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "In adversarial training (AT), the main focus has been the objective and\noptimizer while the model has been less studied, so that the models being used\nare still those classic ones in standard training (ST). Classic network\narchitectures (NAs) are generally worse than searched NAs in ST, which should\nbe the same in AT. In this paper, we argue that NA and AT cannot be handled\nindependently, since given a dataset, the optimal NA in ST would be no longer\noptimal in AT. That being said, AT is time-consuming itself; if we directly\nsearch NAs in AT over large search spaces, the computation will be practically\ninfeasible. Thus, we propose a diverse-structured network (DS-Net), to\nsignificantly reduce the size of the search space: instead of low-level\noperations, we only consider predefined atomic blocks, where an atomic block is\na time-tested building block like the residual block. There are only a few\natomic blocks and thus we can weight all atomic blocks rather than find the\nbest one in a searched block of DS-Net, which is an essential trade-off between\nexploring diverse structures and exploiting the best structures. Empirical\nresults demonstrate the advantages of DS-Net, i.e., weighting the atomic\nblocks.",
          "link": "http://arxiv.org/abs/2102.01886",
          "publishedOn": "2021-06-21T02:07:38.336Z",
          "wordCount": 695,
          "title": "Learning Diverse-Structured Networks for Adversarial Robustness. (arXiv:2102.01886v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10230",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mahapatra_D/0/1/0/all/0/1\">Dwarikanath Mahapatra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_A/0/1/0/all/0/1\">Ankur Singh</a>",
          "description": "While medical image segmentation is an important task for computer aided\ndiagnosis, the high expertise requirement for pixelwise manual annotations\nmakes it a challenging and time consuming task. Since conventional data\naugmentations do not fully represent the underlying distribution of the\ntraining set, the trained models have varying performance when tested on images\ncaptured from different sources. Most prior work on image synthesis for data\naugmentation ignore the interleaved geometric relationship between different\nanatomical labels. We propose improvements over previous GAN-based medical\nimage synthesis methods by learning the relationship between different\nanatomical labels. We use a weakly supervised segmentation method to obtain\npixel level semantic label map of images which is used learn the intrinsic\nrelationship of geometry and shape across semantic labels. Latent space\nvariable sampling results in diverse generated images from a base image and\nimproves robustness. We use the synthetic images from our method to train\nnetworks for segmenting COVID-19 infected areas from lung CT images. The\nproposed method outperforms state-of-the-art segmentation methods on a public\ndataset. Ablation studies also demonstrate benefits of integrating geometry and\ndiversity.",
          "link": "http://arxiv.org/abs/2106.10230",
          "publishedOn": "2021-06-21T02:07:38.328Z",
          "wordCount": 697,
          "title": "CT Image Synthesis Using Weakly Supervised Segmentation and Geometric Inter-Label Relations For COVID Image Analysis. (arXiv:2106.10230v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sixiang_T/0/1/0/all/0/1\">Tan Sixiang</a>",
          "description": "For real-time semantic segmentation, how to increase the speed while\nmaintaining high resolution is a problem that has been discussed and solved.\nBackbone design and fusion design have always been two essential parts of\nreal-time semantic segmentation. We hope to design a light-weight network based\non previous design experience and reach the level of state-of-the-art real-time\nsemantic segmentation without any pre-training. To achieve this goal, a\nencoder-decoder architectures are proposed to solve this problem by applying a\ndecoder network onto a backbone model designed for real-time segmentation tasks\nand designed three different ways to fuse semantics and detailed information in\nthe aggregation phase. We have conducted extensive experiments on two semantic\nsegmentation benchmarks. Experiments on the Cityscapes and CamVid datasets show\nthat the proposed FRFNet strikes a balance between speed calculation and\naccuracy. It achieves 72% Mean Intersection over Union (mIoU%) on the\nCityscapes test dataset with the speed of 144 on a single RTX 1080Ti card. The\nCode is available at https://github.com/favoMJ/FRFNet.",
          "link": "http://arxiv.org/abs/2105.12964",
          "publishedOn": "2021-06-21T02:07:38.308Z",
          "wordCount": 632,
          "title": "Feature Reuse and Fusion for Real-time Semantic segmentation. (arXiv:2105.12964v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09624",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Grohl_J/0/1/0/all/0/1\">Janek Gr&#xf6;hl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1\">Melanie Schellenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dreher_K/0/1/0/all/0/1\">Kris Dreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Holzwarth_N/0/1/0/all/0/1\">Niklas Holzwarth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seitel_A/0/1/0/all/0/1\">Alexander Seitel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>",
          "description": "Photoacoustic imaging has the potential to revolutionise healthcare due to\nthe valuable information on tissue physiology that is contained in\nmultispectral photoacoustic measurements. Clinical translation of the\ntechnology requires conversion of the high-dimensional acquired data into\nclinically relevant and interpretable information. In this work, we present a\ndeep learning-based approach to semantic segmentation of multispectral\nphotoacoustic images to facilitate the interpretability of recorded images.\nManually annotated multispectral photoacoustic imaging data are used as gold\nstandard reference annotations and enable the training of a deep learning-based\nsegmentation algorithm in a supervised manner. Based on a validation study with\nexperimentally acquired data of healthy human volunteers, we show that\nautomatic tissue segmentation can be used to create powerful analyses and\nvisualisations of multispectral photoacoustic images. Due to the intuitive\nrepresentation of high-dimensional information, such a processing algorithm\ncould be a valuable means to facilitate the clinical translation of\nphotoacoustic imaging.",
          "link": "http://arxiv.org/abs/2105.09624",
          "publishedOn": "2021-06-21T02:07:38.300Z",
          "wordCount": 631,
          "title": "Semantic segmentation of multispectral photoacoustic images using deep learning. (arXiv:2105.09624v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaozheng Yin</a>",
          "description": "Recently, autonomous vehicles and those equipped with an Advanced Driver\nAssistance System (ADAS) are emerging. They share the road with regular ones\noperated by human drivers entirely. To ensure guaranteed safety for passengers\nand other road users, it becomes essential for autonomous vehicles and ADAS to\nanticipate traffic accidents from natural driving scenes. The dynamic\nspatial-temporal interaction of the traffic agents is complex, and visual cues\nfor predicting a future accident are embedded deeply in dashcam video data.\nTherefore, early anticipation of traffic accidents remains a challenge. To this\nend, the paper presents a dynamic spatial-temporal attention (DSTA) network for\nearly anticipation of traffic accidents from dashcam videos. The proposed\nDSTA-network learns to select discriminative temporal segments of a video\nsequence with a module named Dynamic Temporal Attention (DTA). It also learns\nto focus on the informative spatial regions of frames with another module named\nDynamic Spatial Attention (DSA). The spatial-temporal relational features of\naccidents, along with scene appearance features, are learned jointly with a\nGated Recurrent Unit (GRU) network. The experimental evaluation of the\nDSTA-network on two benchmark datasets confirms that it has exceeded the\nstate-of-the-art performance. A thorough ablation study evaluates the\ncontributions of individual components of the DSTA-network, revealing how the\nnetwork achieves such performance. Furthermore, this paper proposes a new\nstrategy that fuses the prediction scores from two complementary models and\nverifies its effectiveness in further boosting the performance of early\naccident anticipation.",
          "link": "http://arxiv.org/abs/2106.10197",
          "publishedOn": "2021-06-21T02:07:38.292Z",
          "wordCount": 693,
          "title": "A Dynamic Spatial-temporal Attention Network for Early Anticipation of Traffic Accidents. (arXiv:2106.10197v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.08482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We present a plug-in replacement for batch normalization (BN) called\nexponential moving average normalization (EMAN), which improves the performance\nof existing student-teacher based self- and semi-supervised learning\ntechniques. Unlike the standard BN, where the statistics are computed within\neach batch, EMAN, used in the teacher, updates its statistics by exponential\nmoving average from the BN statistics of the student. This design reduces the\nintrinsic cross-sample dependency of BN and enhances the generalization of the\nteacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2\npoints and semi-supervised learning by about 7/2 points, when 1%/10% supervised\nlabels are available on ImageNet. These improvements are consistent across\nmethods, network architectures, training duration, and datasets, demonstrating\nthe general effectiveness of this technique. The code is available at\nhttps://github.com/amazon-research/exponential-moving-average-normalization.",
          "link": "http://arxiv.org/abs/2101.08482",
          "publishedOn": "2021-06-21T02:07:38.285Z",
          "wordCount": 614,
          "title": "Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning. (arXiv:2101.08482v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zickler_T/0/1/0/all/0/1\">Todd Zickler</a>",
          "description": "Localizing stereo boundaries is difficult because matching cues are absent in\nthe occluded regions that are adjacent to them. We introduce an energy and\nlevel-set optimizer that improves boundaries by encoding the essential geometry\nof occlusions: The spatial extent of an occlusion must equal the amplitude of\nthe disparity jump that causes it. In a collection of figure-ground scenes from\nMiddlebury and Falling Things stereo datasets, the model provides more accurate\nboundaries than previous occlusion-handling techniques.",
          "link": "http://arxiv.org/abs/2006.16094",
          "publishedOn": "2021-06-21T02:07:38.277Z",
          "wordCount": 554,
          "title": "Level Set Stereo for Cooperative Grouping with Occlusion. (arXiv:2006.16094v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1\">Ayush K Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1\">Tarun Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietlmeier_J/0/1/0/all/0/1\">Julia Dietlmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1\">Alan F Smeaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E O&#x27;Connor</a>",
          "description": "Detecting generic, taxonomy-free event boundaries invideos represents a major\nstride forward towards holisticvideo understanding. In this paper we present a\ntechnique forgeneric event boundary detection based on a two stream in-flated\n3D convolutions architecture, which can learn spatio-temporal features from\nvideos. Our work is inspired from theGeneric Event Boundary Detection Challenge\n(part of CVPR2021 Long Form Video Understanding- LOVEU Workshop).Throughout the\npaper we provide an in-depth analysis ofthe experiments performed along with an\ninterpretation ofthe results obtained.",
          "link": "http://arxiv.org/abs/2106.10090",
          "publishedOn": "2021-06-21T02:07:38.258Z",
          "wordCount": 537,
          "title": "Discerning Generic Event Boundaries in Long-Form Wild Videos. (arXiv:2106.10090v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10195",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Uelwer_T/0/1/0/all/0/1\">Tobias Uelwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_T/0/1/0/all/0/1\">Tobias Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harmeling_S/0/1/0/all/0/1\">Stefan Harmeling</a>",
          "description": "Fourier phase retrieval is the problem of reconstructing a signal given only\nthe magnitude of its Fourier transformation. Optimization-based approaches,\nlike the well-established Gerchberg-Saxton or the hybrid input output\nalgorithm, struggle at reconstructing images from magnitudes that are not\noversampled. This motivates the application of learned methods, which allow\nreconstruction from non-oversampled magnitude measurements after a learning\nphase. In this paper, we want to push the limits of these learned methods by\nmeans of a deep neural network cascade that reconstructs the image successively\non different resolutions from its non-oversampled Fourier magnitude. We\nevaluate our method on four different datasets (MNIST, EMNIST, Fashion-MNIST,\nand KMNIST) and demonstrate that it yields improved performance over other\nnon-iterative methods and optimization-based methods.",
          "link": "http://arxiv.org/abs/2106.10195",
          "publishedOn": "2021-06-21T02:07:38.251Z",
          "wordCount": 573,
          "title": "Non-Iterative Phase Retrieval With Cascaded Neural Networks. (arXiv:2106.10195v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.00856",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sarker_M/0/1/0/all/0/1\">Md. Mostafa Kamal Sarker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rashwan_H/0/1/0/all/0/1\">Hatem A. Rashwan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akram_F/0/1/0/all/0/1\">Farhan Akram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_V/0/1/0/all/0/1\">Vivek Kumar Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Banu_S/0/1/0/all/0/1\">Syeda Furruka Banu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_F/0/1/0/all/0/1\">Forhad U H Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choudhury_K/0/1/0/all/0/1\">Kabir Ahmed Choudhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chambon_S/0/1/0/all/0/1\">Sylvie Chambon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Radeva_P/0/1/0/all/0/1\">Petia Radeva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puig_D/0/1/0/all/0/1\">Domenec Puig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdel_Nasser_M/0/1/0/all/0/1\">Mohamed Abdel-Nasser</a>",
          "description": "The determination of precise skin lesion boundaries in dermoscopic images\nusing automated methods faces many challenges, most importantly, the presence\nof hair, inconspicuous lesion edges and low contrast in dermoscopic images, and\nvariability in the color, texture and shapes of skin lesions. Existing deep\nlearning-based skin lesion segmentation algorithms are expensive in terms of\ncomputational time and memory. Consequently, running such segmentation\nalgorithms requires a powerful GPU and high bandwidth memory, which are not\navailable in dermoscopy devices. Thus, this article aims to achieve precise\nskin lesion segmentation with minimum resources: a lightweight, efficient\ngenerative adversarial network (GAN) model called SLSNet, which combines 1-D\nkernel factorized networks, position and channel attention, and multiscale\naggregation mechanisms with a GAN model. The 1-D kernel factorized network\nreduces the computational cost of 2D filtering. The position and channel\nattention modules enhance the discriminative ability between the lesion and\nnon-lesion feature representations in spatial and channel dimensions,\nrespectively. A multiscale block is also used to aggregate the coarse-to-fine\nfeatures of input skin images and reduce the effect of the artifacts. SLSNet is\nevaluated on two publicly available datasets: ISBI 2017 and the ISIC 2018.\nAlthough SLSNet has only 2.35 million parameters, the experimental results\ndemonstrate that it achieves segmentation results on a par with the\nstate-of-the-art skin lesion segmentation methods with an accuracy of 97.61%,\nand Dice and Jaccard similarity coefficients of 90.63% and 81.98%,\nrespectively. SLSNet can run at more than 110 frames per second (FPS) in a\nsingle GTX1080Ti GPU, which is faster than well-known deep learning-based image\nsegmentation models, such as FCN. Therefore, SLSNet can be used for practical\ndermoscopic applications.",
          "link": "http://arxiv.org/abs/1907.00856",
          "publishedOn": "2021-06-21T02:07:38.240Z",
          "wordCount": 776,
          "title": "SLSNet: Skin lesion segmentation using a lightweight generative adversarial network. (arXiv:1907.00856v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lucas Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1\">Bistra Dilkina</a>",
          "description": "Recent work has shown that deep learning models can be used to classify\nland-use data from geospatial satellite imagery. We show that when these deep\nlearning models are trained on data from specific continents/seasons, there is\na high degree of variability in model performance on out-of-sample\ncontinents/seasons. This suggests that just because a model accurately predicts\nland-use classes in one continent or season does not mean that the model will\naccurately predict land-use classes in a different continent or season. We then\nuse clustering techniques on satellite imagery from different continents to\nvisualize the differences in landscapes that make geospatial generalization\nparticularly difficult, and summarize our takeaways for future satellite\nimagery-related applications.",
          "link": "http://arxiv.org/abs/2008.10351",
          "publishedOn": "2021-06-21T02:07:38.212Z",
          "wordCount": 606,
          "title": "Model Generalization in Deep Learning Applications for Land Cover Mapping. (arXiv:2008.10351v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaolin Wu</a>",
          "description": "Nighttime photographers are often troubled by light pollution of unwanted\nartificial lights. Artificial lights, after scattered by aerosols in the\natmosphere, can inundate the starlight and degrade the quality of nighttime\nimages, by reducing contrast and dynamic range and causing hazes. In this paper\nwe develop a physically-based light pollution reduction (LPR) algorithm that\ncan substantially alleviate the aforementioned degradations of perceptual\nquality and restore the pristine state of night sky. The key to the success of\nthe proposed LPR algorithm is an inverse method to estimate the spatial\nradiance distribution and spectral signature of ground artificial lights.\nExtensive experiments are carried out to evaluate the efficacy and limitations\nof the LPR algorithm.",
          "link": "http://arxiv.org/abs/2106.10046",
          "publishedOn": "2021-06-21T02:07:38.197Z",
          "wordCount": 550,
          "title": "Light Pollution Reduction in Nighttime Photography. (arXiv:2106.10046v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuecheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>",
          "description": "Place recognition is critical for both offline mapping and online\nlocalization. However, current single-sensor based place recognition still\nremains challenging in adverse conditions. In this paper, a heterogeneous\nmeasurements based framework is proposed for long-term place recognition, which\nretrieves the query radar scans from the existing lidar maps. To achieve this,\na deep neural network is built with joint training in the learning stage, and\nthen in the testing stage, shared embeddings of radar and lidar are extracted\nfor heterogeneous place recognition. To validate the effectiveness of the\nproposed method, we conduct tests and generalization experiments on the\nmulti-session public datasets compared to other competitive methods. The\nexperimental results indicate that our model is able to perform multiple place\nrecognitions: lidar-to-lidar, radar-to-radar and radar-to-lidar, while the\nlearned model is trained only once. We also release the source code publicly:\nhttps://github.com/ZJUYH/radar-to-lidar-place-recognition.",
          "link": "http://arxiv.org/abs/2102.04960",
          "publishedOn": "2021-06-21T02:07:38.175Z",
          "wordCount": 636,
          "title": "Radar-to-Lidar: Heterogeneous Place Recognition via Joint Learning. (arXiv:2102.04960v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Fenglei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>",
          "description": "Despite its best performance in image denoising, the supervised deep\ndenoising methods require paired noise-clean data, which are often unavailable.\nTo address this challenge, Noise2Noise was designed based on the fact that\npaired noise-clean images can be replaced by paired noise-noise images that are\neasier to collect. However, in many scenarios the collection of paired\nnoise-noise images is still impractical. To bypass labeled images, Noise2Void\nmethods predict masked pixels from their surroundings with single noisy images\nonly and give improved denoising results that still need improvements. An\nobservation on classic denoising methods is that non-local mean (NLM) outcomes\nare typically superior to locally denoised results. In contrast, Noise2Void and\nits variants do not utilize self-similarities in an image as the NLM-based\nmethods do. Here we propose Noise2Sim, an NLM-inspired self-learning method for\nimage denoising. Specifically, Noise2Sim leverages the self-similarity of image\npixels to train the denoising network, requiring single noisy images only. Our\ntheoretical analysis shows that Noise2Sim tends to be equivalent to Noise2Noise\nunder mild conditions. To efficiently manage the computational burden for\nglobally searching similar pixels, we design a two-step procedure to provide\ndata for Noise2Sim training. Extensive experiments demonstrate the superiority\nof Noise2Sim on common benchmark datasets.",
          "link": "http://arxiv.org/abs/2011.03384",
          "publishedOn": "2021-06-21T02:07:38.167Z",
          "wordCount": 687,
          "title": "Noise2Sim -- Similarity-based Self-Learning for Image Denoising. (arXiv:2011.03384v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Byerly_A/0/1/0/all/0/1\">Adam Byerly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalganova_T/0/1/0/all/0/1\">Tatiana Kalganova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dear_I/0/1/0/all/0/1\">Ian Dear</a>",
          "description": "Most capsule network designs rely on traditional matrix multiplication\nbetween capsule layers and computationally expensive routing mechanisms to deal\nwith the capsule dimensional entanglement that the matrix multiplication\nintroduces. By using Homogeneous Vector Capsules (HVCs), which use element-wise\nmultiplication rather than matrix multiplication, the dimensions of the\ncapsules remain unentangled. In this work, we study HVCs as applied to the\nhighly structured MNIST dataset in order to produce a direct comparison to the\ncapsule research direction of Geoffrey Hinton, et al. In our study, we show\nthat a simple convolutional neural network using HVCs performs as well as the\nprior best performing capsule network on MNIST using 5.5x fewer parameters, 4x\nfewer training epochs, no reconstruction sub-network, and requiring no routing\nmechanism. The addition of multiple classification branches to the network\nestablishes a new state of the art for the MNIST dataset with an accuracy of\n99.87% for an ensemble of these models, as well as establishing a new state of\nthe art for a single model (99.83% accurate).",
          "link": "http://arxiv.org/abs/2001.09136",
          "publishedOn": "2021-06-21T02:07:38.160Z",
          "wordCount": 671,
          "title": "No Routing Needed Between Capsules. (arXiv:2001.09136v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10044",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gandikota_K/0/1/0/all/0/1\">Kanchana Vaishnavi Gandikota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1\">Zorah L&#xe4;hner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czaplinski_A/0/1/0/all/0/1\">Adam Czapli&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>",
          "description": "Many applications require the robustness, or ideally the invariance, of a\nneural network to certain transformations of input data. Most commonly, this\nrequirement is addressed by either augmenting the training data, using\nadversarial training, or defining network architectures that include the\ndesired invariance automatically. Unfortunately, the latter often relies on the\nability to enlist all possible transformations, which make such approaches\nlargely infeasible for infinite sets of transformations, such as arbitrary\nrotations or scaling. In this work, we propose a method for provably invariant\nnetwork architectures with respect to group actions by choosing one element\nfrom a (possibly continuous) orbit based on a fixed criterion. In a nutshell,\nwe intend to 'undo' any possible transformation before feeding the data into\nthe actual network. We analyze properties of such approaches, extend them to\nequivariant networks, and demonstrate their advantages in terms of robustness\nas well as computational efficiency in several numerical examples. In\nparticular, we investigate the robustness with respect to rotations of images\n(which can possibly hold up to discretization artifacts only) as well as the\nprovable rotational and scaling invariance of 3D point cloud classification.",
          "link": "http://arxiv.org/abs/2106.10044",
          "publishedOn": "2021-06-21T02:07:38.153Z",
          "wordCount": 630,
          "title": "Training or Architecture? How to Incorporate Invariance in Neural Networks. (arXiv:2106.10044v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fornoni_M/0/1/0/all/0/1\">Marco Fornoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chaochao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Liangchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1\">Kimberly Wilber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stark_A/0/1/0/all/0/1\">Alex Stark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1\">Andrew Howard</a>",
          "description": "When interacting with objects through cameras, or pictures, users often have\na specific intent. For example, they may want to perform a visual search.\nHowever, most object detection models ignore the user intent, relying on image\npixels as their only input. This often leads to incorrect results, such as lack\nof a high-confidence detection on the object of interest, or detection with a\nwrong class label. In this paper we investigate techniques to modulate standard\nobject detectors to explicitly account for the user intent, expressed as an\nembedding of a simple query. Compared to standard object detectors,\nquery-modulated detectors show superior performance at detecting objects for a\ngiven label of interest. Thanks to large-scale training data synthesized from\nstandard object detection annotations, query-modulated detectors can also\noutperform specialized referring expression recognition systems. Furthermore,\nthey can be simultaneously trained to solve for both query-modulated detection\nand standard object detection.",
          "link": "http://arxiv.org/abs/2106.10258",
          "publishedOn": "2021-06-21T02:07:38.146Z",
          "wordCount": 606,
          "title": "Bridging the Gap Between Object Detection and User Intent via Query-Modulation. (arXiv:2106.10258v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivashechkin_M/0/1/0/all/0/1\">Maksym Ivashechkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1\">Daniel Barath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>",
          "description": "We present VSAC, a RANSAC-type robust estimator with a number of novelties.\nIt benefits from the introduction of the concept of independent inliers that\nimproves significantly the efficacy of the dominant plane handling and, also,\nallows near error-free rejection of incorrect models, without false positives.\nThe local optimization process and its application is improved so that it is\nrun on average only once. Further technical improvements include adaptive\nsequential hypothesis verification and efficient model estimation via Gaussian\nelimination. Experiments on four standard datasets show that VSAC is\nsignificantly faster than all its predecessors and runs on average in 1-2 ms,\non a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++,\nthe currently most accurate estimator of two-view geometry. In the repeated\nruns on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.",
          "link": "http://arxiv.org/abs/2106.10240",
          "publishedOn": "2021-06-21T02:07:38.126Z",
          "wordCount": 573,
          "title": "VSAC: Efficient and Accurate Estimator for H and F. (arXiv:2106.10240v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Ci Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_N/0/1/0/all/0/1\">Nima Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1\">Sofia Broom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Maheen Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernlund_E/0/1/0/all/0/1\">Elin Hernlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuffi_S/0/1/0/all/0/1\">Silvia Zuffi</a>",
          "description": "In this paper we present our preliminary work on model-based behavioral\nanalysis of horse motion. Our approach is based on the SMAL model, a 3D\narticulated statistical model of animal shape. We define a novel SMAL model for\nhorses based on a new template, skeleton and shape space learned from $37$\nhorse toys. We test the accuracy of our hSMAL model in reconstructing a horse\nfrom 3D mocap data and images. We apply the hSMAL model to the problem of\nlameness detection from video, where we fit the model to images to recover 3D\npose and train an ST-GCN network on pose data. A comparison with the same\nnetwork trained on mocap points illustrates the benefit of our approach.",
          "link": "http://arxiv.org/abs/2106.10102",
          "publishedOn": "2021-06-21T02:07:38.111Z",
          "wordCount": 576,
          "title": "hSMAL: Detailed Horse Shape and Pose Reconstruction for Motion Pattern Recognition. (arXiv:2106.10102v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Bin-Bin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>",
          "description": "Boundary-based instance segmentation has drawn much attention since of its\nattractive efficiency. However, existing methods suffer from the difficulty in\nlong-distance regression. In this paper, we propose a coarse-to-fine module to\naddress the problem. Approximate boundary points are generated at the coarse\nstage and then features of these points are sampled and fed to a refined\nregressor for fine prediction. It is end-to-end trainable since differential\nsampling operation is well supported in the module. Furthermore, we design a\nholistic boundary-aware branch and introduce instance-agnostic supervision to\nassist regression. Equipped with ResNet-101, our approach achieves 31.7\\% mask\nAP on COCO dataset with single-scale training and testing, outperforming the\nbaseline 1.3\\% mask AP with less than 1\\% additional parameters and GFLOPs.\nExperiments also show that our proposed method achieves competitive performance\ncompared to existing boundary-based methods with a lightweight design and a\nsimple pipeline.",
          "link": "http://arxiv.org/abs/2106.10213",
          "publishedOn": "2021-06-21T02:07:38.104Z",
          "wordCount": 591,
          "title": "A Coarse-to-Fine Instance Segmentation Network with Learning Boundary Representation. (arXiv:2106.10213v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10080",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhangyang_C/0/1/0/all/0/1\">Cao Peibei. Wang Zhangyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kede_M/0/1/0/all/0/1\">Ma Kede</a>",
          "description": "In real-world image enhancement, it is often challenging (if not impossible)\nto acquire ground-truth data, preventing the adoption of distance metrics for\nobjective quality assessment. As a result, one often resorts to subjective\nquality assessment, the most straightforward and reliable means of evaluating\nimage enhancement. Conventional subjective testing requires manually\npre-selecting a small set of visual examples, which may suffer from three\nsources of biases: 1) sampling bias due to the extremely sparse distribution of\nthe selected samples in the image space; 2) algorithmic bias due to potential\noverfitting the selected samples; 3) subjective bias due to further potential\ncherry-picking test results. This eventually makes the field of real-world\nimage enhancement more of an art than a science. Here we take steps towards\ndebiasing conventional subjective assessment by automatically sampling a set of\nadaptive and diverse images for subsequent testing. This is achieved by casting\nsample selection into a joint maximization of the discrepancy between the\nenhancers and the diversity among the selected input images. Careful visual\ninspection on the resulting enhanced images provides a debiased ranking of the\nenhancement algorithms. We demonstrate our subjective assessment method using\nthree popular and practically demanding image enhancement tasks: dehazing,\nsuper-resolution, and low-light enhancement.",
          "link": "http://arxiv.org/abs/2106.10080",
          "publishedOn": "2021-06-21T02:07:38.096Z",
          "wordCount": 641,
          "title": "Debiased Subjective Assessment of Real-World Image Enhancement. (arXiv:2106.10080v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wenqing Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "In this work, we propose a high fidelity face swapping method, called\nHifiFace, which can well preserve the face shape of the source face and\ngenerate photo-realistic results. Unlike other existing face swapping works\nthat only use face recognition model to keep the identity similarity, we\npropose 3D shape-aware identity to control the face shape with the geometric\nsupervision from 3DMM and 3D face reconstruction method. Meanwhile, we\nintroduce the Semantic Facial Fusion module to optimize the combination of\nencoder and decoder features and make adaptive blending, which makes the\nresults more photo-realistic. Extensive experiments on faces in the wild\ndemonstrate that our method can preserve better identity, especially on the\nface shape, and can generate more photo-realistic results than previous\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.09965",
          "publishedOn": "2021-06-21T02:07:38.088Z",
          "wordCount": 585,
          "title": "HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping. (arXiv:2106.09965v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelhalim_A/0/1/0/all/0/1\">Awad Abdelhalim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbas_M/0/1/0/all/0/1\">Montasir Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotha_B/0/1/0/all/0/1\">Bhavi Bharat Kotha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicks_A/0/1/0/all/0/1\">Alfred Wicks</a>",
          "description": "In a previous study, we presented VT-Lane, a three-step framework for\nreal-time vehicle detection, tracking, and turn movement classification at\nurban intersections. In this study, we present a case study incorporating the\nhighly accurate trajectories and movement classification obtained via VT-Lane\nfor the purpose of speed estimation and driver behavior calibration for traffic\nat urban intersections. First, we use a highly instrumented vehicle to verify\nthe estimated speeds obtained from video inference. The results of the speed\nvalidation show that our method can estimate the average travel speed of\ndetected vehicles in real-time with an error of 0.19 m/sec, which is equivalent\nto 2% of the average observed travel speeds in the intersection of the study.\nInstantaneous speeds (at the resolution of 30 Hz) were found to be estimated\nwith an average error of 0.21 m/sec and 0.86 m/sec respectively for\nfree-flowing and congested traffic conditions. We then use the estimated speeds\nto calibrate the parameters of a driver behavior model for the vehicles in the\narea of study. The results show that the calibrated model replicates the\ndriving behavior with an average error of 0.45 m/sec, indicating the high\npotential for using this framework for automated, large-scale calibration of\ncar-following models from roadside traffic video data, which can lead to\nsubstantial improvements in traffic modeling via microscopic simulation.",
          "link": "http://arxiv.org/abs/2106.09932",
          "publishedOn": "2021-06-21T02:07:38.068Z",
          "wordCount": 680,
          "title": "A Framework for Real-time Traffic Trajectory Tracking, Speed Estimation, and Driver Behavior Calibration at Urban Intersections Using Virtual Traffic Lanes. (arXiv:2106.09932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antony_J/0/1/0/all/0/1\">Jibinraj Antony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlather_D/0/1/0/all/0/1\">Dr. Florian Schlather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safronov_G/0/1/0/all/0/1\">Georgij Safronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitz_M/0/1/0/all/0/1\">Markus Schmitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laerhoven_P/0/1/0/all/0/1\">Prof. Dr. Kristof Van Laerhoven</a>",
          "description": "With the rise of deep learning models in the field of computer vision, new\npossibilities for their application in industrial processes proves to return\ngreat benefits. Nevertheless, the actual fit of machine learning for highly\nstandardised industrial processes is still under debate. This paper addresses\nthe challenges on the industrial realization of the AI tools, considering the\nuse case of Laser Beam Welding quality control as an example. We use object\ndetection algorithms from the TensorFlow object detection API and adapt them to\nour use case using transfer learning. The baseline models we develop are used\nas benchmarks and evaluated and compared to models that undergo dataset scaling\nand hyperparameter tuning. We find that moderate scaling of the dataset via\nimage augmentation leads to improvements in intersection over union (IoU) and\nrecall, whereas high levels of augmentation and scaling may lead to\ndeterioration of results. Finally, we put our results into perspective of the\nunderlying use case and evaluate their fit.",
          "link": "http://arxiv.org/abs/2106.10160",
          "publishedOn": "2021-06-21T02:07:38.061Z",
          "wordCount": 616,
          "title": "Toward Fault Detection in Industrial Welding Processes with Deep Learning and Data Augmentation. (arXiv:2106.10160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_S/0/1/0/all/0/1\">Sauptik Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heydari_J/0/1/0/all/0/1\">Javad Heydari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Samarth Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurup_U/0/1/0/all/0/1\">Unmesh Kurup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mohak Shah</a>",
          "description": "Limited availability of labeled-data makes any supervised learning problem\nchallenging. Alternative learning settings like semi-supervised and universum\nlearning alleviate the dependency on labeled data, but still require a large\namount of unlabeled data, which may be unavailable or expensive to acquire.\nGAN-based synthetic data generation methods have recently shown promise by\ngenerating synthetic samples to improve task at hand. However, these samples\ncannot be used for other purposes. In this paper, we propose a GAN game which\nprovides improved discriminator accuracy under limited data settings, while\ngenerating realistic synthetic data. This provides the added advantage that now\nthe generated data can be used for other similar tasks. We provide the\ntheoretical guarantees and empirical results in support of our approach.",
          "link": "http://arxiv.org/abs/2106.09946",
          "publishedOn": "2021-06-21T02:07:38.054Z",
          "wordCount": 574,
          "title": "Evolving GANs: When Contradictions Turn into Compliance. (arXiv:2106.09946v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinzhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tonghuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaqian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Dongdong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">RenGang Li</a>",
          "description": "Interpreting how does deep neural networks (DNNs) make predictions is a vital\nfield in artificial intelligence, which hinders wide applications of DNNs.\nVisualization of learned representations helps we humans understand the vision\nof DNNs. In this work, visualized images that can activate the neural network\nto the target classes are generated by back-propagation method. Here, rotation\nand scaling operations are applied to introduce the transformation invariance\nin the image generating process, which we find a significant improvement on\nvisualization effect. Finally, we show some cases that such method can help us\nto gain insight into neural networks.",
          "link": "http://arxiv.org/abs/2106.09982",
          "publishedOn": "2021-06-21T02:07:38.047Z",
          "wordCount": 551,
          "title": "Towards interpreting computer vision based on transformation invariant optimization. (arXiv:2106.09982v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awiszus_M/0/1/0/all/0/1\">Maren Awiszus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1\">Frederik Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>",
          "description": "This work introduces World-GAN, the first method to perform data-driven\nProcedural Content Generation via Machine Learning in Minecraft from a single\nexample. Based on a 3D Generative Adversarial Network (GAN) architecture, we\nare able to create arbitrarily sized world snippets from a given sample. We\nevaluate our approach on creations from the community as well as structures\ngenerated with the Minecraft World Generator. Our method is motivated by the\ndense representations used in Natural Language Processing (NLP) introduced with\nword2vec [1]. The proposed block2vec representations make World-GAN independent\nfrom the number of different blocks, which can vary a lot in Minecraft, and\nenable the generation of larger levels. Finally, we demonstrate that changing\nthis new representation space allows us to change the generated style of an\nalready trained generator. World-GAN enables its users to generate Minecraft\nworlds based on parts of their creations.",
          "link": "http://arxiv.org/abs/2106.10155",
          "publishedOn": "2021-06-21T02:07:38.024Z",
          "wordCount": 593,
          "title": "World-GAN: a Generative Model for Minecraft Worlds. (arXiv:2106.10155v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangwei Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tingyun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>",
          "description": "In active visual tracking, it is notoriously difficult when distracting\nobjects appear, as distractors often mislead the tracker by occluding the\ntarget or bringing a confusing appearance. To address this issue, we propose a\nmixed cooperative-competitive multi-agent game, where a target and multiple\ndistractors form a collaborative team to play against a tracker and make it\nfail to follow. Through learning in our game, diverse distracting behaviors of\nthe distractors naturally emerge, thereby exposing the tracker's weakness,\nwhich helps enhance the distraction-robustness of the tracker. For effective\nlearning, we then present a bunch of practical methods, including a reward\nfunction for distractors, a cross-modal teacher-student learning strategy, and\na recurrent attention mechanism for the tracker. The experimental results show\nthat our tracker performs desired distraction-robust active visual tracking and\ncan be well generalized to unseen environments. We also show that the\nmulti-agent game can be used to adversarially test the robustness of trackers.",
          "link": "http://arxiv.org/abs/2106.10110",
          "publishedOn": "2021-06-21T02:07:38.000Z",
          "wordCount": 601,
          "title": "Towards Distraction-Robust Active Visual Tracking. (arXiv:2106.10110v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1\">A. Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">J. Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Y. Pang</a>",
          "description": "Pseudo-LiDAR based 3D object detectors have gained popularity due to their\nhigh accuracy. However, these methods need dense depth supervision and suffer\nfrom inferior speed. To solve these two issues, a recently introduced RTS3D\nbuilds an efficient 4D Feature-Consistency Embedding (FCE) space for the\nintermediate representation of object without depth supervision. FCE space\nsplits the entire object region into 3D uniform grid latent space for feature\nsampling point generation, which ignores the importance of different object\nregions. However, we argue that, compared with the inner region, the outer\nregion plays a more important role for accurate 3D detection. To encode more\ninformation from the outer region, we propose a shape prior non-uniform\nsampling strategy that performs dense sampling in outer region and sparse\nsampling in inner region. As a result, more points are sampled from the outer\nregion and more useful features are extracted for 3D detection. Further, to\nenhance the feature discrimination of each sampling point, we propose a\nhigh-level semantic enhanced FCE module to exploit more contextual information\nand suppress noise better. Experiments on the KITTI dataset are performed to\nshow the effectiveness of the proposed method. Compared with the baseline\nRTS3D, our proposed method has 2.57% improvement on AP3d almost without extra\nnetwork parameters. Moreover, our proposed method outperforms the\nstate-of-the-art methods without extra supervision at a real-time speed.",
          "link": "http://arxiv.org/abs/2106.10013",
          "publishedOn": "2021-06-21T02:07:37.993Z",
          "wordCount": 665,
          "title": "Shape Prior Non-Uniform Sampling Guided Real-time Stereo 3D Object Detection. (arXiv:2106.10013v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tropin_D/0/1/0/all/0/1\">D.V. Tropin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ershov_A/0/1/0/all/0/1\">A.M. Ershov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1\">D.P. Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arlazarov_V/0/1/0/all/0/1\">V.V. Arlazarov</a>",
          "description": "The demand for on-device document recognition systems increases in\nconjunction with the emergence of more strict privacy and security\nrequirements. In such systems, there is no data transfer from the end device to\na third-party information processing servers. The response time is vital to the\nuser experience of on-device document recognition. Combined with the\nunavailability of discrete GPUs, powerful CPUs, or a large RAM capacity on\nconsumer-grade end devices such as smartphones, the time limitations put\nsignificant constraints on the computational complexity of the applied\nalgorithms for on-device execution.\n\nIn this work, we consider document location in an image without prior\nknowledge of the document content or its internal structure. In accordance with\nthe published works, at least 5 systems offer solutions for on-device document\nlocation. All these systems use a location method which can be considered\nHough-based. The precision of such systems seems to be lower than that of the\nstate-of-the-art solutions which were not designed to account for the limited\ncomputational resources.\n\nWe propose an advanced Hough-based method. In contrast with other approaches,\nit accounts for the geometric invariants of the central projection model and\ncombines both edge and color features for document boundary detection. The\nproposed method allowed for the second best result for SmartDoc dataset in\nterms of precision, surpassed by U-net like neural network. When evaluated on a\nmore challenging MIDV-500 dataset, the proposed algorithm guaranteed the best\nprecision compared to published methods. Our method retained the applicability\nto on-device computations.",
          "link": "http://arxiv.org/abs/2106.09987",
          "publishedOn": "2021-06-21T02:07:37.973Z",
          "wordCount": 695,
          "title": "Advanced Hough-based method for on-device document localization. (arXiv:2106.09987v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sungwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>",
          "description": "Training a Convolutional Neural Network (CNN) to be robust against rotation\nhas mostly been done with data augmentation. In this paper, another progressive\nvision of research direction is highlighted to encourage less dependence on\ndata augmentation by achieving structural rotational invariance of a network.\nThe deep equivariance-bridged SO(2) invariant network is proposed to echo such\nvision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network\n(SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the\ngraph representation of an image to acquire rotationally equivariant\nrepresentation, as GCN is more suitable for constructing deeper network than\nspectral graph convolution-based approaches. Then, invariant representation is\neventually obtained with Global Average Pooling (GAP), a permutation-invariant\noperation suitable for aggregating high-dimensional representations, over the\nequivariant set of vertices retrieved from SWN-GCN. Our method achieves the\nstate-of-the-art image classification performance on rotated MNIST and CIFAR-10\nimages, where the models are trained with a non-augmented dataset only.\nQuantitative validations over invariance of the representations also\ndemonstrate strong invariance of deep representations of SWN-GCN over\nrotations.",
          "link": "http://arxiv.org/abs/2106.09996",
          "publishedOn": "2021-06-21T02:07:37.966Z",
          "wordCount": 601,
          "title": "Equivariance-bridged SO(2)-Invariant Representation Learning using Graph Convolutional Network. (arXiv:2106.09996v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Tomoki Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>",
          "description": "We propose a novel GAN training scheme that can handle any level of labeling\nin a unified manner. Our scheme introduces a form of artificial labeling that\ncan incorporate manually defined labels, when available, and induce an\nalignment between them. To define the artificial labels, we exploit the\nassumption that neural network generators can be trained more easily to map\nnearby latent vectors to data with semantic similarities, than across separate\ncategories. We use generated data samples and their corresponding artificial\nconditioning labels to train a classifier. The classifier is then used to\nself-label real data. To boost the accuracy of the self-labeling, we also use\nthe exponential moving average of the classifier. However, because the\nclassifier might still make mistakes, especially at the beginning of the\ntraining, we also refine the labels through self-attention, by using the\nlabeling of real data samples only when the classifier outputs a high\nclassification probability score. We evaluate our approach on CIFAR-10, STL-10\nand SVHN, and show that both self-labeling and self-attention consistently\nimprove the quality of generated data. More surprisingly, we find that the\nproposed scheme can even outperform class-conditional GANs.",
          "link": "http://arxiv.org/abs/2106.09914",
          "publishedOn": "2021-06-21T02:07:37.958Z",
          "wordCount": 626,
          "title": "A Unified Generative Adversarial Network Training via Self-Labeling and Self-Attention. (arXiv:2106.09914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>",
          "description": "We present a heterogeneous localization framework for solving radar global\nlocalization and pose tracking on pre-built lidar maps. To bridge the gap of\nsensing modalities, deep neural networks are constructed to create shared\nembedding space for radar scans and lidar maps. Herein learned feature\nembeddings are supportive for similarity measurement, thus improving map\nretrieval and data matching respectively. In RobotCar and MulRan datasets, we\ndemonstrate the effectiveness of the proposed framework with the comparison to\nScan Context and RaLL. In addition, the proposed pose tracking pipeline is with\nless neural networks compared to the original RaLL.",
          "link": "http://arxiv.org/abs/2106.10000",
          "publishedOn": "2021-06-21T02:07:37.939Z",
          "wordCount": 546,
          "title": "Improved Radar Localization on Lidar Maps Using Shared Embedding. (arXiv:2106.10000v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_I/0/1/0/all/0/1\">Indrajit Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_D/0/1/0/all/0/1\">David C. Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimber_O/0/1/0/all/0/1\">Oliver Bimber</a>",
          "description": "Fully autonomous drones have been demonstrated to find lost or injured\npersons under strongly occluding forest canopy. Airborne Optical Sectioning\n(AOS), a novel synthetic aperture imaging technique, together with\ndeep-learning-based classification enables high detection rates under realistic\nsearch-and-rescue conditions. We demonstrate that false detections can be\nsignificantly suppressed and true detections boosted by combining\nclassifications from multiple AOS rather than single integral images. This\nimproves classification rates especially in the presence of occlusion. To make\nthis possible, we modified the AOS imaging process to support large overlaps\nbetween subsequent integrals, enabling real-time and on-board scanning and\nprocessing of groundspeeds up to 10 m/s.",
          "link": "http://arxiv.org/abs/2106.10077",
          "publishedOn": "2021-06-21T02:07:37.932Z",
          "wordCount": 567,
          "title": "Combined Person Classification with Airborne Optical Sectioning. (arXiv:2106.10077v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Collecting training data from untrusted sources exposes machine learning\nservices to poisoning adversaries, who maliciously manipulate training data to\ndegrade the model accuracy. When trained on offline datasets, poisoning\nadversaries have to inject the poisoned data in advance before training, and\nthe order of feeding these poisoned batches into the model is stochastic. In\ncontrast, practical systems are more usually trained/fine-tuned on sequentially\ncaptured real-time data, in which case poisoning adversaries could dynamically\npoison each data batch according to the current model state. In this paper, we\nfocus on the real-time settings and propose a new attacking strategy, which\naffiliates an accumulative phase with poisoning attacks to secretly (i.e.,\nwithout affecting accuracy) magnify the destructive effect of a (poisoned)\ntrigger batch. By mimicking online learning and federated learning on CIFAR-10,\nwe show that the model accuracy will significantly drop by a single update step\non the trigger batch after the accumulative phase. Our work validates that a\nwell-designed but straightforward attacking strategy can dramatically amplify\nthe poisoning effects, with no need to explore complex techniques.",
          "link": "http://arxiv.org/abs/2106.09993",
          "publishedOn": "2021-06-21T02:07:37.919Z",
          "wordCount": 615,
          "title": "Accumulative Poisoning Attacks on Real-time Data. (arXiv:2106.09993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1\">Maura Pintor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demetrio_L/0/1/0/all/0/1\">Luca Demetrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotgiu_A/0/1/0/all/0/1\">Angelo Sotgiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manca_G/0/1/0/all/0/1\">Giovanni Manca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1\">Fabio Roli</a>",
          "description": "Evaluating robustness of machine-learning models to adversarial examples is a\nchallenging problem. Many defenses have been shown to provide a false sense of\nsecurity by causing gradient-based attacks to fail, and they have been broken\nunder more rigorous evaluations. Although guidelines and best practices have\nbeen suggested to improve current adversarial robustness evaluations, the lack\nof automatic testing and debugging tools makes it difficult to apply these\nrecommendations in a systematic manner. In this work, we overcome these\nlimitations by (i) defining a set of quantitative indicators which unveil\ncommon failures in the optimization of gradient-based attacks, and (ii)\nproposing specific mitigation strategies within a systematic evaluation\nprotocol. Our extensive experimental analysis shows that the proposed\nindicators of failure can be used to visualize, debug and improve current\nadversarial robustness evaluations, providing a first concrete step towards\nautomatizing and systematizing current adversarial robustness evaluations. Our\nopen-source code is available at:\nhttps://github.com/pralab/IndicatorsOfAttackFailure.",
          "link": "http://arxiv.org/abs/2106.09947",
          "publishedOn": "2021-06-21T02:07:37.888Z",
          "wordCount": 609,
          "title": "Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples. (arXiv:2106.09947v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyu-Lim Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeong-Soo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Seung-Ri Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun-Ho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_C/0/1/0/all/0/1\">Chul-Min Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Seok Lee</a>",
          "description": "A significant amount of work has been done on adversarial attacks that inject\nimperceptible noise to images to deteriorate the image classification\nperformance of deep models. However, most of the existing studies consider\nattacks in the digital (pixel) domain where an image acquired by an image\nsensor with sampling and quantization has been recorded. This paper, for the\nfirst time, introduces an optical adversarial attack, which physically alters\nthe light field information arriving at the image sensor so that the\nclassification model yields misclassification. More specifically, we modulate\nthe phase of the light in the Fourier domain using a spatial light modulator\nplaced in the photographic system. The operative parameters of the modulator\nare obtained by gradient-based optimization to maximize cross-entropy and\nminimize distortions. We present experiments based on both simulation and a\nreal hardware optical system, from which the feasibility of the proposed\noptical attack is demonstrated. It is also verified that the proposed attack is\ncompletely different from common optical-domain distortions such as spherical\naberration, defocus, and astigmatism in terms of both perturbation patterns and\nclassification results.",
          "link": "http://arxiv.org/abs/2106.09908",
          "publishedOn": "2021-06-21T02:07:37.880Z",
          "wordCount": 624,
          "title": "Light Lies: Optical Adversarial Attack. (arXiv:2106.09908v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1\">Lie Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wanji He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yelin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiufen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>",
          "description": "In medical image segmentation, it is difficult to mark ambiguous areas\naccurately with binary masks, especially when dealing with small lesions.\nTherefore, it is a challenge for radiologists to reach a consensus by using\nbinary masks under the condition of multiple annotations. However, these areas\nmay contain anatomical structures that are conducive to diagnosis. Uncertainty\nis introduced to study these situations. Nevertheless, the uncertainty is\nusually measured by the variances between predictions in a multiple trial way.\nIt is not intuitive, and there is no exact correspondence in the image.\nInspired by image matting, we introduce matting as a soft segmentation method\nand a new perspective to deal with and represent uncertain regions into medical\nscenes, namely medical matting. More specifically, because there is no\navailable medical matting dataset, we first labeled two medical datasets with\nalpha matte. Secondly, the matting method applied to the natural image is not\nsuitable for the medical scene, so we propose a new architecture to generate\nbinary masks and alpha matte in a row. Thirdly, the uncertainty map is\nintroduced to highlight the ambiguous regions from the binary results and\nimprove the matting performance. Evaluated on these datasets, the proposed\nmodel outperformed state-of-the-art matting algorithms by a large margin, and\nalpha matte is proved to be a more efficient labeling form than a binary mask.",
          "link": "http://arxiv.org/abs/2106.09887",
          "publishedOn": "2021-06-21T02:07:37.814Z",
          "wordCount": 675,
          "title": "Medical Matting: A New Perspective on Medical Segmentation with Uncertainty. (arXiv:2106.09887v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1\">Robert Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1\">Peizhen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo E Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chasmai_M/0/1/0/all/0/1\">Mustafa Chasmai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schobs_L/0/1/0/all/0/1\">Lawrence Schobs</a>",
          "description": "Machine learning is a general-purpose technology holding promises for many\ninterdisciplinary research problems. However, significant barriers exist in\ncrossing disciplinary boundaries when most machine learning tools are developed\nin different areas separately. We present Pykale - a Python library for\nknowledge-aware machine learning on graphs, images, texts, and videos to enable\nand accelerate interdisciplinary research. We formulate new green machine\nlearning guidelines based on standard software engineering practices and\npropose a novel pipeline-based application programming interface (API). PyKale\nfocuses on leveraging knowledge from multiple sources for accurate and\ninterpretable prediction, thus supporting multimodal learning and transfer\nlearning (particularly domain adaptation) with latest deep learning and\ndimensionality reduction models. We build PyKale on PyTorch and leverage the\nrich PyTorch ecosystem. Our pipeline-based API design enforces standardization\nand minimalism, embracing green machine learning concepts via reducing\nrepetitions and redundancy, reusing existing resources, and recycling learning\nmodels across areas. We demonstrate its interdisciplinary nature via examples\nin bioinformatics, knowledge graph, image/video recognition, and medical\nimaging.",
          "link": "http://arxiv.org/abs/2106.09756",
          "publishedOn": "2021-06-21T02:07:37.799Z",
          "wordCount": 628,
          "title": "PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python. (arXiv:2106.09756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stember_J/0/1/0/all/0/1\">Joseph Stember</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1\">Hrithwik Shalu</a>",
          "description": "Purpose: Image classification is perhaps the most fundamental task in imaging\nAI. However, labeling images is time-consuming and tedious. We have recently\ndemonstrated that reinforcement learning (RL) can classify 2D slices of MRI\nbrain images with high accuracy. Here we make two important steps toward\nspeeding image classification: Firstly, we automatically extract class labels\nfrom the clinical reports. Secondly, we extend our prior 2D classification work\nto fully 3D image volumes from our institution. Hence, we proceed as follows:\nin Part 1, we extract labels from reports automatically using the SBERT natural\nlanguage processing approach. Then, in Part 2, we use these labels with RL to\ntrain a classification Deep-Q Network (DQN) for 3D image volumes.\n\nMethods: For Part 1, we trained SBERT with 90 radiology report impressions.\nWe then used the trained SBERT to predict class labels for use in Part 2. In\nPart 2, we applied multi-step image classification to allow for combined Deep-Q\nlearning using 3D convolutions and TD(0) Q learning. We trained on a set of 90\nimages. We tested on a separate set of 61 images, again using the classes\npredicted from patient reports by the trained SBERT in Part 1. For comparison,\nwe also trained and tested a supervised deep learning classification network on\nthe same set of training and testing images using the same labels.\n\nResults: Part 1: Upon training with the corpus of radiology reports, the\nSBERT model had 100% accuracy for both normal and metastasis-containing scans.\nPart 2: Then, using these labels, whereas the supervised approach quickly\noverfit the training data and as expected performed poorly on the testing set\n(66% accuracy, just over random guessing), the reinforcement learning approach\nachieved an accuracy of 92%. The results were found to be statistically\nsignificant, with a p-value of 3.1 x 10^-5.",
          "link": "http://arxiv.org/abs/2106.09812",
          "publishedOn": "2021-06-21T02:07:37.792Z",
          "wordCount": 751,
          "title": "Deep reinforcement learning with automated label extraction from clinical reports accurately classifies 3D MRI brain volumes. (arXiv:2106.09812v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qigong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiufang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fanhua Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "The training of deep neural networks (DNNs) always requires intensive\nresources for both computation and data storage. Thus, DNNs cannot be\nefficiently applied to mobile phones and embedded devices, which severely\nlimits their applicability in industrial applications. To address this issue,\nwe propose a novel encoding scheme using {-1, +1} to decompose quantized neural\nnetworks (QNNs) into multi-branch binary networks, which can be efficiently\nimplemented by bitwise operations (i.e., xnor and bitcount) to achieve model\ncompression, computational acceleration, and resource saving. By using our\nmethod, users can achieve different encoding precisions arbitrarily according\nto their requirements and hardware resources. The proposed mechanism is highly\nsuitable for the use of FPGA and ASIC in terms of data storage and computation,\nwhich provides a feasible idea for smart chips. We validate the effectiveness\nof our method on large-scale image classification (e.g., ImageNet), object\ndetection, and semantic segmentation tasks. In particular, our method with\nlow-bit encoding can still achieve almost the same performance as its high-bit\ncounterparts.",
          "link": "http://arxiv.org/abs/2106.09886",
          "publishedOn": "2021-06-21T02:07:37.784Z",
          "wordCount": 619,
          "title": "Quantized Neural Networks via {-1, +1} Encoding Decomposition and Acceleration. (arXiv:2106.09886v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zejiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Kuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>",
          "description": "Deep neural networks (DNNs) are effective in solving many real-world\nproblems. Larger DNN models usually exhibit better quality (e.g., accuracy) but\ntheir excessive computation results in long training and inference time. Model\nsparsification can reduce the computation and memory cost while maintaining\nmodel quality. Most existing sparsification algorithms unidirectionally remove\nweights, while others randomly or greedily explore a small subset of weights in\neach layer. The inefficiency of the algorithms reduces the achievable sparsity\nlevel. In addition, many algorithms still require pre-trained dense models and\nthus suffer from large memory footprint and long training time. In this paper,\nwe propose a novel scheduled grow-and-prune (GaP) methodology without\npre-training the dense models. It addresses the shortcomings of the previous\nworks by repeatedly growing a subset of layers to dense and then pruning back\nto sparse after some training. Experiments have shown that such models can\nmatch or beat the quality of highly optimized dense models at 80% sparsity on a\nvariety of tasks, such as image classification, objective detection, 3D object\npart segmentation, and translation. They also outperform other state-of-the-art\n(SOTA) pruning methods, including pruning from pre-trained dense models. As an\nexample, a 90% sparse ResNet-50 obtained via GaP achieves 77.9% top-1 accuracy\non ImageNet, improving the SOTA results by 1.5%.",
          "link": "http://arxiv.org/abs/2106.09857",
          "publishedOn": "2021-06-21T02:07:37.777Z",
          "wordCount": 669,
          "title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods. (arXiv:2106.09857v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>",
          "description": "To evaluate clustering results is a significant part of cluster analysis.\nSince there are no true class labels for clustering in typical unsupervised\nlearning, many internal cluster validity indices (CVIs), which use predicted\nlabels and data, have been created. Without true labels, to design an effective\nCVI is as difficult as to create a clustering method. And it is crucial to have\nmore CVIs because there are no universal CVIs that can be used to measure all\ndatasets and no specific methods of selecting a proper CVI for clusters without\ntrue labels. Therefore, to apply a variety of CVIs to evaluate clustering\nresults is necessary. In this paper, we propose a novel internal CVI -- the\nDistance-based Separability Index (DSI), based on a data separability measure.\nWe compared the DSI with eight internal CVIs including studies from early Dunn\n(1974) to most recent CVDD (2019) and an external CVI as ground truth, by using\nclustering results of five clustering algorithms on 12 real and 97 synthetic\ndatasets. Results show DSI is an effective, unique, and competitive CVI to\nother compared CVIs. We also summarized the general process to evaluate CVIs\nand created the rank-difference metric for comparison of CVIs' results.",
          "link": "http://arxiv.org/abs/2106.09794",
          "publishedOn": "2021-06-21T02:07:37.759Z",
          "wordCount": 647,
          "title": "A Distance-based Separability Measure for Internal Cluster Validation. (arXiv:2106.09794v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1\">Mark Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Siyuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Maxwell D. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yukun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liangzhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qihang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan L. Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroff_F/0/1/0/all/0/1\">Florian Schroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hartwig Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang-Chieh Chen</a>",
          "description": "DeepLab2 is a TensorFlow library for deep labeling, aiming to provide a\nstate-of-the-art and easy-to-use TensorFlow codebase for general dense pixel\nprediction problems in computer vision. DeepLab2 includes all our recently\ndeveloped DeepLab model variants with pretrained checkpoints as well as model\ntraining and evaluation code, allowing the community to reproduce and further\nimprove upon the state-of-art systems. To showcase the effectiveness of\nDeepLab2, our Panoptic-DeepLab employing Axial-SWideRNet as network backbone\nachieves 68.0% PQ or 83.5% mIoU on Cityscaspes validation set, with only\nsingle-scale inference and ImageNet-1K pretrained checkpoints. We hope that\npublicly sharing our library could facilitate future research on dense pixel\nlabeling tasks and envision new applications of this technology. Code is made\npublicly available at \\url{https://github.com/google-research/deeplab2}.",
          "link": "http://arxiv.org/abs/2106.09748",
          "publishedOn": "2021-06-21T02:07:37.711Z",
          "wordCount": 592,
          "title": "DeepLab2: A TensorFlow Library for Deep Labeling. (arXiv:2106.09748v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenghua Xu</a>",
          "description": "Imbalanced datasets widely exist in practice and area great challenge for\ntraining deep neural models with agood generalization on infrequent classes. In\nthis work, wepropose a new rare-class sample generator (RSG) to solvethis\nproblem. RSG aims to generate some new samplesfor rare classes during training,\nand it has in particularthe following advantages: (1) it is convenient to use\nandhighly versatile, because it can be easily integrated intoany kind of\nconvolutional neural network, and it works wellwhen combined with different\nloss functions, and (2) it isonly used during the training phase, and\ntherefore, no ad-ditional burden is imposed on deep neural networks duringthe\ntesting phase. In extensive experimental evaluations, weverify the\neffectiveness of RSG. Furthermore, by leveragingRSG, we obtain competitive\nresults on Imbalanced CIFARand new state-of-the-art results on Places-LT,\nImageNet-LT, and iNaturalist 2018. The source code is available at\nhttps://github.com/Jianf-Wang/RSG.",
          "link": "http://arxiv.org/abs/2106.09859",
          "publishedOn": "2021-06-21T02:07:37.694Z",
          "wordCount": 606,
          "title": "RSG: A Simple but Effective Module for Learning Imbalanced Datasets. (arXiv:2106.09859v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09832",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1\">Nicol&#xe1;s Gaggion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansilla_L/0/1/0/all/0/1\">Lucas Mansilla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1\">Diego Milone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>",
          "description": "In this work we address the problem of landmark-based segmentation for\nanatomical structures. We propose HybridGNet, an encoder-decoder neural\narchitecture which combines standard convolutions for image feature encoding,\nwith graph convolutional neural networks to decode plausible representations of\nanatomical structures. We benchmark the proposed architecture considering other\nstandard landmark and pixel-based models for anatomical segmentation in chest\nx-ray images, and found that HybridGNet is more robust to image occlusions. We\nalso show that it can be used to construct landmark-based segmentations from\npixel level annotations. Our experimental results suggest that HybridGNet\nproduces accurate and anatomically plausible landmark-based segmentations, by\nnaturally incorporating shape constraints within the decoding process via\nspectral convolutions.",
          "link": "http://arxiv.org/abs/2106.09832",
          "publishedOn": "2021-06-21T02:07:37.671Z",
          "wordCount": 567,
          "title": "Hybrid graph convolutional neural networks for landmark-based anatomical segmentation. (arXiv:2106.09832v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09834",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebrahimian_S/0/1/0/all/0/1\">Shadi Ebrahimian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hengyong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalra_M/0/1/0/all/0/1\">Mannu Kalra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>",
          "description": "By the ALARA (As Low As Reasonably Achievable) principle, ultra-low-dose CT\nreconstruction is a holy grail to minimize cancer risks and genetic damages,\nespecially for children. With the development of medical CT technologies, the\niterative algorithms are widely used to reconstruct decent CT images from a\nlow-dose scan. Recently, artificial intelligence (AI) techniques have shown a\ngreat promise in further reducing CT radiation dose to the next level. In this\npaper, we demonstrate that AI-powered CT reconstruction offers diagnostic image\nquality at an ultra-low-dose level comparable to that of radiography.\nSpecifically, here we develop a Split Unrolled Grid-like Alternative\nReconstruction (SUGAR) network, in which deep learning, physical modeling and\nimage prior are integrated. The reconstruction results from clinical datasets\nshow that excellent images can be reconstructed using SUGAR from 36\nprojections. This approach has a potential to change future healthcare.",
          "link": "http://arxiv.org/abs/2106.09834",
          "publishedOn": "2021-06-21T02:07:37.644Z",
          "wordCount": 597,
          "title": "AI-Enabled Ultra-Low-Dose CT Reconstruction. (arXiv:2106.09834v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "This paper investigates two techniques for developing efficient\nself-supervised vision transformers (EsViT) for visual representation learning.\nFirst, we show through a comprehensive empirical study that multi-stage\narchitectures with sparse self-attentions can significantly reduce modeling\ncomplexity but with a cost of losing the ability to capture fine-grained\ncorrespondences between image regions. Second, we propose a new pre-training\ntask of region matching which allows the model to capture fine-grained region\ndependencies and as a result significantly improves the quality of the learned\nvision representations. Our results show that combining the two techniques,\nEsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation,\noutperforming prior arts with around an order magnitude of higher throughput.\nWhen transferring to downstream linear classification tasks, EsViT outperforms\nits supervised counterpart on 17 out of 18 datasets. The code and models will\nbe publicly available.",
          "link": "http://arxiv.org/abs/2106.09785",
          "publishedOn": "2021-06-21T02:07:37.637Z",
          "wordCount": 595,
          "title": "Efficient Self-supervised Vision Transformers for Representation Learning. (arXiv:2106.09785v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kapishnikov_A/0/1/0/all/0/1\">Andrei Kapishnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1\">Besim Avci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedin_B/0/1/0/all/0/1\">Ben Wedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1\">Michael Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1\">Tolga Bolukbasi</a>",
          "description": "Integrated Gradients (IG) is a commonly used feature attribution method for\ndeep neural networks. While IG has many desirable properties, the method often\nproduces spurious/noisy pixel attributions in regions that are not related to\nthe predicted class when applied to visual models. While this has been\npreviously noted, most existing solutions are aimed at addressing the symptoms\nby explicitly reducing the noise in the resulting attributions. In this work,\nwe show that one of the causes of the problem is the accumulation of noise\nalong the IG path. To minimize the effect of this source of noise, we propose\nadapting the attribution path itself -- conditioning the path not just on the\nimage but also on the model being explained. We introduce Adaptive Path Methods\n(APMs) as a generalization of path methods, and Guided IG as a specific\ninstance of an APM. Empirically, Guided IG creates saliency maps better aligned\nwith the model's prediction and the input image that is being explained. We\nshow through qualitative and quantitative experiments that Guided IG\noutperforms other, related methods in nearly every experiment.",
          "link": "http://arxiv.org/abs/2106.09788",
          "publishedOn": "2021-06-21T02:07:37.630Z",
          "wordCount": 657,
          "title": "Guided Integrated Gradients: An Adaptive Path Method for Removing Noise. (arXiv:2106.09788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>",
          "description": "In recent years, multi-view subspace clustering has achieved impressive\nperformance due to the exploitation of complementary imformation across\nmultiple views. However, multi-view data can be very complicated and are not\neasy to cluster in real-world applications. Most existing methods operate on\nraw data and may not obtain the optimal solution. In this work, we propose a\nnovel multi-view clustering method named smoothed multi-view subspace\nclustering (SMVSC) by employing a novel technique, i.e., graph filtering, to\nobtain a smooth representation for each view, in which similar data points have\nsimilar feature values. Specifically, it retains the graph geometric features\nthrough applying a low-pass filter. Consequently, it produces a\n``clustering-friendly\" representation and greatly facilitates the downstream\nclustering task. Extensive experiments on benchmark datasets validate the\nsuperiority of our approach. Analysis shows that graph filtering increases the\nseparability of classes.",
          "link": "http://arxiv.org/abs/2106.09875",
          "publishedOn": "2021-06-21T02:07:37.609Z",
          "wordCount": 584,
          "title": "Smoothed Multi-View Subspace Clustering. (arXiv:2106.09875v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09759",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "We introduce a new dataset called Synthetic COVID-19 Chest X-ray Dataset for\ntraining machine learning models. The dataset consists of 21,295 synthetic\nCOVID-19 chest X-ray images to be used for computer-aided diagnosis. These\nimages, generated via an unsupervised domain adaptation approach, are of high\nquality. We find that the synthetic images not only improve performance of\nvarious deep learning architectures when used as additional training data under\nheavy imbalance conditions, but also detect the target class with high\nconfidence. We also find that comparable performance can also be achieved when\ntrained only on synthetic images. Further, salient features of the synthetic\nCOVID-19 images indicate that the distribution is significantly different from\nNon-COVID-19 classes, enabling a proper decision boundary. We hope the\navailability of such high fidelity chest X-ray images of COVID-19 will\nencourage advances in the development of diagnostic and/or management tools.",
          "link": "http://arxiv.org/abs/2106.09759",
          "publishedOn": "2021-06-21T02:07:37.584Z",
          "wordCount": 633,
          "title": "Synthetic COVID-19 Chest X-ray Dataset for Computer-Aided Diagnosis. (arXiv:2106.09759v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_R/0/1/0/all/0/1\">Ryan Kim</a>",
          "description": "Throughout the COVID-19 pandemic, the most common symptom displayed by\npatients has been a fever, leading to the use of temperature scanning as a\npreemptive measure to detect potential carriers of the virus. Human employees\nwith handheld thermometers have been used to fulfill this task, however this\nputs them at risk as they cannot be physically distanced and the sequential\nnature of this method leads to great inconveniences and inefficiency. The\nproposed solution is an autonomously navigating robot capable of conversing and\nscanning people's temperature to detect fevers and help screen for COVID-19. To\nsatisfy this objective, the robot must be able to (1) navigate autonomously,\n(2) detect and track people, and (3) get individuals' temperature reading and\nconverse with them if it exceeds 38{\\deg}C. An autonomously navigating mobile\nrobot is used with a manipulator controlled using a face tracking algorithm,\nand an end effector consisting of a thermal camera, smartphone, and chatbot.\nThe goal is to develop a functioning solution that performs the above tasks. In\naddition, technical challenges encountered and their engineering solutions will\nbe presented, and recommendations will be made for enhancements that could be\nincorporated when approaching commercialization.",
          "link": "http://arxiv.org/abs/2106.09894",
          "publishedOn": "2021-06-21T02:07:37.576Z",
          "wordCount": 684,
          "title": "Development of a conversing and body temperature scanning autonomously navigating robot to help screen for COVID-19. (arXiv:2106.09894v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Girgis_R/0/1/0/all/0/1\">Roger Girgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1\">Florian Golemo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codevilla_F/0/1/0/all/0/1\">Felipe Codevilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jim Aldon D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_M/0/1/0/all/0/1\">Martin Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1\">Samira Ebrahimi Kahou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "Robust multi-agent trajectory prediction is essential for the safe control of\nrobots and vehicles that interact with humans. Many existing methods treat\nsocial and temporal information separately and therefore fall short of\nmodelling the joint future trajectories of all agents in a socially consistent\nway. To address this, we propose a new class of Latent Variable Sequential Set\nTransformers which autoregressively model multi-agent trajectories. We refer to\nthese architectures as \"AutoBots\". AutoBots model the contents of sets (e.g.\nrepresenting the properties of agents in a scene) over time and employ\nmulti-head self-attention blocks over these sequences of sets to encode the\nsociotemporal relationships between the different actors of a scene. This\nproduces either the trajectory of one ego-agent or a distribution over the\nfuture trajectories for all agents under consideration. Our approach works for\ngeneral sequences of sets and we provide illustrative experiments modelling the\nsequential structure of the multiple strokes that make up symbols in the\nOmniglot data. For the single-agent prediction case, we validate our model on\nthe NuScenes motion prediction task and achieve competitive results on the\nglobal leaderboard. In the multi-agent forecasting setting, we validate our\nmodel on TrajNet. We find that our method outperforms physical extrapolation\nand recurrent network baselines and generates scene-consistent trajectories.",
          "link": "http://arxiv.org/abs/2104.00563",
          "publishedOn": "2021-06-18T02:06:46.265Z",
          "wordCount": 700,
          "title": "Autobots: Latent Variable Sequential Set Transformers. (arXiv:2104.00563v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1\">Fabio Bellavia</a>",
          "description": "This paper investigates how to step up local image descriptor matching by\nexploiting matching context information. Two main contexts are identified,\noriginated respectively from the descriptor space and from the keypoint space.\nThe former is generally used to design the actual matching strategy while the\nlatter to filter matches according to the local spatial consistency. On this\nbasis, a new matching strategy and a novel local spatial filter, named\nrespectively blob matching and Delaunay Triangulation Matching (DTM) are\ndevised. Blob matching provides a general matching framework by merging\ntogether several strategies, including pre-filtering as well as many-to-many\nand symmetric matching, enabling to achieve a global improvement upon each\nindividual strategy. DTM alternates between Delaunay triangulation contractions\nand expansions to figure out and adjust keypoint neighborhood consistency.\nExperimental evaluation shows that DTM is comparable or better than the\nstate-of-the-art in terms of matching accuracy and robustness, especially for\nnon-planar scenes. Evaluation is carried out according to a new benchmark\ndevised for analyzing the matching pipeline in terms of correct correspondences\non both planar and non-planar scenes, including state-of-the-art methods as\nwell as the common SIFT matching approach for reference. This evaluation can be\nof assistance for future research in this field.",
          "link": "http://arxiv.org/abs/2106.09584",
          "publishedOn": "2021-06-18T02:06:46.258Z",
          "wordCount": 627,
          "title": "SIFT Matching by Context Exposed. (arXiv:2106.09584v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Songtao Lu</a>",
          "description": "Multiple views of data, both naturally acquired (e.g., image and audio) and\nartificially produced (e.g., via adding different noise to data samples), have\nproven useful in enhancing representation learning. Natural views are often\nhandled by multiview analysis tools, e.g., (deep) canonical correlation\nanalysis [(D)CCA], while the artificial ones are frequently used in\nself-supervised learning (SSL) paradigms, e.g., SimCLR and Barlow Twins. Both\ntypes of approaches often involve learning neural feature extractors such that\nthe embeddings of data exhibit high cross-view correlations. Although\nintuitive, the effectiveness of correlation-based neural embedding is only\nempirically validated. This work puts forth a theory-backed framework for\nunsupervised multiview learning. Our development starts with proposing a\nmultiview model, where each view is a nonlinear mixture of shared and private\ncomponents. Consequently, the learning problem boils down to shared/private\ncomponent identification and disentanglement. Under this model, latent\ncorrelation maximization is shown to guarantee the extraction of the shared\ncomponents across views (up to certain ambiguities). In addition, the private\ninformation in each view can be provably disentangled from the shared using\nproper regularization design. The method is tested on a series of tasks, e.g.,\ndownstream clustering, which all show promising performance. Our development\nalso provides a unifying perspective for understanding various DCCA and SSL\nschemes.",
          "link": "http://arxiv.org/abs/2106.07115",
          "publishedOn": "2021-06-18T02:06:35.937Z",
          "wordCount": 679,
          "title": "Latent Correlation-Based Multiview Learning and Self-Supervision: A Unifying Perspective. (arXiv:2106.07115v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellina_C/0/1/0/all/0/1\">Clayton Mellina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>",
          "description": "Semi-supervised learning on class-imbalanced data, although a realistic\nproblem, has been under studied. While existing semi-supervised learning (SSL)\nmethods are known to perform poorly on minority classes, we find that they\nstill generate high precision pseudo-labels on minority classes. By exploiting\nthis property, in this work, we propose Class-Rebalancing Self-Training\n(CReST), a simple yet effective framework to improve existing SSL methods on\nclass-imbalanced data. CReST iteratively retrains a baseline SSL model with a\nlabeled set expanded by adding pseudo-labeled samples from an unlabeled set,\nwhere pseudo-labeled samples from minority classes are selected more frequently\naccording to an estimated class distribution. We also propose a progressive\ndistribution alignment to adaptively adjust the rebalancing strength dubbed\nCReST+. We show that CReST and CReST+ improve state-of-the-art SSL algorithms\non various class-imbalanced datasets and consistently outperform other popular\nrebalancing methods. Code has been made available at\nhttps://github.com/google-research/crest.",
          "link": "http://arxiv.org/abs/2102.09559",
          "publishedOn": "2021-06-18T02:06:35.924Z",
          "wordCount": 613,
          "title": "CReST: A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning. (arXiv:2102.09559v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>",
          "description": "Few-shot learning (FSL) aims to classify images under low-data regimes, where\nthe conventional pooled global representation is likely to lose useful local\ncharacteristics. Recent work has achieved promising performances by using deep\ndescriptors. They generally take all deep descriptors from neural networks into\nconsideration while ignoring that some of them are useless in classification\ndue to their limited receptive field, e.g., task-irrelevant descriptors could\nbe misleading and multiple aggregative descriptors from background clutter\ncould even overwhelm the object's presence. In this paper, we argue that a\nMutual Nearest Neighbor (MNN) relation should be established to explicitly\nselect the query descriptors that are most relevant to each task and discard\nless relevant ones from aggregative clutters in FSL. Specifically, we propose\nDiscriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive\nexperiments demonstrate that our method not only qualitatively selects\ntask-relevant descriptors but also quantitatively outperforms the existing\nstate-of-the-arts by a large margin of 1.8~4.9% on fine-grained CUB, a\nconsiderable margin of 1.4~2.2% on both supervised and semi-supervised\nminiImagenet, and ~1.4% on challenging tieredimagenet.",
          "link": "http://arxiv.org/abs/2103.08160",
          "publishedOn": "2021-06-18T02:06:35.909Z",
          "wordCount": 641,
          "title": "DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network. (arXiv:2103.08160v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Huanqian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Ying Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingxing Wei</a>",
          "description": "Deep neural networks are vulnerable to adversarial examples, which are\ncrafted by adding human-imperceptible perturbations to original images. Most\nexisting adversarial attack methods achieve nearly 100% attack success rates\nunder the white-box setting, but only achieve relatively low attack success\nrates under the black-box setting. To improve the transferability of\nadversarial examples for the black-box setting, several methods have been\nproposed, e.g., input diversity, translation-invariant attack, and\nmomentum-based attack. In this paper, we propose a method named Gradient\nRefining, which can further improve the adversarial transferability by\ncorrecting useless gradients introduced by input diversity through multiple\ntransformations. Our method is generally applicable to many gradient-based\nattack methods combined with input diversity. Extensive experiments are\nconducted on the ImageNet dataset and our method can achieve an average\ntransfer success rate of 82.07% for three different models under single-model\nsetting, which outperforms the other state-of-the-art methods by a large margin\nof 6.0% averagely. And we have applied the proposed method to the competition\nCVPR 2021 Unrestricted Adversarial Attacks on ImageNet organized by Alibaba and\nwon the second place in attack success rates among 1558 teams.",
          "link": "http://arxiv.org/abs/2105.04834",
          "publishedOn": "2021-06-18T02:06:35.875Z",
          "wordCount": 632,
          "title": "Improving Adversarial Transferability with Gradient Refining. (arXiv:2105.04834v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuxuan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1\">Zheng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhenglun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>",
          "description": "With the increasing demand to efficiently deploy DNNs on mobile edge devices,\nit becomes much more important to reduce unnecessary computation and increase\nthe execution speed. Prior methods towards this goal, including model\ncompression and network architecture search (NAS), are largely performed\nindependently and do not fully consider compiler-level optimizations which is a\nmust-do for mobile acceleration. In this work, we first propose (i) a general\ncategory of fine-grained structured pruning applicable to various DNN layers,\nand (ii) a comprehensive, compiler automatic code generation framework\nsupporting different DNNs and different pruning schemes, which bridge the gap\nof model compression and NAS. We further propose NPAS, a compiler-aware unified\nnetwork pruning, and architecture search. To deal with large search space, we\npropose a meta-modeling procedure based on reinforcement learning with fast\nevaluation and Bayesian optimization, ensuring the total number of training\nepochs comparable with representative NAS frameworks. Our framework achieves\n6.7ms, 5.9ms, 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3\nlevel), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an\noff-the-shelf mobile phone, consistently outperforming prior work.",
          "link": "http://arxiv.org/abs/2012.00596",
          "publishedOn": "2021-06-18T02:06:35.841Z",
          "wordCount": 720,
          "title": "NPAS: A Compiler-aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration. (arXiv:2012.00596v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Emunds_C/0/1/0/all/0/1\">Christoph Emunds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauen_N/0/1/0/all/0/1\">Nicolas Pauen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_V/0/1/0/all/0/1\">Veronika Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frisch_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Frisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treeck_C/0/1/0/all/0/1\">Christoph van Treeck</a>",
          "description": "Enhancing interoperability and information exchange between domain-specific\nsoftware products for BIM is an important aspect in the Architecture,\nEngineering, Construction and Operations industry. Recent research started\ninvestigating methods from the areas of machine and deep learning for semantic\nenrichment of BIM models. However, training and evaluation of these machine\nlearning algorithms requires sufficiently large and comprehensive datasets.\nThis work presents IFCNet, a dataset of single-entity IFC files spanning a\nbroad range of IFC classes containing both geometric and semantic information.\nUsing only the geometric information of objects, the experiments show that\nthree different deep learning models are able to achieve good classification\nperformance.",
          "link": "http://arxiv.org/abs/2106.09712",
          "publishedOn": "2021-06-18T02:06:35.815Z",
          "wordCount": 548,
          "title": "IFCNet: A Benchmark Dataset for IFC Entity Classification. (arXiv:2106.09712v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.12352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1\">Letitia Parcalabescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1\">Iacer Calixto</a>",
          "description": "We investigate the reasoning ability of pretrained vision and language (V&L)\nmodels in two tasks that require multimodal integration: (1) discriminating a\ncorrect image-sentence pair from an incorrect one, and (2) counting entities in\nan image. We evaluate three pretrained V&L models on these tasks: ViLBERT,\nViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results\nshow that models solve task (1) very well, as expected, since all models are\npretrained on task (1). However, none of the pretrained V&L models is able to\nadequately solve task (2), our counting probe, and they cannot generalise to\nout-of-distribution quantities. We propose a number of explanations for these\nfindings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of\ncatastrophic forgetting on task (1). Concerning our results on the counting\nprobe, we find evidence that all models are impacted by dataset bias, and also\nfail to individuate entities in the visual input. While a selling point of\npretrained V&L models is their ability to solve complex tasks, our findings\nsuggest that understanding their reasoning and grounding capabilities requires\nmore targeted investigations on specific phenomena.",
          "link": "http://arxiv.org/abs/2012.12352",
          "publishedOn": "2021-06-18T02:06:35.782Z",
          "wordCount": 725,
          "title": "Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks. (arXiv:2012.12352v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1\">Alex Fedorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sylvain_T/0/1/0/all/0/1\">Tristan Sylvain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geenjaar_E/0/1/0/all/0/1\">Eloy Geenjaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luck_M/0/1/0/all/0/1\">Margaux Luck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeRamus_T/0/1/0/all/0/1\">Thomas P. DeRamus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirilin_A/0/1/0/all/0/1\">Alex Kirilin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleklov_D/0/1/0/all/0/1\">Dmitry Bleklov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1\">Vince D. Calhoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1\">Sergey M. Plis</a>",
          "description": "Sensory input from multiple sources is crucial for robust and coherent human\nperception. Different sources contribute complementary explanatory factors.\nSimilarly, research studies often collect multimodal imaging data, each of\nwhich can provide shared and unique information. This observation motivated the\ndesign of powerful multimodal self-supervised representation-learning\nalgorithms. In this paper, we unify recent work on multimodal self-supervised\nlearning under a single framework. Observing that most self-supervised methods\noptimize similarity metrics between a set of model components, we propose a\ntaxonomy of all reasonable ways to organize this process. We first evaluate\nmodels on toy multimodal MNIST datasets and then apply them to a multimodal\nneuroimaging dataset with Alzheimer's disease patients. We find that (1)\nmultimodal contrastive learning has significant benefits over its unimodal\ncounterpart, (2) the specific composition of multiple contrastive objectives is\ncritical to performance on a downstream task, (3) maximization of the\nsimilarity between representations has a regularizing effect on a neural\nnetwork, which can sometimes lead to reduced downstream performance but still\nreveal multimodal relations. Results show that the proposed approach\noutperforms previous self-supervised encoder-decoder methods based on canonical\ncorrelation analysis (CCA) or the mixture-of-experts multimodal variational\nautoEncoder (MMVAE) on various datasets with a linear evaluation protocol.\nImportantly, we find a promising solution to uncover connections between\nmodalities through a jointly shared subspace that can help advance work in our\nsearch for neuroimaging biomarkers.",
          "link": "http://arxiv.org/abs/2012.13623",
          "publishedOn": "2021-06-18T02:06:35.769Z",
          "wordCount": 733,
          "title": "Self-Supervised Multimodal Domino: in Search of Biomarkers for Alzheimer's Disease. (arXiv:2012.13623v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaoyuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weixin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shenhan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>",
          "description": "This paper proposes a framework for the interactive video object segmentation\n(VOS) in the wild where users can choose some frames for annotations\niteratively. Then, based on the user annotations, a segmentation algorithm\nrefines the masks. The previous interactive VOS paradigm selects the frame with\nsome worst evaluation metric, and the ground truth is required for calculating\nthe evaluation metric, which is impractical in the testing phase. In contrast,\nin this paper, we advocate that the frame with the worst evaluation metric may\nnot be exactly the most valuable frame that leads to the most performance\nimprovement across the video. Thus, we formulate the frame selection problem in\nthe interactive VOS as a Markov Decision Process, where an agent is learned to\nrecommend the frame under a deep reinforcement learning framework. The learned\nagent can automatically determine the most valuable frame, making the\ninteractive setting more practical in the wild. Experimental results on the\npublic datasets show the effectiveness of our learned agent without any changes\nto the underlying VOS algorithms. Our data, code, and models are available at\nhttps://github.com/svip-lab/IVOS-W.",
          "link": "http://arxiv.org/abs/2103.10391",
          "publishedOn": "2021-06-18T02:06:35.700Z",
          "wordCount": 667,
          "title": "Learning to Recommend Frame for Interactive Video Object Segmentation in the Wild. (arXiv:2103.10391v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1\">Zhen-Peng Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1\">Lap-Pui Chau</a>",
          "description": "In this paper, we propose a real-time and accurate automatic license plate\nrecognition (ALPR) approach. Our study illustrates the outstanding design of\nALPR with four insights: (1) the resampling-based cascaded framework is\nbeneficial to both speed and accuracy; (2) the highly efficient license plate\nrecognition should abundant additional character segmentation and recurrent\nneural network (RNN), but adopt a plain convolutional neural network (CNN); (3)\nin the case of CNN, taking advantage of vertex information on license plates\nimproves the recognition performance; and (4) the weight-sharing character\nclassifier addresses the lack of training images in small-scale datasets. Based\non these insights, we propose a novel ALPR approach, termed VSNet.\nSpecifically, VSNet includes two CNNs, i.e., VertexNet for license plate\ndetection and SCR-Net for license plate recognition, integrated in a\nresampling-based cascaded manner. In VertexNet, we propose an efficient\nintegration block to extract the spatial features of license plates. With\nvertex supervisory information, we propose a vertex-estimation branch in\nVertexNet such that license plates can be rectified as the input images of\nSCR-Net. In SCR-Net, we introduce a horizontal encoding technique for\nleft-to-right feature extraction and propose a weight-sharing classifier for\ncharacter recognition. Experimental results show that the proposed VSNet\noutperforms state-of-the-art methods by more than 50% relative improvement on\nerror rate, achieving > 99% recognition accuracy on CCPD and AOLP datasets with\n149 FPS inference speed. Moreover, our method illustrates an outstanding\ngeneralization capability when evaluated on the unseen PKUData and CLPD\ndatasets.",
          "link": "http://arxiv.org/abs/2011.14936",
          "publishedOn": "2021-06-18T02:06:35.647Z",
          "wordCount": 724,
          "title": "Rethinking and Designing a High-performing Automatic License Plate Recognition Approach. (arXiv:2011.14936v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Greer_H/0/1/0/all/0/1\">Hastings Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwitt_R/0/1/0/all/0/1\">Roland Kwitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vialard_F/0/1/0/all/0/1\">Francois-Xavier Vialard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>",
          "description": "Learning maps between data samples is fundamental. Applications range from\nrepresentation learning, image translation and generative modeling, to the\nestimation of spatial deformations. Such maps relate feature vectors, or map\nbetween feature spaces. Well-behaved maps should be regular, which can be\nimposed explicitly or may emanate from the data itself. We explore what induces\nregularity for spatial transformations, e.g., when computing image\nregistrations. Classical optimization-based models compute maps between pairs\nof samples and rely on an appropriate regularizer for well-posedness. Recent\ndeep learning approaches have attempted to avoid using such regularizers\naltogether by relying on the sample population instead. We explore if it is\npossible to obtain spatial regularity using an inverse consistency loss only\nand elucidate what explains map regularity in such a context. We find that deep\nnetworks combined with an inverse consistency loss and randomized off-grid\ninterpolation yield well behaved, approximately diffeomorphic, spatial\ntransformations. Despite the simplicity of this approach, our experiments\npresent compelling evidence, on both synthetic and real data, that regular maps\ncan be obtained without carefully tuned explicit regularizers, while achieving\ncompetitive registration performance.",
          "link": "http://arxiv.org/abs/2105.04459",
          "publishedOn": "2021-06-18T02:06:35.637Z",
          "wordCount": 652,
          "title": "ICON: Learning Regular Maps Through Inverse Consistency. (arXiv:2105.04459v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fasfous_N/0/1/0/all/0/1\">Nael Fasfous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemparala_M/0/1/0/all/0/1\">Manoj-Rohit Vemparala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frickenstein_A/0/1/0/all/0/1\">Alexander Frickenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frickenstein_L/0/1/0/all/0/1\">Lukas Frickenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stechele_W/0/1/0/all/0/1\">Walter Stechele</a>",
          "description": "Face masks have long been used in many areas of everyday life to protect\nagainst the inhalation of hazardous fumes and particles. They also offer an\neffective solution in healthcare for bi-directional protection against\nair-borne diseases. Wearing and positioning the mask correctly is essential for\nits function. Convolutional neural networks (CNNs) offer an excellent solution\nfor face recognition and classification of correct mask wearing and\npositioning. In the context of the ongoing COVID-19 pandemic, such algorithms\ncan be used at entrances to corporate buildings, airports, shopping areas, and\nother indoor locations, to mitigate the spread of the virus. These application\nscenarios impose major challenges to the underlying compute platform. The\ninference hardware must be cheap, small and energy efficient, while providing\nsufficient memory and compute power to execute accurate CNNs at a reasonably\nlow latency. To maintain data privacy of the public, all processing must remain\non the edge-device, without any communication with cloud servers. To address\nthese challenges, we present a low-power binary neural network classifier for\ncorrect facial-mask wear and positioning. The classification task is\nimplemented on an embedded FPGA, performing high-throughput binary operations.\nClassification can take place at up to ~6400 frames-per-second, easily enabling\nmulti-camera, speed-gate settings or statistics collection in crowd settings.\nWhen deployed on a single entrance or gate, the idle power consumption is\nreduced to 1.6W, improving the battery-life of the device. We achieve an\naccuracy of up to 98% for four wearing positions of the MaskedFace-Net dataset.\nTo maintain equivalent classification accuracy for all face structures,\nskin-tones, hair types, and mask types, the algorithms are tested for their\nability to generalize the relevant features over all subjects using the\nGrad-CAM approach.",
          "link": "http://arxiv.org/abs/2102.03456",
          "publishedOn": "2021-06-18T02:06:35.621Z",
          "wordCount": 810,
          "title": "BinaryCoP: Binary Neural Network-based COVID-19 Face-Mask Wear and Positioning Predictor on Edge Devices. (arXiv:2102.03456v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1\">Gencer Sumbul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_A/0/1/0/all/0/1\">Arne de Wall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreuziger_T/0/1/0/all/0/1\">Tristan Kreuziger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcelino_F/0/1/0/all/0/1\">Filipe Marcelino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_H/0/1/0/all/0/1\">Hugo Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benevides_P/0/1/0/all/0/1\">Pedro Benevides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caetano_M/0/1/0/all/0/1\">M&#xe1;rio Caetano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markl_V/0/1/0/all/0/1\">Volker Markl</a>",
          "description": "This paper presents the multi-modal BigEarthNet (BigEarthNet-MM) benchmark\narchive made up of 590,326 pairs of Sentinel-1 and Sentinel-2 image patches to\nsupport the deep learning (DL) studies in multi-modal multi-label remote\nsensing (RS) image retrieval and classification. Each pair of patches in\nBigEarthNet-MM is annotated with multi-labels provided by the CORINE Land Cover\n(CLC) map of 2018 based on its thematically most detailed Level-3 class\nnomenclature. Our initial research demonstrates that some CLC classes are\nchallenging to be accurately described by only considering (single-date)\nBigEarthNet-MM images. In this paper, we also introduce an alternative\nclass-nomenclature as an evolution of the original CLC labels to address this\nproblem. This is achieved by interpreting and arranging the CLC Level-3\nnomenclature based on the properties of BigEarthNet-MM images in a new\nnomenclature of 19 classes. In our experiments, we show the potential of\nBigEarthNet-MM for multi-modal multi-label image retrieval and classification\nproblems by considering several state-of-the-art DL models. We also demonstrate\nthat the DL models trained from scratch on BigEarthNet-MM outperform those\npre-trained on ImageNet, especially in relation to some complex classes,\nincluding agriculture and other vegetated and natural environments. We make all\nthe data and the DL models publicly available at https://bigearth.net, offering\nan important resource to support studies on multi-modal image scene\nclassification and retrieval problems in RS.",
          "link": "http://arxiv.org/abs/2105.07921",
          "publishedOn": "2021-06-18T02:06:35.603Z",
          "wordCount": 736,
          "title": "BigEarthNet-MM: A Large Scale Multi-Modal Multi-Label Benchmark Archive for Remote Sensing Image Classification and Retrieval. (arXiv:2105.07921v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+El_Nouby_A/0/1/0/all/0/1\">Alaaeldin El-Nouby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1\">Hugo Touvron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1\">Natalia Neverova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1\">Jakob Verbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1\">Herv&#xe9; Jegou</a>",
          "description": "Following their success in natural language processing, transformers have\nrecently shown much promise for computer vision. The self-attention operation\nunderlying transformers yields global interactions between all tokens ,i.e.\nwords or image patches, and enables flexible modelling of image data beyond the\nlocal interactions of convolutions. This flexibility, however, comes with a\nquadratic complexity in time and memory, hindering application to long\nsequences and high-resolution images. We propose a \"transposed\" version of\nself-attention that operates across feature channels rather than tokens, where\nthe interactions are based on the cross-covariance matrix between keys and\nqueries. The resulting cross-covariance attention (XCA) has linear complexity\nin the number of tokens, and allows efficient processing of high-resolution\nimages. Our cross-covariance image transformer (XCiT) is built upon XCA. It\ncombines the accuracy of conventional transformers with the scalability of\nconvolutional architectures. We validate the effectiveness and generality of\nXCiT by reporting excellent results on multiple vision benchmarks, including\nimage classification and self-supervised feature learning on ImageNet-1k,\nobject detection and instance segmentation on COCO, and semantic segmentation\non ADE20k.",
          "link": "http://arxiv.org/abs/2106.09681",
          "publishedOn": "2021-06-18T02:06:35.571Z",
          "wordCount": 617,
          "title": "XCiT: Cross-Covariance Image Transformers. (arXiv:2106.09681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.04551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dorkenwald_M/0/1/0/all/0/1\">Michael Dorkenwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1\">Timo Milbich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1\">Andreas Blattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1\">Robin Rombach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1\">Konstantinos G. Derpanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "Video understanding calls for a model to learn the characteristic interplay\nbetween static scene content and its dynamics: Given an image, the model must\nbe able to predict a future progression of the portrayed scene and, conversely,\na video should be explained in terms of its static image content and all the\nremaining characteristics not present in the initial frame. This naturally\nsuggests a bijective mapping between the video domain and the static content as\nwell as residual information. In contrast to common stochastic image-to-video\nsynthesis, such a model does not merely generate arbitrary videos progressing\nthe initial image. Given this image, it rather provides a one-to-one mapping\nbetween the residual vectors and the video with stochastic outcomes when\nsampling. The approach is naturally implemented using a conditional invertible\nneural network (cINN) that can explain videos by independently modelling static\nand other video characteristics, thus laying the basis for controlled video\nsynthesis. Experiments on four diverse video datasets demonstrate the\neffectiveness of our approach in terms of both the quality and diversity of the\nsynthesized results. Our project page is available at https://bit.ly/3t66bnU.",
          "link": "http://arxiv.org/abs/2105.04551",
          "publishedOn": "2021-06-18T02:06:35.564Z",
          "wordCount": 654,
          "title": "Stochastic Image-to-Video Synthesis using cINNs. (arXiv:2105.04551v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kangcheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Feng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Ben M. Chen</a>",
          "description": "This work presents FG-Net, a general deep learning framework for large-scale\npoint clouds understanding without voxelizations, which achieves accurate and\nreal-time performance with a single NVIDIA GTX 1080 GPU. First, a novel noise\nand outlier filtering method is designed to facilitate subsequent high-level\ntasks. For effective understanding purpose, we propose a deep convolutional\nneural network leveraging correlated feature mining and deformable convolution\nbased geometric-aware modelling, in which the local feature relationships and\ngeometric patterns can be fully exploited. For the efficiency issue, we put\nforward an inverse density sampling operation and a feature pyramid based\nresidual learning strategy to save the computational cost and memory\nconsumption respectively. Extensive experiments on real-world challenging\ndatasets demonstrated that our approaches outperform state-of-the-art\napproaches in terms of accuracy and efficiency. Moreover, weakly supervised\ntransfer learning is also conducted to demonstrate the generalization capacity\nof our method.",
          "link": "http://arxiv.org/abs/2012.09439",
          "publishedOn": "2021-06-18T02:06:35.556Z",
          "wordCount": 624,
          "title": "FG-Net: Fast Large-Scale LiDAR Point Clouds Understanding Network Leveraging Correlated Feature Mining and Geometric-Aware Modelling. (arXiv:2012.09439v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>",
          "description": "In the past few years, numerous deep learning methods have been proposed to\naddress the task of segmenting salient objects from RGB images. However, these\napproaches depending on single modality fail to achieve the state-of-the-art\nperformance on widely used light field salient object detection (SOD) datasets,\nwhich collect large-scale natural images and provide multiple modalities such\nas multi-view, micro-lens images and depth maps. Most recently proposed light\nfield SOD methods have acquired improving detecting accuracy, yet still predict\nrough objects' structures and perform slow inference speed. To this end, we\npropose CMA-Net, which consists of two novel cascaded mutual attention modules\naiming at fusing the high level features from the modalities of all-in-focus\nand depth. Our proposed CMA-Net outperforms 30 SOD methods (by a large margin)\non two widely applied light field benchmark datasets. Besides, the proposed\nCMA-Net can run at a speed of 53 fps, thus being four times faster than the\nstate-of-the-art multi-modal SOD methods. Extensive quantitative and\nqualitative experiments illustrate both the effectiveness and efficiency of our\nCMA-Net, inspiring future development of multi-modal learning for both the\nRGB-D and light field SOD.",
          "link": "http://arxiv.org/abs/2105.00949",
          "publishedOn": "2021-06-18T02:06:35.510Z",
          "wordCount": 674,
          "title": "CMA-Net: A Cascaded Mutual Attention Network for Light Field Salient Object Detection. (arXiv:2105.00949v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">David Chuan-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martelaro_N/0/1/0/all/0/1\">Nikolas Martelaro</a>",
          "description": "A key task in design work is grasping the client's implicit tastes. Designers\noften do this based on a set of examples from the client. However, recognizing\na common pattern among many intertwining variables such as color, texture, and\nlayout and synthesizing them into a composite preference can be challenging. In\nthis paper, we leverage the pattern recognition capability of computational\nmodels to aid in this task. We offer a set of principles for computationally\nlearning personal style. The principles are manifested in PseudoClient, a deep\nlearning framework that learns a computational model for personal graphic\ndesign style from only a handful of examples. In several experiments, we found\nthat PseudoClient achieves a 79.40% accuracy with only five positive and\nnegative examples, outperforming several alternative methods. Finally, we\ndiscuss how PseudoClient can be utilized as a building block to support the\ndevelopment of future design applications.",
          "link": "http://arxiv.org/abs/2105.14457",
          "publishedOn": "2021-06-18T02:06:35.503Z",
          "wordCount": 604,
          "title": "Learning Personal Style from Few Examples. (arXiv:2105.14457v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bortsova_G/0/1/0/all/0/1\">Gerda Bortsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Gonzalo_C/0/1/0/all/0/1\">Cristina Gonz&#xe1;lez-Gonzalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetstein_S/0/1/0/all/0/1\">Suzanne C. Wetstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubost_F/0/1/0/all/0/1\">Florian Dubost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katramados_I/0/1/0/all/0/1\">Ioannis Katramados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogeweg_L/0/1/0/all/0/1\">Laurens Hogeweg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liefers_B/0/1/0/all/0/1\">Bart Liefers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1\">Josien P.W. Pluim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veta_M/0/1/0/all/0/1\">Mitko Veta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_C/0/1/0/all/0/1\">Clara I. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruijne_M/0/1/0/all/0/1\">Marleen de Bruijne</a>",
          "description": "Adversarial attacks are considered a potentially serious security threat for\nmachine learning systems. Medical image analysis (MedIA) systems have recently\nbeen argued to be vulnerable to adversarial attacks due to strong financial\nincentives and the associated technological infrastructure.\n\nIn this paper, we study previously unexplored factors affecting adversarial\nattack vulnerability of deep learning MedIA systems in three medical domains:\nophthalmology, radiology, and pathology. We focus on adversarial black-box\nsettings, in which the attacker does not have full access to the target model\nand usually uses another model, commonly referred to as surrogate model, to\ncraft adversarial examples. We consider this to be the most realistic scenario\nfor MedIA systems.\n\nFirstly, we study the effect of weight initialization (ImageNet vs. random)\non the transferability of adversarial attacks from the surrogate model to the\ntarget model. Secondly, we study the influence of differences in development\ndata between target and surrogate models. We further study the interaction of\nweight initialization and data differences with differences in model\narchitecture. All experiments were done with a perturbation degree tuned to\nensure maximal transferability at minimal visual perceptibility of the attacks.\n\nOur experiments show that pre-training may dramatically increase the\ntransferability of adversarial examples, even when the target and surrogate's\narchitectures are different: the larger the performance gain using\npre-training, the larger the transferability. Differences in the development\ndata between target and surrogate models considerably decrease the performance\nof the attack; this decrease is further amplified by difference in the model\narchitecture. We believe these factors should be considered when developing\nsecurity-critical MedIA systems planned to be deployed in clinical practice.",
          "link": "http://arxiv.org/abs/2006.06356",
          "publishedOn": "2021-06-18T02:06:35.461Z",
          "wordCount": 788,
          "title": "Adversarial Attack Vulnerability of Medical Image Analysis Systems: Unexplored Factors. (arXiv:2006.06356v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05404",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Guo_S/0/1/0/all/0/1\">Shangjie Guo</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Fritsch_A/0/1/0/all/0/1\">Amilson R. Fritsch</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Greenberg_C/0/1/0/all/0/1\">Craig Greenberg</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Spielman_I/0/1/0/all/0/1\">I. B. Spielman</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zwolak_J/0/1/0/all/0/1\">Justyna P. Zwolak</a>",
          "description": "Most data in cold-atom experiments comes from images, the analysis of which\nis limited by our preconceptions of the patterns that could be present in the\ndata. We focus on the well-defined case of detecting dark solitons -- appearing\nas local density depletions in a Bose-Einstein condensate (BEC) -- using a\nmethodology that is extensible to the general task of pattern recognition in\nimages of cold atoms. Studying soliton dynamics over a wide range of parameters\nrequires the analysis of large datasets, making the existing\nhuman-inspection-based methodology a significant bottleneck. Here we describe\nan automated classification and positioning system for identifying localized\nexcitations in atomic BECs utilizing deep convolutional neural networks to\neliminate the need for human image examination. Furthermore, we openly publish\nour labeled dataset of dark solitons, the first of its kind, for further\nmachine learning research.",
          "link": "http://arxiv.org/abs/2101.05404",
          "publishedOn": "2021-06-18T02:06:35.451Z",
          "wordCount": 626,
          "title": "Machine-learning enhanced dark soliton detection in Bose-Einstein condensates. (arXiv:2101.05404v2 [cond-mat.quant-gas] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.08061",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Modanwal_G/0/1/0/all/0/1\">Gourav Modanwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vellal_A/0/1/0/all/0/1\">Adithya Vellal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej A. Mazurowski</a>",
          "description": "Dynamic Contrast Enhanced-Magnetic Resonance Imaging (DCE-MRI) is widely used\nto complement ultrasound examinations and x-ray mammography during the early\ndetection and diagnosis of breast cancer. However, images generated by various\nMRI scanners (e.g. GE Healthcare vs Siemens) differ both in intensity and noise\ndistribution, preventing algorithms trained on MRIs from one scanner to\ngeneralize to data from other scanners successfully. We propose a method for\nimage normalization to solve this problem. MRI normalization is challenging\nbecause it requires both normalizing intensity values and mapping between the\nnoise distributions of different scanners. We utilize a cycle-consistent\ngenerative adversarial network to learn a bidirectional mapping between MRIs\nproduced by GE Healthcare and Siemens scanners. This allows us learning the\nmapping between two different scanner types without matched data, which is not\ncommonly available. To ensure the preservation of breast shape and structures\nwithin the breast, we propose two technical innovations. First, we incorporate\na mutual information loss with the CycleGAN architecture to ensure that the\nstructure of the breast is maintained. Second, we propose a modified\ndiscriminator architecture which utilizes a smaller field-of-view to ensure the\npreservation of finer details in the breast tissue. Quantitative and\nqualitative evaluations show that the second proposed method was able to\nconsistently preserve a high level of detail in the breast structure while also\nperforming the proper intensity normalization and noise mapping. Our results\ndemonstrate that the proposed model can successfully learn a bidirectional\nmapping between MRIs produced by different vendors, potentially enabling\nimproved accuracy of downstream computational algorithms for diagnosis and\ndetection of breast cancer. All the data used in this study are publicly\navailable.",
          "link": "http://arxiv.org/abs/1912.08061",
          "publishedOn": "2021-06-18T02:06:35.442Z",
          "wordCount": 758,
          "title": "Normalization of breast MRIs using Cycle-Consistent Generative Adversarial Networks. (arXiv:1912.08061v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhongze Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Liguang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangsheng Xu</a>",
          "description": "Natural image matting aims to precisely separate foreground objects from\nbackground using alpha matte. Fully automatic natural image matting without\nexternal annotation is quite challenging. Well-performed matting methods\nusually require accurate labor-intensive handcrafted trimap as extra input,\nwhile the performance of automatic trimap generation method of dilating\nforeground segmentation fluctuates with segmentation quality. Therefore, we\nargue that how to handle trade-off of additional information input is a major\nissue in automatic matting. This paper presents a universal semantic-guided\nautomatic natural image matting pipeline with light-weight non-local attention\nwithout trimap and background image as input. Specifically, guided by semantic\ninformation of coarse foreground segmentation, Trimap Generation Network\nestimates accurate trimap. With estimated trimap and RGB image as input, our\nlight-weight Non-local Matting Network with Refinement produces final alpha\nmatte, whose trimap-guided global aggregation attention block is equipped with\nstride downsampling convolution, reducing computation complexity and promoting\nperformance. Experimental results show that our matting algorithm has\ncompetitive performance with current state-of-the-art methods in both\ntrimap-free and trimap-needed aspects.",
          "link": "http://arxiv.org/abs/2103.17020",
          "publishedOn": "2021-06-18T02:06:35.435Z",
          "wordCount": 631,
          "title": "Semantic-guided Automatic Natural Image Matting with Light-weight Non-local Attention. (arXiv:2103.17020v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1\">Ravi Ramamoorthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Keli Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkis_M/0/1/0/all/0/1\">Michel Sarkis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_N/0/1/0/all/0/1\">Ning Bi</a>",
          "description": "We propose a novel real-time selfie video stabilization method. Our method is\ncompletely automatic and runs at 26 fps. We use a 1D linear convolutional\nnetwork to directly infer the rigid moving least squares warping which\nimplicitly balances between the global rigidity and local flexibility. Our\nnetwork structure is specifically designed to stabilize the background and\nforeground at the same time, while providing optional control of stabilization\nfocus (relative importance of foreground vs. background) to the users. To train\nour network, we collect a selfie video dataset with 1005 videos, which is\nsignificantly larger than previous selfie video datasets. We also propose a\ngrid approximation method to the rigid moving least squares warping that\nenables the real-time frame warping. Our method is fully automatic and produces\nvisually and quantitatively better results than previous real-time general\nvideo stabilization methods. Compared to previous offline selfie video methods,\nour approach produces comparable quality with a speed improvement of orders of\nmagnitude.",
          "link": "http://arxiv.org/abs/2009.02007",
          "publishedOn": "2021-06-18T02:06:35.417Z",
          "wordCount": 615,
          "title": "Real-Time Selfie Video Stabilization. (arXiv:2009.02007v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dua_R/0/1/0/all/0/1\">Radhika Dua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kancheti_S/0/1/0/all/0/1\">Sai Srinivas Kancheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>",
          "description": "Visual Question Answering is a multi-modal task that aims to measure\nhigh-level visual understanding. Contemporary VQA models are restrictive in the\nsense that answers are obtained via classification over a limited vocabulary\n(in the case of open-ended VQA), or via classification over a set of\nmultiple-choice-type answers. In this work, we present a completely generative\nformulation where a multi-word answer is generated for a visual query. To take\nthis a step forward, we introduce a new task: ViQAR (Visual Question Answering\nand Reasoning), wherein a model must generate the complete answer and a\nrationale that seeks to justify the generated answer. We propose an end-to-end\narchitecture to solve this task and describe how to evaluate it. We show that\nour model generates strong answers and rationales through qualitative and\nquantitative evaluation, as well as through a human Turing Test.",
          "link": "http://arxiv.org/abs/2010.12852",
          "publishedOn": "2021-06-18T02:06:35.411Z",
          "wordCount": 604,
          "title": "Beyond VQA: Generating Multi-word Answer and Rationale to Visual Questions. (arXiv:2010.12852v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">James Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balloch_J/0/1/0/all/0/1\">Jonathan Balloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "Modern computer vision applications suffer from catastrophic forgetting when\nincrementally learning new concepts over time. The most successful approaches\nto alleviate this forgetting require extensive replay of previously seen data,\nwhich is problematic when memory constraints or data legality concerns exist.\nIn this work, we consider the high-impact problem of Data-Free\nClass-Incremental Learning (DFCIL), where an incremental learning agent must\nlearn new concepts over time without storing generators or training data from\npast tasks. One approach for DFCIL is to replay synthetic images produced by\ninverting a frozen copy of the learner's classification model, but we show this\napproach fails for common class-incremental benchmarks when using standard\ndistillation strategies. We diagnose the cause of this failure and propose a\nnovel incremental distillation strategy for DFCIL, contributing a modified\ncross-entropy training and importance-weighted feature distillation, and show\nthat our method results in up to a 25.1% increase in final task accuracy\n(absolute difference) compared to SOTA DFCIL methods for common\nclass-incremental benchmarks. Our method even outperforms several standard\nreplay based methods which store a coreset of images.",
          "link": "http://arxiv.org/abs/2106.09701",
          "publishedOn": "2021-06-18T02:06:35.404Z",
          "wordCount": 624,
          "title": "Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning. (arXiv:2106.09701v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.12342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tiantu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Felix Xiaozhu Lin</a>",
          "description": "Low-cost cameras enable powerful analytics. An unexploited opportunity is\nthat most captured videos remain \"cold\" without being queried. For efficiency,\nwe advocate for these cameras to be zero streaming: capturing videos to local\nstorage and communicating with the cloud only when analytics is requested. How\nto query zero-streaming cameras efficiently? Our response is a camera/cloud\nruntime system called DIVA. It addresses two key challenges: to best use\nlimited camera resource during video capture; to rapidly explore massive videos\nduring query execution. DIVA contributes two unconventional techniques. (1)\nWhen capturing videos, a camera builds sparse yet accurate landmark frames,\nfrom which it learns reliable knowledge for accelerating future queries. (2)\nWhen executing a query, a camera processes frames in multiple passes with\nincreasingly more expensive operators. As such, DIVA presents and keeps\nrefining inexact query results throughout the query's execution. On diverse\nqueries over 15 videos lasting 720 hours in total, DIVA runs at more than 100x\nvideo realtime and outperforms competitive alternative designs. To our\nknowledge, DIVA is the first system for querying large videos stored on\nlow-cost remote cameras.",
          "link": "http://arxiv.org/abs/1904.12342",
          "publishedOn": "2021-06-18T02:06:35.398Z",
          "wordCount": 670,
          "title": "Video Analytics with Zero-streaming Cameras. (arXiv:1904.12342v4 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madumal_P/0/1/0/all/0/1\">Prashan Madumal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1\">Tim Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1\">Krista A. Ehinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Benjamin I. P. Rubinstein</a>",
          "description": "Convolutional neural network (CNN) models for computer vision are powerful\nbut lack explainability in their most basic form. This deficiency remains a key\nchallenge when applying CNNs in important domains. Recent work on explanations\nthrough feature importance of approximate linear models has moved from\ninput-level features (pixels or segments) to features from mid-layer feature\nmaps in the form of concept activation vectors (CAVs). CAVs contain\nconcept-level information and could be learned via clustering. In this work, we\nrethink the ACE algorithm of Ghorbani et~al., proposing an alternative\ninvertible concept-based explanation (ICE) framework to overcome its\nshortcomings. Based on the requirements of fidelity (approximate models to\ntarget models) and interpretability (being meaningful to people), we design\nmeasurements and evaluate a range of matrix factorization methods with our\nframework. We find that non-negative concept activation vectors (NCAVs) from\nnon-negative matrix factorization provide superior performance in\ninterpretability and fidelity based on computational and human subject\nexperiments. Our framework provides both local and global concept-level\nexplanations for pre-trained CNN models.",
          "link": "http://arxiv.org/abs/2006.15417",
          "publishedOn": "2021-06-18T02:06:35.390Z",
          "wordCount": 671,
          "title": "Invertible Concept-based Explanations for CNN Models with Non-negative Concept Activation Vectors. (arXiv:2006.15417v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boerdijk_W/0/1/0/all/0/1\">Wout Boerdijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundermeyer_M/0/1/0/all/0/1\">Martin Sundermeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durner_M/0/1/0/all/0/1\">Maximilian Durner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>",
          "description": "We present a novel framework for self-supervised grasped object segmentation\nwith a robotic manipulator. Our method successively learns an agnostic\nforeground segmentation followed by a distinction between manipulator and\nobject solely by observing the motion between consecutive RGB frames. In\ncontrast to previous approaches, we propose a single, end-to-end trainable\narchitecture which jointly incorporates motion cues and semantic knowledge.\nFurthermore, while the motion of the manipulator and the object are substantial\ncues for our algorithm, we present means to robustly deal with distraction\nobjects moving in the background, as well as with completely static scenes. Our\nmethod neither depends on any visual registration of a kinematic robot or 3D\nobject models, nor on precise hand-eye calibration or any additional sensor\ndata. By extensive experimental evaluation we demonstrate the superiority of\nour framework and provide detailed insights on its capability of dealing with\nthe aforementioned extreme cases of motion. We also show that training a\nsemantic segmentation network with the automatically labeled data achieves\nresults on par with manually annotated training data. Code and pretrained model\nare available at https://github.com/DLR-RM/DistinctNet.",
          "link": "http://arxiv.org/abs/2011.03279",
          "publishedOn": "2021-06-18T02:06:35.371Z",
          "wordCount": 657,
          "title": "\"What's This?\" -- Learning to Segment Unknown Objects from Manipulation Sequences. (arXiv:2011.03279v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1\">Abhinanda R. Punnakkal</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1\">Arjun Chandrasekaran</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1\">Nikos Athanasiou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1\">Alejandra Quiros-Ramirez</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a> (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)",
          "description": "Understanding the semantics of human movement -- the what, how and why of the\nmovement -- is an important problem that requires datasets of human actions\nwith semantic labels. Existing datasets take one of two approaches. Large-scale\nvideo datasets contain many action labels but do not contain ground-truth 3D\nhuman motion. Alternatively, motion-capture (mocap) datasets have precise body\nmotions but are limited to a small number of actions. To address this, we\npresent BABEL, a large dataset with language labels describing the actions\nbeing performed in mocap sequences. BABEL consists of action labels for about\n43 hours of mocap sequences from AMASS. Action labels are at two levels of\nabstraction -- sequence labels describe the overall action in the sequence, and\nframe labels describe all actions in every frame of the sequence. Each frame\nlabel is precisely aligned with the duration of the corresponding action in the\nmocap sequence, and multiple actions can overlap. There are over 28k sequence\nlabels, and 63k frame labels in BABEL, which belong to over 250 unique action\ncategories. Labels from BABEL can be leveraged for tasks like action\nrecognition, temporal action localization, motion synthesis, etc. To\ndemonstrate the value of BABEL as a benchmark, we evaluate the performance of\nmodels on 3D action recognition. We demonstrate that BABEL poses interesting\nlearning challenges that are applicable to real-world scenarios, and can serve\nas a useful benchmark of progress in 3D action recognition. The dataset,\nbaseline method, and evaluation code is made available, and supported for\nacademic research purposes at https://babel.is.tue.mpg.de/.",
          "link": "http://arxiv.org/abs/2106.09696",
          "publishedOn": "2021-06-18T02:06:35.363Z",
          "wordCount": 728,
          "title": "BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Germain_H/0/1/0/all/0/1\">Hugo Germain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourmaud_G/0/1/0/all/0/1\">Guillaume Bourmaud</a>",
          "description": "Given a pair of partially overlapping source and target images and a keypoint\nin the source image, the keypoint's correspondent in the target image can be\neither visible, occluded or outside the field of view. Local feature matching\nmethods are only able to identify the correspondent's location when it is\nvisible, while humans can also hallucinate its location when it is occluded or\noutside the field of view through geometric reasoning. In this paper, we bridge\nthis gap by training a network to output a peaked probability distribution over\nthe correspondent's location, regardless of this correspondent being visible,\noccluded, or outside the field of view. We experimentally demonstrate that this\nnetwork is indeed able to hallucinate correspondences on unseen pairs of\nimages. We also apply this network to a camera pose estimation problem and find\nit is significantly more robust than state-of-the-art local feature\nmatching-based competitors.",
          "link": "http://arxiv.org/abs/2106.09711",
          "publishedOn": "2021-06-18T02:06:35.356Z",
          "wordCount": 576,
          "title": "Visual Correspondence Hallucination: Towards Geometric Reasoning. (arXiv:2106.09711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1\">Elijah Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorieul_T/0/1/0/all/0/1\">Titouan Lorieul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1\">Dan Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>",
          "description": "Predicting all applicable labels for a given image is known as multi-label\nclassification. Compared to the standard multi-class case (where each image has\nonly one label), it is considerably more challenging to annotate training data\nfor multi-label classification. When the number of potential labels is large,\nhuman annotators find it difficult to mention all applicable labels for each\ntraining image. Furthermore, in some settings detection is intrinsically\ndifficult e.g. finding small object instances in high resolution images. As a\nresult, multi-label training data is often plagued by false negatives. We\nconsider the hardest version of this problem, where annotators provide only one\nrelevant label for each image. As a result, training sets will have only one\npositive label per image and no confirmed negatives. We explore this special\ncase of learning from missing labels across four different multi-label image\nclassification datasets for both linear classifiers and end-to-end fine-tuned\ndeep networks. We extend existing multi-label losses to this setting and\npropose novel variants that constrain the number of expected positive labels\nduring training. Surprisingly, we show that in some cases it is possible to\napproach the performance of fully labeled classifiers despite training with\nsignificantly fewer confirmed labels.",
          "link": "http://arxiv.org/abs/2106.09708",
          "publishedOn": "2021-06-18T02:06:35.350Z",
          "wordCount": 641,
          "title": "Multi-Label Learning from Single Positive Labels. (arXiv:2106.09708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.09704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zongyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Runsen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>",
          "description": "Over the past several years, we have witnessed impressive progress in the\nfield of learned image compression. Recent learned image codecs are commonly\nbased on autoencoders, that first encode an image into low-dimensional latent\nrepresentations and then decode them for reconstruction purposes. To capture\nspatial dependencies in the latent space, prior works exploit hyperprior and\nspatial context model to build an entropy model, which estimates the bit-rate\nfor end-to-end rate-distortion optimization. However, such an entropy model is\nsuboptimal from two aspects: (1) It fails to capture spatially global\ncorrelations among the latents. (2) Cross-channel relationships of the latents\nare still underexplored. In this paper, we propose the concept of separate\nentropy coding to leverage a serial decoding process for causal contextual\nentropy prediction in the latent space. A causal context model is proposed that\nseparates the latents across channels and makes use of cross-channel\nrelationships to generate highly informative contexts. Furthermore, we propose\na causal global prediction model, which is able to find global reference points\nfor accurate predictions of unknown points. Both these two models facilitate\nentropy estimation without the transmission of overhead. In addition, we\nfurther adopt a new separate attention module to build more powerful transform\nnetworks. Experimental results demonstrate that our full image compression\nmodel outperforms standard VVC/H.266 codec on Kodak dataset in terms of both\nPSNR and MS-SSIM, yielding the state-of-the-art rate-distortion performance.",
          "link": "http://arxiv.org/abs/2011.09704",
          "publishedOn": "2021-06-18T02:06:35.342Z",
          "wordCount": 719,
          "title": "Causal Contextual Prediction for Learned Image Compression. (arXiv:2011.09704v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1\">Avi Schwarzschild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arjun Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John P Dickerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Data poisoning and backdoor attacks manipulate training data in order to\ncause models to fail during inference. A recent survey of industry\npractitioners found that data poisoning is the number one concern among threats\nranging from model stealing to adversarial attacks. However, it remains unclear\nexactly how dangerous poisoning methods are and which ones are more effective\nconsidering that these methods, even ones with identical objectives, have not\nbeen tested in consistent or realistic settings. We observe that data poisoning\nand backdoor attacks are highly sensitive to variations in the testing setup.\nMoreover, we find that existing methods may not generalize to realistic\nsettings. While these existing works serve as valuable prototypes for data\npoisoning, we apply rigorous tests to determine the extent to which we should\nfear them. In order to promote fair comparison in future work, we develop\nstandardized benchmarks for data poisoning and backdoor attacks.",
          "link": "http://arxiv.org/abs/2006.12557",
          "publishedOn": "2021-06-18T02:06:35.311Z",
          "wordCount": 660,
          "title": "Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks. (arXiv:2006.12557v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandikal_P/0/1/0/all/0/1\">Priyanka Mandikal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>",
          "description": "Dexterous robotic hands are appealing for their agility and human-like\nmorphology, yet their high degree of freedom makes learning to manipulate\nchallenging. We introduce an approach for learning dexterous grasping. Our key\nidea is to embed an object-centric visual affordance model within a deep\nreinforcement learning loop to learn grasping policies that favor the same\nobject regions favored by people. Unlike traditional approaches that learn from\nhuman demonstration trajectories (e.g., hand joint sequences captured with a\nglove), the proposed prior is object-centric and image-based, allowing the\nagent to anticipate useful affordance regions for objects unseen during policy\nlearning. We demonstrate our idea with a 30-DoF five-fingered robotic hand\nsimulator on 40 objects from two datasets, where it successfully and\nefficiently learns policies for stable functional grasps. Our affordance-guided\npolicies are significantly more effective, generalize better to novel objects,\ntrain 3 X faster than the baselines, and are more robust to noisy sensor\nreadings and actuation. Our work offers a step towards manipulation agents that\nlearn by watching how people use objects, without requiring state and action\ninformation about the human body. Project website:\nthis http URL",
          "link": "http://arxiv.org/abs/2009.01439",
          "publishedOn": "2021-06-18T02:06:35.305Z",
          "wordCount": 647,
          "title": "Learning Dexterous Grasping with Object-Centric Visual Affordances. (arXiv:2009.01439v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.05937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>",
          "description": "Deep neural models in recent years have been successful in almost every\nfield, including extremely complex problem statements. However, these models\nare huge in size, with millions (and even billions) of parameters, thus\ndemanding more heavy computation power and failing to be deployed on edge\ndevices. Besides, the performance boost is highly dependent on redundant\nlabeled data. To achieve faster speeds and to handle the problems caused by the\nlack of data, knowledge distillation (KD) has been proposed to transfer\ninformation learned from one model to another. KD is often characterized by the\nso-called `Student-Teacher' (S-T) learning framework and has been broadly\napplied in model compression and knowledge transfer. This paper is about KD and\nS-T learning, which are being actively studied in recent years. First, we aim\nto provide explanations of what KD is and how/why it works. Then, we provide a\ncomprehensive survey on the recent progress of KD methods together with S-T\nframeworks typically for vision tasks. In general, we consider some fundamental\nquestions that have been driving this research area and thoroughly generalize\nthe research progress and technical details. Additionally, we systematically\nanalyze the research status of KD in vision applications. Finally, we discuss\nthe potentials and open challenges of existing methods and prospect the future\ndirections of KD and S-T learning.",
          "link": "http://arxiv.org/abs/2004.05937",
          "publishedOn": "2021-06-18T02:06:35.299Z",
          "wordCount": 761,
          "title": "Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks. (arXiv:2004.05937v7 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shiksha Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_P/0/1/0/all/0/1\">Puspita Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Richa Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatsa_M/0/1/0/all/0/1\">Mayank Vatsa</a>",
          "description": "Due to the COVID-19 pandemic, wearing face masks has become a mandate in\npublic places worldwide. Face masks occlude a significant portion of the facial\nregion. Additionally, people wear different types of masks, from simple ones to\nones with graphics and prints. These pose new challenges to face recognition\nalgorithms. Researchers have recently proposed a few masked face datasets for\ndesigning algorithms to overcome the challenges of masked face recognition.\nHowever, existing datasets lack the cultural diversity and collection in the\nunrestricted settings. Country like India with attire diversity, people are not\nlimited to wearing traditional masks but also clothing like a thin cotton\nprinted towel (locally called as ``gamcha''), ``stoles'', and ``handkerchiefs''\nto cover their faces. In this paper, we present a novel \\textbf{Indian Masked\nFaces in the Wild (IMFW)} dataset which contains images with variations in\npose, illumination, resolution, and the variety of masks worn by the subjects.\nWe have also benchmarked the performance of existing face recognition models on\nthe proposed IMFW dataset. Experimental results demonstrate the limitations of\nexisting algorithms in presence of diverse conditions.",
          "link": "http://arxiv.org/abs/2106.09670",
          "publishedOn": "2021-06-18T02:06:35.292Z",
          "wordCount": 658,
          "title": "Indian Masked Faces in the Wild Dataset. (arXiv:2106.09670v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Fanyi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>",
          "description": "We present MoDist as a novel method to explicitly distill motion information\ninto self-supervised video representations. Compared to previous video\nrepresentation learning methods that mostly focus on learning motion cues\nimplicitly from RGB inputs, we show that the representation learned with our\nMoDist method focus more on foreground motion regions and thus generalizes\nbetter to downstream tasks. To achieve this, MoDist enriches standard\ncontrastive learning objectives for RGB video clips with a cross-modal learning\nobjective between a Motion pathway and a Visual pathway. We evaluate MoDist on\nseveral datasets for both action recognition (UCF101/HMDB51/SSv2) as well as\naction detection (AVA), and demonstrate state-of-the-art self-supervised\nperformance on all datasets. Furthermore, we show that MoDist representation\ncan be as effective as (in some cases even better than) representations learned\nwith full supervision. Given its simplicity, we hope MoDist could serve as a\nstrong baseline for future research in self-supervised video representation\nlearning.",
          "link": "http://arxiv.org/abs/2106.09703",
          "publishedOn": "2021-06-18T02:06:35.286Z",
          "wordCount": 584,
          "title": "MoDist: Motion Distillation for Self-supervised Video Representation Learning. (arXiv:2106.09703v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">De-An Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>",
          "description": "Generalization has been a long-standing challenge for reinforcement learning\n(RL). Visual RL, in particular, can be easily distracted by irrelevant factors\nin high-dimensional observation space. In this work, we consider robust policy\nlearning which targets zero-shot generalization to unseen visual environments\nwith large distributional shift. We propose SECANT, a novel self-expert cloning\ntechnique that leverages image augmentation in two stages to decouple robust\nrepresentation learning from policy optimization. Specifically, an expert\npolicy is first trained by RL from scratch with weak augmentations. A student\nnetwork then learns to mimic the expert policy by supervised learning with\nstrong augmentations, making its representation more robust against visual\nvariations compared to the expert. Extensive experiments demonstrate that\nSECANT significantly advances the state of the art in zero-shot generalization\nacross 4 challenging domains. Our average reward improvements over prior SOTAs\nare: DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based\nautonomous driving (+47.7%), and indoor object navigation (+15.8%). Code\nrelease and video are available at https://linxifan.github.io/secant-site/.",
          "link": "http://arxiv.org/abs/2106.09678",
          "publishedOn": "2021-06-18T02:06:35.265Z",
          "wordCount": 621,
          "title": "SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual Policies. (arXiv:2106.09678v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1\">Wenpeng Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zaifeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>",
          "description": "Image-based geometric modeling and novel view synthesis based on sparse,\nlarge-baseline samplings are challenging but important tasks for emerging\nmultimedia applications such as virtual reality and immersive telepresence.\nExisting methods fail to produce satisfactory results due to the limitation on\ninferring reliable depth information over such challenging reference\nconditions. With the popularization of commercial light field (LF) cameras,\ncapturing LF images (LFIs) is as convenient as taking regular photos, and\ngeometry information can be reliably inferred. This inspires us to use a sparse\nset of LF captures to render high-quality novel views globally. However, fusion\nof LF captures from multiple angles is challenging due to the scale\ninconsistency caused by various capture settings. To overcome this challenge,\nwe propose a novel scale-consistent volume rescaling algorithm that robustly\naligns the disparity probability volumes (DPV) among different captures for\nscale-consistent global geometry fusion. Based on the fused DPV projected to\nthe target camera frustum, novel learning-based modules have been proposed\n(i.e., the attention-guided multi-scale residual fusion module, and the\ndisparity field guided deep re-regularization module) which comprehensively\nregularize noisy observations from heterogeneous captures for high-quality\nrendering of novel LFIs. Both quantitative and qualitative experiments over the\nStanford Lytro Multi-view LF dataset show that the proposed method outperforms\nstate-of-the-art methods significantly under different experiment settings for\ndisparity inference and LF synthesis.",
          "link": "http://arxiv.org/abs/2106.09548",
          "publishedOn": "2021-06-18T02:06:35.256Z",
          "wordCount": 658,
          "title": "Scale-Consistent Fusion: from Heterogeneous Local Sampling to Global Immersive Rendering. (arXiv:2106.09548v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzi_E/0/1/0/all/0/1\">Ed Pizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papakipos_Z/0/1/0/all/0/1\">Zo&#xeb; Papakipos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_L/0/1/0/all/0/1\">Lowik Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenicek_T/0/1/0/all/0/1\">Tomas Jenicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maximov_M/0/1/0/all/0/1\">Maxim Maximov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1\">Ond&#x159;ej Chum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>",
          "description": "This paper introduces a new benchmark for large-scale image similarity\ndetection. This benchmark is used for the Image Similarity Challenge at\nNeurIPS'21 (ISC2021). The goal is to determine whether a query image is a\nmodified copy of any image in a reference corpus of size 1~million. The\nbenchmark features a variety of image transformations such as automated\ntransformations, hand-crafted image edits and machine-learning based\nmanipulations. This mimics real-life cases appearing in social media, for\nexample for integrity-related problems dealing with misinformation and\nobjectionable content. The strength of the image manipulations, and therefore\nthe difficulty of the benchmark, is calibrated according to the performance of\na set of baseline approaches. Both the query and reference set contain a\nmajority of ``distractor'' images that do not match, which corresponds to a\nreal-life needle-in-haystack setting, and the evaluation metric reflects that.\nWe expect the DISC21 benchmark to promote image copy detection as an important\nand challenging computer vision task and refresh the state of the art.",
          "link": "http://arxiv.org/abs/2106.09672",
          "publishedOn": "2021-06-18T02:06:35.249Z",
          "wordCount": 614,
          "title": "The 2021 Image Similarity Dataset and Challenge. (arXiv:2106.09672v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziko_I/0/1/0/all/0/1\">Imtiaz Masud Ziko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1\">Malik Boudiaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>",
          "description": "We investigate a general formulation for clustering and transductive few-shot\nlearning, which integrates prototype-based objectives, Laplacian regularization\nand supervision constraints from a few labeled data points. We propose a\nconcave-convex relaxation of the problem, and derive a computationally\nefficient block-coordinate bound optimizer, with convergence guarantee. At each\niteration,our optimizer computes independent (parallel) updates for each\npoint-to-cluster assignment. Therefore, it could be trivially distributed for\nlarge-scale clustering and few-shot tasks. Furthermore, we provides a thorough\nconvergence analysis based on point-to-set maps. Were port comprehensive\nclustering and few-shot learning experiments over various data sets, showing\nthat our method yields competitive performances, in term of accuracy and\noptimization quality, while scaling up to large problems. Using standard\ntraining on the base classes, without resorting to complex meta-learning and\nepisodic-training strategies, our approach outperforms state-of-the-art\nfew-shot methods by significant margins, across various models, settings and\ndata sets. Surprisingly, we found that even standard clustering procedures\n(e.g., K-means), which correspond to particular, non-regularized cases of our\ngeneral model, already achieve competitive performances in comparison to the\nstate-of-the-art in few-shot learning. These surprising results point to the\nlimitations of the current few-shot benchmarks, and question the viability of a\nlarge body of convoluted few-shot learning techniques in the recent literature.",
          "link": "http://arxiv.org/abs/2106.09516",
          "publishedOn": "2021-06-18T02:06:35.242Z",
          "wordCount": 644,
          "title": "Transductive Few-Shot Learning: Clustering is All You Need?. (arXiv:2106.09516v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sanghyun Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joon-Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "Temporal correspondence - linking pixels or objects across frames - is a\nfundamental supervisory signal for the video models. For the panoptic\nunderstanding of dynamic scenes, we further extend this concept to every\nsegment. Specifically, we aim to learn coarse segment-level matching and fine\npixel-level matching together. We implement this idea by designing two novel\nlearning objectives. To validate our proposals, we adopt a deep siamese model\nand train the model to learn the temporal correspondence on two different\nlevels (i.e., segment and pixel) along with the target task. At inference time,\nthe model processes each frame independently without any extra computation and\npost-processing. We show that our per-frame inference model can achieve new\nstate-of-the-art results on Cityscapes-VPS and VIPER datasets. Moreover, due to\nits high efficiency, the model runs in a fraction of time (3x) compared to the\nprevious state-of-the-art approach.",
          "link": "http://arxiv.org/abs/2106.09453",
          "publishedOn": "2021-06-18T02:06:35.235Z",
          "wordCount": 585,
          "title": "Learning to Associate Every Segment for Video Panoptic Segmentation. (arXiv:2106.09453v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisdom_S/0/1/0/all/0/1\">Scott Wisdom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershey_J/0/1/0/all/0/1\">John R. Hershey</a>",
          "description": "We introduce a state-of-the-art audio-visual on-screen sound separation\nsystem which is capable of learning to separate sounds and associate them with\non-screen objects by looking at in-the-wild videos. We identify limitations of\nprevious work on audiovisual on-screen sound separation, including the\nsimplicity and coarse resolution of spatio-temporal attention, and poor\nconvergence of the audio separation model. Our proposed model addresses these\nissues using cross-modal and self-attention modules that capture audio-visual\ndependencies at a finer resolution over time, and by unsupervised pre-training\nof audio separation model. These improvements allow the model to generalize to\na much wider set of unseen videos. For evaluation and semi-supervised training,\nwe collected human annotations of on-screen audio from a large database of\nin-the-wild videos (YFCC100M). Our results show marked improvements in\non-screen separation performance, in more general conditions than previous\nmethods.",
          "link": "http://arxiv.org/abs/2106.09669",
          "publishedOn": "2021-06-18T02:06:35.227Z",
          "wordCount": 581,
          "title": "Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention. (arXiv:2106.09669v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09662",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Samei_G/0/1/0/all/0/1\">Golnoosh Samei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karimi_D/0/1/0/all/0/1\">Davood Karimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kesch_C/0/1/0/all/0/1\">Claudia Kesch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salcudean_S/0/1/0/all/0/1\">Septimiu Salcudean</a>",
          "description": "In this work we propose to segment the prostate on a challenging dataset of\ntrans-rectal ultrasound (TRUS) images using convolutional neural networks\n(CNNs) and statistical shape models (SSMs). TRUS is commonly used for a number\nof image-guided interventions on the prostate. Fast and accurate segmentation\non the organ in these images is crucial to planning and fusion with other\nmodalities such as magnetic resonance images (MRIs) . However, TRUS has limited\nsoft tissue contrast and signal to noise ratio which makes the task of\nsegmenting the prostate challenging and subject to inter-observer and\nintra-observer variability. This is especially problematic at the base and apex\nwhere the gland boundary is hard to define. In this paper, we aim to tackle\nthis problem by taking advantage of shape priors learnt on an MR dataset which\nhas higher soft tissue contrast allowing the prostate to be contoured more\naccurately. We use this shape prior in combination with a prostate tissue\nprobability map computed by a CNN for segmentation.",
          "link": "http://arxiv.org/abs/2106.09662",
          "publishedOn": "2021-06-18T02:06:35.208Z",
          "wordCount": 629,
          "title": "Automatic Segmentation of the Prostate on 3D Trans-rectal Ultrasound Images using Statistical Shape Models and Convolutional Neural Networks. (arXiv:2106.09662v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1\">Yuhang Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "Instance segmentation has witnessed a remarkable progress on class-balanced\nbenchmarks. However, they fail to perform as accurately in real-world\nscenarios, where the category distribution of objects naturally comes with a\nlong tail. Instances of head classes dominate a long-tailed dataset and they\nserve as negative samples of tail categories. The overwhelming gradients of\nnegative samples on tail classes lead to a biased learning process for\nclassifiers. Consequently, objects of tail categories are more likely to be\nmisclassified as backgrounds or head categories. To tackle this problem, we\npropose Seesaw Loss to dynamically re-balance gradients of positive and\nnegative samples for each category, with two complementary factors, i.e.,\nmitigation factor and compensation factor. The mitigation factor reduces\npunishments to tail categories w.r.t. the ratio of cumulative training\ninstances between different categories. Meanwhile, the compensation factor\nincreases the penalty of misclassified instances to avoid false positives of\ntail categories. We conduct extensive experiments on Seesaw Loss with\nmainstream frameworks and different data sampling strategies. With a simple\nend-to-end training pipeline, Seesaw Loss obtains significant gains over\nCross-Entropy Loss, and achieves state-of-the-art performance on LVIS dataset\nwithout bells and whistles. Code is available at\nhttps://github.com/open-mmlab/mmdetection.",
          "link": "http://arxiv.org/abs/2008.10032",
          "publishedOn": "2021-06-18T02:06:35.200Z",
          "wordCount": 693,
          "title": "Seesaw Loss for Long-Tailed Instance Segmentation. (arXiv:2008.10032v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marnerides_D/0/1/0/all/0/1\">Demetris Marnerides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashford_Rogers_T/0/1/0/all/0/1\">Thomas Bashford-Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debattista_K/0/1/0/all/0/1\">Kurt Debattista</a>",
          "description": "Inverse Tone Mapping (ITM) methods attempt to reconstruct High Dynamic Range\n(HDR) information from Low Dynamic Range (LDR) image content. The dynamic range\nof well-exposed areas must be expanded and any missing information due to\nover/under-exposure must be recovered (hallucinated). The majority of methods\nfocus on the former and are relatively successful, while most attempts on the\nlatter are not of sufficient quality, even ones based on Convolutional Neural\nNetworks (CNNs). A major factor for the reduced inpainting quality in some\nworks is the choice of loss function. Work based on Generative Adversarial\nNetworks (GANs) shows promising results for image synthesis and LDR inpainting,\nsuggesting that GAN losses can improve inverse tone mapping results. This work\npresents a GAN-based method that hallucinates missing information from badly\nexposed areas in LDR images and compares its efficacy with alternative\nvariations. The proposed method is quantitatively competitive with\nstate-of-the-art inverse tone mapping methods, providing good dynamic range\nexpansion for well-exposed areas and plausible hallucinations for saturated and\nunder-exposed areas. A density-based normalisation method, targeted for HDR\ncontent, is also proposed, as well as an HDR data augmentation method targeted\nfor HDR hallucination.",
          "link": "http://arxiv.org/abs/2106.09486",
          "publishedOn": "2021-06-18T02:06:35.193Z",
          "wordCount": 631,
          "title": "Deep HDR Hallucination for Inverse Tone Mapping. (arXiv:2106.09486v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pham_K/0/1/0/all/0/1\">Khoi Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kafle_K/0/1/0/all/0/1\">Kushal Kafle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhihong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Scott Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>",
          "description": "Visual attributes constitute a large portion of information contained in a\nscene. Objects can be described using a wide variety of attributes which\nportray their visual appearance (color, texture), geometry (shape, size,\nposture), and other intrinsic properties (state, action). Existing work is\nmostly limited to study of attribute prediction in specific domains. In this\npaper, we introduce a large-scale in-the-wild visual attribute prediction\ndataset consisting of over 927K attribute annotations for over 260K object\ninstances. Formally, object attribute prediction is a multi-label\nclassification problem where all attributes that apply to an object must be\npredicted. Our dataset poses significant challenges to existing methods due to\nlarge number of attributes, label sparsity, data imbalance, and object\nocclusion. To this end, we propose several techniques that systematically\ntackle these challenges, including a base model that utilizes both low- and\nhigh-level CNN features with multi-hop attention, reweighting and resampling\ntechniques, a novel negative label expansion scheme, and a novel supervised\nattribute-aware contrastive learning algorithm. Using these techniques, we\nachieve near 3.7 mAP and 5.7 overall F1 points improvement over the current\nstate of the art. Further details about the VAW dataset can be found at\nthis http URL",
          "link": "http://arxiv.org/abs/2106.09707",
          "publishedOn": "2021-06-18T02:06:35.187Z",
          "wordCount": 642,
          "title": "Learning to Predict Visual Attributes in the Wild. (arXiv:2106.09707v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1\">Lucas Caccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1\">Marc&#x27;Aurelio Ranzato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denoyer_L/0/1/0/all/0/1\">Ludovic Denoyer</a>",
          "description": "Classical machine learning frameworks assume access to a possibly large\ndataset in order to train a predictive model. In many practical applications\nhowever, data does not arrive all at once, but in batches over time. This\ncreates a natural trade-off between accuracy of a model and time to obtain such\na model. A greedy predictor could produce non-trivial predictions by\nimmediately training on batches as soon as these become available but, it may\nalso make sub-optimal use of future data. On the other hand, a tardy predictor\ncould wait for a long time to aggregate several batches into a larger dataset,\nbut ultimately deliver a much better performance. In this work, we consider\nsuch a streaming learning setting, which we dub {\\em anytime learning at\nmacroscale} (ALMA). It is an instance of anytime learning applied not at the\nlevel of a single chunk of data, but at the level of the entire sequence of\nlarge batches. We first formalize this learning setting, we then introduce\nmetrics to assess how well learners perform on the given task for a given\nmemory and compute budget, and finally we test several baseline approaches on\nstandard benchmarks repurposed for anytime learning at macroscale. The general\nfinding is that bigger models always generalize better. In particular, it is\nimportant to grow model capacity over time if the initial model is relatively\nsmall. Moreover, updating the model at an intermediate rate strikes the best\ntrade off between accuracy and time to obtain a useful predictor.",
          "link": "http://arxiv.org/abs/2106.09563",
          "publishedOn": "2021-06-18T02:06:35.181Z",
          "wordCount": 681,
          "title": "On Anytime Learning at Macroscale. (arXiv:2106.09563v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kaihua Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1\">Mingyuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>",
          "description": "Adversarial training is the de facto most promising defense against\nadversarial examples. Yet, its passive nature inevitably prevents it from being\nimmune to unknown attackers. To achieve a proactive defense, we need a more\nfundamental understanding of adversarial examples, beyond the popular bounded\nthreat model. In this paper, we provide a causal viewpoint of adversarial\nvulnerability: the cause is the confounder ubiquitously existing in learning,\nwhere attackers are precisely exploiting the confounding effect. Therefore, a\nfundamental solution for adversarial robustness is causal intervention. As the\nconfounder is unobserved in general, we propose to use the instrumental\nvariable that achieves intervention without the need for confounder\nobservation. We term our robust training method as Causal intervention by\ninstrumental Variable (CiiV). It has a differentiable retinotopic sampling\nlayer and a consistency loss, which is stable and guaranteed not to suffer from\ngradient obfuscation. Extensive experiments on a wide spectrum of attackers and\nsettings applied in MNIST, CIFAR-10, and mini-ImageNet datasets empirically\ndemonstrate that CiiV is robust to adaptive attacks.",
          "link": "http://arxiv.org/abs/2106.09534",
          "publishedOn": "2021-06-18T02:06:35.160Z",
          "wordCount": 609,
          "title": "Adversarial Visual Robustness by Causal Intervention. (arXiv:2106.09534v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunlu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morel_Forster_A/0/1/0/all/0/1\">Andreas Morel-Forster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetter_T/0/1/0/all/0/1\">Thomas Vetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1\">Bernhard Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>",
          "description": "3D face reconstruction from a single image is challenging due to its\nill-posed nature. Model-based face autoencoders address this issue effectively\nby fitting a face model to the target image in a weakly supervised manner.\nHowever, in unconstrained environments occlusions distort the face\nreconstruction because the model often erroneously tries to adapt to occluded\nface regions. Supervised occlusion segmentation is a viable solution to avoid\nthe fitting of occluded face regions, but it requires a large amount of\nannotated training data. In this work, we enable model-based face autoencoders\nto segment occluders accurately without requiring any additional supervision\nduring training, and this separates regions where the model will be fitted from\nthose where it will not be fitted. To achieve this, we extend face autoencoders\nwith a segmentation network. The segmentation network decides which regions the\nmodel should adapt to by reaching balances in a trade-off between including\npixels and adapting the model to them, and excluding pixels so that the model\nfitting is not negatively affected and reaches higher overall reconstruction\naccuracy on pixels showing the face. This leads to a synergistic effect, in\nwhich the occlusion segmentation guides the training of the face autoencoder to\nconstrain the fitting in the non-occluded regions, while the improved fitting\nenables the segmentation model to better predict the occluded face regions.\nQualitative and quantitative experiments on the CelebA-HQ database and the AR\ndatabase verify the effectiveness of our model in improving 3D face\nreconstruction under occlusions and in enabling accurate occlusion segmentation\nfrom weak supervision only. Code available at\nhttps://github.com/unibas-gravis/Occlusion-Robust-MoFA.",
          "link": "http://arxiv.org/abs/2106.09614",
          "publishedOn": "2021-06-18T02:06:35.153Z",
          "wordCount": 712,
          "title": "To fit or not to fit: Model-based Face Reconstruction and Occlusion Segmentation from Weak Supervision. (arXiv:2106.09614v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barros_T/0/1/0/all/0/1\">Tiago Barros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrote_L/0/1/0/all/0/1\">Lu&#xed;s Garrote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_R/0/1/0/all/0/1\">Ricardo Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premebida_C/0/1/0/all/0/1\">Cristiano Premebida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunes_U/0/1/0/all/0/1\">Urbano J. Nunes</a>",
          "description": "Deep networks have been progressively adapted to new sensor modalities,\nnamely to 3D LiDAR, which led to unprecedented achievements in autonomous\nvehicle-related applications such as place recognition. One of the main\nchallenges of deep models in place recognition is to extract efficient and\ndescriptive feature representations that relate places based on their\nsimilarity. To address the problem of place recognition using LiDAR data, this\npaper proposes a novel 3D LiDAR-based deep learning network (named AttDLNet)\nthat comprises an encoder network and exploits an attention mechanism to\nselectively focus on long-range context and interfeature relationships. The\nproposed network is trained and validated on the KITTI dataset, using the\ncosine loss for training and a retrieval-based place recognition pipeline for\nvalidation. Additionally, an ablation study is presented to assess the best\nnetwork configuration. Results show that the encoder network features are\nalready very descriptive, but adding attention to the network further improves\nperformance. From the ablation study, results indicate that the middle encoder\nlayers have the highest mean performance, while deeper layers are more robust\nto orientation change. The code is publicly available on the project website:\nhttps://github.com/Cybonic/ AttDLNet",
          "link": "http://arxiv.org/abs/2106.09637",
          "publishedOn": "2021-06-18T02:06:35.147Z",
          "wordCount": 629,
          "title": "AttDLNet: Attention-based DL Network for 3D LiDAR Place Recognition. (arXiv:2106.09637v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1\">Guangyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathaki_T/0/1/0/all/0/1\">Tania Stathaki</a>",
          "description": "RGB-D salient object detection(SOD) demonstrates its superiority on detecting\nin complex environments due to the additional depth information introduced in\nthe data. Inevitably, an independent stream is introduced to extract features\nfrom depth images, leading to extra computation and parameters. This\nmethodology which sacrifices the model size to improve the detection accuracy\nmay impede the practical application of SOD problems. To tackle this dilemma,\nwe propose a dynamic distillation method along with a lightweight framework,\nwhich significantly reduces the parameters. This method considers the factors\nof both teacher and student performance within the training stage and\ndynamically assigns the distillation weight instead of applying a fixed weight\non the student model. Extensive experiments are conducted on five public\ndatasets to demonstrate that our method can achieve competitive performance\ncompared to 10 prior methods through a 78.2MB lightweight structure.",
          "link": "http://arxiv.org/abs/2106.09517",
          "publishedOn": "2021-06-18T02:06:35.139Z",
          "wordCount": 578,
          "title": "Dynamic Knowledge Distillation with A Single Stream Structure for RGB-DSalient Object Detection. (arXiv:2106.09517v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1\">Koushik Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Shilpak Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Ashish Kumar Pandey</a>",
          "description": "We have proposed orthogonal-Pad\\'e activation functions, which are trainable\nactivation functions and show that they have faster learning capability and\nimproves the accuracy in standard deep learning datasets and models. Based on\nour experiments, we have found two best candidates out of six orthogonal-Pad\\'e\nactivations, which we call safe Hermite-Pade (HP) activation functions, namely\nHP-1 and HP-2. When compared to ReLU, HP-1 and HP-2 has an increment in top-1\naccuracy by 5.06% and 4.63% respectively in PreActResNet-34, by 3.02% and 2.75%\nrespectively in MobileNet V2 model on CIFAR100 dataset while on CIFAR10 dataset\ntop-1 accuracy increases by 2.02% and 1.78% respectively in PreActResNet-34, by\n2.24% and 2.06% respectively in LeNet, by 2.15% and 2.03% respectively in\nEfficientnet B0.",
          "link": "http://arxiv.org/abs/2106.09693",
          "publishedOn": "2021-06-18T02:06:35.131Z",
          "wordCount": 581,
          "title": "Orthogonal-Pad\\'e Activation Functions: Trainable Activation functions for smooth and faster convergence in deep networks. (arXiv:2106.09693v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Minhao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_M/0/1/0/all/0/1\">Matthis Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciceri_T/0/1/0/all/0/1\">Tommaso Ciceri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbera_G/0/1/0/all/0/1\">Giammarco La Barbera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloch_I/0/1/0/all/0/1\">Isabelle Bloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>",
          "description": "The joint use of multiple imaging modalities for medical image segmentation\nhas been widely studied in recent years. The fusion of information from\ndifferent modalities has demonstrated to improve the segmentation accuracy,\nwith respect to mono-modal segmentations, in several applications. However,\nacquiring multiple modalities is usually not possible in a clinical setting due\nto a limited number of physicians and scanners, and to limit costs and scan\ntime. Most of the time, only one modality is acquired. In this paper, we\npropose KD-Net, a framework to transfer knowledge from a trained multi-modal\nnetwork (teacher) to a mono-modal one (student). The proposed method is an\nadaptation of the generalized distillation framework where the student network\nis trained on a subset (1 modality) of the teacher's inputs (n modalities). We\nillustrate the effectiveness of the proposed framework in brain tumor\nsegmentation with the BraTS 2018 dataset. Using different architectures, we\nshow that the student network effectively learns from the teacher and always\noutperforms the baseline mono-modal network in terms of segmentation accuracy.",
          "link": "http://arxiv.org/abs/2106.09564",
          "publishedOn": "2021-06-18T02:06:35.109Z",
          "wordCount": 629,
          "title": "Knowledge distillation from multi-modal to mono-modal segmentation networks. (arXiv:2106.09564v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mokady_R/0/1/0/all/0/1\">Ron Mokady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzaban_R/0/1/0/all/0/1\">Rotem Tzaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1\">Sagie Benaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>",
          "description": "The task of unsupervised motion retargeting in videos has seen substantial\nadvancements through the use of deep neural networks. While early works\nconcentrated on specific object priors such as a human face or body, recent\nwork considered the unsupervised case. When the source and target videos,\nhowever, are of different shapes, current methods fail. To alleviate this\nproblem, we introduce JOKR - a JOint Keypoint Representation that captures the\nmotion common to both the source and target videos, without requiring any\nobject prior or data collection. By employing a domain confusion term, we\nenforce the unsupervised keypoint representations of both videos to be\nindistinguishable. This encourages disentanglement between the parts of the\nmotion that are common to the two domains, and their distinctive appearance and\nmotion, enabling the generation of videos that capture the motion of the one\nwhile depicting the style of the other. To enable cases where the objects are\nof different proportions or orientations, we apply a learned affine\ntransformation between the JOKRs. This augments the representation to be affine\ninvariant, and in practice broadens the variety of possible retargeting pairs.\nThis geometry-driven representation enables further intuitive control, such as\ntemporal coherence and manual editing. Through comprehensive experimentation,\nwe demonstrate the applicability of our method to different challenging\ncross-domain video pairs. We evaluate our method both qualitatively and\nquantitatively, and demonstrate that our method handles various cross-domain\nscenarios, such as different animals, different flowers, and humans. We also\ndemonstrate superior temporal coherency and visual quality compared to\nstate-of-the-art alternatives, through statistical metrics and a user study.\nSource code and videos can be found at https://rmokady.github.io/JOKR/ .",
          "link": "http://arxiv.org/abs/2106.09679",
          "publishedOn": "2021-06-18T02:06:35.089Z",
          "wordCount": 711,
          "title": "JOKR: Joint Keypoint Representation for Unsupervised Cross-Domain Motion Retargeting. (arXiv:2106.09679v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seyedi_S/0/1/0/all/0/1\">Salman Seyedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levey_A/0/1/0/all/0/1\">Allan Levey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1\">Gari D. Clifford</a>",
          "description": "The expanding usage of complex machine learning methods like deep learning\nhas led to an explosion in human activity recognition, particularly applied to\nhealth. In particular, as part of a larger body sensor network system, face and\nfull-body analysis is becoming increasingly common for evaluating health\nstatus. However, complex models which handle private and sometimes protected\ndata, raise concerns about the potential leak of identifiable data. In this\nwork, we focus on the case of a deep network model trained on images of\nindividual faces. Full-face video recordings taken from 493 individuals\nundergoing an eye-tracking based evaluation of neurological function were used.\nOutputs, gradients, intermediate layer outputs, loss, and labels were used as\ninputs for a deep network with an added support vector machine emission layer\nto recognize membership in the training data. The inference attack method and\nassociated mathematical analysis indicate that there is a low likelihood of\nunintended memorization of facial features in the deep learning model. In this\nstudy, it is showed that the named model preserves the integrity of training\ndata with reasonable confidence. The same process can be implemented in similar\nconditions for different models.",
          "link": "http://arxiv.org/abs/2106.09621",
          "publishedOn": "2021-06-18T02:06:35.080Z",
          "wordCount": 624,
          "title": "Privacy-Preserving Eye-tracking Using Deep Learning. (arXiv:2106.09621v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhenzhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Current state-of-the-art image captioning models adopt autoregressive\ndecoders, \\ie they generate each word by conditioning on previously generated\nwords, which leads to heavy latency during inference. To tackle this issue,\nnon-autoregressive image captioning models have recently been proposed to\nsignificantly accelerate the speed of inference by generating all words in\nparallel. However, these non-autoregressive models inevitably suffer from large\ngeneration quality degradation since they remove words dependence excessively.\nTo make a better trade-off between speed and quality, we introduce a\nsemi-autoregressive model for image captioning~(dubbed as SATIC), which keeps\nthe autoregressive property in global but generates words parallelly in local.\nBased on Transformer, there are only a few modifications needed to implement\nSATIC. Extensive experiments on the MSCOCO image captioning benchmark show that\nSATIC can achieve a better trade-off without bells and whistles. Code is\navailable at {\\color{magenta}\\url{https://github.com/YuanEZhou/satic}}.",
          "link": "http://arxiv.org/abs/2106.09436",
          "publishedOn": "2021-06-18T02:06:35.028Z",
          "wordCount": 570,
          "title": "Semi-Autoregressive Transformer for Image Captioning. (arXiv:2106.09436v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eisenberger_M/0/1/0/all/0/1\">Marvin Eisenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1\">David Novotny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerchenbaum_G/0/1/0/all/0/1\">Gael Kerchenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1\">Patrick Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1\">Natalia Neverova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>",
          "description": "We present NeuroMorph, a new neural network architecture that takes as input\ntwo 3D shapes and produces in one go, i.e. in a single feed forward pass, a\nsmooth interpolation and point-to-point correspondences between them. The\ninterpolation, expressed as a deformation field, changes the pose of the source\nshape to resemble the target, but leaves the object identity unchanged.\nNeuroMorph uses an elegant architecture combining graph convolutions with\nglobal feature pooling to extract local features. During training, the model is\nincentivized to create realistic deformations by approximating geodesics on the\nunderlying shape space manifold. This strong geometric prior allows to train\nour model end-to-end and in a fully unsupervised manner without requiring any\nmanual correspondence annotations. NeuroMorph works well for a large variety of\ninput shapes, including non-isometric pairs from different object categories.\nIt obtains state-of-the-art results for both shape correspondence and\ninterpolation tasks, matching or surpassing the performance of recent\nunsupervised and supervised methods on multiple benchmarks.",
          "link": "http://arxiv.org/abs/2106.09431",
          "publishedOn": "2021-06-18T02:06:35.022Z",
          "wordCount": 615,
          "title": "NeuroMorph: Unsupervised Shape Interpolation and Correspondence in One Go. (arXiv:2106.09431v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rangwani_H/0/1/0/all/0/1\">Harsh Rangwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1\">Konda Reddy Mopuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>",
          "description": "Generative Adversarial Networks (GANs) have swiftly evolved to imitate\nincreasingly complex image distributions. However, majority of the developments\nfocus on performance of GANs on balanced datasets. We find that the existing\nGANs and their training regimes which work well on balanced datasets fail to be\neffective in case of imbalanced (i.e. long-tailed) datasets. In this work we\nintroduce a novel theoretically motivated Class Balancing regularizer for\ntraining GANs. Our regularizer makes use of the knowledge from a pre-trained\nclassifier to ensure balanced learning of all the classes in the dataset. This\nis achieved via modelling the effective class frequency based on the\nexponential forgetting observed in neural networks and encouraging the GAN to\nfocus on underrepresented classes. We demonstrate the utility of our\nregularizer in learning representations for long-tailed distributions via\nachieving better performance than existing approaches over multiple datasets.\nSpecifically, when applied to an unconditional GAN, it improves the FID from\n$13.03$ to $9.01$ on the long-tailed iNaturalist-$2019$ dataset.",
          "link": "http://arxiv.org/abs/2106.09402",
          "publishedOn": "2021-06-18T02:06:35.003Z",
          "wordCount": 606,
          "title": "Class Balancing GAN with a Classifier in the Loop. (arXiv:2106.09402v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1\">Yuki Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyata_K/0/1/0/all/0/1\">Kazunori Miyata</a>",
          "description": "Along the rapid development of deep learning techniques in generative models,\nit is becoming an urgent issue to combine machine intelligence with human\nintelligence to solve the practical applications. Motivated by this\nmethodology, this work aims to adjust the machine generated character fonts\nwith the effort of human workers in the perception study. Although numerous\nfonts are available online for public usage, it is difficult and challenging to\ngenerate and explore a font to meet the preferences for common users. To solve\nthe specific issue, we propose the perceptual manifold of fonts to visualize\nthe perceptual adjustment in the latent space of a generative model of fonts.\nIn our framework, we adopt the variational autoencoder network for the font\ngeneration. Then, we conduct a perceptual study on the generated fonts from the\nmulti-dimensional latent space of the generative model. After we obtained the\ndistribution data of specific preferences, we utilize manifold learning\napproach to visualize the font distribution. In contrast to the conventional\nuser interface in our user study, the proposed font-exploring user interface is\nefficient and helpful in the designated user preference.",
          "link": "http://arxiv.org/abs/2106.09198",
          "publishedOn": "2021-06-18T02:06:34.987Z",
          "wordCount": 613,
          "title": "Learning Perceptual Manifold of Fonts. (arXiv:2106.09198v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1\">Mert Seker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannisto_A/0/1/0/all/0/1\">Anssi M&#xe4;nnist&#xf6;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1\">Jenni Raitoharju</a>",
          "description": "Main characters in images are the most important humans that catch the\nviewer's attention upon first look, and they are emphasized by properties such\nas size, position, color saturation, and sharpness of focus. Identifying the\nmain character in images plays an important role in traditional photographic\nstudies and media analysis, but the task is performed manually and can be slow\nand laborious. Furthermore, selection of main characters can be sometimes\nsubjective. In this paper, we analyze the feasibility of solving the main\ncharacter recognition needed for photographic studies automatically and propose\na method for identifying the main characters. The proposed method uses machine\nlearning based human pose estimation along with traditional computer vision\napproaches for this task. We approach the task as a binary classification\nproblem where each detected human is classified either as a main character or\nnot. To evaluate both the subjectivity of the task and the performance of our\nmethod, we collected a dataset of 300 varying images from multiple sources and\nasked five people, a photographic researcher and four other persons, to\nannotate the main characters. Our analysis showed a relatively high agreement\nbetween different annotators. The proposed method achieved a promising F1 score\nof 0.83 on the full image set and 0.96 on a subset evaluated as most clear and\nimportant cases by the photographic researcher.",
          "link": "http://arxiv.org/abs/2106.09064",
          "publishedOn": "2021-06-18T02:06:34.979Z",
          "wordCount": 666,
          "title": "Automatic Main Character Recognition for Photographic Studies. (arXiv:2106.09064v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huttunen_H/0/1/0/all/0/1\">Heikki Huttunen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elomaa_T/0/1/0/all/0/1\">Tapio Elomaa</a>",
          "description": "Age estimation is an essential challenge in computer vision. With the\nadvances of convolutional neural networks, the performance of age estimation\nhas been dramatically improved. Existing approaches usually treat age\nestimation as a classification problem. However, the age labels are ambiguous,\nthus make the classification task difficult. In this paper, we propose a simple\nyet effective approach for age estimation, which improves the performance\ncompared to classification-based methods. The method combines four\nclassification losses and one regression loss representing different class\ngranularities together, and we name it as Age-Granularity-Net. We validate the\nAge-Granularity-Net framework on the CVPR Chalearn 2016 dataset, and extensive\nexperiments show that the proposed approach can reduce the prediction error\ncompared to any individual loss. The source code link is\nhttps://github.com/yipersevere/age-estimation.",
          "link": "http://arxiv.org/abs/2106.09393",
          "publishedOn": "2021-06-18T02:06:34.971Z",
          "wordCount": 559,
          "title": "using multiple losses for accurate facial age estimation. (arXiv:2106.09393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Springstein_M/0/1/0/all/0/1\">Matthias Springstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1\">Eric M&#xfc;ller-Budack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>",
          "description": "The recognition of handwritten mathematical expressions in images and video\nframes is a difficult and unsolved problem yet. Deep convectional neural\nnetworks are basically a promising approach, but typically require a large\namount of labeled training data. However, such a large training dataset does\nnot exist for the task of handwritten formula recognition. In this paper, we\nintroduce a system that creates a large set of synthesized training examples of\nmathematical expressions which are derived from LaTeX documents. For this\npurpose, we propose a novel attention-based generative adversarial network to\ntranslate rendered equations to handwritten formulas. The datasets generated by\nthis approach contain hundreds of thousands of formulas, making it ideal for\npretraining or the design of more complex models. We evaluate our synthesized\ndataset and the recognition approach on the CROHME 2014 benchmark dataset.\nExperimental results demonstrate the feasibility of the approach.",
          "link": "http://arxiv.org/abs/2106.09432",
          "publishedOn": "2021-06-18T02:06:34.953Z",
          "wordCount": 605,
          "title": "Unsupervised Training Data Generation of Handwritten Formulas using Generative Adversarial Networks with Self-Attention. (arXiv:2106.09432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bay_A/0/1/0/all/0/1\">Alessandro Bay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_B/0/1/0/all/0/1\">Biswa Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirabile_A/0/1/0/all/0/1\">Andrea Mirabile</a>",
          "description": "Modern neural networks are highly uncalibrated. It poses a significant\nchallenge for safety-critical systems to utilise deep neural networks (DNNs),\nreliably. Many recently proposed approaches have demonstrated substantial\nprogress in improving DNN calibration. However, they hardly touch upon\nrefinement, which historically has been an essential aspect of calibration.\nRefinement indicates separability of a network's correct and incorrect\npredictions. This paper presents a theoretically and empirically supported\nexposition for reviewing a model's calibration and refinement. Firstly, we show\nthe breakdown of expected calibration error (ECE), into predicted confidence\nand refinement. Connecting with this result, we highlight that regularisation\nbased calibration only focuses on naively reducing a model's confidence. This\nlogically has a severe downside to a model's refinement. We support our claims\nthrough rigorous empirical evaluations of many state of the art calibration\napproaches on standard datasets. We find that many calibration approaches with\nthe likes of label smoothing, mixup etc. lower the utility of a DNN by\ndegrading its refinement. Even under natural data shift, this\ncalibration-refinement trade-off holds for the majority of calibration methods.\nThese findings call for an urgent retrospective into some popular pathways\ntaken for modern DNN calibration.",
          "link": "http://arxiv.org/abs/2106.09385",
          "publishedOn": "2021-06-18T02:06:34.947Z",
          "wordCount": 638,
          "title": "On the Dark Side of Calibration for Modern Neural Networks. (arXiv:2106.09385v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1\">Guolin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>",
          "description": "For a target task where labeled data is unavailable, domain adaptation can\ntransfer a learner from a different source domain. Previous deep domain\nadaptation methods mainly learn a global domain shift, i.e., align the global\nsource and target distributions without considering the relationships between\ntwo subdomains within the same category of different domains, leading to\nunsatisfying transfer learning performance without capturing the fine-grained\ninformation. Recently, more and more researchers pay attention to Subdomain\nAdaptation which focuses on accurately aligning the distributions of the\nrelevant subdomains. However, most of them are adversarial methods which\ncontain several loss functions and converge slowly. Based on this, we present\nDeep Subdomain Adaptation Network (DSAN) which learns a transfer network by\naligning the relevant subdomain distributions of domain-specific layer\nactivations across different domains based on a local maximum mean discrepancy\n(LMMD). Our DSAN is very simple but effective which does not need adversarial\ntraining and converges fast. The adaptation can be achieved easily with most\nfeed-forward network models by extending them with LMMD loss, which can be\ntrained efficiently via back-propagation. Experiments demonstrate that DSAN can\nachieve remarkable results on both object recognition tasks and digit\nclassification tasks. Our code will be available at:\nhttps://github.com/easezyc/deep-transfer-learning",
          "link": "http://arxiv.org/abs/2106.09388",
          "publishedOn": "2021-06-18T02:06:34.937Z",
          "wordCount": 654,
          "title": "Deep Subdomain Adaptation Network for Image Classification. (arXiv:2106.09388v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09222",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dia_O/0/1/0/all/0/1\">Ousmane Amadou Dia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karaletsos_T/0/1/0/all/0/1\">Theofanis Karaletsos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hazirbas_C/0/1/0/all/0/1\">Caner Hazirbas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kabul_I/0/1/0/all/0/1\">Ilknur Kaynar Kabul</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meijer_E/0/1/0/all/0/1\">Erik Meijer</a>",
          "description": "The susceptibility of deep learning models to adversarial perturbations has\nstirred renewed attention in adversarial examples resulting in a number of\nattacks. However, most of these attacks fail to encompass a large spectrum of\nadversarial perturbations that are imperceptible to humans. In this paper, we\npresent localized uncertainty attacks, a novel class of threat models against\ndeterministic and stochastic classifiers. Under this threat model, we create\nadversarial examples by perturbing only regions in the inputs where a\nclassifier is uncertain. To find such regions, we utilize the predictive\nuncertainty of the classifier when the classifier is stochastic or, we learn a\nsurrogate model to amortize the uncertainty when it is deterministic. Unlike\n$\\ell_p$ ball or functional attacks which perturb inputs indiscriminately, our\ntargeted changes can be less perceptible. When considered under our threat\nmodel, these attacks still produce strong adversarial examples; with the\nexamples retaining a greater degree of similarity with the inputs.",
          "link": "http://arxiv.org/abs/2106.09222",
          "publishedOn": "2021-06-18T02:06:34.922Z",
          "wordCount": 607,
          "title": "Localized Uncertainty Attacks. (arXiv:2106.09222v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dewen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yawen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinrong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haiyun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Meiping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jian Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingtong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>",
          "description": "The success of deep learning heavily depends on the availability of large\nlabeled training sets. However, it is hard to get large labeled datasets in\nmedical image domain because of the strict privacy concern and costly labeling\nefforts. Contrastive learning, an unsupervised learning technique, has been\nproved powerful in learning image-level representations from unlabeled data.\nThe learned encoder can then be transferred or fine-tuned to improve the\nperformance of downstream tasks with limited labels. A critical step in\ncontrastive learning is the generation of contrastive data pairs, which is\nrelatively simple for natural image classification but quite challenging for\nmedical image segmentation due to the existence of the same tissue or organ\nacross the dataset. As a result, when applied to medical image segmentation,\nmost state-of-the-art contrastive learning frameworks inevitably introduce a\nlot of false-negative pairs and result in degraded segmentation quality. To\naddress this issue, we propose a novel positional contrastive learning (PCL)\nframework to generate contrastive data pairs by leveraging the position\ninformation in volumetric medical images. Experimental results on CT and MRI\ndatasets demonstrate that the proposed PCL method can substantially improve the\nsegmentation performance compared to existing methods in both semi-supervised\nsetting and transfer learning setting.",
          "link": "http://arxiv.org/abs/2106.09157",
          "publishedOn": "2021-06-18T02:06:34.888Z",
          "wordCount": 651,
          "title": "Positional Contrastive Learning for VolumetricMedical Image Segmentation. (arXiv:2106.09157v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seybold_B/0/1/0/all/0/1\">Bryan Seybold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sud_A/0/1/0/all/0/1\">Avneesh Sud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruby_G/0/1/0/all/0/1\">Graham Ruby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>",
          "description": "We present a method to infer the 3D pose of mice, including the limbs and\nfeet, from monocular videos. Many human clinical conditions and their\ncorresponding animal models result in abnormal motion, and accurately measuring\n3D motion at scale offers insights into health. The 3D poses improve\nclassification of health-related attributes over 2D representations. The\ninferred poses are accurate enough to estimate stride length even when the feet\nare mostly occluded. This method could be applied as part of a continuous\nmonitoring system to non-invasively measure animal health.",
          "link": "http://arxiv.org/abs/2106.09251",
          "publishedOn": "2021-06-18T02:06:34.881Z",
          "wordCount": 530,
          "title": "Optical Mouse: 3D Mouse Pose From Single-View Video. (arXiv:2106.09251v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolter_M/0/1/0/all/0/1\">Moritz Wolter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanke_F/0/1/0/all/0/1\">Felix Blanke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyt_C/0/1/0/all/0/1\">Charles Tapley Hoyt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcke_J/0/1/0/all/0/1\">Jochen Garcke</a>",
          "description": "As neural networks become more able to generate realistic artificial images,\nthey have the potential to improve movies, music, video games and make the\ninternet an even more creative and inspiring place. Yet, at the same time, the\nlatest technology potentially enables new digital ways to lie. In response, the\nneed for a diverse and reliable toolbox arises to identify artificial images\nand other content. Previous work primarily relies on pixel-space CNN or the\nFourier transform. To the best of our knowledge, wavelet-based gan analysis and\ndetection methods have been absent thus far. This paper aims to fill this gap\nand describes a wavelet-based approach to gan-generated image analysis and\ndetection. We evaluate our method on FFHQ, CelebA, and LSUN source\nidentification problems and find improved or competitive performance.",
          "link": "http://arxiv.org/abs/2106.09369",
          "publishedOn": "2021-06-18T02:06:34.849Z",
          "wordCount": 568,
          "title": "Wavelet-Packet Powered Deepfake Image Detection. (arXiv:2106.09369v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>",
          "description": "Multimodal image-language transformers have achieved impressive results on a\nvariety of tasks that rely on fine-tuning (e.g., visual question answering and\nimage retrieval). We are interested in shedding light on the quality of their\npretrained representations -- in particular, if these models can distinguish\ndifferent types of verbs or if they rely solely on nouns in a given sentence.\nTo do so, we collect a dataset of image-sentence pairs (in English) consisting\nof 421 verbs that are either visual or commonly found in the pretraining data\n(i.e., the Conceptual Captions dataset). We use this dataset to evaluate\npretrained image-language transformers and find that they fail more in\nsituations that require verb understanding compared to other parts of speech.\nWe also investigate what category of verbs are particularly challenging.",
          "link": "http://arxiv.org/abs/2106.09141",
          "publishedOn": "2021-06-18T02:06:34.840Z",
          "wordCount": 560,
          "title": "Probing Image-Language Transformers for Verb Understanding. (arXiv:2106.09141v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangbing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>",
          "description": "Few-shot learning aims to learn a classifier using a few labelled instances\nfor each class. Metric-learning approaches for few-shot learning embed\ninstances into a high-dimensional space and conduct classification based on\ndistances among instance embeddings. However, such instance embeddings are\nusually shared across all episodes and thus lack the discriminative power to\ngeneralize classifiers according to episode-specific features. In this paper,\nwe propose a novel approach, namely \\emph{Episode Adaptive Embedding Network}\n(EAEN), to learn episode-specific embeddings of instances. By leveraging the\nprobability distributions of all instances in an episode at each channel-pixel\nembedding dimension, EAEN can not only alleviate the overfitting issue\nencountered in few-shot learning tasks, but also capture discriminative\nfeatures specific to an episode. To empirically verify the effectiveness and\nrobustness of EAEN, we have conducted extensive experiments on three widely\nused benchmark datasets, under various combinations of different generic\nembedding backbones and different classifiers. The results show that EAEN\nsignificantly improves classification accuracy about $10\\%$ to $20\\%$ in\ndifferent settings over the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.09398",
          "publishedOn": "2021-06-18T02:06:34.826Z",
          "wordCount": 602,
          "title": "Episode Adaptive Embedding Networks for Few-shot Learning. (arXiv:2106.09398v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chengjun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1\">Ziheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>",
          "description": "Homotopy model is an excellent tool exploited by diverse research works in\nthe field of machine learning. However, its flexibility is limited due to lack\nof adaptiveness, i.e., manual fixing or tuning the appropriate homotopy\ncoefficients. To address the problem above, we propose a novel adaptive\nhomotopy framework (AH) in which the Maclaurin duality is employed, such that\nthe homotopy parameters can be adaptively obtained. Accordingly, the proposed\nAH can be widely utilized to enhance the homotopy-based algorithm. In\nparticular, in this paper, we apply AH to contrastive learning (AHCL) such that\nit can be effectively transferred from weak-supervised learning (given label\npriori) to unsupervised learning, where soft labels of contrastive learning are\ndirectly and adaptively learned. Accordingly, AHCL has the adaptive ability to\nextract deep features without any sort of prior information. Consequently, the\naffinity matrix formulated by the related adaptive labels can be constructed as\nthe deep Laplacian graph that incorporates the topology of deep representations\nfor the inputs. Eventually, extensive experiments on benchmark datasets\nvalidate the superiority of our method.",
          "link": "http://arxiv.org/abs/2106.09244",
          "publishedOn": "2021-06-18T02:06:34.801Z",
          "wordCount": 613,
          "title": "Deep Contrastive Graph Representation via Adaptive Homotopy Learning. (arXiv:2106.09244v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yutian Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Sheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jueming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongming Liu</a>",
          "description": "To evaluate the robustness gain of Bayesian neural networks on image\nclassification tasks, we perform input perturbations, and adversarial attacks\nto the state-of-the-art Bayesian neural networks, with a benchmark CNN model as\nreference. The attacks are selected to simulate signal interference and\ncyberattacks towards CNN-based machine learning systems. The result shows that\na Bayesian neural network achieves significantly higher robustness against\nadversarial attacks generated against a deterministic neural network model,\nwithout adversarial training. The Bayesian posterior can act as the safety\nprecursor of ongoing malicious activities. Furthermore, we show that the\nstochastic classifier after the deterministic CNN extractor has sufficient\nrobustness enhancement rather than a stochastic feature extractor before the\nstochastic classifier. This advises on utilizing stochastic layers in building\ndecision-making pipelines within a safety-critical domain.",
          "link": "http://arxiv.org/abs/2106.09223",
          "publishedOn": "2021-06-18T02:06:34.794Z",
          "wordCount": 570,
          "title": "Evaluating the Robustness of Bayesian Neural Networks Against Different Types of Attacks. (arXiv:2106.09223v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanfir_M/0/1/0/all/0/1\">Mihai Zanfir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanfir_A/0/1/0/all/0/1\">Andrei Zanfir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazavan_E/0/1/0/all/0/1\">Eduard Gabriel Bazavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukthankar_R/0/1/0/all/0/1\">Rahul Sukthankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>",
          "description": "We present THUNDR, a transformer-based deep neural network methodology to\nreconstruct the 3d pose and shape of people, given monocular RGB images. Key to\nour methodology is an intermediate 3d marker representation, where we aim to\ncombine the predictive power of model-free-output architectures and the\nregularizing, anthropometrically-preserving properties of a statistical human\nsurface model like GHUM -- a recently introduced, expressive full body\nstatistical 3d human model, trained end-to-end. Our novel transformer-based\nprediction pipeline can focus on image regions relevant to the task, supports\nself-supervised regimes, and ensures that solutions are consistent with human\nanthropometry. We show state-of-the-art results on Human3.6M and 3DPW, for both\nthe fully-supervised and the self-supervised models, for the task of inferring\n3d human shape, joint positions, and global translation. Moreover, we observe\nvery solid 3d reconstruction performance for difficult human poses collected in\nthe wild.",
          "link": "http://arxiv.org/abs/2106.09336",
          "publishedOn": "2021-06-18T02:06:34.787Z",
          "wordCount": 580,
          "title": "THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers. (arXiv:2106.09336v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumawat_S/0/1/0/all/0/1\">Sudhakar Kumawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_G/0/1/0/all/0/1\">Gagan Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>",
          "description": "Deep neural networks have enormous representational power which leads them to\noverfit on most datasets. Thus, regularizing them is important in order to\nreduce overfitting and enhance their generalization capabilities. Recently,\nchannel shuffle operation has been introduced for mixing channels in group\nconvolutions in resource efficient networks in order to reduce memory and\ncomputations. This paper studies the operation of channel shuffle as a\nregularization technique in deep convolutional networks. We show that while\nrandom shuffling of channels during training drastically reduce their\nperformance, however, randomly shuffling small patches between channels\nsignificantly improves their performance. The patches to be shuffled are picked\nfrom the same spatial locations in the feature maps such that a patch, when\ntransferred from one channel to another, acts as structured noise for the later\nchannel. We call this method \"ShuffleBlock\". The proposed ShuffleBlock module\nis easy to implement and improves the performance of several baseline networks\non the task of image classification on CIFAR and ImageNet datasets. It also\nachieves comparable and in many cases better performance than many other\nregularization methods. We provide several ablation studies on selecting\nvarious hyperparameters of the ShuffleBlock module and propose a new scheduling\nmethod that further enhances its performance.",
          "link": "http://arxiv.org/abs/2106.09358",
          "publishedOn": "2021-06-18T02:06:34.780Z",
          "wordCount": 635,
          "title": "ShuffleBlock: Shuffle to Regularize Deep Convolutional Neural Networks. (arXiv:2106.09358v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaves_L/0/1/0/all/0/1\">Levy Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bissoto_A/0/1/0/all/0/1\">Alceu Bissoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1\">Eduardo Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>",
          "description": "Self-supervised pre-training appears as an advantageous alternative to\nsupervised pre-trained for transfer learning. By synthesizing annotations on\npretext tasks, self-supervision allows to pre-train models on large amounts of\npseudo-labels before fine-tuning them on the target task. In this work, we\nassess self-supervision for the diagnosis of skin lesions, comparing three\nself-supervised pipelines to a challenging supervised baseline, on five test\ndatasets comprising in- and out-of-distribution samples. Our results show that\nself-supervision is competitive both in improving accuracies and in reducing\nthe variability of outcomes. Self-supervision proves particularly useful for\nlow training data scenarios ($<1\\,500$ and $<150$ samples), where its ability\nto stabilize the outcomes is essential to provide sound results.",
          "link": "http://arxiv.org/abs/2106.09229",
          "publishedOn": "2021-06-18T02:06:34.774Z",
          "wordCount": 551,
          "title": "An Evaluation of Self-Supervised Pre-Training for Skin-Lesion Analysis. (arXiv:2106.09229v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorfman_A/0/1/0/all/0/1\">Adam Dorfman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McInnis_P/0/1/0/all/0/1\">Paul McInnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>",
          "description": "In this study, we take a departure and explore an explainability-driven\nstrategy to data auditing, where actionable insights into the data at hand are\ndiscovered through the eyes of quantitative explainability on the behaviour of\na dummy model prototype when exposed to data. We demonstrate this strategy by\nauditing two popular medical benchmark datasets, and discover hidden data\nquality issues that lead deep learning models to make predictions for the wrong\nreasons. The actionable insights gained from this explainability driven data\nauditing strategy is then leveraged to address the discovered issues to enable\nthe creation of high-performing deep learning models with appropriate\nprediction behaviour. The hope is that such an explainability-driven strategy\ncan be complimentary to data-driven strategies to facilitate for more\nresponsible development of machine learning algorithms for computer vision\napplications.",
          "link": "http://arxiv.org/abs/2106.09177",
          "publishedOn": "2021-06-18T02:06:34.745Z",
          "wordCount": 593,
          "title": "Insights into Data through Model Behaviour: An Explainability-driven Strategy for Data Auditing for Responsible Computer Vision Applications. (arXiv:2106.09177v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09303",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bourbia_S/0/1/0/all/0/1\">Salima Bourbia</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Karine_A/0/1/0/all/0/1\">Ayoub Karine</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Chetouani_A/0/1/0/all/0/1\">Aladine Chetouani</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Hassouni_M/0/1/0/all/0/1\">Mohammed El Hassouni</a> (1 and 4) ((1) LRIT, Mohammed V University in Rabat, Rabat, Morocco, (2) L@bISEN, ISEN Yncrea Ouest, 33 Quater Chemin du Champ de Manoeuvre, 44470 Carquefou, France, (3) Laboratoire PRISME, Universite d&#x27;Orl&#xe9;ans, France, (4) FLSH, Mohammed V University in Rabat, Rabat, Morocco)",
          "description": "This paper addresses the problem of blind stereoscopic image quality\nassessment (NR-SIQA) using a new multi-task deep learning based-method. In the\nfield of stereoscopic vision, the information is fairly distributed between the\nleft and right views as well as the binocular phenomenon. In this work, we\npropose to integrate these characteristics to estimate the quality of\nstereoscopic images without reference through a convolutional neural network.\nOur method is based on two main tasks: the first task predicts naturalness\nanalysis based features adapted to stereo images, while the second task\npredicts the quality of such images. The former, so-called auxiliary task, aims\nto find more robust and relevant features to improve the quality prediction. To\ndo this, we compute naturalness-based features using a Natural Scene Statistics\n(NSS) model in the complex wavelet domain. It allows to capture the statistical\ndependency between pairs of the stereoscopic images. Experiments are conducted\non the well known LIVE PHASE I and LIVE PHASE II databases. The results\nobtained show the relevance of our method when comparing with those of the\nstate-of-the-art. Our code is available online on\n\\url{https://github.com/Bourbia-Salima/multitask-cnn-nrsiqa_2021}.",
          "link": "http://arxiv.org/abs/2106.09303",
          "publishedOn": "2021-06-18T02:06:34.734Z",
          "wordCount": 687,
          "title": "A Multi-task convolutional neural network for blind stereoscopic image quality assessment using naturalness analysis. (arXiv:2106.09303v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09311",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Owsianko_H/0/1/0/all/0/1\">Haley Owsianko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cassayre_F/0/1/0/all/0/1\">Florian Cassayre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Q/0/1/0/all/0/1\">Qiyuan Liang</a>",
          "description": "Image denoising is a classic restoration problem. Yet, current deep learning\nmethods are subject to the problems of generalization and interpretability. To\nmitigate these problems, in this project, we present a framework that is\ncapable of controllable, confidence-based noise removal. The framework is based\non the fusion between two different denoised images, both derived from the same\nnoisy input. One of the two is denoised using generic algorithms (e.g.\nGaussian), which make few assumptions on the input images, therefore,\ngeneralize in all scenarios. The other is denoised using deep learning,\nperforming well on seen datasets. We introduce a set of techniques to fuse the\ntwo components smoothly in the frequency domain. Beyond that, we estimate the\nconfidence of a deep learning denoiser to allow users to interpret the output,\nand provide a fusion strategy that safeguards them against out-of-distribution\ninputs. Through experiments, we demonstrate the effectiveness of the proposed\nframework in different use cases.",
          "link": "http://arxiv.org/abs/2106.09311",
          "publishedOn": "2021-06-18T02:06:34.727Z",
          "wordCount": 588,
          "title": "Controllable Confidence-Based Image Denoising. (arXiv:2106.09311v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dror_A/0/1/0/all/0/1\">Amir Ben Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zehngut_N/0/1/0/all/0/1\">Niv Zehngut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_A/0/1/0/all/0/1\">Avraham Raviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artyomov_E/0/1/0/all/0/1\">Evgeny Artyomov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitek_R/0/1/0/all/0/1\">Ran Vitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jevnisek_R/0/1/0/all/0/1\">Roy Jevnisek</a>",
          "description": "Despite the increasing prevalence of deep neural networks, their\napplicability in resource-constrained devices is limited due to their\ncomputational load. While modern devices exhibit a high level of parallelism,\nreal-time latency is still highly dependent on networks' depth. Although recent\nworks show that below a certain depth, the width of shallower networks must\ngrow exponentially, we presume that neural networks typically exceed this\nminimal depth to accelerate convergence and incrementally increase accuracy.\nThis motivates us to transform pre-trained deep networks that already exploit\nsuch advantages into shallower forms. We propose a method that learns whether\nnon-linear activations can be removed, allowing to fold consecutive linear\nlayers into one. We apply our method to networks pre-trained on CIFAR-10 and\nCIFAR-100 and find that they can all be transformed into shallower forms that\nshare a similar depth. Finally, we use our method to provide more efficient\nalternatives to MobileNetV2 and EfficientNet-Lite architectures on the ImageNet\nclassification task.",
          "link": "http://arxiv.org/abs/2106.09309",
          "publishedOn": "2021-06-18T02:06:34.712Z",
          "wordCount": 597,
          "title": "Layer Folding: Neural Network Depth Reduction using Activation Linearization. (arXiv:2106.09309v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao%2A_Y/0/1/0/all/0/1\">Yulong Cao*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang%2A_N/0/1/0/all/0/1\">Ningfei Wang*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao%2A_C/0/1/0/all/0/1\">Chaowei Xiao*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang%2A_D/0/1/0/all/0/1\">Dawei Yang*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Alfred Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a> (*co-first authors)",
          "description": "In Autonomous Driving (AD) systems, perception is both security and safety\ncritical. Despite various prior studies on its security issues, all of them\nonly consider attacks on camera- or LiDAR-based AD perception alone. However,\nproduction AD systems today predominantly adopt a Multi-Sensor Fusion (MSF)\nbased design, which in principle can be more robust against these attacks under\nthe assumption that not all fusion sources are (or can be) attacked at the same\ntime. In this paper, we present the first study of security issues of MSF-based\nperception in AD systems. We directly challenge the basic MSF design assumption\nabove by exploring the possibility of attacking all fusion sources\nsimultaneously. This allows us for the first time to understand how much\nsecurity guarantee MSF can fundamentally provide as a general defense strategy\nfor AD perception.\n\nWe formulate the attack as an optimization problem to generate a\nphysically-realizable, adversarial 3D-printed object that misleads an AD system\nto fail in detecting it and thus crash into it. We propose a novel attack\npipeline that addresses two main design challenges: (1) non-differentiable\ntarget camera and LiDAR sensing systems, and (2) non-differentiable cell-level\naggregated features popularly used in LiDAR-based AD perception. We evaluate\nour attack on MSF included in representative open-source industry-grade AD\nsystems in real-world driving scenarios. Our results show that the attack\nachieves over 90% success rate across different object types and MSF. Our\nattack is also found stealthy, robust to victim positions, transferable across\nMSF algorithms, and physical-world realizable after being 3D-printed and\ncaptured by LiDAR and camera devices. To concretely assess the end-to-end\nsafety impact, we further perform simulation evaluation and show that it can\ncause a 100% vehicle collision rate for an industry-grade AD system.",
          "link": "http://arxiv.org/abs/2106.09249",
          "publishedOn": "2021-06-18T02:06:34.705Z",
          "wordCount": 769,
          "title": "Invisible for both Camera and LiDAR: Security of Multi-Sensor Fusion based Perception in Autonomous Driving Under Physical-World Attacks. (arXiv:2106.09249v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09302",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ross_T/0/1/0/all/0/1\">Tobias Ro&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_P/0/1/0/all/0/1\">Pierangela Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinke_A/0/1/0/all/0/1\">Annika Reinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1\">Manuel Wiesenfarth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koeppel_L/0/1/0/all/0/1\">Lisa Koeppel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Full_P/0/1/0/all/0/1\">Peter M. Full</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pekdemir_B/0/1/0/all/0/1\">B&#xfc;nyamin Pekdemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1\">Patrick Godau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trofimova_D/0/1/0/all/0/1\">Darya Trofimova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1\">Sara Moccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calimeri_F/0/1/0/all/0/1\">Francesco Calimeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1\">Beat P. M&#xfc;ller-Stich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_Schneider_A/0/1/0/all/0/1\">Annette Kopp-Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>",
          "description": "Challenges have become the state-of-the-art approach to benchmark image\nanalysis algorithms in a comparative manner. While the validation on identical\ndata sets was a great step forward, results analysis is often restricted to\npure ranking tables, leaving relevant questions unanswered. Specifically,\nlittle effort has been put into the systematic investigation on what\ncharacterizes images in which state-of-the-art algorithms fail. To address this\ngap in the literature, we (1) present a statistical framework for learning from\nchallenges and (2) instantiate it for the specific task of instrument instance\nsegmentation in laparoscopic videos. Our framework relies on the semantic meta\ndata annotation of images, which serves as foundation for a General Linear\nMixed Models (GLMM) analysis. Based on 51,542 meta data annotations performed\non 2,728 images, we applied our approach to the results of the Robust Medical\nInstrument Segmentation Challenge (ROBUST-MIS) challenge 2019 and revealed\nunderexposure, motion and occlusion of instruments as well as the presence of\nsmoke or other objects in the background as major sources of algorithm failure.\nOur subsequent method development, tailored to the specific remaining issues,\nyielded a deep learning model with state-of-the-art overall performance and\nspecific strengths in the processing of images in which previous methods tended\nto fail. Due to the objectivity and generic applicability of our approach, it\ncould become a valuable tool for validation in the field of medical image\nanalysis and beyond. and segmentation of small, crossing, moving and\ntransparent instrument(s) (parts).",
          "link": "http://arxiv.org/abs/2106.09302",
          "publishedOn": "2021-06-18T02:06:34.698Z",
          "wordCount": 712,
          "title": "How can we learn (more) from challenges? A statistical approach to driving future algorithm development. (arXiv:2106.09302v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Sadegh R Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yu-Chi Hu</a>",
          "description": "Purpose: Radiotherapy presents unique challenges and clinical requirements\nfor longitudinal tumor and organ-at-risk (OAR) prediction during treatment. The\nchallenges include tumor inflammation/edema and radiation-induced changes in\norgan geometry, whereas the clinical requirements demand flexibility in\ninput/output sequence timepoints to update the predictions on rolling basis and\nthe grounding of all predictions in relationship to the pre-treatment imaging\ninformation for response and toxicity assessment in adaptive radiotherapy.\nMethods: To deal with the aforementioned challenges and to comply with the\nclinical requirements, we present a novel 3D sequence-to-sequence model based\non Convolution Long Short Term Memory (ConvLSTM) that makes use of series of\ndeformation vector fields (DVF) between individual timepoints and reference\npre-treatment/planning CTs to predict future anatomical deformations and\nchanges in gross tumor volume as well as critical OARs. High-quality DVF\ntraining data is created by employing hyper-parameter optimization on the\nsubset of the training data with DICE coefficient and mutual information\nmetric. We validated our model on two radiotherapy datasets: a publicly\navailable head-and-neck dataset (28 patients with manually contoured pre-,\nmid-, and post-treatment CTs), and an internal non-small cell lung cancer\ndataset (63 patients with manually contoured planning CT and 6 weekly CBCTs).\nResults: The use of DVF representation and skip connections overcomes the\nblurring issue of ConvLSTM prediction with the traditional image\nrepresentation. The mean and standard deviation of DICE for predictions of lung\nGTV at week 4, 5, and 6 were 0.83$\\pm$0.09, 0.82$\\pm$0.08, and 0.81$\\pm$0.10,\nrespectively, and for post-treatment ipsilateral and contralateral parotids,\nwere 0.81$\\pm$0.06 and 0.85$\\pm$0.02.",
          "link": "http://arxiv.org/abs/2106.09076",
          "publishedOn": "2021-06-18T02:06:34.690Z",
          "wordCount": 706,
          "title": "Deformation Driven Seq2Seq Longitudinal Tumor and Organs-at-Risk Prediction for Radiotherapy. (arXiv:2106.09076v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wei Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miaomiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>",
          "description": "Human motion prediction aims to forecast future human poses given a\nhistorical motion. Whether based on recurrent or feed-forward neural networks,\nexisting learning based methods fail to model the observation that human motion\ntends to repeat itself, even for complex sports actions and cooking activities.\nHere, we introduce an attention based feed-forward network that explicitly\nleverages this observation. In particular, instead of modeling frame-wise\nattention via pose similarity, we propose to extract motion attention to\ncapture the similarity between the current motion context and the historical\nmotion sub-sequences. In this context, we study the use of different types of\nattention, computed at joint, body part, and full pose levels. Aggregating the\nrelevant past motions and processing the result with a graph convolutional\nnetwork allows us to effectively exploit motion patterns from the long-term\nhistory to predict the future poses. Our experiments on Human3.6M, AMASS and\n3DPW validate the benefits of our approach for both periodical and\nnon-periodical actions. Thanks to our attention model, it yields\nstate-of-the-art results on all three datasets. Our code is available at\nhttps://github.com/wei-mao-2019/HisRepItself.",
          "link": "http://arxiv.org/abs/2106.09300",
          "publishedOn": "2021-06-18T02:06:34.659Z",
          "wordCount": 625,
          "title": "Multi-level Motion Attention for Human Motion Prediction. (arXiv:2106.09300v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_A/0/1/0/all/0/1\">Anjana Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barmaki_R/0/1/0/all/0/1\">Roghayeh Barmaki</a>",
          "description": "Autism spectrum disorder (ASD) is a developmental disorder that influences\nthe communication and social behavior of a person in a way that those in the\nspectrum have difficulty in perceiving other people's facial expressions, as\nwell as presenting and communicating emotions and affect via their own faces\nand bodies. Some efforts have been made to predict and improve children with\nASD's affect states in play therapy, a common method to improve children's\nsocial skills via play and games. However, many previous works only used\npre-trained models on benchmark emotion datasets and failed to consider the\ndistinction in emotion between typically developing children and children with\nautism. In this paper, we present an open-source two-stage multi-modal approach\nleveraging acoustic and visual cues to predict three main affect states of\nchildren with ASD's affect states (positive, negative, and neutral) in\nreal-world play therapy scenarios, and achieved an overall accuracy of 72:40%.\nThis work presents a novel way to combine human expertise and machine\nintelligence for ASD affect recognition by proposing a two-stage schema.",
          "link": "http://arxiv.org/abs/2106.09199",
          "publishedOn": "2021-06-18T02:06:34.633Z",
          "wordCount": 636,
          "title": "A Two-stage Multi-modal Affect Analysis Framework for Children with Autism Spectrum Disorder. (arXiv:2106.09199v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Joonyoung Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>",
          "description": "Unsupervised image-to-image translation methods such as CycleGAN learn to\nconvert images from one domain to another using unpaired training data sets\nfrom different domains. Unfortunately, these approaches still require centrally\ncollected unpaired records, potentially violating privacy and security issues.\nAlthough the recent federated learning (FL) allows a neural network to be\ntrained without data exchange, the basic assumption of the FL is that all\nclients have their own training data from a similar domain, which is different\nfrom our image-to-image translation scenario in which each client has images\nfrom its unique domain and the goal is to learn image translation between\ndifferent domains without accessing the target domain data. To address this,\nhere we propose a novel federated CycleGAN architecture that can learn image\ntranslation in an unsupervised manner while maintaining the data privacy.\nSpecifically, our approach arises from a novel observation that CycleGAN loss\ncan be decomposed into the sum of client specific local objectives that can be\nevaluated using only their data. This local objective decomposition allows\nmultiple clients to participate in federated CycleGAN training without\nsacrificing performance. Furthermore, our method employs novel switchable\ngenerator and discriminator architecture using Adaptive Instance Normalization\n(AdaIN) that significantly reduces the band-width requirement of the federated\nlearning. Our experimental results on various unsupervised image translation\ntasks show that our federated CycleGAN provides comparable performance compared\nto the non-federated counterpart.",
          "link": "http://arxiv.org/abs/2106.09246",
          "publishedOn": "2021-06-18T02:06:34.623Z",
          "wordCount": 666,
          "title": "Federated CycleGAN for Privacy-Preserving Image-to-Image Translation. (arXiv:2106.09246v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yun-Hao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>",
          "description": "This paper starts by revealing a surprising finding: without any learning, a\nrandomly initialized CNN can localize objects surprisingly well. That is, a CNN\nhas an inductive bias to naturally focus on objects, named as Tobias (``The\nobject is at sight'') in this paper. This empirical inductive bias is further\nanalyzed and successfully applied to self-supervised learning. A CNN is\nencouraged to learn representations that focus on the foreground object, by\ntransforming every image into various versions with different backgrounds,\nwhere the foreground and background separation is guided by Tobias.\nExperimental results show that the proposed Tobias significantly improves\ndownstream tasks, especially for object detection. This paper also shows that\nTobias has consistent improvements on training sets of different sizes, and is\nmore resilient to changes in image augmentations. Our codes will be available\nat https://github.com/CupidJay/Tobias.",
          "link": "http://arxiv.org/abs/2106.09259",
          "publishedOn": "2021-06-18T02:06:34.581Z",
          "wordCount": 591,
          "title": "A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications. (arXiv:2106.09259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdev_V/0/1/0/all/0/1\">Vandana Sachdev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antani_S/0/1/0/all/0/1\">Sameer Antani</a>",
          "description": "Accurate segmentation of medical images into anatomically meaningful regions\nis critical for the extraction of quantitative indices or biomarkers. The\ncommon pipeline for segmentation comprises regions of interest detection stage\nand segmentation stage, which are independent of each other and typically\nperformed using separate deep learning networks. The performance of the\nsegmentation stage highly relies on the extracted set of spatial features and\nthe receptive fields. In this work, we propose an end-to-end network, called\nTrilateral Attention Network (TaNet), for real-time detection and segmentation\nin medical images. TaNet has a module for region localization, and three\nsegmentation pathways: 1) handcrafted pathway with hand-designed convolutional\nkernels, 2) detail pathway with regular convolutional kernels, and 3) a global\npathway to enlarge the receptive field. The first two pathways encode rich\nhandcrafted and low-level features extracted by hand-designed and regular\nkernels while the global pathway encodes high-level context information. By\njointly training the network for localization and segmentation using different\nsets of features, TaNet achieved superior performance, in terms of accuracy and\nspeed, when evaluated on an echocardiography dataset for cardiac segmentation.\nThe code and models will be made publicly available in TaNet Github page.",
          "link": "http://arxiv.org/abs/2106.09201",
          "publishedOn": "2021-06-18T02:06:34.533Z",
          "wordCount": 625,
          "title": "Trilateral Attention Network for Real-time Medical Image Segmentation. (arXiv:2106.09201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jiahao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>",
          "description": "Enforcing orthogonality in neural networks is an antidote for gradient\nvanishing/exploding problems, sensitivity by adversarial perturbation, and\nbounding generalization errors. However, many previous approaches are\nheuristic, and the orthogonality of convolutional layers is not systematically\nstudied: some of these designs are not exactly orthogonal, while others only\nconsider standard convolutional layers and propose specific classes of their\nrealizations. To address this problem, we propose a theoretical framework for\northogonal convolutional layers, which establishes the equivalence between\nvarious orthogonal convolutional layers in the spatial domain and the\nparaunitary systems in the spectral domain. Since there exists a complete\nspectral factorization of paraunitary systems, any orthogonal convolution layer\ncan be parameterized as convolutions of spatial filters. Our framework endows\nhigh expressive power to various convolutional layers while maintaining their\nexact orthogonality. Furthermore, our layers are memory and computationally\nefficient for deep networks compared to previous designs. Our versatile\nframework, for the first time, enables the study of architecture designs for\ndeep orthogonal networks, such as choices of skip connection, initialization,\nstride, and dilation. Consequently, we scale up orthogonal networks to deep\narchitectures, including ResNet, WideResNet, and ShuffleNet, substantially\nincreasing the performance over the traditional shallow orthogonal networks.",
          "link": "http://arxiv.org/abs/2106.09121",
          "publishedOn": "2021-06-18T02:06:34.526Z",
          "wordCount": 638,
          "title": "Scaling-up Diverse Orthogonal Convolutional Networks with a Paraunitary Framework. (arXiv:2106.09121v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1\">Lucas Caccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>",
          "description": "This paper presents SPeCiaL: a method for unsupervised pretraining of\nrepresentations tailored for continual learning. Our approach devises a\nmeta-learning objective that differentiates through a sequential learning\nprocess. Specifically, we train a linear model over the representations to\nmatch different augmented views of the same image together, each view presented\nsequentially. The linear model is then evaluated on both its ability to\nclassify images it just saw, and also on images from previous iterations. This\ngives rise to representations that favor quick knowledge retention with minimal\nforgetting. We evaluate SPeCiaL in the Continual Few-Shot Learning setting, and\nshow that it can match or outperform other supervised pretraining approaches.",
          "link": "http://arxiv.org/abs/2106.09065",
          "publishedOn": "2021-06-18T02:06:34.505Z",
          "wordCount": 539,
          "title": "SPeCiaL: Self-Supervised Pretraining for Continual Learning. (arXiv:2106.09065v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Paul Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1\">Christoph H. Lampert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bickel_B/0/1/0/all/0/1\">Bernd Bickel</a>",
          "description": "Our goal in this work is to generate realistic videos given just one initial\nframe as input. Existing unsupervised approaches to this task do not consider\nthe fact that a video typically shows a 3D environment, and that this should\nremain coherent from frame to frame even as the camera and objects move. We\naddress this by developing a model that first estimates the latent 3D structure\nof the scene, including the segmentation of any moving objects. It then\npredicts future frames by simulating the object and camera dynamics, and\nrendering the resulting views. Importantly, it is trained end-to-end using only\nthe unsupervised objective of predicting future frames, without any 3D\ninformation nor segmentation annotations. Experiments on two challenging\ndatasets of natural videos show that our model can estimate 3D structure and\nmotion segmentation from a single frame, and hence generate plausible and\nvaried predictions.",
          "link": "http://arxiv.org/abs/2106.09051",
          "publishedOn": "2021-06-18T02:06:34.499Z",
          "wordCount": 598,
          "title": "Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure. (arXiv:2106.09051v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Du Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>",
          "description": "Video transformers have recently emerged as a competitive alternative to 3D\nCNNs for video understanding. However, due to their large number of parameters\nand reduced inductive biases, these models require supervised pretraining on\nlarge-scale image datasets to achieve top performance. In this paper, we\nempirically demonstrate that self-supervised pretraining of video transformers\non video-only datasets can lead to action recognition results that are on par\nor better than those obtained with supervised pretraining on large-scale image\ndatasets, even massive ones such as ImageNet-21K. Since transformer-based\nmodels are effective at capturing dependencies over extended temporal spans, we\npropose a simple learning procedure that forces the model to match a long-term\nview to a short-term view of the same video. Our approach, named Long-Short\nTemporal Contrastive Learning (LSTCL), enables video transformers to learn an\neffective clip-level representation by predicting temporal context captured\nfrom a longer temporal extent. To demonstrate the generality of our findings,\nwe implement and validate our approach under three different self-supervised\ncontrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct\nvideo-transformer architectures, including an improved variant of the Swin\nTransformer augmented with space-time attention. We conduct a thorough ablation\nstudy and show that LSTCL achieves competitive performance on multiple video\nbenchmarks and represents a convincing alternative to supervised image-based\npretraining.",
          "link": "http://arxiv.org/abs/2106.09212",
          "publishedOn": "2021-06-18T02:06:34.491Z",
          "wordCount": 652,
          "title": "Long-Short Temporal Contrastive Learning of Video Transformers. (arXiv:2106.09212v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonnaire_T/0/1/0/all/0/1\">Tony Bonnaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decelle_A/0/1/0/all/0/1\">Aur&#xe9;lien Decelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghanim_N/0/1/0/all/0/1\">Nabila Aghanim</a>",
          "description": "A regularized version of Mixture Models is proposed to learn a principal\ngraph from a distribution of $D$-dimensional data points. In the particular\ncase of manifold learning for ridge detection, we assume that the underlying\nmanifold can be modeled as a graph structure acting like a topological prior\nfor the Gaussian clusters turning the problem into a maximum a posteriori\nestimation. Parameters of the model are iteratively estimated through an\nExpectation-Maximization procedure making the learning of the structure\ncomputationally efficient with guaranteed convergence for any graph prior in a\npolynomial time. We also embed in the formalism a natural way to make the\nalgorithm robust to outliers of the pattern and heteroscedasticity of the\nmanifold sampling coherently with the graph structure. The method uses a graph\nprior given by the minimum spanning tree that we extend using random\nsub-samplings of the dataset to take into account cycles that can be observed\nin the spatial distribution.",
          "link": "http://arxiv.org/abs/2106.09035",
          "publishedOn": "2021-06-18T02:06:34.475Z",
          "wordCount": 605,
          "title": "Regularization of Mixture Models for Robust Principal Graph Learning. (arXiv:2106.09035v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingchuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mira_R/0/1/0/all/0/1\">Rodrigo Mira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Stavros Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>",
          "description": "The large amount of audiovisual content being shared online today has drawn\nsubstantial attention to the prospect of audiovisual self-supervised learning.\nRecent works have focused on each of these modalities separately, while others\nhave attempted to model both simultaneously in a cross-modal fashion. However,\ncomparatively little attention has been given to leveraging one modality as a\ntraining objective to learn from the other. In this work, we propose Learning\nvisual speech Representations from Audio via self-supervision (LiRA).\nSpecifically, we train a ResNet+Conformer model to predict acoustic features\nfrom unlabelled visual speech. We find that this pre-trained model can be\nleveraged towards word-level and sentence-level lip-reading through feature\nextraction and fine-tuning experiments. We show that our approach significantly\noutperforms other self-supervised methods on the Lip Reading in the Wild (LRW)\ndataset and achieves state-of-the-art performance on Lip Reading Sentences 2\n(LRS2) using only a fraction of the total labelled data.",
          "link": "http://arxiv.org/abs/2106.09171",
          "publishedOn": "2021-06-18T02:06:34.469Z",
          "wordCount": 608,
          "title": "LiRA: Learning Visual Speech Representations from Audio through Self-supervision. (arXiv:2106.09171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kay_J/0/1/0/all/0/1\">Justin Kay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrifield_M/0/1/0/all/0/1\">Matt Merrifield</a>",
          "description": "Camera-based electronic monitoring (EM) systems are increasingly being\ndeployed onboard commercial fishing vessels to collect essential data for\nfisheries management and regulation. These systems generate large quantities of\nvideo data which must be reviewed on land by human experts. Computer vision can\nassist this process by automatically detecting and classifying fish species,\nhowever the lack of existing public data in this domain has hindered progress.\nTo address this, we present the Fishnet Open Images Database, a large dataset\nof EM imagery for fish detection and fine-grained categorization onboard\ncommercial fishing vessels. The dataset consists of 86,029 images containing 34\nobject classes, making it the largest and most diverse public dataset of\nfisheries EM imagery to-date. It includes many of the characteristic challenges\nof EM data: visual similarity between species, skewed class distributions,\nharsh weather conditions, and chaotic crew activity. We evaluate the\nperformance of existing detection and classification algorithms and demonstrate\nthat the dataset can serve as a challenging benchmark for development of\ncomputer vision algorithms in fisheries. The dataset is available at\nhttps://www.fishnet.ai/.",
          "link": "http://arxiv.org/abs/2106.09178",
          "publishedOn": "2021-06-18T02:06:34.427Z",
          "wordCount": 637,
          "title": "The Fishnet Open Images Database: A Dataset for Fish Detection and Fine-Grained Categorization in Fisheries. (arXiv:2106.09178v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Recently, transformation-based self-supervised learning has been applied to\ngenerative adversarial networks (GANs) to mitigate the catastrophic forgetting\nproblem of discriminator by learning stable representations. However, the\nseparate self-supervised tasks in existing self-supervised GANs cause an\ninconsistent goal with generative modeling due to the learning of the generator\nfrom their generator distribution-agnostic classifiers. To address this issue,\nwe propose a novel self-supervised GANs framework with label augmentation,\ni.e., augmenting the GAN labels (real or fake) with the self-supervised\npseudo-labels. In particular, the discriminator and the self-supervised\nclassifier are unified to learn a single task that predicts the augmented label\nsuch that the discriminator/classifier is aware of the generator distribution,\nwhile the generator tries to confuse the discriminator/classifier by optimizing\nthe discrepancy between the transformed real and generated distributions.\nTheoretically, we prove that the generator, at the equilibrium point, converges\nto replicate the data distribution. Empirically, we demonstrate that the\nproposed method significantly outperforms competitive baselines on both\ngenerative modeling and representation learning across benchmark datasets.",
          "link": "http://arxiv.org/abs/2106.08601",
          "publishedOn": "2021-06-17T16:16:41.579Z",
          "wordCount": 589,
          "title": "Self-supervised GANs with Label Augmentation. (arXiv:2106.08601v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaomeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1\">Michael Potter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gaurav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Chan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1\">V. Ratna Saripalli</a>",
          "description": "It is no secret amongst deep learning researchers that finding the right data\naugmentation strategy during training can mean the difference between a\nstate-of-the-art result and a run-of-the-mill ranking. To that end, the\ncommunity has seen many efforts to automate the process of finding the perfect\naugmentation procedure for any task at hand. Unfortunately, even recent\ncutting-edge methods bring massive computational overhead, requiring as many as\n100 full model trainings to settle on an ideal configuration. We show how to\nachieve even better performance in just 7: with Random Unidimensional\nAugmentation. Source code is available at https://github.com/fastestimator/RUA",
          "link": "http://arxiv.org/abs/2106.08756",
          "publishedOn": "2021-06-17T16:16:41.547Z",
          "wordCount": 528,
          "title": "Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1\">Sophia Bano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casella_A/0/1/0/all/0/1\">Alessandro Casella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisco Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1\">Sara Moccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attilakos_G/0/1/0/all/0/1\">George Attilakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wimalasundera_R/0/1/0/all/0/1\">Ruwan Wimalasundera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paladini_D/0/1/0/all/0/1\">Dario Paladini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deprest_J/0/1/0/all/0/1\">Jan Deprest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momi_E/0/1/0/all/0/1\">Elena De Momi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattos_L/0/1/0/all/0/1\">Leonardo S. Mattos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>",
          "description": "Fetoscopy laser photocoagulation is a widely used procedure for the treatment\nof Twin-to-Twin Transfusion Syndrome (TTTS), that occur in mono-chorionic\nmultiple pregnancies due to placental vascular anastomoses. This procedure is\nparticularly challenging due to limited field of view, poor manoeuvrability of\nthe fetoscope, poor visibility due to fluid turbidity, variability in light\nsource, and unusual position of the placenta. This may lead to increased\nprocedural time and incomplete ablation, resulting in persistent TTTS.\nComputer-assisted intervention may help overcome these challenges by expanding\nthe fetoscopic field of view through video mosaicking and providing better\nvisualization of the vessel network. However, the research and development in\nthis domain remain limited due to unavailability of high-quality data to encode\nthe intra- and inter-procedure variability. Through the \\textit{Fetoscopic\nPlacental Vessel Segmentation and Registration (FetReg)} challenge, we present\na large-scale multi-centre dataset for the development of generalized and\nrobust semantic segmentation and video mosaicking algorithms for the fetal\nenvironment with a focus on creating drift-free mosaics from long duration\nfetoscopy videos. In this paper, we provide an overview of the FetReg dataset,\nchallenge tasks, evaluation metrics and baseline methods for both segmentation\nand registration. Baseline methods results on the FetReg dataset shows that our\ndataset poses interesting challenges, offering large opportunity for the\ncreation of novel methods and models through a community effort initiative\nguided by the FetReg challenge.",
          "link": "http://arxiv.org/abs/2106.05923",
          "publishedOn": "2021-06-17T15:44:16.763Z",
          "wordCount": 708,
          "title": "FetReg: Placental Vessel Segmentation and Registration in Fetoscopy Challenge Dataset. (arXiv:2106.05923v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Min-Fong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao-Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Min-Hung Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yu-Syuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hsien-Kai Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Min Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hung-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jou_K/0/1/0/all/0/1\">Kevin Jou</a>",
          "description": "Network spaces have been known as a critical factor in both handcrafted\nnetwork designs or defining search spaces for Neural Architecture Search (NAS).\nHowever, an effective space involves tremendous prior knowledge and/or manual\neffort, and additional constraints are required to discover efficiency-aware\narchitectures. In this paper, we define a new problem, Network Space Search\n(NSS), as searching for favorable network spaces instead of a single\narchitecture. We propose an NSS method to directly search for efficient-aware\nnetwork spaces automatically, reducing the manual effort and immense cost in\ndiscovering satisfactory ones. The resultant network spaces, named Elite\nSpaces, are discovered from Expanded Search Space with minimal human expertise\nimposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front\nunder various complexity constraints and can be further served as NAS search\nspaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an\naveragely 2.3% lower error rate and 3.7% closer to target constraint than the\nbaseline with around 90% fewer samples required to find satisfactory networks).\nMoreover, our NSS approach is capable of searching for superior spaces in\nfuture unexplored spaces, revealing great potential in searching for network\nspaces automatically. Website:\nhttps://minhungchen.netlify.app/publication/nss/.",
          "link": "http://arxiv.org/abs/2104.11014",
          "publishedOn": "2021-06-17T15:44:16.740Z",
          "wordCount": 697,
          "title": "Network Space Search for Pareto-Efficient Spaces. (arXiv:2104.11014v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yicheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yongqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiahui Zhu</a>",
          "description": "Recovering 3D human pose from 2D joints is a highly unconstrained problem,\nespecially without any video or multi-view information. We present an\nunsupervised GAN-based model to recover 3D human pose from 2D joint locations\nextracted from a single image. Our model uses a GAN to learn the mapping of\ndistribution from 2D poses to 3D poses, not the simple 2D-3D correspondence.\nConsidering the reprojection constraint, our model can estimate the camera so\nthat we can reproject the estimated 3D pose to the original 2D pose. Based on\nthis reprojection method, we can rotate and reproject the generated pose to get\nour \"new\" 2D pose and then use a weight sharing generator to estimate the \"new\"\n3D pose and a \"new\" camera. Through the above estimation process, we can define\nthe single-view-multi-angle consistency loss during training to simulate\nmulti-view consistency, which means the 3D poses and cameras estimated from two\nangles of a single view should be able to be mixed to generate rich 2D\nreprojections, and the 2D reprojections reprojected from the same 3D pose\nshould be consistent. The experimental results on Human3.6M show that our\nmethod outperforms all the state-of-the-art methods, and results on\nMPI-INF-3DHP show that our method outperforms state-of-the-art by approximately\n15.0%.",
          "link": "http://arxiv.org/abs/2106.05616",
          "publishedOn": "2021-06-17T15:44:16.717Z",
          "wordCount": 653,
          "title": "SVMA: A GAN-based model for Monocular 3D Human Pose Estimation. (arXiv:2106.05616v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shichong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moazeni_A/0/1/0/all/0/1\">Alireza Moazeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>",
          "description": "Deep generative models such as GANs have driven impressive advances in\nconditional image synthesis in recent years. A persistent challenge has been to\ngenerate diverse versions of output images from the same input image, due to\nthe problem of mode collapse: because only one ground truth output image is\ngiven per input image, only one mode of the conditional distribution is\nmodelled. In this paper, we focus on this problem of multimodal conditional\nimage synthesis and build on the recently proposed technique of Implicit\nMaximum Likelihood Estimation (IMLE). Prior IMLE-based methods required\ndifferent architectures for different tasks, which limit their applicability,\nand were lacking in fine details in the generated images. We propose CAM-Net, a\nunified architecture that can be applied to a broad range of tasks.\nAdditionally, it is capable of generating convincing high frequency details,\nachieving a reduction of the Frechet Inception Distance (FID) by up to 45.3%\ncompared to the baseline.",
          "link": "http://arxiv.org/abs/2106.09015",
          "publishedOn": "2021-06-17T01:58:45.215Z",
          "wordCount": 627,
          "title": "Cascading Modular Network (CAM-Net) for Multimodal Image Synthesis. (arXiv:2106.09015v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guerdan_L/0/1/0/all/0/1\">Luke Guerdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raymond_A/0/1/0/all/0/1\">Alex Raymond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1\">Hatice Gunes</a>",
          "description": "As machine learning approaches are increasingly used to augment human\ndecision-making, eXplainable Artificial Intelligence (XAI) research has\nexplored methods for communicating system behavior to humans. However, these\napproaches often fail to account for the emotional responses of humans as they\ninteract with explanations. Facial affect analysis, which examines human facial\nexpressions of emotions, is one promising lens for understanding how users\nengage with explanations. Therefore, in this work, we aim to (1) identify which\nfacial affect features are pronounced when people interact with XAI interfaces,\nand (2) develop a multitask feature embedding for linking facial affect signals\nwith participants' use of explanations. Our analyses and results show that the\noccurrence and values of facial AU1 and AU4, and Arousal are heightened when\nparticipants fail to use explanations effectively. This suggests that facial\naffect analysis should be incorporated into XAI to personalize explanations to\nindividuals' interaction styles and to adapt explanations based on the\ndifficulty of the task performed.",
          "link": "http://arxiv.org/abs/2106.08761",
          "publishedOn": "2021-06-17T01:58:45.117Z",
          "wordCount": 595,
          "title": "Toward Affective XAI: Facial Affect Analysis for Understanding Explainable Human-AI Interactions. (arXiv:2106.08761v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1\">Joni Korpihalkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1\">Tuomo Sipola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puuska_S/0/1/0/all/0/1\">Samir Puuska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1\">Tero Kokkonen</a>",
          "description": "Computer vision and machine learning can be used to automate various tasks in\ncancer diagnostic and detection. If an attacker can manipulate the automated\nprocessing, the results can be devastating and in the worst case lead to wrong\ndiagnosis and treatment. In this research, the goal is to demonstrate the use\nof one-pixel attacks in a real-life scenario with a real pathology dataset,\nTUPAC16, which consists of digitized whole-slide images. We attack against the\nIBM CODAIT's MAX breast cancer detector using adversarial images. These\nadversarial examples are found using differential evolution to perform the\none-pixel modification to the images in the dataset. The results indicate that\na minor one-pixel modification of a whole slide image under analysis can affect\nthe diagnosis by reversing the automatic diagnosis result. The attack poses a\nthreat from the cyber security perspective: the one-pixel method can be used as\nan attack vector by a motivated attacker.",
          "link": "http://arxiv.org/abs/2012.00517",
          "publishedOn": "2021-06-17T01:58:45.110Z",
          "wordCount": 632,
          "title": "One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer. (arXiv:2012.00517v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1\">Poorya Aghdaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1\">Baaria Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "This work investigates the well-known problem of morphing attacks, which has\ndrawn considerable attention in the biometrics community. Morphed images have\nexposed face recognition systems' susceptibility to false acceptance, resulting\nin dire consequences, especially for national security applications. To detect\nmorphing attacks, we propose a method which is based on a discriminative 2D\nDiscrete Wavelet Transform (2D-DWT). A discriminative wavelet sub-band can\nhighlight inconsistencies between a real and a morphed image. We observe that\nthere is a salient discrepancy between the entropy of a given sub-band in a\nbona fide image, and the same sub-band's entropy in a morphed sample.\nConsidering this dissimilarity between these two entropy values, we find the\nKullback-Leibler divergence between the two distributions, namely the entropy\nof the bona fide and the corresponding morphed images. The most discriminative\nwavelet sub-bands are those with the highest corresponding KL-divergence\nvalues. Accordingly, 22 sub-bands are selected as the most discriminative ones\nin terms of morph detection. We show that a Deep Neural Network (DNN) trained\non the 22 discriminative sub-bands can detect morphed samples precisely. Most\nimportantly, the effectiveness of our algorithm is validated through\nexperiments on three datasets: VISAPP17, LMA, and MorGAN. We also performed an\nablation study on the sub-band selection.",
          "link": "http://arxiv.org/abs/2106.08565",
          "publishedOn": "2021-06-17T01:58:45.062Z",
          "wordCount": 638,
          "title": "Detection of Morphed Face Images Using Discriminative Wavelet Sub-bands. (arXiv:2106.08565v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mirza_M/0/1/0/all/0/1\">Muhammad Jehanzeb Mirza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buerkle_C/0/1/0/all/0/1\">Cornelius Buerkle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarquin_J/0/1/0/all/0/1\">Julio Jarquin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opitz_M/0/1/0/all/0/1\">Michael Opitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oboril_F/0/1/0/all/0/1\">Fabian Oboril</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholl_K/0/1/0/all/0/1\">Kay-Ulrich Scholl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1\">Horst Bischof</a>",
          "description": "State-of-the-art object detection systems for autonomous driving achieve\npromising results in clear weather conditions. However, such autonomous safety\ncritical systems also need to work in degrading weather conditions, such as\nrain, fog and snow. Unfortunately, most approaches evaluate only on the KITTI\ndataset, which consists only of clear weather scenes. In this paper we address\nthis issue and perform one of the most detailed evaluation on single and dual\nmodality architectures on data captured in real weather conditions. We analyse\nthe performance degradation of these architectures in degrading weather\nconditions. We demonstrate that an object detection architecture performing\ngood in clear weather might not be able to handle degrading weather conditions.\nWe also perform ablation studies on the dual modality architectures and show\ntheir limitations.",
          "link": "http://arxiv.org/abs/2106.08795",
          "publishedOn": "2021-06-17T01:58:45.055Z",
          "wordCount": 569,
          "title": "Robustness of Object Detectors in Degrading Weather Conditions. (arXiv:2106.08795v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Shuyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1\">Matan Protter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimerman_G/0/1/0/all/0/1\">Gadi Zimerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghui Xu</a>",
          "description": "Recent deep generative models have achieved promising performance in image\ninpainting. However, it is still very challenging for a neural network to\ngenerate realistic image details and textures, due to its inherent spectral\nbias. By our understanding of how artists work, we suggest to adopt a\n`structure first detail next' workflow for image inpainting. To this end, we\npropose to build a Pyramid Generator by stacking several sub-generators, where\nlower-layer sub-generators focus on restoring image structures while the\nhigher-layer sub-generators emphasize image details. Given an input image, it\nwill be gradually restored by going through the entire pyramid in a bottom-up\nfashion. Particularly, our approach has a learning scheme of progressively\nincreasing hole size, which allows it to restore large-hole images. In\naddition, our method could fully exploit the benefits of learning with\nhigh-resolution images, and hence is suitable for high-resolution image\ninpainting. Extensive experimental results on benchmark datasets have validated\nthe effectiveness of our approach compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.08905",
          "publishedOn": "2021-06-17T01:58:45.039Z",
          "wordCount": 602,
          "title": "Structure First Detail Next: Image Inpainting with Pyramid Generator. (arXiv:2106.08905v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sangwon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chaewon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>",
          "description": "Monocular depth estimation is an especially important task in robotics and\nautonomous driving, where 3D structural information is essential. However,\nextreme lighting conditions and complex surface objects make it difficult to\npredict depth in a single image. Therefore, to generate accurate depth maps, it\nis important for the model to learn structural information about the scene. We\npropose a novel Patch-Wise EdgeConv Module (PEM) and EdgeConv Attention Module\n(EAM) to solve the difficulty of monocular depth estimation. The proposed\nmodules extract structural information by learning the relationship between\nimage patches close to each other in space using edge convolution. Our method\nis evaluated on two popular datasets, the NYU Depth V2 and the KITTI Eigen\nsplit, achieving state-of-the-art performance. We prove that the proposed model\npredicts depth robustly in challenging scenes through various comparative\nexperiments.",
          "link": "http://arxiv.org/abs/2106.08615",
          "publishedOn": "2021-06-17T01:58:44.910Z",
          "wordCount": 570,
          "title": "EdgeConv with Attention Module for Monocular Depth Estimation. (arXiv:2106.08615v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Multi-task learning (MTL) aims to improve the generalization of several\nrelated tasks by learning them jointly. As a comparison, in addition to the\njoint training scheme, modern meta-learning allows unseen tasks with limited\nlabels during the test phase, in the hope of fast adaptation over them. Despite\nthe subtle difference between MTL and meta-learning in the problem formulation,\nboth learning paradigms share the same insight that the shared structure\nbetween existing training tasks could lead to better generalization and\nadaptation. In this paper, we take one important step further to understand the\nclose connection between these two learning paradigms, through both theoretical\nanalysis and empirical investigation. Theoretically, we first demonstrate that\nMTL shares the same optimization formulation with a class of gradient-based\nmeta-learning (GBML) algorithms. We then prove that for over-parameterized\nneural networks with sufficient depth, the learned predictive functions of MTL\nand GBML are close. In particular, this result implies that the predictions\ngiven by these two models are similar over the same unseen task. Empirically,\nwe corroborate our theoretical findings by showing that, with proper\nimplementation, MTL is competitive against state-of-the-art GBML algorithms on\na set of few-shot image classification benchmarks. Since existing GBML\nalgorithms often involve costly second-order bi-level optimization, our\nfirst-order MTL method is an order of magnitude faster on large-scale datasets\nsuch as mini-ImageNet. We believe this work could help bridge the gap between\nthese two learning paradigms, and provide a computationally efficient\nalternative to GBML that also supports fast task adaptation.",
          "link": "http://arxiv.org/abs/2106.09017",
          "publishedOn": "2021-06-17T01:58:44.019Z",
          "wordCount": 699,
          "title": "Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation. (arXiv:2106.09017v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1810.12941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baruch_E/0/1/0/all/0/1\">Elad Ben Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1\">Yosi Keller</a>",
          "description": "In this work, we propose a novel Convolutional Neural Network (CNN)\narchitecture for the joint detection and matching of feature points in images\nacquired by different sensors using a single forward pass. The resulting\nfeature detector is tightly coupled with the feature descriptor, in contrast to\nclassical approaches (SIFT, etc.), where the detection phase precedes and\ndiffers from computing the descriptor. Our approach utilizes two CNN\nsubnetworks, the first being a Siamese CNN and the second, consisting of dual\nnon-weight-sharing CNNs. This allows simultaneous processing and fusion of the\njoint and disjoint cues in the multimodal image patches. The proposed approach\nis experimentally shown to outperform contemporary state-of-the-art schemes\nwhen applied to multiple datasets of multimodal images. It is also shown to\nprovide repeatable feature points detections across multisensor images,\noutperforming state-of-the-art detectors. To the best of our knowledge, it is\nthe first unified approach for the detection and matching of such images.",
          "link": "http://arxiv.org/abs/1810.12941",
          "publishedOn": "2021-06-17T01:58:43.503Z",
          "wordCount": 622,
          "title": "Joint detection and matching of feature points in multimodal images. (arXiv:1810.12941v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lanlan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Recent work introduced progressive network growing as a promising way to ease\nthe training for large GANs, but the model design and architecture-growing\nstrategy still remain under-explored and needs manual design for different\nimage data. In this paper, we propose a method to dynamically grow a GAN during\ntraining, optimizing the network architecture and its parameters together with\nautomation. The method embeds architecture search techniques as an interleaving\nstep with gradient-based training to periodically seek the optimal\narchitecture-growing strategy for the generator and discriminator. It enjoys\nthe benefits of both eased training because of progressive growing and improved\nperformance because of broader architecture design space. Experimental results\ndemonstrate new state-of-the-art of image generation. Observations in the\nsearch procedure also provide constructive insights into the GAN model design\nsuch as generator-discriminator balance and convolutional layer choices.",
          "link": "http://arxiv.org/abs/2106.08505",
          "publishedOn": "2021-06-17T01:58:43.368Z",
          "wordCount": 567,
          "title": "Dynamically Grown Generative Adversarial Networks. (arXiv:2106.08505v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yicheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yongqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiahui Zhu</a>",
          "description": "3D human pose estimation is still a challenging problem despite the large\namount of work that has been done in this field. Generally, most methods\ndirectly use neural networks and ignore certain constraints (e.g., reprojection\nconstraints and joint angle and bone length constraints). This paper proposes a\nweakly supervised GAN-based model for 3D human pose estimation that considers\n3D information along with 2D information simultaneously, in which a\nreprojection network is employed to learn the mapping of the distribution from\n3D poses to 2D poses. In particular, we train the reprojection network and the\ngenerative adversarial network synchronously. Furthermore, inspired by the\ntypical kinematic chain space (KCS) matrix, we propose a weighted KCS matrix,\nwhich is added into the discriminator's input to impose joint angle and bone\nlength constraints. The experimental results on Human3.6M show that our method\noutperforms state-of-the-art methods by approximately 5.1\\%.",
          "link": "http://arxiv.org/abs/2106.04274",
          "publishedOn": "2021-06-17T01:58:43.315Z",
          "wordCount": 592,
          "title": "A Synchronized Reprojection-based Model for 3D Human Pose Estimation. (arXiv:2106.04274v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozbulak_U/0/1/0/all/0/1\">Utku Ozbulak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anzaku_E/0/1/0/all/0/1\">Esla Timothy Anzaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neve_W/0/1/0/all/0/1\">Wesley De Neve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messem_A/0/1/0/all/0/1\">Arnout Van Messem</a>",
          "description": "Although the adoption rate of deep neural networks (DNNs) has tremendously\nincreased in recent years, a solution for their vulnerability against\nadversarial examples has not yet been found. As a result, substantial research\nefforts are dedicated to fix this weakness, with many studies typically using a\nsubset of source images to generate adversarial examples, treating every image\nin this subset as equal. We demonstrate that, in fact, not every source image\nis equally suited for this kind of assessment. To do so, we devise a\nlarge-scale model-to-model transferability scenario for which we meticulously\nanalyze the properties of adversarial examples, generated from every suitable\nsource image in ImageNet by making use of two of the most frequently deployed\nattacks. In this transferability scenario, which involves seven distinct DNN\nmodels, including the recently proposed vision transformers, we reveal that it\nis possible to have a difference of up to $12.5\\%$ in model-to-model\ntransferability success, $1.01$ in average $L_2$ perturbation, and $0.03$\n($8/225$) in average $L_{\\infty}$ perturbation when $1,000$ source images are\nsampled randomly among all suitable candidates. We then take one of the first\nsteps in evaluating the robustness of images used to create adversarial\nexamples, proposing a number of simple but effective methods to identify\nunsuitable source images, thus making it possible to mitigate extreme cases in\nexperimentation and support high-quality benchmarking.",
          "link": "http://arxiv.org/abs/2106.07141",
          "publishedOn": "2021-06-17T01:58:43.309Z",
          "wordCount": 681,
          "title": "Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks. (arXiv:2106.07141v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yajing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoxian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1\">Bruno Lepri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1\">Marco De Nadai</a>",
          "description": "Image-to-Image (I2I) multi-domain translation models are usually evaluated\nalso using the quality of their semantic interpolation results. However,\nstate-of-the-art models frequently show abrupt changes in the image appearance\nduring interpolation, and usually perform poorly in interpolations across\ndomains. In this paper, we propose a new training protocol based on three\nspecific losses which help a translation network to learn a smooth and\ndisentangled latent style space in which: 1) Both intra- and inter-domain\ninterpolations correspond to gradual changes in the generated images and 2) The\ncontent of the source image is better preserved during the translation.\nMoreover, we propose a novel evaluation metric to properly measure the\nsmoothness of latent style space of I2I translation models. The proposed method\ncan be plugged into existing translation approaches, and our extensive\nexperiments on different datasets show that it can significantly boost the\nquality of the generated images and the graduality of the interpolations.",
          "link": "http://arxiv.org/abs/2106.09016",
          "publishedOn": "2021-06-17T01:58:43.303Z",
          "wordCount": 606,
          "title": "Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation. (arXiv:2106.09016v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_P/0/1/0/all/0/1\">Praveen Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovvuri_R/0/1/0/all/0/1\">Rama Kovvuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassilev_B/0/1/0/all/0/1\">Boris Vassilev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1\">Tal Hassner</a>",
          "description": "We present a novel approach for disentangling the content of a text image\nfrom all aspects of its appearance. The appearance representation we derive can\nthen be applied to new content, for one-shot transfer of the source style to\nnew content. We learn this disentanglement in a self-supervised manner. Our\nmethod processes entire word boxes, without requiring segmentation of text from\nbackground, per-character processing, or making assumptions on string lengths.\nWe show results in different text domains which were previously handled by\nspecialized methods, e.g., scene text, handwritten text. To these ends, we make\na number of technical contributions: (1) We disentangle the style and content\nof a textual image into a non-parametric, fixed-dimensional vector. (2) We\npropose a novel approach inspired by StyleGAN but conditioned over the example\nstyle at different resolution and content. (3) We present novel self-supervised\ntraining criteria which preserve both source style and target content using a\npre-trained font classifier and text recognizer. Finally, (4) we also introduce\nImgur5K, a new challenging dataset for handwritten word images. We offer\nnumerous qualitative photo-realistic results of our method. We further show\nthat our method surpasses previous work in quantitative tests on scene text and\nhandwriting datasets, as well as in a user study.",
          "link": "http://arxiv.org/abs/2106.08385",
          "publishedOn": "2021-06-17T01:58:43.297Z",
          "wordCount": 645,
          "title": "TextStyleBrush: Transfer of Text Aesthetics from a Single Example. (arXiv:2106.08385v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhaoyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Minghao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guodong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kehuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "Recent works have shown that interval bound propagation (IBP) can be used to\ntrain verifiably robust neural networks. Reseachers observe an intriguing\nphenomenon on these IBP trained networks: CROWN, a bounding method based on\ntight linear relaxation, often gives very loose bounds on these networks. We\nalso observe that most neurons become dead during the IBP training process,\nwhich could hurt the representation capability of the network. In this paper,\nwe study the relationship between IBP and CROWN, and prove that CROWN is always\ntighter than IBP when choosing appropriate bounding lines. We further propose a\nrelaxed version of CROWN, linear bound propagation (LBP), that can be used to\nverify large networks to obtain lower verified errors than IBP. We also design\na new activation function, parameterized ramp function (ParamRamp), which has\nmore diversity of neuron status than ReLU. We conduct extensive experiments on\nMNIST, CIFAR-10 and Tiny-ImageNet with ParamRamp activation and achieve\nstate-of-the-art verified robustness. Code and the appendix are available at\nhttps://github.com/ZhaoyangLyu/VerifiablyRobustNN.",
          "link": "http://arxiv.org/abs/2104.00447",
          "publishedOn": "2021-06-17T01:58:43.273Z",
          "wordCount": 654,
          "title": "Towards Evaluating and Training Verifiably Robust Neural Networks. (arXiv:2104.00447v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francois_A/0/1/0/all/0/1\">Anton Fran&#xe7;ois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaunes_J/0/1/0/all/0/1\">Joan Glaun&#xe8;s</a>",
          "description": "In this paper, we propose an implementation of both Large Deformation\nDiffeomorphic Metric Mapping (LDDMM) and Metamorphosis image registration using\na semi-Lagrangian scheme for geodesic shooting. We propose to solve both\nproblems as an inexact matching providing a single and unifying cost function.\nWe demonstrate that for image registration the use of a semi-Lagrangian scheme\nis more stable than a standard Eulerian scheme. Our GPU implementation is based\non PyTorch, which greatly simplifies and accelerates the computations thanks to\nits powerful automatic differentiation engine. It will be freely available at\nhttps://github.com/antonfrancois/Demeter_metamorphosis.",
          "link": "http://arxiv.org/abs/2106.08817",
          "publishedOn": "2021-06-17T01:58:43.234Z",
          "wordCount": 530,
          "title": "Metamorphic image registration using a semi-Lagrangian scheme. (arXiv:2106.08817v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.02854",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vonikakis_V/0/1/0/all/0/1\">Vassilios Vonikakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neo_D/0/1/0/all/0/1\">Dexter Neo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1\">Stefan Winkler</a>",
          "description": "Emotion recognition and understanding is a vital component in human-machine\ninteraction. Dimensional models of affect such as those using valence and\narousal have advantages over traditional categorical ones due to the complexity\nof emotional states in humans. However, dimensional emotion annotations are\ndifficult and expensive to collect, therefore they are not as prevalent in the\naffective computing community. To address these issues, we propose a method to\ngenerate synthetic images from existing categorical emotion datasets using face\nmorphing as well as dimensional labels in the circumplex space with full\ncontrol over the resulting sample distribution, while achieving augmentation\nfactors of at least 20x or more.",
          "link": "http://arxiv.org/abs/2103.02854",
          "publishedOn": "2021-06-17T01:58:43.209Z",
          "wordCount": 580,
          "title": "Morphset:Augmenting categorical emotion datasets with dimensional affect labels using face morphing. (arXiv:2103.02854v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_N/0/1/0/all/0/1\">Navdeep Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Sadegh R Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Si-Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yezzi_A/0/1/0/all/0/1\">Anthony Yezzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>",
          "description": "Purpose: In current clinical practice, noisy and artifact-ridden weekly\ncone-beam computed tomography (CBCT) images are only used for patient setup\nduring radiotherapy. Treatment planning is done once at the beginning of the\ntreatment using high-quality planning CT (pCT) images and manual contours for\norgans-at-risk (OARs) structures. If the quality of the weekly CBCT images can\nbe improved while simultaneously segmenting OAR structures, this can provide\ncritical information for adapting radiotherapy mid-treatment as well as for\nderiving biomarkers for treatment response. Methods: Using a novel\nphysics-based data augmentation strategy, we synthesize a large dataset of\nperfectly/inherently registered planning CT and synthetic-CBCT pairs for\nlocally advanced lung cancer patient cohort, which are then used in a multitask\n3D deep learning framework to simultaneously segment and translate real weekly\nCBCT images to high-quality planning CT-like images. Results: We compared the\nsynthetic CT and OAR segmentations generated by the model to real planning CT\nand manual OAR segmentations and showed promising results. The real week 1\n(baseline) CBCT images which had an average MAE of 162.77 HU compared to pCT\nimages are translated to synthetic CT images that exhibit a drastically\nimproved average MAE of 29.31 HU and average structural similarity of 92% with\nthe pCT images. The average DICE scores of the 3D organs-at-risk segmentations\nare: lungs 0.96, heart 0.88, spinal cord 0.83 and esophagus 0.66. Conclusions:\nWe demonstrate an approach to translate artifact-ridden CBCT images to high\nquality synthetic CT images while simultaneously generating good quality\nsegmentation masks for different organs-at-risk. This approach could allow\nclinicians to adjust treatment plans using only the routine low-quality CBCT\nimages, potentially improving patient outcomes.",
          "link": "http://arxiv.org/abs/2103.05690",
          "publishedOn": "2021-06-17T01:58:42.949Z",
          "wordCount": 743,
          "title": "Multitask 3D CBCT-to-CT Translation and Organs-at-Risk Segmentation Using Physics-Based Data Augmentation. (arXiv:2103.05690v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1\">Alexander Hepburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1\">Raul Santos-Rodriguez</a>",
          "description": "Explaining the decisions of models is becoming pervasive in the image\nprocessing domain, whether it is by using post-hoc methods or by creating\ninherently interpretable models. While the widespread use of surrogate\nexplainers is a welcome addition to inspect and understand black-box models,\nassessing the robustness and reliability of the explanations is key for their\nsuccess. Additionally, whilst existing work in the explainability field\nproposes various strategies to address this problem, the challenges of working\nwith data in the wild is often overlooked. For instance, in image\nclassification, distortions to images can not only affect the predictions\nassigned by the model, but also the explanation. Given a clean and a distorted\nversion of an image, even if the prediction probabilities are similar, the\nexplanation may still be different. In this paper we propose a methodology to\nevaluate the effect of distortions in explanations by embedding perceptual\ndistances that tailor the neighbourhoods used to training surrogate explainers.\nWe also show that by operating in this way, we can make the explanations more\nrobust to distortions. We generate explanations for images in the Imagenet-C\ndataset and demonstrate how using a perceptual distances in the surrogate\nexplainer creates more coherent explanations for the distorted and reference\nimages.",
          "link": "http://arxiv.org/abs/2102.10951",
          "publishedOn": "2021-06-17T01:58:42.901Z",
          "wordCount": 682,
          "title": "Explainers in the Wild: Making Surrogate Explainers Robust to Distortions through Perception. (arXiv:2102.10951v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Niharika Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1\">Alberto Olmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sailik Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manikonda_L/0/1/0/all/0/1\">Lydia Manikonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1\">Subbarao Kambhampati</a>",
          "description": "In this paper, we show that popular Generative Adversarial Networks (GANs)\nexacerbate biases along the axes of gender and skin tone when given a skewed\ndistribution of face-shots. While practitioners celebrate synthetic data\ngeneration using GANs as an economical way to augment data for training\ndata-hungry machine learning models, it is unclear whether they recognize the\nperils of such techniques when applied to real world datasets biased along\nlatent dimensions. Specifically, we show that (1) traditional GANs further skew\nthe distribution of a dataset consisting of engineering faculty headshots,\ngenerating minority modes less often and of worse quality and (2)\nimage-to-image translation (conditional) GANs also exacerbate biases by\nlightening skin color of non-white faces and transforming female facial\nfeatures to be masculine when generating faces of engineering professors. Thus,\nour study is meant to serve as a cautionary tale.",
          "link": "http://arxiv.org/abs/2001.09528",
          "publishedOn": "2021-06-17T01:58:42.880Z",
          "wordCount": 638,
          "title": "Imperfect ImaGANation: Implications of GANs Exacerbating Biases on Facial Data Augmentation and Snapchat Selfie Lenses. (arXiv:2001.09528v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dora Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Angelina Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>",
          "description": "Image captioning is an important task for benchmarking visual reasoning and\nfor enabling accessibility for people with vision impairments. However, as in\nmany machine learning settings, social biases can influence image captioning in\nundesirable ways. In this work, we study bias propagation pathways within image\ncaptioning, focusing specifically on the COCO dataset. Prior work has analyzed\ngender bias in captions using automatically-derived gender labels; here we\nexamine racial and intersectional biases using manual annotations. Our first\ncontribution is in annotating the perceived gender and skin color of 28,315 of\nthe depicted people after obtaining IRB approval. Using these annotations, we\ncompare racial biases present in both manual and automatically-generated image\ncaptions. We demonstrate differences in caption performance, sentiment, and\nword choice between images of lighter versus darker-skinned people. Further, we\nfind the magnitude of these differences to be greater in modern captioning\nsystems compared to older ones, thus leading to concerns that without proper\nconsideration and mitigation these differences will only become increasingly\nprevalent. Code and data is available at\nhttps://princetonvisualai.github.io/imagecaptioning-bias .",
          "link": "http://arxiv.org/abs/2106.08503",
          "publishedOn": "2021-06-17T01:58:42.865Z",
          "wordCount": 601,
          "title": "Understanding and Evaluating Racial Biases in Image Captioning. (arXiv:2106.08503v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abualsaud_H/0/1/0/all/0/1\">Hala Abualsaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sean Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">David Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Situ_K/0/1/0/all/0/1\">Kenny Situ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1\">Akshay Rangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan M. Trivedi</a>",
          "description": "This study presents an approach to lane detection involving the prediction of\nbinary segmentation masks and per-pixel affinity fields. These affinity fields,\nalong with the binary masks, can then be used to cluster lane pixels\nhorizontally and vertically into corresponding lane instances in a\npost-processing step. This clustering is achieved through a simple row-by-row\ndecoding process with little overhead; such an approach allows LaneAF to detect\na variable number of lanes without assuming a fixed or maximum number of lanes.\nMoreover, this form of clustering is more interpretable in comparison to\nprevious visual clustering approaches, and can be analyzed to identify and\ncorrect sources of error. Qualitative and quantitative results obtained on\npopular lane detection datasets demonstrate the model's ability to detect and\ncluster lanes effectively and robustly. Our proposed approach sets a new\nstate-of-the-art on the challenging CULane dataset and the recently introduced\nUnsupervised LLAMAS dataset.",
          "link": "http://arxiv.org/abs/2103.12040",
          "publishedOn": "2021-06-17T01:58:42.840Z",
          "wordCount": 621,
          "title": "LaneAF: Robust Multi-Lane Detection with Affinity Fields. (arXiv:2103.12040v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Previous studies have verified that the functionality of black-box models can\nbe stolen with full probability outputs. However, under the more practical\nhard-label setting, we observe that existing methods suffer from catastrophic\nperformance degradation. We argue this is due to the lack of rich information\nin the probability prediction and the overfitting caused by hard labels. To\nthis end, we propose a novel hard-label model stealing method termed\n\\emph{black-box dissector}, which consists of two erasing-based modules. One is\na CAM-driven erasing strategy that is designed to increase the information\ncapacity hidden in hard labels from the victim model. The other is a\nrandom-erasing-based self-knowledge distillation module that utilizes soft\nlabels from the substitute model to mitigate overfitting. Extensive experiments\non four widely-used datasets consistently demonstrate that our method\noutperforms state-of-the-art methods, with an improvement of at most $8.27\\%$.\nWe also validate the effectiveness and practical potential of our method on\nreal-world APIs and defense methods. Furthermore, our method promotes other\ndownstream tasks, \\emph{i.e.}, transfer adversarial attacks.",
          "link": "http://arxiv.org/abs/2105.00623",
          "publishedOn": "2021-06-17T01:58:42.779Z",
          "wordCount": 631,
          "title": "Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack. (arXiv:2105.00623v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.02077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1\">Akshay Rangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan M. Trivedi</a>",
          "description": "A driver's gaze is critical for determining their attention, state,\nsituational awareness, and readiness to take over control from partially\nautomated vehicles. Estimating the gaze direction is the most obvious way to\ngauge a driver's state under ideal conditions when limited to using\nnon-intrusive imaging sensors. Unfortunately, the vehicular environment\nintroduces a variety of challenges that are usually unaccounted for - harsh\nillumination, nighttime conditions, and reflective eyeglasses. Relying on head\npose alone under such conditions can prove to be unreliable and erroneous. In\nthis study, we offer solutions to address these problems encountered in the\nreal world. To solve issues with lighting, we demonstrate that using an\ninfrared camera with suitable equalization and normalization suffices. To\nhandle eyeglasses and their corresponding artifacts, we adopt image-to-image\ntranslation using generative adversarial networks to pre-process images prior\nto gaze estimation. Our proposed Gaze Preserving CycleGAN (GPCycleGAN) is\ntrained to preserve the driver's gaze while removing potential eyeglasses from\nface images. GPCycleGAN is based on the well-known CycleGAN approach - with the\naddition of a gaze classifier and a gaze consistency loss for additional\nsupervision. Our approach exhibits improved performance, interpretability,\nrobustness and superior qualitative results on challenging real-world datasets.",
          "link": "http://arxiv.org/abs/2002.02077",
          "publishedOn": "2021-06-17T01:58:42.757Z",
          "wordCount": 699,
          "title": "Gaze Preserving CycleGANs for Eyeglass Removal & Persistent Gaze Estimation. (arXiv:2002.02077v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaohong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong-Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "Face recognition is an important yet challenging problem in computer vision.\nA major challenge in practical face recognition applications lies in\nsignificant variations between profile and frontal faces. Traditional\ntechniques address this challenge either by synthesizing frontal faces or by\npose invariant learning. In this paper, we propose a novel method with Lie\nalgebra theory to explore how face rotation in the 3D space affects the deep\nfeature generation process of convolutional neural networks (CNNs). We prove\nthat face rotation in the image space is equivalent to an additive residual\ncomponent in the feature space of CNNs, which is determined solely by the\nrotation. Based on this theoretical finding, we further design a Lie Algebraic\nResidual Network (LARNet) for tackling pose robust face recognition. Our LARNet\nconsists of a residual subnet for decoding rotation information from input face\nimages, and a gating subnet to learn rotation magnitude for controlling the\nstrength of the residual component contributing to the feature learning\nprocess. Comprehensive experimental evaluations on both frontal-profile face\ndatasets and general face recognition datasets convincingly demonstrate that\nour method consistently outperforms the state-of-the-art ones.",
          "link": "http://arxiv.org/abs/2103.08147",
          "publishedOn": "2021-06-17T01:58:42.751Z",
          "wordCount": 649,
          "title": "LARNet: Lie Algebra Residual Network for Face Recognition. (arXiv:2103.08147v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuge Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaoxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>",
          "description": "Recent studies reveal that Convolutional Neural Networks (CNNs) are typically\nvulnerable to adversarial attacks, which pose a threat to security-sensitive\napplications. Many adversarial defense methods improve robustness at the cost\nof accuracy, raising the contradiction between standard and adversarial\naccuracies. In this paper, we observe an interesting phenomenon that feature\nstatistics change monotonically and smoothly w.r.t the rising of attacking\nstrength. Based on this observation, we propose the adaptive feature alignment\n(AFA) to generate features of arbitrary attacking strengths. Our method is\ntrained to automatically align features of arbitrary attacking strength. This\nis done by predicting a fusing weight in a dual-BN architecture. Unlike\nprevious works that need to either retrain the model or manually tune a\nhyper-parameters for different attacking strengths, our method can deal with\narbitrary attacking strengths with a single model without introducing any\nhyper-parameter. Importantly, our method improves the model robustness against\nadversarial samples without incurring much loss in standard accuracy.\nExperiments on CIFAR-10, SVHN, and tiny-ImageNet datasets demonstrate that our\nmethod outperforms the state-of-the-art under a wide range of attacking\nstrengths.",
          "link": "http://arxiv.org/abs/2105.15157",
          "publishedOn": "2021-06-17T01:58:42.744Z",
          "wordCount": 628,
          "title": "Adaptive Feature Alignment for Adversarial Training. (arXiv:2105.15157v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1\">Mert Seker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannisto_A/0/1/0/all/0/1\">Anssi M&#xe4;nnist&#xf6;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1\">Jenni Raitoharju</a>",
          "description": "The COVID-19 virus has caused a global pandemic since March 2020. The World\nHealth Organization (WHO) has provided guidelines on how to reduce the spread\nof the virus and one of the most important measures is social distancing.\nMaintaining a minimum of one meter distance from other people is strongly\nsuggested to reduce the risk of infection. This has created a strong interest\nin monitoring the social distances either as a safety measure or to study how\nthe measures have affected human behavior and country-wise differences in this.\nThe need for automatic social distance estimation algorithms is evident, but\nthere is no suitable test benchmark for such algorithms. Collecting images with\nmeasured ground-truth pair-wise distances between all the people using\ndifferent camera settings is cumbersome. Furthermore, performance evaluation\nfor social distance estimation algorithms is not straightforward and there is\nno widely accepted evaluation protocol. In this paper, we provide a dataset of\nvarying images with measured pair-wise social distances under different camera\npositionings and focal length values. We suggest a performance evaluation\nprotocol and provide a benchmark to easily evaluate social distance estimation\nalgorithms. We also propose a method for automatic social distance estimation.\nOur method takes advantage of object detection and human pose estimation. It\ncan be applied on any single image as long as focal length and sensor size\ninformation are known. The results on our benchmark are encouraging with 92%\nhuman detection rate and only 28.9% average error in distance estimation among\nthe detected people.",
          "link": "http://arxiv.org/abs/2103.06759",
          "publishedOn": "2021-06-17T01:58:42.733Z",
          "wordCount": 774,
          "title": "Automatic Social Distance Estimation From Images: Performance Evaluation, Test Benchmark, and Algorithm. (arXiv:2103.06759v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szymanowicz_S/0/1/0/all/0/1\">Stanislaw Szymanowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charles_J/0/1/0/all/0/1\">James Charles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1\">Roberto Cipolla</a>",
          "description": "Our objective is to detect anomalies in video while also automatically\nexplaining the reason behind the detector's response. In a practical sense,\nexplainability is crucial for this task as the required response to an anomaly\ndepends on its nature and severity. However, most leading methods (based on\ndeep neural networks) are not interpretable and hide the decision making\nprocess in uninterpretable feature representations. In an effort to tackle this\nproblem we make the following contributions: (1) we show how to build\ninterpretable feature representations suitable for detecting anomalies with\nstate of the art performance, (2) we propose an interpretable probabilistic\nanomaly detector which can describe the reason behind it's response using high\nlevel concepts, (3) we are the first to directly consider object interactions\nfor anomaly detection and (4) we propose a new task of explaining anomalies and\nrelease a large dataset for evaluating methods on this task. Our method\ncompetes well with the state of the art on public datasets while also providing\nanomaly explanation based on objects and their interactions.",
          "link": "http://arxiv.org/abs/2106.08856",
          "publishedOn": "2021-06-17T01:58:42.690Z",
          "wordCount": 616,
          "title": "X-MAN: Explaining multiple sources of anomalies in video. (arXiv:2106.08856v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jongmin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Hyeontaek Oh</a>",
          "description": "This paper addresses unsupervised person re-identification (Re-ID) using\nmulti-label prediction and classification based on graph-structural insight.\nOur method extracts features from person images and produces a graph that\nconsists of the features and a pairwise similarity of them as nodes and edges,\nrespectively. Based on the graph, the proposed graph structure based\nmulti-label prediction (GSMLP) method predicts multi-labels by considering the\npairwise similarity and the adjacency node distribution of each node. The\nmulti-labels created by GSMLP are applied to the proposed selective multi-label\nclassification (SMLC) loss. SMLC integrates a hard-sample mining scheme and a\nmulti-label classification. The proposed GSMLP and SMLC boost the performance\nof unsupervised person Re-ID without any pre-labelled dataset. Experimental\nresults justify the superiority of the proposed method in unsupervised person\nRe-ID by producing state-of-the-art performance. The source code for this paper\nis publicly available on 'https://github.com/uknownpioneer/GSMLP-SMLC.git'.",
          "link": "http://arxiv.org/abs/2106.08798",
          "publishedOn": "2021-06-17T01:58:42.664Z",
          "wordCount": 583,
          "title": "Unsupervised Person Re-identification via Multi-Label Prediction and Classification based on Graph-Structural Insight. (arXiv:2106.08798v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiaqi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Sheng Tang</a>",
          "description": "Rapid pace of generative models has brought about new threats to visual\nforensics such as malicious personation and digital copyright infringement,\nwhich promotes works on fake image attribution. Existing works on fake image\nattribution mainly rely on a direct classification framework. Without\nadditional supervision, the extracted features could include many\ncontent-relevant components and generalize poorly. Meanwhile, how to obtain an\ninterpretable GAN fingerprint to explain the decision remains an open question.\nAdopting a multi-task framework, we propose a GAN Fingerprint Disentangling\nNetwork (GFD-Net) to simultaneously disentangle the fingerprint from\nGAN-generated images and produce a content-irrelevant representation for fake\nimage attribution. A series of constraints are provided to guarantee the\nstability and discriminability of the fingerprint, which in turn helps\ncontent-irrelevant feature extraction. Further, we perform comprehensive\nanalysis on GAN fingerprint, providing some clues about the properties of GAN\nfingerprint and which factors dominate the fingerprint in GAN architecture.\nExperiments show that our GFD-Net achieves superior fake image attribution\nperformance in both closed-world and open-world testing. We also apply our\nmethod in binary fake image detection and exhibit a significant generalization\nability on unseen generators.",
          "link": "http://arxiv.org/abs/2106.08749",
          "publishedOn": "2021-06-17T01:58:42.651Z",
          "wordCount": 627,
          "title": "Learning to Disentangle GAN Fingerprint for Fake Image Attribution. (arXiv:2106.08749v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09054",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Huanyu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>",
          "description": "Pan-sharpening aims at fusing a low-resolution (LR) multi-spectral (MS) image\nand a high-resolution (HR) panchromatic (PAN) image acquired by a satellite to\ngenerate an HR MS image. Many deep learning based methods have been developed\nin the past few years. However, since there are no intended HR MS images as\nreferences for learning, almost all of the existing methods down-sample the MS\nand PAN images and regard the original MS images as targets to form a\nsupervised setting for training. These methods may perform well on the\ndown-scaled images, however, they generalize poorly to the full-resolution\nimages. To conquer this problem, we design an unsupervised framework that is\nable to learn directly from the full-resolution images without any\npreprocessing. The model is built based on a novel generative multi-adversarial\nnetwork. We use a two-stream generator to extract the modality-specific\nfeatures from the PAN and MS images, respectively, and develop a\ndual-discriminator to preserve the spectral and spatial information of the\ninputs when performing fusion. Furthermore, a novel loss function is introduced\nto facilitate training under the unsupervised setting. Experiments and\ncomparisons with other state-of-the-art methods on GaoFen-2 and QuickBird\nimages demonstrate that the proposed method can obtain much better fusion\nresults on the full-resolution images.",
          "link": "http://arxiv.org/abs/2012.09054",
          "publishedOn": "2021-06-17T01:58:42.627Z",
          "wordCount": 666,
          "title": "PGMAN: An Unsupervised Generative Multi-adversarial Network for Pan-sharpening. (arXiv:2012.09054v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Naiyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yanhu Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiqi Huang</a>",
          "description": "Panoptic segmentation (PS) is a complex scene understanding task that\nrequires providing high-quality segmentation for both thing objects and stuff\nregions. Previous methods handle these two classes with semantic and instance\nsegmentation modules separately, following with heuristic fusion or additional\nmodules to resolve the conflicts between the two outputs. This work simplifies\nthis pipeline of PS by consistently modeling the two classes with a novel PS\nframework, which extends a detection model with an extra module to predict\ncategory- and instance-aware pixel embedding (CIAE). CIAE is a novel pixel-wise\nembedding feature that encodes both semantic-classification and\ninstance-distinction information. At the inference process, PS results are\nsimply derived by assigning each pixel to a detected instance or a stuff class\naccording to the learned embedding. Our method not only demonstrates fast\ninference speed but also the first one-stage method to achieve comparable\nperformance to two-stage methods on the challenging COCO benchmark.",
          "link": "http://arxiv.org/abs/2009.13342",
          "publishedOn": "2021-06-17T01:58:42.617Z",
          "wordCount": 612,
          "title": "Learning Category- and Instance-Aware Pixel Embedding for Fast Panoptic Segmentation. (arXiv:2009.13342v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheema_G/0/1/0/all/0/1\">Gullal S. Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimov_S/0/1/0/all/0/1\">Sherzod Hakimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1\">Eric M&#xfc;ller-Budack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>",
          "description": "Opinion and sentiment analysis is a vital task to characterize subjective\ninformation in social media posts. In this paper, we present a comprehensive\nexperimental evaluation and comparison with six state-of-the-art methods, from\nwhich we have re-implemented one of them. In addition, we investigate different\ntextual and visual feature embeddings that cover different aspects of the\ncontent, as well as the recently introduced multimodal CLIP embeddings.\nExperimental results are presented for two different publicly available\nbenchmark datasets of tweets and corresponding images. In contrast to the\nevaluation methodology of previous work, we introduce a reproducible and fair\nevaluation scheme to make results comparable. Finally, we conduct an error\nanalysis to outline the limitations of the methods and possibilities for the\nfuture work.",
          "link": "http://arxiv.org/abs/2106.08829",
          "publishedOn": "2021-06-17T01:58:42.610Z",
          "wordCount": 583,
          "title": "A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment Analysis Methods. (arXiv:2106.08829v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2012.04293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ates_T/0/1/0/all/0/1\">Tayfun Ates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atesoglu_M/0/1/0/all/0/1\">Muhammed Samil Atesoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yigit_C/0/1/0/all/0/1\">Cagatay Yigit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1\">Ilker Kesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobas_M/0/1/0/all/0/1\">Mert Kobas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksun_T/0/1/0/all/0/1\">Tilbe Goksun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>",
          "description": "Humans are able to perceive, understand and reason about physical events.\nDeveloping models with similar physical understanding capabilities is a\nlong-standing goal of artificial intelligence. As a step towards this goal, in\nthis work, we introduce CRAFT, a new visual question answering dataset that\nrequires causal reasoning about physical forces and object interactions. It\ncontains 58K video and question pairs that are generated from 10K videos from\n20 different virtual environments, containing various objects in motion that\ninteract with each other and the scene. Two question categories from CRAFT\ninclude previously studied descriptive and counterfactual questions. Besides,\ninspired by the theories of force dynamics in cognitive linguistics, we\nintroduce new question categories that involve understanding the interactions\nof objects through the notions of cause, enable, and prevent. Our results\ndemonstrate that even though these tasks seem to be simple and intuitive for\nhumans, the evaluated baseline models, including existing state-of-the-art\nmethods, do not yet deal with the challenges posed in our benchmark dataset.",
          "link": "http://arxiv.org/abs/2012.04293",
          "publishedOn": "2021-06-17T01:58:42.604Z",
          "wordCount": 659,
          "title": "CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions. (arXiv:2012.04293v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>",
          "description": "With the complexity of the network structure, uncertainty inference has\nbecome an important task to improve the classification accuracy for artificial\nintelligence systems. For image classification tasks, we propose a structured\nDropConnect (SDC) framework to model the output of a deep neural network by a\nDirichlet distribution. We introduce a DropConnect strategy on weights in the\nfully connected layers during training. In test, we split the network into\nseveral sub-networks, and then model the Dirichlet distribution by match its\nmoments with the mean and variance of the outputs of these sub-networks. The\nentropy of the estimated Dirichlet distribution is finally utilized for\nuncertainty inference. In this paper, this framework is implemented on LeNet$5$\nand VGG$16$ models for misclassification detection and out-of-distribution\ndetection on MNIST and CIFAR-$10$ datasets. Experimental results show that the\nperformance of the proposed SDC can be comparable to other uncertainty\ninference methods. Furthermore, the SDC is adapted well to different network\nstructures with certain generalization capabilities and research prospects.",
          "link": "http://arxiv.org/abs/2106.08624",
          "publishedOn": "2021-06-17T01:58:42.598Z",
          "wordCount": 611,
          "title": "Structured DropConnect for Uncertainty Inference in Image Classification. (arXiv:2106.08624v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>",
          "description": "Recently, the Siamese-based method has stood out from multitudinous tracking\nmethods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to\nvarious special challenges in UAV tracking, \\textit{e.g.}, severe occlusion,\nand fast motion, most existing Siamese-based trackers hardly combine superior\nperformance with high efficiency. To this concern, in this paper, a novel\nattentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking.\nBy virtue of the attention mechanism, the attentional aggregation network (AAN)\nis conducted with self-AAN and cross-AAN, raising the expression ability of\nfeatures eventually. The former AAN aggregates and models the self-semantic\ninterdependencies of the single feature map via spatial and channel dimensions.\nThe latter aims to aggregate the cross-interdependencies of different semantic\nfeatures including the location information of anchors. In addition, the dual\nfeatures version of the anchor proposal network is proposed to raise the\nrobustness of proposing anchors, increasing the perception ability to objects\nwith various scales. Experiments on two well-known authoritative benchmarks are\nconducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA\ntrackers. Besides, real-world tests onboard a typical embedded platform\ndemonstrate that SiamAPN++ achieves promising tracking results with real-time\nspeed.",
          "link": "http://arxiv.org/abs/2106.08816",
          "publishedOn": "2021-06-17T01:58:42.591Z",
          "wordCount": 619,
          "title": "SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking. (arXiv:2106.08816v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1\">Daniele Cattaneo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1\">Matteo Vaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Loop closure detection is an essential component of Simultaneous Localization\nand Mapping (SLAM) systems, which reduces the drift accumulated over time. Over\nthe years, several deep learning approaches have been proposed to address this\ntask, however their performance has been subpar compared to handcrafted\ntechniques, especially while dealing with reverse loops. In this paper, we\nintroduce the novel LCDNet that effectively detects loop closures in LiDAR\npoint clouds by simultaneously identifying previously visited places and\nestimating the 6-DoF relative transformation between the current scan and the\nmap. LCDNet is composed of a shared encoder, a place recognition head that\nextracts global descriptors, and a relative pose head that estimates the\ntransformation between two point clouds. We introduce a novel relative pose\nhead based on the unbalanced optimal transport theory that we implement in a\ndifferentiable manner to allow for end-to-end training. Extensive evaluations\nof LCDNet on multiple real-world autonomous driving datasets show that our\napproach outperforms state-of-the-art loop closure detection and point cloud\nregistration techniques by a large margin, especially while dealing with\nreverse loops. Moreover, we integrate our proposed loop closure detection\napproach into a LiDAR SLAM library to provide a complete mapping system and\ndemonstrate the generalization ability using different sensor setup in an\nunseen city.",
          "link": "http://arxiv.org/abs/2103.05056",
          "publishedOn": "2021-06-17T01:58:42.584Z",
          "wordCount": 675,
          "title": "LCDNet: Deep Loop Closure Detection and Point Cloud Registration for LiDAR SLAM. (arXiv:2103.05056v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sourya Dipta Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nisarg A. Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Saikat Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_H/0/1/0/all/0/1\">Himanshu Kumar</a>",
          "description": "Custom and natural lighting conditions can be emulated in images of the scene\nduring post-editing. Extraordinary capabilities of the deep learning framework\ncan be utilized for such purpose. Deep image relighting allows automatic photo\nenhancement by illumination-specific retouching. Most of the state-of-the-art\nmethods for relighting are run-time intensive and memory inefficient. In this\npaper, we propose an efficient, real-time framework Deep Stacked Relighting\nNetwork (DSRN) for image relighting by utilizing the aggregated features from\ninput image at different scales. Our model is very lightweight with total size\nof about 42 MB and has an average inference time of about 0.0116s for image of\nresolution $1024 \\times 1024$ which is faster as compared to other multi-scale\nmodels. Our solution is quite robust for translating image color temperature\nfrom input image to target image and also performs moderately for light\ngradient generation with respect to the target image. Additionally, we show\nthat if images illuminated from opposite directions are used as input, the\nqualitative results improve over using a single input image.",
          "link": "http://arxiv.org/abs/2102.09242",
          "publishedOn": "2021-06-17T01:58:42.566Z",
          "wordCount": 637,
          "title": "DSRN: an Efficient Deep Network for Image Relighting. (arXiv:2102.09242v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Muhammad Zaigham Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">Arif Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1\">Marcella Astrid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung-Ik Lee</a>",
          "description": "Learning to detect real-world anomalous events through video-level labels is\na challenging task due to the rare occurrence of anomalies as well as noise in\nthe labels. In this work, we propose a weakly supervised anomaly detection\nmethod which has manifold contributions including1) a random batch based\ntraining procedure to reduce inter-batch correlation, 2) a normalcy suppression\nmechanism to minimize anomaly scores of the normal regions of a video by taking\ninto account the overall information available in one training batch, and 3) a\nclustering distance based loss to contribute towards mitigating the label noise\nand to produce better anomaly representations by encouraging our model to\ngenerate distinct normal and anomalous clusters. The proposed method\nobtains83.03% and 89.67% frame-level AUC performance on the UCF Crime and\nShanghaiTech datasets respectively, demonstrating its superiority over the\nexisting state-of-the-art algorithms.",
          "link": "http://arxiv.org/abs/2011.12077",
          "publishedOn": "2021-06-17T01:58:42.537Z",
          "wordCount": 679,
          "title": "CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy Suppression for Anomalous Event Detection. (arXiv:2011.12077v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Numair Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Min H. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1\">James Tompkin</a>",
          "description": "We present a method to estimate dense depth by optimizing a sparse set of\npoints such that their diffusion into a depth map minimizes a multi-view\nreprojection error from RGB supervision. We optimize point positions, depths,\nand weights with respect to the loss by differential splatting that models\npoints as Gaussians with analytic transmittance. Further, we develop an\nefficient optimization routine that can simultaneously optimize the 50k+ points\nrequired for complex scene reconstruction. We validate our routine using ground\ntruth data and show high reconstruction quality. Then, we apply this to light\nfield and wider baseline images via self supervision, and show improvements in\nboth average and outlier error for depth maps diffused from inaccurate sparse\npoints. Finally, we compare qualitative and quantitative results to image\nprocessing and deep learning methods.",
          "link": "http://arxiv.org/abs/2106.08917",
          "publishedOn": "2021-06-17T01:58:42.529Z",
          "wordCount": 566,
          "title": "Differentiable Diffusion for Dense Depth Estimation from Multi-view Images. (arXiv:2106.08917v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.13681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mani_A/0/1/0/all/0/1\">Arjun Mani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinthorn_W/0/1/0/all/0/1\">Will Hinthorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_N/0/1/0/all/0/1\">Nobline Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>",
          "description": "Visual Question Answering (VQA) has become one of the key benchmarks of\nvisual recognition progress. Multiple VQA extensions have been explored to\nbetter simulate real-world settings: different question formulations, changing\ntraining and test distributions, conversational consistency in dialogues, and\nexplanation-based answering. In this work, we further expand this space by\nconsidering visual questions that include a spatial point of reference.\nPointing is a nearly universal gesture among humans, and real-world VQA is\nlikely to involve a gesture towards the target region.\n\nConcretely, we (1) introduce and motivate point-input questions as an\nextension of VQA, (2) define three novel classes of questions within this\nspace, and (3) for each class, introduce both a benchmark dataset and a series\nof baseline models to handle its unique challenges. There are two key\ndistinctions from prior work. First, we explicitly design the benchmarks to\nrequire the point input, i.e., we ensure that the visual question cannot be\nanswered accurately without the spatial reference. Second, we explicitly\nexplore the more realistic point spatial input rather than the standard but\nunnatural bounding box input. Through our exploration we uncover and address\nseveral visual recognition challenges, including the ability to infer human\nintent, reason both locally and globally about the image, and effectively\ncombine visual, language and spatial inputs. Code is available at:\nhttps://github.com/princetonvisualai/pointingqa .",
          "link": "http://arxiv.org/abs/2011.13681",
          "publishedOn": "2021-06-17T01:58:42.514Z",
          "wordCount": 678,
          "title": "Point and Ask: Incorporating Pointing into Visual Question Answering. (arXiv:2011.13681v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fuping Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Unsupervised domain adaptation is useful in medical image segmentation.\nParticularly, when ground truths of the target images are not available, domain\nadaptation can train a target-specific model by utilizing the existing labeled\nimages from other modalities. Most of the reported works mapped images of both\nthe source and target domains into a common latent feature space, and then\nreduced their discrepancy either implicitly with adversarial training or\nexplicitly by directly minimizing a discrepancy metric. In this work, we\npropose a new framework, where the latent features of both domains are driven\ntowards a common and parameterized variational form, whose conditional\ndistribution given the image is Gaussian. This is achieved by two networks\nbased on variational auto-encoders (VAEs) and a regularization for this\nvariational approximation. Both of the VAEs, each for one domain, contain a\nsegmentation module, where the source segmentation is trained in a supervised\nmanner, while the target one is trained unsupervisedly. We validated the\nproposed domain adaptation method using two cardiac segmentation tasks, i.e.,\nthe cross-modality (CT and MR) whole heart segmentation and the cross-sequence\ncardiac MR segmentation. Results show that the proposed method achieved better\naccuracies compared to two state-of-the-art approaches and demonstrated good\npotential for cardiac segmentation. Furthermore, the proposed explicit\nregularization was shown to be effective and efficient in narrowing down the\ndistribution gap between domains, which is useful for unsupervised domain\nadaptation. Our code and data has been released via\nhttps://zmiclab.github.io/projects.html.",
          "link": "http://arxiv.org/abs/2106.08752",
          "publishedOn": "2021-06-17T01:58:42.492Z",
          "wordCount": 674,
          "title": "Unsupervised Domain Adaptation with Variational Approximation for Cardiac Segmentation. (arXiv:2106.08752v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengde Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "This paper presents an end-to-end semi-supervised object detection approach,\nin contrast to previous more complex multi-stage methods. The end-to-end\ntraining gradually improves pseudo label qualities during the curriculum, and\nthe more and more accurate pseudo labels in turn benefit object detection\ntraining. We also propose two simple yet effective techniques within this\nframework: a soft teacher mechanism where the classification loss of each\nunlabeled bounding box is weighed by the classification score produced by the\nteacher network; a box jittering approach to select reliable pseudo boxes for\nthe learning of box regression. On COCO benchmark, the proposed approach\noutperforms previous methods by a large margin under various labeling ratios,\ni.e. 1\\%, 5\\% and 10\\%. Moreover, our approach proves to perform also well when\nthe amount of labeled data is relatively large. For example, it can improve a\n40.9 mAP baseline detector trained using the full COCO training set by +3.6\nmAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the\nstate-of-the-art Swin Transformer-based object detector (58.9 mAP on test-dev),\nit can still significantly improve the detection accuracy by +1.5 mAP, reaching\n60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching\n52.4 mAP, pushing the new state-of-the-art.",
          "link": "http://arxiv.org/abs/2106.09018",
          "publishedOn": "2021-06-17T01:58:42.478Z",
          "wordCount": 642,
          "title": "End-to-End Semi-Supervised Object Detection with Soft Teacher. (arXiv:2106.09018v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1\">Veronika A. Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Left atrial (LA) segmentation from late gadolinium enhanced magnetic\nresonance imaging (LGE MRI) is a crucial step needed for planning the treatment\nof atrial fibrillation. However, automatic LA segmentation from LGE MRI is\nstill challenging, due to the poor image quality, high variability in LA\nshapes, and unclear LA boundary. Though deep learning-based methods can provide\npromising LA segmentation results, they often generalize poorly to unseen\ndomains, such as data from different scanners and/or sites. In this work, we\ncollect 210 LGE MRIs from different centers with different levels of image\nquality. To evaluate the domain generalization ability of models on the LA\nsegmentation task, we employ four commonly used semantic segmentation networks\nfor the LA segmentation from multi-center LGE MRIs. Besides, we investigate\nthree domain generalization strategies, i.e., histogram matching, mutual\ninformation based disentangled representation, and random style transfer, where\na simple histogram matching is proved to be most effective.",
          "link": "http://arxiv.org/abs/2106.08727",
          "publishedOn": "2021-06-17T01:58:42.460Z",
          "wordCount": 594,
          "title": "AtrialGeneral: Domain Generalization for Left Atrial Segmentation of Multi-Center LGE MRIs. (arXiv:2106.08727v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>",
          "description": "Video-grounded dialogue systems aim to integrate video understanding and\ndialogue understanding to generate responses that are relevant to both the\ndialogue and video context. Most existing approaches employ deep learning\nmodels and have achieved remarkable performance, given the relatively small\ndatasets available. However, the results are partly accomplished by exploiting\nbiases in the datasets rather than developing multimodal reasoning, resulting\nin limited generalization. In this paper, we propose a novel approach of\nCompositional Counterfactual Contrastive Learning ($C^3$) to develop\ncontrastive training between factual and counterfactual samples in\nvideo-grounded dialogues. Specifically, we design factual/counterfactual\nsampling based on the temporal steps in videos and tokens in dialogues and\npropose contrastive loss functions that exploit object-level or action-level\nvariance. Different from prior approaches, we focus on contrastive hidden state\nrepresentations among compositional output tokens to optimize the\nrepresentation space in a generation setting. We achieved promising performance\ngains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the\nbenefits of our approach in grounding video and dialogue context.",
          "link": "http://arxiv.org/abs/2106.08914",
          "publishedOn": "2021-06-17T01:58:42.454Z",
          "wordCount": 608,
          "title": "$C^3$: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.07978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_A/0/1/0/all/0/1\">A. H. Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">A. Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karstoft_H/0/1/0/all/0/1\">H. Karstoft</a>",
          "description": "Forecasting the formation and development of clouds is a central element of\nmodern weather forecasting systems. Incorrect clouds forecasts can lead to\nmajor uncertainty in the overall accuracy of weather forecasts due to their\nintrinsic role in the Earth's climate system. Few studies have tackled this\nchallenging problem from a machine learning point-of-view due to a shortage of\nhigh-resolution datasets with many historical observations globally. In this\npaper, we present a novel satellite-based dataset called ``CloudCast''. It\nconsists of 70,080 images with 10 different cloud types for multiple layers of\nthe atmosphere annotated on a pixel level. The spatial resolution of the\ndataset is 928 x 1530 pixels (3x3 km per pixel) with 15-min intervals between\nframes for the period 2017-01-01 to 2018-12-31. All frames are centered and\nprojected over Europe. To supplement the dataset, we conduct an evaluation\nstudy with current state-of-the-art video prediction methods such as\nconvolutional long short-term memory networks, generative adversarial networks,\nand optical flow-based extrapolation methods. As the evaluation of video\nprediction is difficult in practice, we aim for a thorough evaluation in the\nspatial and temporal domain. Our benchmark models show promising results but\nwith ample room for improvement. This is the first publicly available\nglobal-scale dataset with high-resolution cloud types on a high temporal\ngranularity to the authors' best knowledge.",
          "link": "http://arxiv.org/abs/2007.07978",
          "publishedOn": "2021-06-17T01:58:42.448Z",
          "wordCount": 711,
          "title": "CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds. (arXiv:2007.07978v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chengchao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Youtan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xubin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>",
          "description": "Generative Adversarial Networks (GANs) have demonstrated unprecedented\nsuccess in various image generation tasks. The encouraging results, however,\ncome at the price of a cumbersome training process, during which the generator\nand discriminator are alternately updated in two stages. In this paper, we\ninvestigate a general training scheme that enables training GANs efficiently in\nonly one stage. Based on the adversarial losses of the generator and\ndiscriminator, we categorize GANs into two classes, Symmetric GANs and\nAsymmetric GANs, and introduce a novel gradient decomposition method to unify\nthe two, allowing us to train both classes in one stage and hence alleviate the\ntraining effort. We also computationally analyze the efficiency of the proposed\nmethod, and empirically demonstrate that, the proposed method yields a solid\n$1.5\\times$ acceleration across various datasets and network architectures.\nFurthermore, we show that the proposed method is readily applicable to other\nadversarial-training scenarios, such as data-free knowledge distillation. The\ncode is available at https://github.com/zju-vipa/OSGAN.",
          "link": "http://arxiv.org/abs/2103.00430",
          "publishedOn": "2021-06-17T01:58:42.440Z",
          "wordCount": 645,
          "title": "Training Generative Adversarial Networks in One Stage. (arXiv:2103.00430v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00356",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Somraj_N/0/1/0/all/0/1\">Nagabhushan Somraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kashi_M/0/1/0/all/0/1\">Manoj Surya Kashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arun_S/0/1/0/all/0/1\">S. P. Arun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soundararajan_R/0/1/0/all/0/1\">Rajiv Soundararajan</a>",
          "description": "The study of video prediction models is believed to be a fundamental approach\nto representation learning for videos. While a plethora of generative models\nfor predicting the future frame pixel values given the past few frames exist,\nthe quantitative evaluation of the predicted frames has been found to be\nextremely challenging. In this context, we introduce the problem of naturalness\nevaluation, which refers to how natural or realistic a predicted video looks.\nWe create the Indian Institute of Science VIdeo Naturalness Evaluation (IISc\nVINE) Database consisting of 300 videos, obtained by applying different\nprediction models on different datasets, and accompanying human opinion scores.\nWe collected subjective ratings of naturalness from 50 human participants for\nthese videos. Our subjective study reveals that human observers were highly\nconsistent in their judgments of naturalness. We benchmark several popularly\nused measures for evaluating video prediction and show that they do not\nadequately correlate with these subjective scores. We introduce two new\nfeatures to effectively capture naturalness, motion-compensated cosine\nsimilarities of deep features of predicted frames with past frames, and deep\nfeatures extracted from rescaled frame differences. We show that our feature\ndesign leads to state of the art naturalness prediction in accordance with\nhuman judgments on our IISc VINE Database. The database and code are publicly\navailable on our project website:\nhttps://nagabhushansn95.github.io/publications/2020/vine",
          "link": "http://arxiv.org/abs/2005.00356",
          "publishedOn": "2021-06-17T01:58:42.434Z",
          "wordCount": 693,
          "title": "A Naturalness Evaluation Database for Video Prediction Models. (arXiv:2005.00356v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ehsanpour_M/0/1/0/all/0/1\">Mahsa Ehsanpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleh_F/0/1/0/all/0/1\">Fatemeh Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>",
          "description": "The availability of large-scale video action understanding datasets has\nfacilitated advances in the interpretation of visual scenes containing people.\nHowever, learning to recognize human activities in an unconstrained real-world\nenvironment, with potentially highly unbalanced and long-tailed distributed\ndata remains a significant challenge, not least owing to the lack of a\nreflective large-scale dataset. Most existing large-scale datasets are either\ncollected from a specific or constrained environment, e.g. kitchens or rooms,\nor video sharing platforms such as YouTube. In this paper, we introduce\nJRDB-Act, a multi-modal dataset, as an extension of the existing JRDB, which is\ncaptured by asocial mobile manipulator and reflects a real distribution of\nhuman daily life actions in a university campus environment. JRDB-Act has been\ndensely annotated with atomic actions, comprises over 2.8M action labels,\nconstituting a large-scale spatio-temporal action detection dataset. Each human\nbounding box is labelled with one pose-based action label and multiple\n(optional) interaction-based action labels. Moreover JRDB-Act comes with social\ngroup identification annotations conducive to the task of grouping individuals\nbased on their interactions in the scene to infer their social activities\n(common activities in each social group).",
          "link": "http://arxiv.org/abs/2106.08827",
          "publishedOn": "2021-06-17T01:58:42.411Z",
          "wordCount": 627,
          "title": "JRDB-Act: A Large-scale Multi-modal Dataset for Spatio-temporal Action, Social Group and Activity Detection. (arXiv:2106.08827v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.01955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Z/0/1/0/all/0/1\">Zhaoyang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OMay_C/0/1/0/all/0/1\">Callum Biggs O&#x27;May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1\">Bhavin Choksi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1\">Rufin VanRullen</a>",
          "description": "Modern feedforward convolutional neural networks (CNNs) can now solve some\ncomputer vision tasks at super-human levels. However, these networks only\nroughly mimic human visual perception. One difference from human vision is that\nthey do not appear to perceive illusory contours (e.g. Kanizsa squares) in the\nsame way humans do. Physiological evidence from visual cortex suggests that the\nperception of illusory contours could involve feedback connections. Would\nrecurrent feedback neural networks perceive illusory contours like humans? In\nthis work we equip a deep feedforward convolutional network with brain-inspired\nrecurrent dynamics. The network was first pretrained with an unsupervised\nreconstruction objective on a natural image dataset, to expose it to natural\nobject contour statistics. Then, a classification decision layer was added and\nthe model was finetuned on a form discrimination task: squares vs. randomly\noriented inducer shapes (no illusory contour). Finally, the model was tested\nwith the unfamiliar ''illusory contour'' configuration: inducer shapes oriented\nto form an illusory square. Compared with feedforward baselines, the iterative\n''predictive coding'' feedback resulted in more illusory contours being\nclassified as physical squares. The perception of the illusory contour was\nmeasurable in the luminance profile of the image reconstructions produced by\nthe model, demonstrating that the model really ''sees'' the illusion. Ablation\nstudies revealed that natural image pretraining and feedback error correction\nare both critical to the perception of the illusion. Finally we validated our\nconclusions in a deeper network (VGG): adding the same predictive coding\nfeedback dynamics again leads to the perception of illusory contours.",
          "link": "http://arxiv.org/abs/2102.01955",
          "publishedOn": "2021-06-17T01:58:42.399Z",
          "wordCount": 725,
          "title": "Predictive coding feedback results in perceived illusory contours in a recurrent neural network. (arXiv:2102.01955v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08936",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1\">Luka Murn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blasi_S/0/1/0/all/0/1\">Saverio Blasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smeaton_A/0/1/0/all/0/1\">Alan F. Smeaton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>",
          "description": "The versatility of recent machine learning approaches makes them ideal for\nimprovement of next generation video compression solutions. Unfortunately,\nthese approaches typically bring significant increases in computational\ncomplexity and are difficult to interpret into explainable models, affecting\ntheir potential for implementation within practical video coding applications.\nThis paper introduces a novel explainable neural network-based inter-prediction\nscheme, to improve the interpolation of reference samples needed for fractional\nprecision motion compensation. The approach requires a single neural network to\nbe trained from which a full quarter-pixel interpolation filter set is derived,\nas the network is easily interpretable due to its linear structure. A novel\ntraining framework enables each network branch to resemble a specific\nfractional shift. This practical solution makes it very efficient to use\nalongside conventional video coding schemes. When implemented in the context of\nthe state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and\n2.25% BD-rate savings can be achieved on average for lower resolution sequences\nunder the random access, low-delay B and low-delay P configurations,\nrespectively, while the complexity of the learned interpolation schemes is\nsignificantly reduced compared to the interpolation with full CNNs.",
          "link": "http://arxiv.org/abs/2106.08936",
          "publishedOn": "2021-06-17T01:58:42.374Z",
          "wordCount": 664,
          "title": "Improved CNN-based Learning of Interpolation Filters for Low-Complexity Inter Prediction in Video Coding. (arXiv:2106.08936v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengfei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Puyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shanshan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Reconstructing magnetic resonance (MR) images from undersampled data is a\nchallenging problem due to various artifacts introduced by the under-sampling\noperation. Recent deep learning-based methods for MR image reconstruction\nusually leverage a generic auto-encoder architecture which captures low-level\nfeatures at the initial layers and high?level features at the deeper layers.\nSuch networks focus much on global features which may not be optimal to\nreconstruct the fully-sampled image. In this paper, we propose an\nOver-and-Under Complete Convolu?tional Recurrent Neural Network (OUCR), which\nconsists of an overcomplete and an undercomplete Convolutional Recurrent Neural\nNetwork(CRNN). The overcomplete branch gives special attention in learning\nlocal structures by restraining the receptive field of the network. Combining\nit with the undercomplete branch leads to a network which focuses more on\nlow-level features without losing out on the global structures. Extensive\nexperiments on two datasets demonstrate that the proposed method achieves\nsignificant improvements over the compressed sensing and popular deep\nlearning-based methods with less number of trainable parameters. Our code is\navailable at https://github.com/guopengf/OUCR.",
          "link": "http://arxiv.org/abs/2106.08886",
          "publishedOn": "2021-06-17T01:58:42.368Z",
          "wordCount": 608,
          "title": "Over-and-Under Complete Convolutional RNN for MRI Reconstruction. (arXiv:2106.08886v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Friebel_A/0/1/0/all/0/1\">Adrian Friebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johann_T/0/1/0/all/0/1\">Tim Johann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drasdo_D/0/1/0/all/0/1\">Dirk Drasdo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoehme_S/0/1/0/all/0/1\">Stefan Hoehme</a>",
          "description": "We present a novel approach that combines machine learning based interactive\nimage segmentation with a two-stage clustering method to identify similarly\ncolored images for efficient batch image segmentation by guided reuse of\nclassifiers. The segmentation task is formulated as a supervised machine\nlearning problem working on homogeneous groups of voxels termed supervoxels.\nClassifiers are interactively trained from sparse annotations in an iterative\nprocess of annotation refinement. Resulting models can be used for batch\nprocessing of previously unseen images. By clustering images into subsets of\nsimilar colorization, we identify a minimal set of prototype images and\ndemonstrate that using only classifiers trained on these prototype images for\ntheir color-cluster significantly improves the average segmentation performance\nof batch processing. The presented methods are applicable for almost any image\ntype and therefore represent a useful tool for image analysis tasks in general.",
          "link": "http://arxiv.org/abs/2005.07662",
          "publishedOn": "2021-06-17T01:58:42.361Z",
          "wordCount": 614,
          "title": "Guided interactive image segmentation using machine learning and color based data set clustering. (arXiv:2005.07662v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Souri_H/0/1/0/all/0/1\">Hossein Souri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1\">Liam Fowl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "As the curation of data for machine learning becomes increasingly automated,\ndataset tampering is a mounting threat. Backdoor attackers tamper with training\ndata to embed a vulnerability in models that are trained on that data. This\nvulnerability is then activated at inference time by placing a \"trigger\" into\nthe model's input. Typical backdoor attacks insert the trigger directly into\nthe training data, although the presence of such an attack may be visible upon\ninspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning\nwithout placing a trigger into the training data at all. However, this hidden\ntrigger attack is ineffective at poisoning neural networks trained from\nscratch. We develop a new hidden trigger attack, Sleeper Agent, which employs\ngradient matching, data selection, and target model re-training during the\ncrafting process. Sleeper Agent is the first hidden trigger backdoor attack to\nbe effective against neural networks trained from scratch. We demonstrate its\neffectiveness on ImageNet and in black-box settings. Our implementation code\ncan be found at https://github.com/hsouri/Sleeper-Agent.",
          "link": "http://arxiv.org/abs/2106.08970",
          "publishedOn": "2021-06-17T01:58:42.347Z",
          "wordCount": 613,
          "title": "Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch. (arXiv:2106.08970v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1\">Apostolos Modas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1\">Alessio Xompero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Matilla_R/0/1/0/all/0/1\">Ricardo Sanchez-Matilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>",
          "description": "We investigate the problem of classifying - from a single image - the level\nof content in a cup or a drinking glass. This problem is made challenging by\nseveral ambiguities caused by transparencies, shape variations and partial\nocclusions, and by the availability of only small training datasets. In this\npaper, we tackle this problem with an appropriate strategy for transfer\nlearning. Specifically, we use adversarial training in a generic source dataset\nand then refine the training with a task-specific dataset. We also discuss and\nexperimentally evaluate several training strategies and their combination on a\nrange of container types of the CORSMAL Containers Manipulation dataset. We\nshow that transfer learning with adversarial training in the source domain\nconsistently improves the classification accuracy on the test set and limits\nthe overfitting of the classifier to specific features of the training data.",
          "link": "http://arxiv.org/abs/2102.04057",
          "publishedOn": "2021-06-17T01:58:42.334Z",
          "wordCount": 616,
          "title": "Improving filling level classification with adversarial training. (arXiv:2102.04057v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaoxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Y/0/1/0/all/0/1\">Yu She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_B/0/1/0/all/0/1\">Branden Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelson_E/0/1/0/all/0/1\">Edward Adelson</a>",
          "description": "Vision-based tactile sensors have the potential to provide important contact\ngeometry to localize the objective with visual occlusion. However, it is\nchallenging to measure high-resolution 3D contact geometry for a compact robot\nfinger, to simultaneously meet optical and mechanical constraints. In this\nwork, we present the GelSight Wedge sensor, which is optimized to have a\ncompact shape for robot fingers, while achieving high-resolution 3D\nreconstruction. We evaluate the 3D reconstruction under different lighting\nconfigurations, and extend the method from 3 lights to 1 or 2 lights. We\ndemonstrate the flexibility of the design by shrinking the sensor to the size\nof a human finger for fine manipulation tasks. We also show the effectiveness\nand potential of the reconstructed 3D geometry for pose tracking in the 3D\nspace.",
          "link": "http://arxiv.org/abs/2106.08851",
          "publishedOn": "2021-06-17T01:58:42.317Z",
          "wordCount": 567,
          "title": "GelSight Wedge: Measuring High-Resolution 3D Contact Geometry with a Compact Robot Finger. (arXiv:2106.08851v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2009.04709",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lanfredi_R/0/1/0/all/0/1\">Ricardo Bigolin Lanfredi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schroeder_J/0/1/0/all/0/1\">Joyce D. Schroeder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tasdizen_T/0/1/0/all/0/1\">Tolga Tasdizen</a>",
          "description": "Adversarial training, especially projected gradient descent (PGD), has been a\nsuccessful approach for improving robustness against adversarial attacks. After\nadversarial training, gradients of models with respect to their inputs have a\npreferential direction. However, the direction of alignment is not\nmathematically well established, making it difficult to evaluate\nquantitatively. We propose a novel definition of this direction as the\ndirection of the vector pointing toward the closest point of the support of the\nclosest inaccurate class in decision space. To evaluate the alignment with this\ndirection after adversarial training, we apply a metric that uses generative\nadversarial networks to produce the smallest residual needed to change the\nclass present in the image. We show that PGD-trained models have a higher\nalignment than the baseline according to our definition, that our metric\npresents higher alignment values than a competing metric formulation, and that\nenforcing this alignment increases the robustness of models.",
          "link": "http://arxiv.org/abs/2009.04709",
          "publishedOn": "2021-06-17T01:58:42.311Z",
          "wordCount": 693,
          "title": "Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent. (arXiv:2009.04709v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dufumier_B/0/1/0/all/0/1\">Benoit Dufumier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Victor_J/0/1/0/all/0/1\">Julie Victor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigis_A/0/1/0/all/0/1\">Antoine Grigis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wessa_M/0/1/0/all/0/1\">Michel Wessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brambilla_P/0/1/0/all/0/1\">Paolo Brambilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favre_P/0/1/0/all/0/1\">Pauline Favre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polosan_M/0/1/0/all/0/1\">Mircea Polosan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_C/0/1/0/all/0/1\">Colm McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piguet_C/0/1/0/all/0/1\">Camille Marie Piguet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duchesnay_E/0/1/0/all/0/1\">Edouard Duchesnay</a>",
          "description": "Traditional supervised learning with deep neural networks requires a\ntremendous amount of labelled data to converge to a good solution. For 3D\nmedical images, it is often impractical to build a large homogeneous annotated\ndataset for a specific pathology. Self-supervised methods offer a new way to\nlearn a representation of the images in an unsupervised manner with a neural\nnetwork. In particular, contrastive learning has shown great promises by\n(almost) matching the performance of fully-supervised CNN on vision tasks.\nNonetheless, this method does not take advantage of available meta-data, such\nas participant's age, viewed as prior knowledge. Here, we propose to leverage\ncontinuous proxy metadata, in the contrastive learning framework, by\nintroducing a new loss called y-Aware InfoNCE loss. Specifically, we improve\nthe positive sampling during pre-training by adding more positive examples with\nsimilar proxy meta-data with the anchor, assuming they share similar\ndiscriminative semantic features.With our method, a 3D CNN model pre-trained on\n$10^4$ multi-site healthy brain MRI scans can extract relevant features for\nthree classification tasks: schizophrenia, bipolar diagnosis and Alzheimer's\ndetection. When fine-tuned, it also outperforms 3D CNN trained from scratch on\nthese tasks, as well as state-of-the-art self-supervised methods. Our code is\nmade publicly available here.",
          "link": "http://arxiv.org/abs/2106.08808",
          "publishedOn": "2021-06-17T01:58:42.304Z",
          "wordCount": 661,
          "title": "Contrastive Learning with Continuous Proxy Meta-Data for 3D MRI Classification. (arXiv:2106.08808v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DInverno_G/0/1/0/all/0/1\">Giuseppe Alessio D&#x27;Inverno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchini_M/0/1/0/all/0/1\">Monica Bianchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampoli_M/0/1/0/all/0/1\">Maria Lucia Sampoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1\">Franco Scarselli</a>",
          "description": "Graph Neural Networks (GNNs) are a wide class of connectionist models for\ngraph processing. They perform an iterative message passing operation on each\nnode and its neighbors, to solve classification/ clustering tasks --- on some\nnodes or on the whole graph --- collecting all such messages, regardless of\ntheir order. Despite the differences among the various models belonging to this\nclass, most of them adopt the same computation scheme, based on a local\naggregation mechanism and, intuitively, the local computation framework is\nmainly responsible for the expressive power of GNNs. In this paper, we prove\nthat the Weisfeiler--Lehman test induces an equivalence relationship on the\ngraph nodes that exactly corresponds to the unfolding equivalence, defined on\nthe original GNN model. Therefore, the results on the expressive power of the\noriginal GNNs can be extended to general GNNs which, under mild conditions, can\nbe proved capable of approximating, in probability and up to any precision, any\nfunction on graphs that respects the unfolding equivalence.",
          "link": "http://arxiv.org/abs/2106.08992",
          "publishedOn": "2021-06-17T01:58:42.297Z",
          "wordCount": 611,
          "title": "An unifying point of view on expressive power of GNNs. (arXiv:2106.08992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.09899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Mingzhu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shaoqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>",
          "description": "User data confidentiality protection is becoming a rising challenge in the\npresent deep learning research. Without access to data, conventional\ndata-driven model compression faces a higher risk of performance degradation.\nRecently, some works propose to generate images from a specific pretrained\nmodel to serve as training data. However, the inversion process only utilizes\nbiased feature statistics stored in one model and is from low-dimension to\nhigh-dimension. As a consequence, it inevitably encounters the difficulties of\ngeneralizability and inexact inversion, which leads to unsatisfactory\nperformance. To address these problems, we propose MixMix based on two simple\nyet effective techniques: (1) Feature Mixing: utilizes various models to\nconstruct a universal feature space for generalized inversion; (2) Data Mixing:\nmixes the synthesized images and labels to generate exact label information. We\nprove the effectiveness of MixMix from both theoretical and empirical\nperspectives. Extensive experiments show that MixMix outperforms existing\nmethods on the mainstream compression tasks, including quantization, knowledge\ndistillation, and pruning. Specifically, MixMix achieves up to 4% and 20%\naccuracy uplift on quantization and pruning, respectively, compared to existing\ndata-free compression work.",
          "link": "http://arxiv.org/abs/2011.09899",
          "publishedOn": "2021-06-17T01:58:42.280Z",
          "wordCount": 653,
          "title": "MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing. (arXiv:2011.09899v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Illing_B/0/1/0/all/0/1\">Bernd Illing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventura_J/0/1/0/all/0/1\">Jean Ventura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1\">Guillaume Bellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1\">Wulfram Gerstner</a>",
          "description": "Learning in the brain is poorly understood and learning rules that respect\nbiological constraints, yet yield deep hierarchical representations, are still\nunknown. Here, we propose a learning rule that takes inspiration from\nneuroscience and recent advances in self-supervised deep learning. Learning\nminimizes a simple layer-specific loss function and does not need to\nback-propagate error signals within or between layers. Instead, weight updates\nfollow a local, Hebbian, learning rule that only depends on pre- and\npost-synaptic neuronal activity, predictive dendritic input and widely\nbroadcasted modulation factors which are identical for large groups of neurons.\nThe learning rule applies contrastive predictive learning to a causal,\nbiological setting using saccades (i.e. rapid shifts in gaze direction). We\nfind that networks trained with this self-supervised and local rule build deep\nhierarchical representations of images, speech and video.",
          "link": "http://arxiv.org/abs/2010.08262",
          "publishedOn": "2021-06-17T01:58:42.274Z",
          "wordCount": 624,
          "title": "Local plasticity rules can learn deep representations using self-supervised contrastive predictions. (arXiv:2010.08262v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cascante_Bonilla_P/0/1/0/all/0/1\">Paola Cascante-Bonilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1\">Arshdeep Sekhon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>",
          "description": "Convolutional neural networks for visual recognition require large amounts of\ntraining samples and usually benefit from data augmentation. This paper\nproposes PatchMix, a data augmentation method that creates new samples by\ncomposing patches from pairs of images in a grid-like pattern. These new\nsamples' ground truth labels are set as proportional to the number of patches\nfrom each image. We then add a set of additional losses at the patch-level to\nregularize and to encourage good representations at both the patch and image\nlevels. A ResNet-50 model trained on ImageNet using PatchMix exhibits superior\ntransfer learning capabilities across a wide array of benchmarks. Although\nPatchMix can rely on random pairings and random grid-like patterns for mixing,\nwe explore evolutionary search as a guiding strategy to discover optimal\ngrid-like patterns and image pairing jointly. For this purpose, we conceive a\nfitness function that bypasses the need to re-train a model to evaluate each\nchoice. In this way, PatchMix outperforms a base model on CIFAR-10 (+1.91),\nCIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16) by significant\nmargins, also outperforming previous state-of-the-art pairwise augmentation\nstrategies.",
          "link": "http://arxiv.org/abs/2106.09011",
          "publishedOn": "2021-06-17T01:58:42.253Z",
          "wordCount": 619,
          "title": "Evolving Image Compositions for Feature Representation Learning. (arXiv:2106.09011v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rozumnyi_D/0/1/0/all/0/1\">Denys Rozumnyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>",
          "description": "We address the novel task of jointly reconstructing the 3D shape, texture,\nand motion of an object from a single motion-blurred image. While previous\napproaches address the deblurring problem only in the 2D image domain, our\nproposed rigorous modeling of all object properties in the 3D domain enables\nthe correct description of arbitrary object motion. This leads to significantly\nbetter image decomposition and sharper deblurring results. We model the\nobserved appearance of a motion-blurred object as a combination of the\nbackground and a 3D object with constant translation and rotation. Our method\nminimizes a loss on reconstructing the input image via differentiable rendering\nwith suitable regularizers. This enables estimating the textured 3D mesh of the\nblurred object with high fidelity. Our method substantially outperforms\ncompeting approaches on several benchmarks for fast moving objects deblurring.\nQualitative results show that the reconstructed 3D mesh generates high-quality\ntemporal super-resolution and novel views of the deblurred object.",
          "link": "http://arxiv.org/abs/2106.08762",
          "publishedOn": "2021-06-17T01:58:42.247Z",
          "wordCount": 600,
          "title": "Shape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving Objects. (arXiv:2106.08762v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shuangyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chengsong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagavathiannan_M/0/1/0/all/0/1\">Muthukumar Bagavathiannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dezhen Song</a>",
          "description": "To enable robotic weed control, we develop algorithms to detect nutsedge weed\nfrom bermudagrass turf. Due to the similarity between the weed and the\nbackground turf, manual data labeling is expensive and error-prone.\nConsequently, directly applying deep learning methods for object detection\ncannot generate satisfactory results. Building on an instance detection\napproach (i.e. Mask R-CNN), we combine synthetic data with raw data to train\nthe network. We propose an algorithm to generate high fidelity synthetic data,\nadopting different levels of annotations to reduce labeling cost. Moreover, we\nconstruct a nutsedge skeleton-based probabilistic map (NSPM) as the neural\nnetwork input to reduce the reliance on pixel-wise precise labeling. We also\nmodify loss function from cross entropy to Kullback-Leibler divergence which\naccommodates uncertainty in the labeling process. We implement the proposed\nalgorithm and compare it with both Faster R-CNN and Mask R-CNN. The results\nshow that our design can effectively overcome the impact of imprecise and\ninsufficient training sample issues and significantly outperform the Faster\nR-CNN counterpart with a false negative rate of only 0.4%. In particular, our\napproach also reduces labeling time by 95% while achieving better performance\nif comparing with the original Mask R-CNN approach.",
          "link": "http://arxiv.org/abs/2106.08897",
          "publishedOn": "2021-06-17T01:58:42.230Z",
          "wordCount": 642,
          "title": "Toward Robotic Weed Control: Detection of Nutsedge Weed in Bermudagrass Turf Using Inaccurate and Insufficient Training Data. (arXiv:2106.08897v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jacky Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kit-Yung Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lik-Hang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_P/0/1/0/all/0/1\">Pan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiang Su</a>",
          "description": "Mobile Augmented Reality (MAR) integrates computer-generated virtual objects\nwith physical environments for mobile devices. MAR systems enable users to\ninteract with MAR devices, such as smartphones and head-worn wearables, and\nperforms seamless transitions from the physical world to a mixed world with\ndigital entities. These MAR systems support user experiences by using MAR\ndevices to provide universal accessibility to digital contents. Over the past\n20 years, a number of MAR systems have been developed, however, the studies and\ndesign of MAR frameworks have not yet been systematically reviewed from the\nperspective of user-centric design. This article presents the first effort of\nsurveying existing MAR frameworks (count: 37) and further discusses the latest\nstudies on MAR through a top-down approach: 1) MAR applications; 2) MAR\nvisualisation techniques adaptive to user mobility and contexts; 3) systematic\nevaluation of MAR frameworks including supported platforms and corresponding\nfeatures such as tracking, feature extraction plus sensing capabilities; and 4)\nunderlying machine learning approaches supporting intelligent operations within\nMAR systems. Finally, we summarise the development of emerging research fields,\ncurrent state-of-the-art, and discuss the important open challenges and\npossible theoretical and technical directions. This survey aims to benefit both\nresearchers and MAR system developers alike.",
          "link": "http://arxiv.org/abs/2106.08710",
          "publishedOn": "2021-06-17T01:58:42.224Z",
          "wordCount": 658,
          "title": "Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence. (arXiv:2106.08710v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1\">Timoleon Moraitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1\">Abu Sebastian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eleftheriou_E/0/1/0/all/0/1\">Evangelos Eleftheriou</a> (IBM Research - Zurich)",
          "description": "Biological neurons and their in-silico emulations for neuromorphic artificial\nintelligence (AI) use extraordinarily energy-efficient mechanisms, such as\nspike-based communication and local synaptic plasticity. It remains unclear\nwhether these neuronal mechanisms only offer efficiency or also underlie the\nsuperiority of biological intelligence. Here, we prove rigorously that, indeed,\nthe Bayes-optimal prediction and inference of randomly but continuously\ntransforming environments, a common natural setting, relies on short-term\nspike-timing-dependent plasticity, a hallmark of biological synapses. Further,\nthis dynamic Bayesian inference through plasticity enables circuits of the\ncerebral cortex in simulations to recognize previously unseen, highly distorted\ndynamic stimuli. Strikingly, this also introduces a biologically-modelled AI,\nthe first to overcome multiple limitations of deep learning and outperform\nartificial neural networks in a visual task. The cortical-like network is\nspiking and event-based, trained only with unsupervised and local plasticity,\non a small, narrow, and static training dataset, but achieves recognition of\nunseen, transformed, and dynamic data better than deep neural networks with\ncontinuous activations, trained with supervised backpropagation on the\ntransforming data. These results link short-term plasticity to high-level\ncortical function, suggest optimality of natural intelligence for natural\nenvironments, and repurpose neuromorphic AI from mere efficiency to\ncomputational supremacy altogether.",
          "link": "http://arxiv.org/abs/2009.06808",
          "publishedOn": "2021-06-17T01:58:42.206Z",
          "wordCount": 687,
          "title": "Optimality of short-term synaptic plasticity in modelling certain dynamic environments. (arXiv:2009.06808v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suleymanov_T/0/1/0/all/0/1\">Tarlan Suleymanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadd_M/0/1/0/all/0/1\">Matthew Gadd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martini_D/0/1/0/all/0/1\">Daniele De Martini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_P/0/1/0/all/0/1\">Paul Newman</a>",
          "description": "In this paper we present the Oxford Road Boundaries Dataset, designed for\ntraining and testing machine-learning-based road-boundary detection and\ninference approaches. We have hand-annotated two of the 10 km-long forays from\nthe Oxford Robotcar Dataset and generated from other forays several thousand\nfurther examples with semi-annotated road-boundary masks. To boost the number\nof training samples in this way, we used a vision-based localiser to project\nlabels from the annotated datasets to other traversals at different times and\nweather conditions. As a result, we release 62605 labelled samples, of which\n47639 samples are curated. Each of these samples contains both raw and\nclassified masks for left and right lenses. Our data contains images from a\ndiverse set of scenarios such as straight roads, parked cars, junctions, etc.\nFiles for download and tools for manipulating the labelled data are available\nat: oxford-robotics-institute.github.io/road-boundaries-dataset",
          "link": "http://arxiv.org/abs/2106.08983",
          "publishedOn": "2021-06-17T01:58:42.191Z",
          "wordCount": 589,
          "title": "The Oxford Road Boundaries Dataset. (arXiv:2106.08983v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Adversarial training has been the topic of dozens of studies and a leading\nmethod for defending against adversarial attacks. Yet, it remains largely\nunknown (a) how adversarially-robust ImageNet classifiers (R classifiers)\ngeneralize to out-of-distribution examples; and (b) how their generalization\ncapability relates to their hidden representations. In this paper, we perform a\nthorough, systematic study to answer these two questions across AlexNet,\nGoogLeNet, and ResNet-50 architectures. We found that while standard ImageNet\nclassifiers have a strong texture bias, their R counterparts rely heavily on\nshapes. Remarkably, adversarial training induces three simplicity biases into\nhidden neurons in the process of 'robustifying' the network. That is, each\nconvolutional neuron in R networks often changes to detecting (1) pixel-wise\nsmoother patterns i.e. a mechanism that blocks high-frequency noise from\npassing through the network; (2) more lower-level features i.e. textures and\ncolors (instead of objects); and (3) fewer types of inputs. Our findings reveal\nthe interesting mechanisms that made networks more adversarially robust and\nalso explain some recent findings. Our findings reveal the interesting\nmechanisms that made networks more adversarially robust and also explain some\nrecent findings e.g. why R networks benefit from much larger capacity (Xie and\nYuille, 2020) and can act as a strong image prior in image synthesis (Santurkar\net al., 2019).",
          "link": "http://arxiv.org/abs/2006.09373",
          "publishedOn": "2021-06-17T01:58:42.186Z",
          "wordCount": 693,
          "title": "The shape and simplicity biases of adversarially robust ImageNet-trained CNNs. (arXiv:2006.09373v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1\">Jiajun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>",
          "description": "Attention has been proved to be an efficient mechanism to capture long-range\ndependencies. However, so far it has not been deployed in invertible networks.\nThis is due to the fact that in order to make a network invertible, every\ncomponent within the network needs to be a bijective transformation, but a\nnormal attention block is not. In this paper, we propose invertible attention\nthat can be plugged into existing invertible models. We mathematically and\nexperimentally prove that the invertibility of an attention model can be\nachieved by carefully constraining its Lipschitz constant. We validate the\ninvertibility of our invertible attention on image reconstruction task with 3\npopular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible\nattention achieves similar performance in comparison with normal non-invertible\nattention on dense prediction tasks.",
          "link": "http://arxiv.org/abs/2106.09003",
          "publishedOn": "2021-06-17T01:58:42.143Z",
          "wordCount": 555,
          "title": "Invertible Attention. (arXiv:2106.09003v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.10817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foo_L/0/1/0/all/0/1\">Lin Geng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>",
          "description": "We consider the problem of segmenting cell nuclei instances from Hematoxylin\nand Eosin (H&E) stains with dot annotations only. While most recent works focus\non improving the segmentation quality, this is usually insufficient for\ninstance segmentation of cell instances clustered together or with a small\nsize. In this work, we propose a simple two-step post-processing procedure,\nSplit and Expand, that directly improves the conversion of segmentation maps to\ninstances. In the splitting step, we generate fine-grained cell instances from\nthe segmentation map with the guidance of cell-center predictions. For the\nexpansion step, we utilize Layer-wise Relevance Propagation (LRP) explanation\nresults to add small cells that are not captured in the segmentation map.\nAlthough we additionally train an output head to predict cell-centers, the\npost-processing procedure itself is not explicitly trained and is executed at\ninference-time only. A feature re-weighting loss based on LRP is proposed to\nimprove our method even further. We test our procedure on the MoNuSeg and TNBC\ndatasets and show quantitatively and qualitatively that our proposed method\nimproves object-level metrics substantially.",
          "link": "http://arxiv.org/abs/2007.10817",
          "publishedOn": "2021-06-17T01:58:42.082Z",
          "wordCount": 649,
          "title": "Split and Expand: An inference-time improvement for Weakly Supervised Cell Instance Segmentation. (arXiv:2007.10817v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsaregorodtsev_A/0/1/0/all/0/1\">Alexander Tsaregorodtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>",
          "description": "We present an automated data augmentation approach for image classification.\nWe formulate the problem as Monte Carlo sampling where our goal is to\napproximate the optimal augmentation policies. We propose a particle filtering\nformulation to find optimal augmentation policies and their schedules during\nmodel training. Our performance measurement procedure relies on a validation\nsubset of our training set, while the policy transition model depends on a\nGaussian prior and an optional augmentation velocity parameter. In our\nexperiments, we show that our formulation for automated augmentation reaches\npromising results on CIFAR-10, CIFAR-100, and ImageNet datasets using the\nstandard network architectures for this problem. By comparing with the related\nwork, we also show that our method reaches a balance between the computational\ncost of policy search and the model performance.",
          "link": "http://arxiv.org/abs/2106.08693",
          "publishedOn": "2021-06-17T01:58:42.075Z",
          "wordCount": 555,
          "title": "ParticleAugment: Sampling-Based Data Augmentation. (arXiv:2106.08693v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.12616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Freeman_I/0/1/0/all/0/1\">Ido Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummert_A/0/1/0/all/0/1\">Anton Kummert</a>",
          "description": "The rising demand for Active Safety systems in automotive applications\nstresses the need for a reliable short to mid-term trajectory prediction.\nAnticipating the unfolding path of road users, one can act to increase the\noverall safety. In this work, we propose to train artificial neural networks\nfor movement understanding by predicting trajectories in their natural form, as\na function of time. Predicting polynomial coefficients allows us to increased\naccuracy and improve generalisation.",
          "link": "http://arxiv.org/abs/2101.12616",
          "publishedOn": "2021-06-17T01:58:42.068Z",
          "wordCount": 540,
          "title": "Polynomial Trajectory Predictions for Improved Learning Performance. (arXiv:2101.12616v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1\">Kale-ab Tessera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1\">Benjamin Rosman</a>",
          "description": "Training sparse networks to converge to the same performance as dense neural\narchitectures has proven to be elusive. Recent work suggests that\ninitialization is the key. However, while this direction of research has had\nsome success, focusing on initialization alone appears to be inadequate. In\nthis paper, we take a broader view of training sparse networks and consider the\nrole of regularization, optimization, and architecture choices on sparse\nmodels. We propose a simple experimental framework, Same Capacity Sparse vs\nDense Comparison (SC-SDC), that allows for a fair comparison of sparse and\ndense networks. Furthermore, we propose a new measure of gradient flow,\nEffective Gradient Flow (EGF), that better correlates to performance in sparse\nnetworks. Using top-line metrics, SC-SDC and EGF, we show that default choices\nof optimizers, activation functions and regularizers used for dense networks\ncan disadvantage sparse networks. Based upon these findings, we show that\ngradient flow in sparse networks can be improved by reconsidering aspects of\nthe architecture design and the training regime. Our work suggests that\ninitialization is only one piece of the puzzle and taking a wider view of\ntailoring optimization to sparse networks yields promising results.",
          "link": "http://arxiv.org/abs/2102.01670",
          "publishedOn": "2021-06-17T01:58:42.062Z",
          "wordCount": 655,
          "title": "Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization. (arXiv:2102.01670v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zihan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fuyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1\">Fan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>",
          "description": "Using generative models to synthesize visual features from semantic\ndistribution is one of the most popular solutions to ZSL image classification\nin recent years. The triplet loss (TL) is popularly used to generate realistic\nvisual distributions from semantics by automatically searching discriminative\nrepresentations. However, the traditional TL cannot search reliable unseen\ndisentangled representations due to the unavailability of unseen classes in\nZSL. To alleviate this drawback, we propose in this work a multi-modal triplet\nloss (MMTL) which utilizes multimodal information to search a disentangled\nrepresentation space. As such, all classes can interplay which can benefit\nlearning disentangled class representations in the searched space. Furthermore,\nwe develop a novel model called Disentangling Class Representation Generative\nAdversarial Network (DCR-GAN) focusing on exploiting the disentangled\nrepresentations in training, feature synthesis, and final recognition stages.\nBenefiting from the disentangled representations, DCR-GAN could fit a more\nrealistic distribution over both seen and unseen features. Extensive\nexperiments show that our proposed model can lead to superior performance to\nthe state-of-the-arts on four benchmark datasets. Our code is available at\nhttps://github.com/FouriYe/DCRGAN-TMM.",
          "link": "http://arxiv.org/abs/2106.08605",
          "publishedOn": "2021-06-17T01:58:42.047Z",
          "wordCount": 616,
          "title": "Disentangling Semantic-to-visual Confusion for Zero-shot Learning. (arXiv:2106.08605v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08706",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pandey_L/0/1/0/all/0/1\">Laxmi Pandey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arif_A/0/1/0/all/0/1\">Ahmed Sabbir Arif</a>",
          "description": "Speech sounds of spoken language are obtained by varying configuration of the\narticulators surrounding the vocal tract. They contain abundant information\nthat can be utilized to better understand the underlying mechanism of human\nspeech production. We propose a novel deep neural network-based learning\nframework that understands acoustic information in the variable-length sequence\nof vocal tract shaping during speech production, captured by real-time magnetic\nresonance imaging (rtMRI), and translate it into text. The proposed framework\ncomprises of spatiotemporal convolutions, a recurrent network, and the\nconnectionist temporal classification loss, trained entirely end-to-end. On the\nUSC-TIMIT corpus, the model achieved a 40.6% PER at sentence-level, much better\ncompared to the existing models. To the best of our knowledge, this is the\nfirst study that demonstrates the recognition of entire spoken sentence based\non an individual's articulatory motions captured by rtMRI video. We also\nperformed an analysis of variations in the geometry of articulation in each\nsub-regions of the vocal tract (i.e., pharyngeal, velar and dorsal, hard\npalate, labial constriction region) with respect to different emotions and\ngenders. Results suggest that each sub-regions distortion is affected by both\nemotion and gender.",
          "link": "http://arxiv.org/abs/2106.08706",
          "publishedOn": "2021-06-17T01:58:42.010Z",
          "wordCount": 656,
          "title": "Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in Real-Time MRI. (arXiv:2106.08706v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zilong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guozhong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Bin Fu</a>",
          "description": "This is a short technical report introducing the solution of the Team\nTCParser for Short-video Face Parsing Track of The 3rd Person in Context (PIC)\nWorkshop and Challenge at CVPR 2021. In this paper, we introduce a strong\nbackbone which is cross-window based Shuffle Transformer for presenting\naccurate face parsing representation. To further obtain the finer segmentation\nresults, especially on the edges, we introduce a Feature Alignment Aggregation\n(FAA) module. It can effectively relieve the feature misalignment issue caused\nby multi-resolution feature aggregation. Benefiting from the stronger backbone\nand better feature aggregation, the proposed method achieves 86.9519% score in\nthe Short-video Face Parsing track of the 3rd Person in Context (PIC) Workshop\nand Challenge, ranked the first place.",
          "link": "http://arxiv.org/abs/2106.08650",
          "publishedOn": "2021-06-17T01:58:41.996Z",
          "wordCount": 558,
          "title": "Shuffle Transformer with Feature Alignment for Video Face Parsing. (arXiv:2106.08650v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "In this work, we address the task of referring image segmentation (RIS),\nwhich aims at predicting a segmentation mask for the object described by a\nnatural language expression. Most existing methods focus on establishing\nunidirectional or directional relationships between visual and linguistic\nfeatures to associate two modalities together, while the multi-scale context is\nignored or insufficiently modeled. Multi-scale context is crucial to localize\nand segment those objects that have large scale variations during the\nmulti-modal fusion process. To solve this problem, we propose a simple yet\neffective Cascaded Multi-modal Fusion (CMF) module, which stacks multiple\natrous convolutional layers in parallel and further introduces a cascaded\nbranch to fuse visual and linguistic features. The cascaded branch can\nprogressively integrate multi-scale contextual information and facilitate the\nalignment of two modalities during the multi-modal fusion process. Experimental\nresults on four benchmark datasets demonstrate that our method outperforms most\nstate-of-the-art methods. Code is available at\nhttps://github.com/jianhua2022/CMF-Refseg.",
          "link": "http://arxiv.org/abs/2106.08617",
          "publishedOn": "2021-06-17T01:58:41.990Z",
          "wordCount": 588,
          "title": "CMF: Cascaded Multi-model Fusion for Referring Image Segmentation. (arXiv:2106.08617v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yueming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaolin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_B/0/1/0/all/0/1\">Bing Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_T/0/1/0/all/0/1\">Tengfei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yawei Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Haojin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>",
          "description": "In an autonomous driving system, it is essential to recognize vehicles,\npedestrians and cyclists from images. Besides the high accuracy of the\nprediction, the requirement of real-time running brings new challenges for\nconvolutional network models. In this report, we introduce a real-time method\nto detect the 2D objects from images. We aggregate several popular one-stage\nobject detectors and train the models of variety input strategies\nindependently, to yield better performance for accurate multi-scale detection\nof each category, especially for small objects. For model acceleration, we\nleverage TensorRT to optimize the inference time of our detection pipeline. As\nshown in the leaderboard, our proposed detection framework ranks the 2nd place\nwith 75.00% L1 mAP and 69.72% L2 mAP in the real-time 2D detection track of the\nWaymo Open Dataset Challenges, while our framework achieves the latency of\n45.8ms/frame on an Nvidia Tesla V100 GPU.",
          "link": "http://arxiv.org/abs/2106.08713",
          "publishedOn": "2021-06-17T01:58:41.983Z",
          "wordCount": 597,
          "title": "2nd Place Solution for Waymo Open Dataset Challenge - Real-time 2D Object Detection. (arXiv:2106.08713v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quande Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongzheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>",
          "description": "Federated learning (FL) has emerged with increasing popularity to collaborate\ndistributed medical institutions for training deep networks. However, despite\nexisting FL algorithms only allow the supervised training setting, most\nhospitals in realistic usually cannot afford the intricate data labeling due to\nabsence of budget or expertise. This paper studies a practical yet challenging\nFL problem, named \\textit{Federated Semi-supervised Learning} (FSSL), which\naims to learn a federated model by jointly utilizing the data from both labeled\nand unlabeled clients (i.e., hospitals). We present a novel approach for this\nproblem, which improves over traditional consistency regularization mechanism\nwith a new inter-client relation matching scheme. The proposed learning scheme\nexplicitly connects the learning across labeled and unlabeled clients by\naligning their extracted disease relationships, thereby mitigating the\ndeficiency of task knowledge at unlabeled clients and promoting discriminative\ninformation from unlabeled samples. We validate our method on two large-scale\nmedical image classification datasets. The effectiveness of our method has been\ndemonstrated with the clear improvements over state-of-the-arts as well as the\nthorough ablation analysis on both tasks\\footnote{Code will be made available\nat \\url{https://github.com/liuquande/FedIRM}}.",
          "link": "http://arxiv.org/abs/2106.08600",
          "publishedOn": "2021-06-17T01:58:41.968Z",
          "wordCount": 617,
          "title": "Federated Semi-supervised Medical Image Classification via Inter-client Relation Matching. (arXiv:2106.08600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_B/0/1/0/all/0/1\">Boyang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenhui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guanqun Ding</a>",
          "description": "Anomaly detection has attracted considerable search attention. However,\nexisting anomaly detection databases encounter two major problems. Firstly,\nthey are limited in scale. Secondly, training sets contain only video-level\nlabels indicating the existence of an abnormal event during the full video\nwhile lacking annotations of precise time durations. To tackle these problems,\nwe contribute a new Large-scale Anomaly Detection (LAD) database as the\nbenchmark for anomaly detection in video sequences, which is featured in two\naspects. 1) It contains 2000 video sequences including normal and abnormal\nvideo clips with 14 anomaly categories including crash, fire, violence, etc.\nwith large scene varieties, making it the largest anomaly analysis database to\ndate. 2) It provides the annotation data, including video-level labels\n(abnormal/normal video, anomaly type) and frame-level labels (abnormal/normal\nvideo frame) to facilitate anomaly detection. Leveraging the above benefits\nfrom the LAD database, we further formulate anomaly detection as a\nfully-supervised learning problem and propose a multi-task deep neural network\nto solve it. We first obtain the local spatiotemporal contextual feature by\nusing an Inflated 3D convolutional (I3D) network. Then we construct a recurrent\nconvolutional neural network fed the local spatiotemporal contextual feature to\nextract the spatiotemporal contextual feature. With the global spatiotemporal\ncontextual feature, the anomaly type and score can be computed simultaneously\nby a multi-task neural network. Experimental results show that the proposed\nmethod outperforms the state-of-the-art anomaly detection methods on our\ndatabase and other public databases of anomaly detection. Codes are available\nat https://github.com/wanboyang/anomaly_detection_LAD2000.",
          "link": "http://arxiv.org/abs/2106.08570",
          "publishedOn": "2021-06-17T01:58:41.938Z",
          "wordCount": 688,
          "title": "Anomaly Detection in Video Sequences: A Benchmark and Computational Model. (arXiv:2106.08570v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nunn_E/0/1/0/all/0/1\">Eric J. Nunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadivi_P/0/1/0/all/0/1\">Pejman Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>",
          "description": "Generative adversarial networks or GANs are a type of generative modeling\nframework. GANs involve a pair of neural networks engaged in a competition in\niteratively creating fake data, indistinguishable from the real data. One\nnotable application of GANs is developing fake human faces, also known as \"deep\nfakes,\" due to the deep learning algorithms at the core of the GAN framework.\nMeasuring the quality of the generated images is inherently subjective but\nattempts to objectify quality using standardized metrics have been made. One\nexample of objective metrics is the Frechet Inception Distance (FID), which\nmeasures the difference between distributions of feature vectors for two\nseparate datasets of images. There are situations that images with low\nperceptual qualities are not assigned appropriate FID scores. We propose to\nimprove the robustness of the evaluation process by integrating lower-level\nfeatures to cover a wider array of visual defects. Our proposed method\nintegrates three levels of feature abstractions to evaluate the quality of\ngenerated images. Experimental evaluations show better performance of the\nproposed method for distorted images.",
          "link": "http://arxiv.org/abs/2106.08575",
          "publishedOn": "2021-06-17T01:58:41.929Z",
          "wordCount": 612,
          "title": "Compound Frechet Inception Distance for Quality Assessment of GAN Created Images. (arXiv:2106.08575v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hankyu Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1\">Heng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Didari_S/0/1/0/all/0/1\">Sima Didari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jae Oh Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangert_P/0/1/0/all/0/1\">Patrick Bangert</a>",
          "description": "We demonstrate that frequently appearing objects can be discovered by\ntraining randomly sampled patches from a small number of images (100 to 200) by\nself-supervision. Key to this approach is the pattern space, a latent space of\npatterns that represents all possible sub-images of the given image data. The\ndistance structure in the pattern space captures the co-occurrence of patterns\ndue to the frequent objects. The pattern space embedding is learned by\nminimizing the contrastive loss between randomly generated adjacent patches. To\nprevent the embedding from learning the background, we modulate the contrastive\nloss by color-based object saliency and background dissimilarity. The learned\ndistance structure serves as object memory, and the frequent objects are simply\ndiscovered by clustering the pattern vectors from the random patches sampled\nfor inference. Our image representation based on image patches naturally\nhandles the position and scale invariance property that is crucial to\nmulti-object discovery. The method has been proven surprisingly effective, and\nsuccessfully applied to finding multiple human faces and bodies from natural\nimages.",
          "link": "http://arxiv.org/abs/2106.08599",
          "publishedOn": "2021-06-17T01:58:41.922Z",
          "wordCount": 610,
          "title": "PatchNet: Unsupervised Object Discovery based on Patch Embedding. (arXiv:2106.08599v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chaewon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">MyeongAh Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>",
          "description": "Video anomaly detection has gained significant attention due to the\nincreasing requirements of automatic monitoring for surveillance videos.\nEspecially, the prediction based approach is one of the most studied methods to\ndetect anomalies by predicting frames that include abnormal events in the test\nset after learning with the normal frames of the training set. However, a lot\nof prediction networks are computationally expensive owing to the use of\npre-trained optical flow networks, or fail to detect abnormal situations\nbecause of their strong generative ability to predict even the anomalies. To\naddress these shortcomings, we propose spatial rotation transformation (SRT)\nand temporal mixing transformation (TMT) to generate irregular patch cuboids\nwithin normal frame cuboids in order to enhance the learning of normal\nfeatures. Additionally, the proposed patch transformation is used only during\nthe training phase, allowing our model to detect abnormal frames at fast speed\nduring inference. Our model is evaluated on three anomaly detection benchmarks,\nachieving competitive accuracy and surpassing all the previous works in terms\nof speed.",
          "link": "http://arxiv.org/abs/2106.08613",
          "publishedOn": "2021-06-17T01:58:41.911Z",
          "wordCount": 600,
          "title": "FastAno: Fast Anomaly Detection via Spatio-temporal Patch Transformation. (arXiv:2106.08613v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaobing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>",
          "description": "Deep learning-based multi-source unsupervised domain adaptation (MUDA) has\nbeen actively studied in recent years. Compared with single-source unsupervised\ndomain adaptation (SUDA), domain shift in MUDA exists not only between the\nsource and target domains but also among multiple source domains. Most existing\nMUDA algorithms focus on extracting domain-invariant representations among all\ndomains whereas the task-specific decision boundaries among classes are largely\nneglected. In this paper, we propose an end-to-end trainable network that\nexploits domain Consistency Regularization for unsupervised Multi-source domain\nAdaptive classification (CRMA). CRMA aligns not only the distributions of each\npair of source and target domains but also that of all domains. For each pair\nof source and target domains, we employ an intra-domain consistency to\nregularize a pair of domain-specific classifiers to achieve intra-domain\nalignment. In addition, we design an inter-domain consistency that targets\njoint inter-domain alignment among all domains. To address different\nsimilarities between multiple source domains and the target domain, we design\nan authorization strategy that assigns different authorities to domain-specific\nclassifiers adaptively for optimal pseudo label prediction and self-training.\nExtensive experiments show that CRMA tackles unsupervised domain adaptation\neffectively under a multi-source setup and achieves superior adaptation\nconsistently across multiple MUDA datasets.",
          "link": "http://arxiv.org/abs/2106.08590",
          "publishedOn": "2021-06-17T01:58:41.895Z",
          "wordCount": 629,
          "title": "Domain Consistency Regularization for Unsupervised Multi-source Domain Adaptive Classification. (arXiv:2106.08590v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yueyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haofeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaying Liu</a>",
          "description": "Visual analytics have played an increasingly critical role in the Internet of\nThings, where massive visual signals have to be compressed and fed into\nmachines. But facing such big data and constrained bandwidth capacity, existing\nimage/video compression methods lead to very low-quality representations, while\nexisting feature compression techniques fail to support diversified visual\nanalytics applications/tasks with low-bit-rate representations. In this paper,\nwe raise and study the novel problem of supporting multiple machine vision\nanalytics tasks with the compressed visual representation, namely, the\ninformation compression problem in analytics taxonomy. By utilizing the\nintrinsic transferability among different tasks, our framework successfully\nconstructs compact and expressive representations at low bit-rates to support a\ndiversified set of machine vision tasks, including both high-level\nsemantic-related tasks and mid-level geometry analytic tasks. In order to\nimpose compactness in the representations, we propose a codebook-based\nhyperprior, which helps map the representation into a low-dimensional manifold.\nAs it well fits the signal structure of the deep visual feature, it facilitates\nmore accurate entropy estimation, and results in higher compression efficiency.\nWith the proposed framework and the codebook-based hyperprior, we further\ninvestigate the relationship of different task features owning different levels\nof abstraction granularity. Experimental results demonstrate that with the\nproposed scheme, a set of diversified tasks can be supported at a significantly\nlower bit-rate, compared with existing compression schemes.",
          "link": "http://arxiv.org/abs/2106.08512",
          "publishedOn": "2021-06-17T01:58:41.871Z",
          "wordCount": 653,
          "title": "Revisit Visual Representation in Analytics Taxonomy: A Compression Perspective. (arXiv:2106.08512v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_H/0/1/0/all/0/1\">Hidetoshi Matsuo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Nishio_M/0/1/0/all/0/1\">Mizuho Nishio</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Nogami_M/0/1/0/all/0/1\">Munenobu Nogami</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1\">Feibi Zeng</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kurimoto_T/0/1/0/all/0/1\">Takako Kurimoto</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1\">Sandeep Kaushik</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Wiesinger_F/0/1/0/all/0/1\">Florian Wiesinger</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Kono_A/0/1/0/all/0/1\">Atsushi K Kono</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1\">Takamichi Murakami</a> (1) ((1) Department of Radiology, Kobe University Graduate School of Medicine, Kobe, Japan, (2) GE Healthcare, Hino, Japan and (3) GE Healthcare, Munich, Germany)",
          "description": "The integrated positron emission tomography/magnetic resonance imaging\n(PET/MRI) scanner facilitates the simultaneous acquisition of metabolic\ninformation via PET and morphological information with high soft-tissue\ncontrast using MRI. Although PET/MRI facilitates the capture of high-accuracy\nfusion images, its major drawback can be attributed to the difficulty\nencountered when performing attenuation correction, which is necessary for\nquantitative PET evaluation. The combined PET/MRI scanning requires the\ngeneration of attenuation-correction maps from MRI owing to no direct\nrelationship between the gamma-ray attenuation information and MRIs. While\nMRI-based bone-tissue segmentation can be readily performed for the head and\npelvis regions, the realization of accurate bone segmentation via chest CT\ngeneration remains a challenging task. This can be attributed to the\nrespiratory and cardiac motions occurring in the chest as well as its\nanatomically complicated structure and relatively thin bone cortex. This paper\npresents a means to minimise the anatomical structural changes without human\nannotation by adding structural constraints using a modality-independent\nneighbourhood descriptor (MIND) to a generative adversarial network (GAN) that\ncan transform unpaired images. The results obtained in this study revealed the\nproposed U-GAT-IT + MIND approach to outperform all other competing approaches.\nThe findings of this study hint towards possibility of synthesising clinically\nacceptable CT images from chest MRI without human annotation, thereby\nminimising the changes in the anatomical structure.",
          "link": "http://arxiv.org/abs/2106.08557",
          "publishedOn": "2021-06-17T01:58:41.862Z",
          "wordCount": 703,
          "title": "Unsupervised-learning-based method for chest MRI-CT transformation using structure constrained unsupervised generative attention networks. (arXiv:2106.08557v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalayeh_M/0/1/0/all/0/1\">Mahdi M. Kalayeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_N/0/1/0/all/0/1\">Nagendra Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrashekar_A/0/1/0/all/0/1\">Ashok Chandrashekar</a>",
          "description": "The abundance and ease of utilizing sound, along with the fact that auditory\nclues reveal so much about what happens in the scene, make the audio-visual\nspace a perfectly intuitive choice for self-supervised representation learning.\nHowever, the current literature suggests that training on \\textit{uncurated}\ndata yields considerably poorer representations compared to the\n\\textit{curated} alternatives collected in supervised manner, and the gap only\nnarrows when the volume of data significantly increases. Furthermore, the\nquality of learned representations is known to be heavily influenced by the\nsize and taxonomy of the curated datasets used for self-supervised training.\nThis begs the question of whether we are celebrating too early on catching up\nwith supervised learning when our self-supervised efforts still rely almost\nexclusively on curated data. In this paper, we study the efficacy of learning\nfrom Movies and TV Shows as forms of uncurated data for audio-visual\nself-supervised learning. We demonstrate that a simple model based on\ncontrastive learning, trained on a collection of movies and TV shows, not only\ndramatically outperforms more complex methods which are trained on orders of\nmagnitude larger uncurated datasets, but also performs very competitively with\nthe state-of-the-art that learns from large-scale curated data. We identify\nthat audiovisual patterns like the appearance of the main character or\nprominent scenes and mise-en-sc\\`ene which frequently occur through the whole\nduration of a movie, lead to an overabundance of easy negative instances in the\ncontrastive learning formulation. Capitalizing on such observation, we propose\na hierarchical sampling policy, which despite its simplicity, effectively\nimproves the performance, particularly when learning from TV shows which\nnaturally face less semantic diversity.",
          "link": "http://arxiv.org/abs/2106.08513",
          "publishedOn": "2021-06-17T01:58:41.855Z",
          "wordCount": 710,
          "title": "Watching Too Much Television is Good: Self-Supervised Audio-Visual Representation Learning from Movies and TV Shows. (arXiv:2106.08513v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dietrich_M/0/1/0/all/0/1\">Maximilian Dietrich</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Seidlitz_S/0/1/0/all/0/1\">Silvia Seidlitz</a> (2, 3), <a href=\"http://arxiv.org/find/cs/1/au:+Schreck_N/0/1/0/all/0/1\">Nicholas Schreck</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1\">Manuel Wiesenfarth</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1\">Patrick Godau</a> (2, 3), <a href=\"http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu Tizabi</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Sellner_J/0/1/0/all/0/1\">Jan Sellner</a> (2, 3), <a href=\"http://arxiv.org/find/cs/1/au:+Marx_S/0/1/0/all/0/1\">Sebastian Marx</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Knodler_S/0/1/0/all/0/1\">Samuel Kn&#xf6;dler</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Allers_M/0/1/0/all/0/1\">Michael M. Allers</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Ayala_L/0/1/0/all/0/1\">Leonardo Ayala</a> (2, 7), <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_K/0/1/0/all/0/1\">Karsten Schmidt</a> (8), <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_T/0/1/0/all/0/1\">Thorsten Brenner</a> (8), <a href=\"http://arxiv.org/find/cs/1/au:+Studier_Fischer_A/0/1/0/all/0/1\">Alexander Studier-Fischer</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Nickel_F/0/1/0/all/0/1\">Felix Nickel</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1\">Beat P. M&#xfc;ller-Stich</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_Schneider_A/0/1/0/all/0/1\">Annette Kopp-Schneider</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Weigand_M/0/1/0/all/0/1\">Markus A. Weigand</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a> (2, 6, 7) ((1) Department of Anesthesiology, Heidelberg University Hospital, Heidelberg, Germany, (2) Division of Computer Assisted Medical Interventions, German Cancer Research Center (DKFZ), Heidelberg, Germany, (3) HIDSS4Health - Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany (4) Division of Biostatistics, German Cancer Research Center (DKFZ), Heidelberg, Germany, (5) Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany, (6) Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany, (7) Medical Faculty, Heidelberg University, Heidelberg, Germany, (8) Department of Anesthesiology and Intensive Care Medicine, University Hospital Essen, University Duisburg-Essen, Essen, Germany)",
          "description": "Sepsis is a leading cause of mortality and critical illness worldwide. While\nrobust biomarkers for early diagnosis are still missing, recent work indicates\nthat hyperspectral imaging (HSI) has the potential to overcome this bottleneck\nby monitoring microcirculatory alterations. Automated machine learning-based\ndiagnosis of sepsis based on HSI data, however, has not been explored to date.\nGiven this gap in the literature, we leveraged an existing data set to (1)\ninvestigate whether HSI-based automated diagnosis of sepsis is possible and (2)\nput forth a list of possible confounders relevant for HSI-based tissue\nclassification. While we were able to classify sepsis with an accuracy of over\n$98\\,\\%$ using the existing data, our research also revealed several subject-,\ntherapy- and imaging-related confounders that may lead to an overestimation of\nalgorithm performance when not balanced across the patient groups. We conclude\nthat further prospective studies, carefully designed with respect to these\nconfounders, are necessary to confirm the preliminary results obtained in this\nstudy.",
          "link": "http://arxiv.org/abs/2106.08445",
          "publishedOn": "2021-06-17T01:58:41.843Z",
          "wordCount": 770,
          "title": "Machine learning-based analysis of hyperspectral images for automated sepsis diagnosis. (arXiv:2106.08445v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chuah_W/0/1/0/all/0/1\">WeiQin Chuah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1\">Ruwan Tennakoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1\">Alireza Bab-Hadiashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1\">David Suter</a>",
          "description": "Learning-based stereo matching and depth estimation networks currently excel\non public benchmarks with impressive results. However, state-of-the-art\nnetworks often fail to generalize from synthetic imagery to more challenging\nreal data domains. This paper is an attempt to uncover hidden secrets of\nachieving domain robustness and in particular, discovering the important\ningredients of generalization success of stereo matching networks by analyzing\nthe effect of synthetic image learning on real data performance. We provide\nevidence that demonstrates that learning of features in the synthetic domain by\na stereo matching network is heavily influenced by two \"shortcuts\" presented in\nthe synthetic data: (1) identical local statistics (RGB colour features)\nbetween matching pixels in the synthetic stereo images and (2) lack of realism\nin synthetic textures on 3D objects simulated in game engines. We will show\nthat by removing such shortcuts, we can achieve domain robustness in the\nstate-of-the-art stereo matching frameworks and produce a remarkable\nperformance on multiple realistic datasets, despite the fact that the networks\nwere trained on synthetic data, only. Our experimental results point to the\nfact that eliminating shortcuts from the synthetic data is key to achieve\ndomain-invariant generalization between synthetic and real data domains.",
          "link": "http://arxiv.org/abs/2106.08486",
          "publishedOn": "2021-06-17T01:58:41.823Z",
          "wordCount": 640,
          "title": "Achieving Domain Robustness in Stereo Matching Networks by Removing Shortcut Learning. (arXiv:2106.08486v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ying-Tian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuan-Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi-Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song-Hai Zhang</a>",
          "description": "In this paper, we present a novel implicit glyph shape representation, which\nmodels glyphs as shape primitives enclosed by quadratic curves, and naturally\nenables generating glyph images at arbitrary high resolutions. Experiments on\nfont reconstruction and interpolation tasks verified that this structured\nimplicit representation is suitable for describing both structure and style\nfeatures of glyphs. Furthermore, based on the proposed representation, we\ndesign a simple yet effective disentangled network for the challenging one-shot\nfont style transfer problem, and achieve the best results comparing to\nstate-of-the-art alternatives in both quantitative and qualitative comparisons.\nBenefit from this representation, our generated glyphs have the potential to be\nconverted to vector fonts through post-processing, reducing the gap between\nrasterized images and vector graphics. We hope this work can provide a powerful\ntool for 2D shape analysis and synthesis, and inspire further exploitation in\nimplicit representations for 2D shape modeling.",
          "link": "http://arxiv.org/abs/2106.08573",
          "publishedOn": "2021-06-17T01:58:41.817Z",
          "wordCount": 570,
          "title": "Learning Implicit Glyph Shape Representation. (arXiv:2106.08573v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_V/0/1/0/all/0/1\">VanThong Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Guee-Sang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hyung-Jeong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo-Huyng Kim</a>",
          "description": "This paper presents an approach for Evoked Expressions from Videos (EEV)\nchallenge, which aims to predict evoked facial expressions from video. We take\nadvantage of pre-trained models on large-scale datasets in computer vision and\naudio signals to extract the deep representation of timestamps in the video. A\ntemporal convolution network, rather than an RNN like architecture, is used to\nexplore temporal relationships due to its advantage in memory consumption and\nparallelism. Furthermore, to address the missing annotations of some\ntimestamps, positional encoding is employed to ensure continuity of input data\nwhen discarding these timestamps during training. We achieved state-of-the-art\nresults on the EEV challenge with a Pearson correlation coefficient of 0.05477,\nthe first ranked performance in the EEV 2021 challenge.",
          "link": "http://arxiv.org/abs/2106.08596",
          "publishedOn": "2021-06-17T01:58:41.801Z",
          "wordCount": 578,
          "title": "Temporal Convolution Networks with Positional Encoding for Evoked Expression Estimation. (arXiv:2106.08596v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finlay_C/0/1/0/all/0/1\">Chris Finlay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1\">Adam Oberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "Recent work has shown that Neural Ordinary Differential Equations (ODEs) can\nserve as generative models of images using the perspective of Continuous\nNormalizing Flows (CNFs). Such models offer exact likelihood calculation, and\ninvertible generation/density estimation. In this work we introduce a\nMulti-Resolution variant of such models (MRCNF), by characterizing the\nconditional distribution over the additional information required to generate a\nfine image that is consistent with the coarse image. We introduce a\ntransformation between resolutions that allows for no change in the log\nlikelihood. We show that this approach yields comparable likelihood values for\nvarious image datasets, with improved performance at higher resolutions, with\nfewer parameters, using only 1 GPU.",
          "link": "http://arxiv.org/abs/2106.08462",
          "publishedOn": "2021-06-17T01:58:41.795Z",
          "wordCount": 552,
          "title": "Multi-Resolution Continuous Normalizing Flows. (arXiv:2106.08462v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sangmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1\">Junhyug Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kangil Kim</a>",
          "description": "In this work, we seek new insights into the underlying challenges of the\nScene Graph Generation (SGG) task. Quantitative and qualitative analysis of the\nVisual Genome dataset implies -- 1) Ambiguity: even if inter-object\nrelationship contains the same object (or predicate), they may not be visually\nor semantically similar, 2) Asymmetry: despite the nature of the relationship\nthat embodied the direction, it was not well addressed in previous studies, and\n3) Higher-order contexts: leveraging the identities of certain graph elements\ncan help to generate accurate scene graphs. Motivated by the analysis, we\ndesign a novel SGG framework, Local-to-Global Interaction Networks (LOGIN).\nLocally, interactions extract the essence between three instances - subject,\nobject, and background - while baking direction awareness into the network by\nconstraining the input order. Globally, interactions encode the contexts\nbetween every graph components -- nodes and edges. Also we introduce Attract &\nRepel loss which finely adjusts predicate embeddings. Our framework enables\npredicting the scene graph in a local-to-global manner by design, leveraging\nthe possible complementariness. To quantify how much LOGIN is aware of\nrelational direction, we propose a new diagnostic task called Bidirectional\nRelationship Classification (BRC). We see that LOGIN can successfully\ndistinguish relational direction than existing methods (in BRC task) while\nshowing state-of-the-art results on the Visual Genome benchmark (in SGG task).",
          "link": "http://arxiv.org/abs/2106.08543",
          "publishedOn": "2021-06-17T01:58:41.788Z",
          "wordCount": 675,
          "title": "Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions. (arXiv:2106.08543v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Junior_C/0/1/0/all/0/1\">Celso A. M. Lopes Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_R/0/1/0/all/0/1\">Ricardo B. das Neves Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bezerra_B/0/1/0/all/0/1\">Byron L. D. Bezerra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toselli_A/0/1/0/all/0/1\">Alejandro H. Toselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Impedovo_D/0/1/0/all/0/1\">Donato Impedovo</a>",
          "description": "This paper describes the short-term competition on Components Segmentation\nTask of Document Photos that was prepared in the context of the 16th\nInternational Conference on Document Analysis and Recognition (ICDAR 2021).\nThis competition aims to bring together researchers working on the filed of\nidentification document image processing and provides them a suitable benchmark\nto compare their techniques on the component segmentation task of document\nimages. Three challenge tasks were proposed entailing different segmentation\nassignments to be performed on a provided dataset. The collected data are from\nseveral types of Brazilian ID documents, whose personal information was\nconveniently replaced. There were 16 participants whose results obtained for\nsome or all the three tasks show different rates for the adopted metrics, like\nDice Similarity Coefficient ranging from 0.06 to 0.99. Different Deep Learning\nmodels were applied by the entrants with diverse strategies to achieve the best\nresults in each of the tasks. Obtained results show that the current applied\nmethods for solving one of the proposed tasks (document boundary detection) are\nalready well stablished. However, for the other two challenge tasks (text zone\nand handwritten sign detection) research and development of more robust\napproaches are still required to achieve acceptable results.",
          "link": "http://arxiv.org/abs/2106.08499",
          "publishedOn": "2021-06-17T01:58:41.771Z",
          "wordCount": 659,
          "title": "ICDAR 2021 Competition on Components Segmentation Task of Document Photos. (arXiv:2106.08499v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoshan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhe Ma</a>",
          "description": "Recently, the transductive graph-based methods have achieved great success in\nthe few-shot classification task. However, most existing methods ignore\nexploring the class-level knowledge that can be easily learned by humans from\njust a handful of samples. In this paper, we propose an Explicit Class\nKnowledge Propagation Network (ECKPN), which is composed of the comparison,\nsqueeze and calibration modules, to address this problem. Specifically, we\nfirst employ the comparison module to explore the pairwise sample relations to\nlearn rich sample representations in the instance-level graph. Then, we squeeze\nthe instance-level graph to generate the class-level graph, which can help\nobtain the class-level visual knowledge and facilitate modeling the relations\nof different classes. Next, the calibration module is adopted to characterize\nthe relations of the classes explicitly to obtain the more discriminative\nclass-level knowledge representations. Finally, we combine the class-level\nknowledge with the instance-level sample representations to guide the inference\nof the query samples. We conduct extensive experiments on four few-shot\nclassification benchmarks, and the experimental results show that the proposed\nECKPN significantly outperforms the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.08523",
          "publishedOn": "2021-06-17T01:58:41.765Z",
          "wordCount": 619,
          "title": "ECKPN: Explicit Class Knowledge Propagation Network for Transductive Few-shot Learning. (arXiv:2106.08523v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1\">Jiquan Ngiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Benjamin Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Vijay Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hao-Tien Lewis Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1\">Jeffrey Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1\">Alex Bewley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1\">Ashish Venugopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1\">David Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1\">Ben Sapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jonathon Shlens</a>",
          "description": "Predicting the future motion of multiple agents is necessary for planning in\ndynamic environments. This task is challenging for autonomous driving since\nagents (e.g., vehicles and pedestrians) and their associated behaviors may be\ndiverse and influence each other. Most prior work has focused on first\npredicting independent futures for each agent based on all past motion, and\nthen planning against these independent predictions. However, planning against\nfixed predictions can suffer from the inability to represent the future\ninteraction possibilities between different agents, leading to sub-optimal\nplanning. In this work, we formulate a model for predicting the behavior of all\nagents jointly in real-world driving environments in a unified manner. Inspired\nby recent language modeling approaches, we use a masking strategy as the query\nto our model, enabling one to invoke a single model to predict agent behavior\nin many ways, such as potentially conditioned on the goal or full future\ntrajectory of the autonomous vehicle or the behavior of other agents in the\nenvironment. Our model architecture fuses heterogeneous world state in a\nunified Transformer architecture by employing attention across road elements,\nagent interactions and time steps. We evaluate our approach on autonomous\ndriving datasets for behavior prediction, and achieve state-of-the-art\nperformance. Our work demonstrates that formulating the problem of behavior\nprediction in a unified architecture with a masking strategy may allow us to\nhave a single model that can perform multiple motion prediction and planning\nrelated tasks effectively.",
          "link": "http://arxiv.org/abs/2106.08417",
          "publishedOn": "2021-06-17T01:58:41.759Z",
          "wordCount": 703,
          "title": "Scene Transformer: A unified multi-task model for behavior prediction and planning. (arXiv:2106.08417v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingmin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsen_P/0/1/0/all/0/1\">Peder A. Olsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Ranveer Chandra</a>",
          "description": "This paper presents a neural-network-based solution to recover pixels\noccluded by clouds in satellite images. We leverage radio frequency (RF)\nsignals in the ultra/super-high frequency band that penetrate clouds to help\nreconstruct the occluded regions in multispectral images. We introduce the\nfirst multi-modal multi-temporal cloud removal model. Our model uses publicly\navailable satellite observations and produces daily cloud-free images.\nExperimental results show that our system significantly outperforms baselines\nby 8dB in PSNR. We also demonstrate use cases of our system in digital\nagriculture, flood monitoring, and wildfire detection. We will release the\nprocessed dataset to facilitate future research.",
          "link": "http://arxiv.org/abs/2106.08408",
          "publishedOn": "2021-06-17T01:58:41.752Z",
          "wordCount": 522,
          "title": "Seeing Through Clouds in Satellite Images. (arXiv:2106.08408v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junshen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Eric Z. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanhui Sun</a>",
          "description": "Image registration plays an important role in medical image analysis.\nConventional optimization based methods provide an accurate estimation due to\nthe iterative process at the cost of expensive computation. Deep learning\nmethods such as learn-to-map are much faster but either iterative or\ncoarse-to-fine approach is required to improve accuracy for handling large\nmotions. In this work, we proposed to learn a registration optimizer via a\nmulti-scale neural ODE model. The inference consists of iterative gradient\nupdates similar to a conventional gradient descent optimizer but in a much\nfaster way, because the neural ODE learns from the training data to adapt the\ngradient efficiently at each iteration. Furthermore, we proposed to learn a\nmodal-independent similarity metric to address image appearance variations\nacross different image contrasts. We performed evaluations through extensive\nexperiments in the context of multi-contrast 3D MR images from both public and\nprivate data sources and demonstrate the superior performance of our proposed\nmethods.",
          "link": "http://arxiv.org/abs/2106.08493",
          "publishedOn": "2021-06-17T01:58:41.726Z",
          "wordCount": 587,
          "title": "Multi-scale Neural ODEs for 3D Medical Image Registration. (arXiv:2106.08493v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruinian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1\">Fu-Jen Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vela_P/0/1/0/all/0/1\">Patricio A. Vela</a>",
          "description": "Contemporary grasp detection approaches employ deep learning to achieve\nrobustness to sensor and object model uncertainty. The two dominant approaches\ndesign either grasp-quality scoring or anchor-based grasp recognition networks.\nThis paper presents a different approach to grasp detection by treating it as\nkeypoint detection. The deep network detects each grasp candidate as a pair of\nkeypoints, convertible to the grasp representation g = {x, y, w, {\\theta}}^T,\nrather than a triplet or quartet of corner points. Decreasing the detection\ndifficulty by grouping keypoints into pairs boosts performance. To further\npromote dependencies between keypoints, the general non-local module is\nincorporated into the proposed learning framework. A final filtering strategy\nbased on discrete and continuous orientation prediction removes false\ncorrespondences and further improves grasp detection performance. GKNet, the\napproach presented here, achieves the best balance of accuracy and speed on the\nCornell and the abridged Jacquard dataset (96.9% and 98.39% at 41.67 and 23.26\nfps). Follow-up experiments on a manipulator evaluate GKNet using 4 types of\ngrasping experiments reflecting different nuisance sources: static grasping,\ndynamic grasping, grasping at varied camera angles, and bin picking. GKNet\noutperforms reference baselines in static and dynamic grasping experiments\nwhile showing robustness to varied camera viewpoints and bin picking\nexperiments. The results confirm the hypothesis that grasp keypoints are an\neffective output representation for deep grasp networks that provide robustness\nto expected nuisance factors.",
          "link": "http://arxiv.org/abs/2106.08497",
          "publishedOn": "2021-06-17T01:58:41.720Z",
          "wordCount": 662,
          "title": "GKNet: grasp keypoint network for grasp candidates detection. (arXiv:2106.08497v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_A/0/1/0/all/0/1\">Anthony Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1\">Max Paul Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resch_M/0/1/0/all/0/1\">Michael Resch</a>",
          "description": "With the increasing safety validation requirements for the release of a\nself-driving car, alternative approaches, such as simulation-based testing, are\nemerging in addition to conventional real-world testing. In order to rely on\nvirtual tests the employed sensor models have to be validated. For this reason,\nit is necessary to quantify the discrepancy between simulation and reality in\norder to determine whether a certain fidelity is sufficient for a desired\nintended use. There exists no sound method to measure this\nsimulation-to-reality gap of radar perception for autonomous driving. We\naddress this problem by introducing a multi-layered evaluation approach, which\nconsists of a combination of an explicit and an implicit sensor model\nevaluation. The former directly evaluates the realism of the synthetically\ngenerated sensor data, while the latter refers to an evaluation of a downstream\ntarget application. In order to demonstrate the method, we evaluated the\nfidelity of three typical radar model types (ideal, data-driven, ray\ntracing-based) and their applicability for virtually testing radar-based\nmulti-object tracking. We have shown the effectiveness of the proposed approach\nin terms of providing an in-depth sensor model assessment that renders existing\ndisparities visible and enables a realistic estimation of the overall model\nfidelity across different scenarios.",
          "link": "http://arxiv.org/abs/2106.08372",
          "publishedOn": "2021-06-17T01:58:41.714Z",
          "wordCount": 663,
          "title": "A Multi-Layered Approach for Measuring the Simulation-to-Reality Gap of Radar Perception for Autonomous Driving. (arXiv:2106.08372v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tamboli_D/0/1/0/all/0/1\">Dipesh Tamboli</a>",
          "description": "This document summarizes different visual explanations methods such as CAM,\nGrad-CAM, Localization using Multiple Instance Learning - Saliency-based\nmethods, Saliency-driven Class-Impressions, Muting pixels in input image -\nAdversarial methods and Activation visualization, Convolution filter\nvisualization - Feature-based methods. We have also shown the results produced\nby different methods and a comparison between CAM, GradCAM, and Guided\nBackpropagation.",
          "link": "http://arxiv.org/abs/2106.08366",
          "publishedOn": "2021-06-17T01:58:41.677Z",
          "wordCount": 482,
          "title": "Explaining decision of model from its prediction. (arXiv:2106.08366v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "Attention mechanism of late has been quite popular in the computer vision\ncommunity. A lot of work has been done to improve the performance of the\nnetwork, although almost always it results in increased computational\ncomplexity. In this paper, we propose a new attention module that not only\nachieves the best performance but also has lesser parameters compared to most\nexisting models. Our attention module can easily be integrated with other\nconvolutional neural networks because of its lightweight nature. The proposed\nnetwork named Dual Multi Scale Attention Network (DMSANet) is comprised of two\nparts: the first part is used to extract features at various scales and\naggregate them, the second part uses spatial and channel attention modules in\nparallel to adaptively integrate local features with their global dependencies.\nWe benchmark our network performance for Image Classification on ImageNet\ndataset, Object Detection and Instance Segmentation both on MS COCO dataset.",
          "link": "http://arxiv.org/abs/2106.08382",
          "publishedOn": "2021-06-17T01:58:41.661Z",
          "wordCount": 583,
          "title": "DMSANet: Dual Multi Scale Attention Network. (arXiv:2106.08382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">C.-H. Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1\">Mohit Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Y.-C. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Quan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1\">Tomoaki Yoshinaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1\">Tomokazu Murakami</a>",
          "description": "Physical processes, camera movement, and unpredictable environmental\nconditions like the presence of dust can induce noise and artifacts in video\nfeeds. We observe that popular unsupervised MOT methods are dependent on\nnoise-free inputs. We show that the addition of a small amount of artificial\nrandom noise causes a sharp degradation in model performance on benchmark\nmetrics. We resolve this problem by introducing a robust unsupervised\nmulti-object tracking (MOT) model: AttU-Net. The proposed single-head attention\nmodel helps limit the negative impact of noise by learning visual\nrepresentations at different segment scales. AttU-Net shows better unsupervised\nMOT tracking performance over variational inference-based state-of-the-art\nbaselines. We evaluate our method in the MNIST-MOT and the Atari game video\nbenchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''\nwhich consists of moving Japanese characters and ``Fashion-MNIST MOT'' to\nvalidate the effectiveness of the MOT models.",
          "link": "http://arxiv.org/abs/2105.10005",
          "publishedOn": "2021-06-16T01:21:07.143Z",
          "wordCount": 644,
          "title": "Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaskirat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Generation of stroke-based non-photorealistic imagery, is an important\nproblem in the computer vision community. As an endeavor in this direction,\nsubstantial recent research efforts have been focused on teaching machines \"how\nto paint\", in a manner similar to a human painter. However, the applicability\nof previous methods has been limited to datasets with little variation in\nposition, scale and saliency of the foreground object. As a consequence, we\nfind that these methods struggle to cover the granularity and diversity\npossessed by real world images. To this end, we propose a Semantic Guidance\npipeline with 1) a bi-level painting procedure for learning the distinction\nbetween foreground and background brush strokes at training time. 2) We also\nintroduce invariance to the position and scale of the foreground object through\na neural alignment model, which combines object localization and spatial\ntransformer networks in an end to end manner, to zoom into a particular\nsemantic instance. 3) The distinguishing features of the in-focus object are\nthen amplified by maximizing a novel guided backpropagation based focus reward.\nThe proposed agent does not require any supervision on human stroke-data and\nsuccessfully handles variations in foreground object attributes, thus,\nproducing much higher quality canvases for the CUB-200 Birds and Stanford\nCars-196 datasets. Finally, we demonstrate the further efficacy of our method\non complex datasets with multiple foreground object instances by evaluating an\nextension of our method on the challenging Virtual-KITTI dataset. Source code\nand models are available at https://github.com/1jsingh/semantic-guidance.",
          "link": "http://arxiv.org/abs/2011.12589",
          "publishedOn": "2021-06-16T01:21:06.998Z",
          "wordCount": 728,
          "title": "Combining Semantic Guidance and Deep Reinforcement Learning For Generating Human Level Paintings. (arXiv:2011.12589v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brima_Y/0/1/0/all/0/1\">Yusuf Brima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tushar_M/0/1/0/all/0/1\">Mossadek Hossain Kamal Tushar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_U/0/1/0/all/0/1\">Upama Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tariqul Islam</a>",
          "description": "Magnetic Resonance Imaging (MRI) is a principal diagnostic approach used in\nthe field of radiology to create images of the anatomical and physiological\nstructure of patients. MRI is the prevalent medical imaging practice to find\nabnormalities in soft tissues. Traditionally they are analyzed by a radiologist\nto detect abnormalities in soft tissues, especially the brain. The process of\ninterpreting a massive volume of patient's MRI is laborious. Hence, the use of\nMachine Learning methodologies can aid in detecting abnormalities in soft\ntissues with considerable accuracy. In this research, we have curated a novel\ndataset and developed a framework that uses Deep Transfer Learning to perform a\nmulti-classification of tumors in the brain MRI images. In this paper, we\nadopted the Deep Residual Convolutional Neural Network (ResNet50) architecture\nfor the experiments along with discriminative learning techniques to train the\nmodel. Using the novel dataset and two publicly available MRI brain datasets,\nthis proposed approach attained a classification accuracy of 86.40% on the\ncurated dataset, 93.80% on the Harvard Whole Brain Atlas dataset, and 97.05%\naccuracy on the School of Biomedical Engineering dataset. Results of our\nexperiments significantly demonstrate our proposed framework for transfer\nlearning is a potential and effective method for brain tumor\nmulti-classification tasks.",
          "link": "http://arxiv.org/abs/2106.07333",
          "publishedOn": "2021-06-16T01:21:06.943Z",
          "wordCount": 701,
          "title": "Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class Classification. (arXiv:2106.07333v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+True_J/0/1/0/all/0/1\">Julian True</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naimul Khan</a>",
          "description": "Despite the continued successes of computationally efficient deep neural\nnetwork architectures for video object detection, performance continually\narrives at the great trilemma of speed versus accuracy versus computational\nresources (pick two). Current attempts to exploit temporal information in video\ndata to overcome this trilemma are bottlenecked by the state-of-the-art in\nobject detection models. We present, a technique which performs video object\ndetection through the use of off-the-shelf object detectors alongside existing\noptical flow based motion estimation techniques in parallel. Through a set of\nexperiments on the benchmark MOT20 dataset, we demonstrate that our approach\nsignificantly reduces the baseline latency of any given object detector without\nsacrificing any accuracy. Further latency reduction, up to 25x lower than the\noriginal latency, can be achieved with minimal accuracy loss. MOVEX enables low\nlatency video object detection on common CPU based systems, thus allowing for\nhigh performance video object detection beyond the domain of GPU computing. The\ncode is available at https://github.com/juliantrue/movex.",
          "link": "http://arxiv.org/abs/2104.08918",
          "publishedOn": "2021-06-16T01:21:06.774Z",
          "wordCount": 607,
          "title": "Motion Vector Extrapolation for Video Object Detection. (arXiv:2104.08918v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_V/0/1/0/all/0/1\">Vinu Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1\">Shoaib Ahmed Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskara_A/0/1/0/all/0/1\">Aditya Bhaskara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_G/0/1/0/all/0/1\">Ganesh Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_S/0/1/0/all/0/1\">Saurav Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garland_M/0/1/0/all/0/1\">Michael Garland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sheraz Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>",
          "description": "With the rise in edge-computing devices, there has been an increasing demand\nto deploy energy and resource-efficient models. A large body of research has\nbeen devoted to developing methods that can reduce the size of the model\nconsiderably without affecting the standard metrics such as top-1 accuracy.\nHowever, these pruning approaches tend to result in a significant mismatch in\nother metrics such as fairness across classes and explainability. To combat\nsuch misalignment, we propose a novel multi-part loss function inspired by the\nknowledge-distillation literature. Through extensive experiments, we\ndemonstrate the effectiveness of our approach across different compression\nalgorithms, architectures, tasks as well as datasets. In particular, we obtain\nup to $4.1\\times$ reduction in the number of prediction mismatches between the\ncompressed and reference models, and up to $5.7\\times$ in cases where the\nreference model makes the correct prediction; all while making no changes to\nthe compression algorithm, and minor modifications to the loss function.\nFurthermore, we demonstrate how inducing simple alignment between the\npredictions of the models naturally improves the alignment on other metrics\nincluding fairness and attributions. Our framework can thus serve as a simple\nplug-and-play component for compression algorithms in the future.",
          "link": "http://arxiv.org/abs/2012.01604",
          "publishedOn": "2021-06-16T01:21:06.764Z",
          "wordCount": 673,
          "title": "Going Beyond Classification Accuracy Metrics in Model Compression. (arXiv:2012.01604v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Navarro_J/0/1/0/all/0/1\">Julia Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabater_N/0/1/0/all/0/1\">Neus Sabater</a>",
          "description": "Recently, learning methods have been designed to create Multiplane Images\n(MPIs) for view synthesis. While MPIs are extremely powerful and facilitate\nhigh quality renderings, a great amount of memory is required, making them\nimpractical for many applications. In this paper, we propose a learning method\nthat optimizes the available memory to render compact and adaptive MPIs. Our\nMPIs avoid redundant information and take into account the scene geometry to\ndetermine the depth sampling.",
          "link": "http://arxiv.org/abs/2102.10086",
          "publishedOn": "2021-06-16T01:21:06.729Z",
          "wordCount": 529,
          "title": "Compact and adaptive multiplane images for view synthesis. (arXiv:2102.10086v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1\">Abhishek Bhandwaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shari Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1\">Dan Gutfreund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1\">Elizabeth Spelke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1\">Tomer D. Ullman</a>",
          "description": "For machine agents to successfully interact with humans in real-world\nsettings, they will need to develop an understanding of human mental life.\nIntuitive psychology, the ability to reason about hidden mental variables that\ndrive observable actions, comes naturally to people: even pre-verbal infants\ncan tell agents from objects, expecting agents to act efficiently to achieve\ngoals given constraints. Despite recent interest in machine agents that reason\nabout other agents, it is not clear if such agents learn or hold the core\npsychology principles that drive human reasoning. Inspired by cognitive\ndevelopment studies on intuitive psychology, we present a benchmark consisting\nof a large dataset of procedurally generated 3D animations, AGENT (Action,\nGoal, Efficiency, coNstraint, uTility), structured around four scenarios (goal\npreferences, action efficiency, unobserved constraints, and cost-reward\ntrade-offs) that probe key concepts of core intuitive psychology. We validate\nAGENT with human-ratings, propose an evaluation protocol emphasizing\ngeneralization, and compare two strong baselines built on Bayesian inverse\nplanning and a Theory of Mind neural network. Our results suggest that to pass\nthe designed tests of core intuitive psychology at human levels, a model must\nacquire or have built-in representations of how agents plan, combining utility\ncomputations and core knowledge of objects and physics.",
          "link": "http://arxiv.org/abs/2102.12321",
          "publishedOn": "2021-06-16T01:21:06.722Z",
          "wordCount": 697,
          "title": "AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wensheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyazono_T/0/1/0/all/0/1\">Taiga Miyazono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1\">Brian Kenji Iwana</a>",
          "description": "Book covers are intentionally designed and provide an introduction to a book.\nHowever, they typically require professional skills to design and produce the\ncover images. Thus, we propose a generative neural network that can produce\nbook covers based on an easy-to-use layout graph. The layout graph contains\nobjects such as text, natural scene objects, and solid color spaces. This\nlayout graph is embedded using a graph convolutional neural network and then\nused with a mask proposal generator and a bounding-box generator and filled\nusing an object proposal generator. Next, the objects are compiled into a\nsingle image and the entire network is trained using a combination of\nadversarial training, perceptual training, and reconstruction. Finally, a Style\nRetention Network (SRNet) is used to transfer the learned font style onto the\ndesired text. Using the proposed method allows for easily controlled and unique\nbook covers.",
          "link": "http://arxiv.org/abs/2105.11088",
          "publishedOn": "2021-06-16T01:21:06.692Z",
          "wordCount": 608,
          "title": "Towards Book Cover Design via Layout Graphs. (arXiv:2105.11088v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Abhishek Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ayush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1\">Vivek Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garza_E/0/1/0/all/0/1\">Ethan Garza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Emily Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vepakomma_P/0/1/0/all/0/1\">Praneeth Vepakomma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>",
          "description": "Recent deep learning models have shown remarkable performance in image\nclassification. While these deep learning systems are getting closer to\npractical deployment, the common assumption made about data is that it does not\ncarry any sensitive information. This assumption may not hold for many\npractical cases, especially in the domain where an individual's personal\ninformation is involved, like healthcare and facial recognition systems. We\nposit that selectively removing features in this latent space can protect the\nsensitive information and provide a better privacy-utility trade-off.\nConsequently, we propose DISCO which learns a dynamic and data driven pruning\nfilter to selectively obfuscate sensitive information in the feature space. We\npropose diverse attack schemes for sensitive inputs \\& attributes and\ndemonstrate the effectiveness of DISCO against state-of-the-art methods through\nquantitative and qualitative evaluation. Finally, we also release an evaluation\nbenchmark dataset of 1 million sensitive representations to encourage rigorous\nexploration of novel attack schemes.",
          "link": "http://arxiv.org/abs/2012.11025",
          "publishedOn": "2021-06-16T01:21:06.676Z",
          "wordCount": 632,
          "title": "DISCO: Dynamic and Invariant Sensitive Channel Obfuscation for deep neural networks. (arXiv:2012.11025v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1\">Ivan Grubi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1\">Marin Or&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>",
          "description": "Semi-supervised learning is especially interesting in the dense prediction\ncontext due to high cost of pixel-level ground truth. Unfortunately, most such\napproaches are evaluated on outdated architectures which hamper research due to\nvery slow training and high requirements on GPU RAM. We address this concern by\npresenting a simple and effective baseline which works very well both on\nstandard and efficient architectures. Our baseline is based on one-way\nconsistency and non-linear geometric and photometric perturbations. We show\nadvantage of perturbing only the student branch and present a plausible\nexplanation of such behaviour. Experiments on Cityscapes and CIFAR-10\ndemonstrate competitive performance with respect to prior work.",
          "link": "http://arxiv.org/abs/2106.07075",
          "publishedOn": "2021-06-16T01:21:06.669Z",
          "wordCount": 558,
          "title": "A baseline for semi-supervised learning of efficient semantic segmentation models. (arXiv:2106.07075v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "Most neural network pruning methods, such as filter-level and layer-level\nprunings, prune the network model along one dimension (depth, width, or\nresolution) solely to meet a computational budget. However, such a pruning\npolicy often leads to excessive reduction of that dimension, thus inducing a\nhuge accuracy loss. To alleviate this issue, we argue that pruning should be\nconducted along three dimensions comprehensively. For this purpose, our pruning\nframework formulates pruning as an optimization problem. Specifically, it first\ncasts the relationships between a certain model's accuracy and\ndepth/width/resolution into a polynomial regression and then maximizes the\npolynomial to acquire the optimal values for the three dimensions. Finally, the\nmodel is pruned along the three optimal dimensions accordingly. In this\nframework, since collecting too much data for training the regression is very\ntime-costly, we propose two approaches to lower the cost: 1) specializing the\npolynomial to ensure an accurate regression even with less training data; 2)\nemploying iterative pruning and fine-tuning to collect the data faster.\nExtensive experiments show that our proposed algorithm surpasses\nstate-of-the-art pruning algorithms and even neural architecture search-based\nalgorithms.",
          "link": "http://arxiv.org/abs/2010.04879",
          "publishedOn": "2021-06-16T01:21:06.659Z",
          "wordCount": 671,
          "title": "Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework. (arXiv:2010.04879v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1\">Suvrit Sra</a>",
          "description": "The label shift problem refers to the supervised learning setting where the\ntrain and test label distributions do not match. Existing work addressing label\nshift usually assumes access to an \\emph{unlabelled} test sample. This sample\nmay be used to estimate the test label distribution, and to then train a\nsuitably re-weighted classifier. While approaches using this idea have proven\neffective, their scope is limited as it is not always feasible to access the\ntarget domain; further, they require repeated retraining if the model is to be\ndeployed in \\emph{multiple} test environments. Can one instead learn a\n\\emph{single} classifier that is robust to arbitrary label shifts from a broad\nfamily? In this paper, we answer this question by proposing a model that\nminimises an objective based on distributionally robust optimisation (DRO). We\nthen design and analyse a gradient descent-proximal mirror ascent algorithm\ntailored for large-scale problems to optimise the proposed objective. %, and\nestablish its convergence. Finally, through experiments on CIFAR-100 and\nImageNet, we show that our technique can significantly improve performance over\na number of baselines in settings where label shift is present.",
          "link": "http://arxiv.org/abs/2010.12230",
          "publishedOn": "2021-06-16T01:21:06.650Z",
          "wordCount": 656,
          "title": "Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bohong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengdi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Though network sparsity emerges as a promising direction to overcome the\ndrastically increasing size of neural networks, it remains an open problem to\nconcurrently maintain model accuracy as well as achieve significant speedups on\ngeneral CPUs. In this paper, we propose one novel concept of $1\\times N$ block\nsparsity pattern (block pruning) to break this limitation. In particular,\nconsecutive $N$ output kernels with the same input channel index are grouped\ninto one block, which serves as a basic pruning granularity of our pruning\npattern. Our $1 \\times N$ sparsity pattern prunes these blocks considered\nunimportant. We also provide a workflow of filter rearrangement that first\nrearranges the weight matrix in the output channel dimension to derive more\ninfluential blocks for accuracy improvements, and then applies similar\nrearrangement to the next-layer weights in the input channel dimension to\nensure correct convolutional operations. Moreover, the output computation after\nour $1 \\times N$ block sparsity can be realized via a parallelized block-wise\nvectorized operation, leading to significant speedups on general CPUs-based\nplatforms. The efficacy of our pruning pattern is proved with experiments on\nILSVRC-2012. For example, in the case of 50% sparsity and $N=4$, our pattern\nobtains about 3.0% improvements over filter pruning in the top-1 accuracy of\nMobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU\nover weight pruning. Code is available at https://github.com/lmbxmu/1xN.",
          "link": "http://arxiv.org/abs/2105.14713",
          "publishedOn": "2021-06-16T01:21:06.630Z",
          "wordCount": 691,
          "title": "1$\\times$N Block Pattern for Network Sparsity. (arXiv:2105.14713v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>",
          "description": "In this paper, we introduce a new perspective on training deep neural\nnetworks capable of state-of-the-art performance without the need for the\nexpensive over-parameterization by proposing the concept of In-Time\nOver-Parameterization (ITOP) in sparse training. By starting from a random\nsparse network and continuously exploring sparse connectivities during\ntraining, we can perform an Over-Parameterization in the space-time manifold,\nclosing the gap in the expressibility between sparse training and dense\ntraining. We further use ITOP to understand the underlying mechanism of Dynamic\nSparse Training (DST) and indicate that the benefits of DST come from its\nability to consider across time all possible parameters when searching for the\noptimal sparse connectivity. As long as there are sufficient parameters that\nhave been reliably explored during training, DST can outperform the dense\nneural network by a large margin. We present a series of experiments to support\nour conjecture and achieve the state-of-the-art sparse training performance\nwith ResNet-50 on ImageNet. More impressively, our method achieves dominant\nperformance over the overparameterization-based sparse methods at extreme\nsparsity levels. When trained on CIFAR-100, our method can match the\nperformance of the dense model even at an extreme sparsity (98%). Code can be\nfound https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization.",
          "link": "http://arxiv.org/abs/2102.02887",
          "publishedOn": "2021-06-16T01:21:06.622Z",
          "wordCount": 713,
          "title": "Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training. (arXiv:2102.02887v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1\">Ramon Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>",
          "description": "Speech-based image retrieval has been studied as a proxy for joint\nrepresentation learning, usually without emphasis on retrieval itself. As such,\nit is unclear how well speech-based retrieval can work in practice -- both in\nan absolute sense and versus alternative strategies that combine automatic\nspeech recognition (ASR) with strong text encoders. In this work, we\nextensively study and expand choices of encoder architectures, training\nmethodology (including unimodal and multimodal pretraining), and other factors.\nOur experiments cover different types of speech in three datasets: Flickr\nAudio, Places Audio, and Localized Narratives. Our best model configuration\nachieves large gains over state of the art, e.g., pushing recall-at-one from\n21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also\nshow our best speech-based models can match or exceed cascaded ASR-to-text\nencoding when speech is spontaneous, accented, or otherwise hard to\nautomatically transcribe.",
          "link": "http://arxiv.org/abs/2104.01894",
          "publishedOn": "2021-06-16T01:21:06.601Z",
          "wordCount": 631,
          "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kangning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yiqiu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1\">Jakub Ch&#x142;&#x119;dowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>",
          "description": "In the last few years, deep learning classifiers have shown promising results\nin image-based medical diagnosis. However, interpreting the outputs of these\nmodels remains a challenge. In cancer diagnosis, interpretability can be\nachieved by localizing the region of the input image responsible for the\noutput, i.e. the location of a lesion. Alternatively, segmentation or detection\nmodels can be trained with pixel-wise annotations indicating the locations of\nmalignant lesions. Unfortunately, acquiring such labels is labor-intensive and\nrequires medical expertise. To overcome this difficulty, weakly-supervised\nlocalization can be utilized. These methods allow neural network classifiers to\noutput saliency maps highlighting the regions of the input most relevant to the\nclassification task (e.g. malignant lesions in mammograms) using only\nimage-level labels (e.g. whether the patient has cancer or not) during\ntraining. When applied to high-resolution images, existing methods produce\nlow-resolution saliency maps. This is problematic in applications in which\nsuspicious lesions are small in relation to the image size. In this work, we\nintroduce a novel neural network architecture to perform weakly-supervised\nsegmentation of high-resolution images. The proposed model selects regions of\ninterest via coarse-level localization, and then performs fine-grained\nsegmentation of those regions. We apply this model to breast cancer diagnosis\nwith screening mammography, and validate it on a large clinically-realistic\ndataset. Measured by Dice similarity score, our approach outperforms existing\nmethods by a large margin in terms of localization performance of benign and\nmalignant lesions, relatively improving the performance by 39.6% and 20.0%,\nrespectively. Code and the weights of some of the models are available at\nhttps://github.com/nyukat/GLAM",
          "link": "http://arxiv.org/abs/2106.07049",
          "publishedOn": "2021-06-16T01:21:06.580Z",
          "wordCount": 734,
          "title": "Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis. (arXiv:2106.07049v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_L/0/1/0/all/0/1\">Lakshman Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wurst_J/0/1/0/all/0/1\">Jonas Wurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1\">Michael Botsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Ke Deng</a>",
          "description": "Traffic scenario categorisation is an essential component of automated\ndriving, for e.\\,g., in motion planning algorithms and their validation.\nFinding new relevant scenarios without handcrafted steps reduce the required\nresources for the development of autonomous driving dramatically. In this work,\na method is proposed to address this challenge by introducing a clustering\ntechnique based on a novel data-adaptive similarity measure, called Random\nForest Activation Pattern (RFAP) similarity. The RFAP similarity is generated\nusing a tree encoding scheme in a Random Forest algorithm. The clustering\nmethod proposed in this work takes into account that there are labelled\nscenarios available and the information from the labelled scenarios can help to\nguide the clustering of unlabelled scenarios. It consists of three steps.\nFirst, a self-supervised Convolutional Neural Network~(CNN) is trained on all\navailable traffic scenarios using a defined self-supervised objective. Second,\nthe CNN is fine-tuned for classification of the labelled scenarios. Third,\nusing the labelled and unlabelled scenarios an iterative optimisation procedure\nis performed for clustering. In the third step at each epoch of the iterative\noptimisation, the CNN is used as a feature generator for an unsupervised Random\nForest. The trained forest, in turn, provides the RFAP similarity to adapt\niteratively the feature generation process implemented by the CNN. Extensive\nexperiments and ablation studies have been done on the highD dataset. The\nproposed method shows superior performance compared to baseline clustering\ntechniques.",
          "link": "http://arxiv.org/abs/2105.07639",
          "publishedOn": "2021-06-16T01:21:06.573Z",
          "wordCount": 709,
          "title": "Traffic Scenario Clustering by Iterative Optimisation of Self-Supervised Networks Using a Random Forest Activation Pattern Similarity. (arXiv:2105.07639v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuejin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>",
          "description": "Human can infer the 3D geometry of a scene from a sketch instead of a\nrealistic image, which indicates that the spatial structure plays a fundamental\nrole in understanding the depth of scenes. We are the first to explore the\nlearning of a depth-specific structural representation, which captures the\nessential feature for depth estimation and ignores irrelevant style\ninformation. Our S2R-DepthNet (Synthetic to Real DepthNet) can be well\ngeneralized to unseen real-world data directly even though it is only trained\non synthetic data. S2R-DepthNet consists of: a) a Structure Extraction (STE)\nmodule which extracts a domaininvariant structural representation from an image\nby disentangling the image into domain-invariant structure and domain-specific\nstyle components, b) a Depth-specific Attention (DSA) module, which learns\ntask-specific knowledge to suppress depth-irrelevant structures for better\ndepth estimation and generalization, and c) a depth prediction module (DP) to\npredict depth from the depth-specific representation. Without access of any\nreal-world images, our method even outperforms the state-of-the-art\nunsupervised domain adaptation methods which use real-world images of the\ntarget domain for training. In addition, when using a small amount of labeled\nreal-world data, we achieve the state-ofthe-art performance under the\nsemi-supervised setting. The code and trained models are available at\nhttps://github.com/microsoft/S2R-DepthNet.",
          "link": "http://arxiv.org/abs/2104.00877",
          "publishedOn": "2021-06-16T01:21:06.566Z",
          "wordCount": 663,
          "title": "S2R-DepthNet: Learning a Generalizable Depth-specific Structural Representation. (arXiv:2104.00877v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heijden_B/0/1/0/all/0/1\">Bas van der Heijden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferranti_L/0/1/0/all/0/1\">Laura Ferranti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1\">Jens Kober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1\">Robert Babuska</a>",
          "description": "This paper presents DeepKoCo, a novel model-based agent that learns a latent\nKoopman representation from images. This representation allows DeepKoCo to plan\nefficiently using linear control methods, such as linear model predictive\ncontrol. Compared to traditional agents, DeepKoCo is robust to task-irrelevant\ndynamics, thanks to the use of a tailored lossy autoencoder network that allows\nDeepKoCo to learn latent dynamics that reconstruct and predict only observed\ncosts, rather than all observed dynamics. As our results show, DeepKoCo\nachieves a similar final performance as traditional model-free methods on\ncomplex control tasks, while being considerably more robust to distractor\ndynamics, making the proposed agent more amenable for real-life applications.",
          "link": "http://arxiv.org/abs/2011.12690",
          "publishedOn": "2021-06-16T01:21:06.559Z",
          "wordCount": 581,
          "title": "DeepKoCo: Efficient latent planning with a robust Koopman representation. (arXiv:2011.12690v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1\">Christoph Angermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_S/0/1/0/all/0/1\">Steinbj&#xf6;rn J&#xf3;nsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravova_A/0/1/0/all/0/1\">Ad&#xe9;la Moravov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laubichler_C/0/1/0/all/0/1\">Christian Laubichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesling_C/0/1/0/all/0/1\">Constantin Kiesling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kober_M/0/1/0/all/0/1\">Martin Kober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fimml_W/0/1/0/all/0/1\">Wolfgang Fimml</a>",
          "description": "Digitalization offers a large number of promising tools for large internal\ncombustion engines such as condition monitoring or condition-based maintenance.\nThis includes the status evaluation of key engine components such as cylinder\nliners, whose inner surfaces are subject to constant wear due to their movement\nrelative to the pistons. Existing state-of-the-art methods for quantifying wear\nrequire disassembly and cutting of the examined liner followed by a\nhigh-resolution microscopic surface depth measurement that quantitatively\nevaluates wear based on bearing load curves (also known as Abbott-Firestone\ncurves). Such reference methods are destructive, time-consuming and costly. The\ngoal of the research presented here is to develop nondestructive yet reliable\nmethods for quantifying the surface condition. A deep-learning framework is\nproposed that allows computation of the bearing load curves from reflection RGB\nimages of the liner surface that can be collected with a wide variety of simple\nimaging devices, without the need to remove and destroy the investigated liner.\nFor this purpose, a convolutional neural network is trained to predict the\nbearing load curve of the corresponding depth profile from the collected RGB\nimages, which in turn can be used for further wear evaluation. Training of the\nnetwork is performed using a custom-built database containing depth profiles\nand reflection images of liner surfaces of large gas engines. The results of\nthe proposed method are visually examined and quantified considering several\nprobabilistic distance metrics and comparison of roughness indicators between\nground truth and model predictions. The observed success of the proposed method\nsuggests its great potential for quantitative wear assessment on engines during\nservice directly on site.",
          "link": "http://arxiv.org/abs/2103.08482",
          "publishedOn": "2021-06-16T01:21:06.552Z",
          "wordCount": 733,
          "title": "Machine Learning for Nondestructive Wear Assessment in Large Internal Combustion Engines. (arXiv:2103.08482v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tailin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glossner_J/0/1/0/all/0/1\">John Glossner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaobo Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaotong Zhang</a>",
          "description": "Deep neural networks have been applied in many applications exhibiting\nextraordinary abilities in the field of computer vision. However, complex\nnetwork architectures challenge efficient real-time deployment and require\nsignificant computation resources and energy costs. These challenges can be\novercome through optimizations such as network compression. Network compression\ncan often be realized with little loss of accuracy. In some cases accuracy may\neven improve. This paper provides a survey on two types of network compression:\npruning and quantization. Pruning can be categorized as static if it is\nperformed offline or dynamic if it is performed at run-time. We compare pruning\ntechniques and describe criteria used to remove redundant computations. We\ndiscuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise,\nlayer-wise and even network-wise pruning. Quantization reduces computations by\nreducing the precision of the datatype. Weights, biases, and activations may be\nquantized typically to 8-bit integers although lower bit width implementations\nare also discussed including binary neural networks. Both pruning and\nquantization can be used independently or combined. We compare current\ntechniques, analyze their strengths and weaknesses, present compressed network\naccuracy results on a number of frameworks, and provide practical guidance for\ncompressing networks.",
          "link": "http://arxiv.org/abs/2101.09671",
          "publishedOn": "2021-06-16T01:21:06.531Z",
          "wordCount": 668,
          "title": "Pruning and Quantization for Deep Neural Network Acceleration: A Survey. (arXiv:2101.09671v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial examples contain carefully crafted perturbations that can fool\ndeep neural networks (DNNs) into making wrong predictions. Enhancing the\nadversarial robustness of DNNs has gained considerable interest in recent\nyears. Although image transformation-based defenses were widely considered at\nan earlier time, most of them have been defeated by adaptive attacks. In this\npaper, we propose a new image transformation defense based on error diffusion\nhalftoning, and combine it with adversarial training to defend against\nadversarial examples. Error diffusion halftoning projects an image into a 1-bit\nspace and diffuses quantization error to neighboring pixels. This process can\nremove adversarial perturbations from a given image while maintaining\nacceptable image quality in the meantime in favor of recognition. Experimental\nresults demonstrate that the proposed method is able to improve adversarial\nrobustness even under advanced adaptive attacks, while most of the other image\ntransformation-based defenses do not. We show that a proper image\ntransformation can still be an effective defense approach. Code:\nhttps://github.com/shaoyuanlo/Halftoning-Defense",
          "link": "http://arxiv.org/abs/2101.09451",
          "publishedOn": "2021-06-16T01:21:06.523Z",
          "wordCount": 639,
          "title": "Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sener_F/0/1/0/all/0/1\">Fadime Sener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_D/0/1/0/all/0/1\">Dibyadip Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>",
          "description": "This technical report extends our work presented in [9] with more\nexperiments. In [9], we tackle long-term video understanding, which requires\nreasoning from current and past or future observations and raises several\nfundamental questions. How should temporal or sequential relationships be\nmodelled? What temporal extent of information and context needs to be\nprocessed? At what temporal scale should they be derived? [9] addresses these\nquestions with a flexible multi-granular temporal aggregation framework. In\nthis report, we conduct further experiments with this framework on different\ntasks and a new dataset, EPIC-KITCHENS-100.",
          "link": "http://arxiv.org/abs/2106.03152",
          "publishedOn": "2021-06-16T01:21:06.515Z",
          "wordCount": 529,
          "title": "Technical Report: Temporal Aggregate Representations. (arXiv:2106.03152v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10762",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "Random field and random cluster theory are used to prove certain mathematical\nresults concerning the probability distribution of image pixel intensities\ncharacterized as generic $2D$ integer arrays. The size of the smallest bounded\nregion within an image is estimated for segmenting an image, from which, the\nequilibrium distribution of intensities can be recovered. From the estimated\nbounded regions, properties of the sub-optimal and equilibrium distributions of\nintensities are derived, which leads to an image compression methodology\nwhereby only slightly more than half of all pixels are required for a\nworst-case reconstruction of the original image. An example in unsupervised\nobject detection illustrates the mathematical results.",
          "link": "http://arxiv.org/abs/2104.10762",
          "publishedOn": "2021-06-16T01:21:06.508Z",
          "wordCount": 611,
          "title": "Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v6 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.12019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Huanhou Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jinglun Shi</a>",
          "description": "Automatically describing video content with text description is challenging\nbut important task, which has been attracting a lot of attention in computer\nvision community. Previous works mainly strive for the accuracy of the\ngenerated sentences, while ignoring the sentences diversity, which is\ninconsistent with human behavior. In this paper, we aim to caption each video\nwith multiple descriptions and propose a novel framework. Concretely, for a\ngiven video, the intermediate latent variables of conventional encode-decode\nprocess are utilized as input to the conditional generative adversarial network\n(CGAN) with the purpose of generating diverse sentences. We adopt different\nConvolutional Neural Networks (CNNs) as our generator that produces\ndescriptions conditioned on latent variables and discriminator that assesses\nthe quality of generated sentences. Simultaneously, a novel DCE metric is\ndesigned to assess the diverse captions. We evaluate our method on the\nbenchmark datasets, where it demonstrates its ability to generate diverse\ndescriptions and achieves superior results against other state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/1910.12019",
          "publishedOn": "2021-06-16T01:21:06.484Z",
          "wordCount": 645,
          "title": "Diverse Video Captioning Through Latent Variable Expansion. (arXiv:1910.12019v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.00923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1\">Tao Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hefeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "To address the problem of data inconsistencies among different facial\nexpression recognition (FER) datasets, many cross-domain FER methods (CD-FERs)\nhave been extensively devised in recent years. Although each declares to\nachieve superior performance, fair comparisons are lacking due to the\ninconsistent choices of the source/target datasets and feature extractors. In\nthis work, we first analyze the performance effect caused by these inconsistent\nchoices, and then re-implement some well-performing CD-FER and recently\npublished domain adaptation algorithms. We ensure that all these algorithms\nadopt the same source datasets and feature extractors for fair CD-FER\nevaluations. We find that most of the current leading algorithms use\nadversarial learning to learn holistic domain-invariant features to mitigate\ndomain shifts. However, these algorithms ignore local features, which are more\ntransferable across different datasets and carry more detailed content for\nfine-grained adaptation. To address these issues, we integrate graph\nrepresentation propagation with adversarial learning for cross-domain\nholistic-local feature co-adaptation by developing a novel adversarial graph\nrepresentation adaptation (AGRA) framework. Specifically, it first builds two\ngraphs to correlate holistic and local regions within each domain and across\ndifferent domains, respectively. Then, it extracts holistic-local features from\nthe input image and uses learnable per-class statistical distributions to\ninitialize the corresponding graph nodes. Finally, two stacked graph\nconvolution networks (GCNs) are adopted to propagate holistic-local features\nwithin each domain to explore their interaction and across different domains\nfor holistic-local feature co-adaptation. We conduct extensive and fair\nevaluations on several popular benchmarks and show that the proposed AGRA\nframework outperforms previous state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2008.00923",
          "publishedOn": "2021-06-16T01:21:06.432Z",
          "wordCount": 785,
          "title": "Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark and Adversarial Graph Learning. (arXiv:2008.00923v7 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>",
          "description": "Visual attention is one of the most significant characteristics for selecting\nand understanding the outside redundancy world. The nature of complex scenes\nincludes enormous redundancy. The human vision system can not process all\ninformation simultaneously because of visual information bottleneck. The human\nvisual system mainly focuses on dominant parts of the scenes to reduce the\ninput visual redundancy information. It is commonly known as visual attention\nprediction or visual saliency map. This paper proposes a new psychophysical\nsaliency prediction architecture, WECSF, inspired by human low-level visual\ncortex function. The model consists of opponent color channels, wavelet\ntransform, wavelet energy map, and contrast sensitivity function for extracting\nlow-level image features and maximum approximation to the human visual system.\nThe proposed model is evaluated several datasets, including MIT1003, MIT300,\nTORONTO, SID4VAM and UCF Sports dataset to explain its efficiency. We also\nquantitatively and qualitatively compared the performance of saliency\nprediction with other state-of-the-art models. Our model achieved very stable\nand good performance. Second, we also confirmed that Fourier and\nspectral-inspired saliency prediction models achieved outperformance compared\nto other start-of-the-art non-neural networks and even deep neural network\nmodels on psychophysical synthesis images. Finally, the proposed model also can\nbe applied to spatial-temporal saliency prediction and got better performance.",
          "link": "http://arxiv.org/abs/2011.04076",
          "publishedOn": "2021-06-16T01:21:06.287Z",
          "wordCount": 715,
          "title": "A Psychophysical Oriented Saliency Map Prediction Model. (arXiv:2011.04076v9 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1\">Markus Nagel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fournarakis_M/0/1/0/all/0/1\">Marios Fournarakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1\">Rana Ali Amjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondarenko_Y/0/1/0/all/0/1\">Yelysei Bondarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baalen_M/0/1/0/all/0/1\">Mart van Baalen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1\">Tijmen Blankevoort</a>",
          "description": "While neural networks have advanced the frontiers in many applications, they\noften come at a high computational cost. Reducing the power and latency of\nneural network inference is key if we want to integrate modern networks into\nedge devices with strict power and compute requirements. Neural network\nquantization is one of the most effective ways of achieving these savings but\nthe additional noise it induces can lead to accuracy degradation. In this white\npaper, we introduce state-of-the-art algorithms for mitigating the impact of\nquantization noise on the network's performance while maintaining low-bit\nweights and activations. We start with a hardware motivated introduction to\nquantization and then consider two main classes of algorithms: Post-Training\nQuantization (PTQ) and Quantization-Aware-Training (QAT). PTQ requires no\nre-training or labelled data and is thus a lightweight push-button approach to\nquantization. In most cases, PTQ is sufficient for achieving 8-bit quantization\nwith close to floating-point accuracy. QAT requires fine-tuning and access to\nlabeled training data but enables lower bit quantization with competitive\nresults. For both solutions, we provide tested pipelines based on existing\nliterature and extensive experimentation that lead to state-of-the-art\nperformance for common deep learning models and tasks.",
          "link": "http://arxiv.org/abs/2106.08295",
          "publishedOn": "2021-06-16T01:21:06.270Z",
          "wordCount": 630,
          "title": "A White Paper on Neural Network Quantization. (arXiv:2106.08295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1\">Vanessa D&#x27;Amario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via\ncomposition of modules that tackle a sub-task. NMNs are a promising strategy to\nachieve systematic generalization, i.e. overcoming biasing factors in the\ntraining distribution. However, the aspects of NMNs that facilitate systematic\ngeneralization are not fully understood. In this paper, we demonstrate that the\nstage and the degree at which modularity is defined has large influence on\nsystematic generalization. In a series of experiments on three VQA datasets\n(MNIST with multiple attributes, SQOOP, and CLEVR-CoGenT), our results reveal\nthat tuning the degree of modularity in the network, especially at the image\nencoder stage, reaches substantially higher systematic generalization. These\nfindings lead to new NMN architectures that outperform previous ones in terms\nof systematic generalization.",
          "link": "http://arxiv.org/abs/2106.08170",
          "publishedOn": "2021-06-16T01:21:06.263Z",
          "wordCount": 557,
          "title": "How Modular Should Neural Module Networks Be for Systematic Generalization?. (arXiv:2106.08170v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1\">Dimitrios Vytiniotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1\">Grzegorz Swirszcz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1\">Viorica Patraucean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Joao Carreira</a>",
          "description": "How can neural networks be trained on large-volume temporal data efficiently?\nTo compute the gradients required to update parameters, backpropagation blocks\ncomputations until the forward and backward passes are completed. For temporal\nsignals, this introduces high latency and hinders real-time learning. It also\ncreates a coupling between consecutive layers, which limits model parallelism\nand increases memory consumption. In this paper, we build upon Sideways, which\navoids blocking by propagating approximate gradients forward in time, and we\npropose mechanisms for temporal integration of information based on different\nvariants of skip connections. We also show how to decouple computation and\ndelegate individual neural modules to different devices, allowing distributed\nand parallel training. The proposed Skip-Sideways achieves low latency\ntraining, model parallelism, and, importantly, is capable of extracting\ntemporal features, leading to more stable training and improved performance on\nreal-world action recognition video datasets such as HMDB51, UCF101, and the\nlarge-scale Kinetics-600. Finally, we also show that models trained with\nSkip-Sideways generate better future frames than Sideways models, and hence\nthey can better utilize motion cues.",
          "link": "http://arxiv.org/abs/2106.08318",
          "publishedOn": "2021-06-16T01:21:06.254Z",
          "wordCount": 632,
          "title": "Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>",
          "description": "Legacy black-and-white photos are riddled with people's nostalgia and\nglorious memories of the past. To better relive the elapsed frozen moments, in\nthis paper, we present a deep exemplar-based image colorization approach named\nColor2Style to resurrect these grayscale image media by filling them with\nvibrant colors. Generally, for exemplar-based colorization, unsupervised and\nunpaired training are usually adopted, due to the difficulty of obtaining input\nand ground truth image pairs. To train an exemplar-based colorization model,\ncurrent algorithms usually strive to achieve two procedures: i) retrieving a\nlarge number of reference images with high similarity in advance, which is\ninevitably time-consuming and tedious; ii) designing complicated modules to\ntransfer the colors of the reference image to the grayscale image, by\ncalculating and leveraging the deep semantic correspondence between them (e.g.,\nnon-local operation). Contrary to the previous methods, we solve and simplify\nthe above two steps in one end-to-end learning procedure. First, we adopt a\nself-augmented self-reference training scheme, where the reference image is\ngenerated by graphical transformations from the original colorful one whereby\nthe training can be formulated in a paired manner. Second, instead of computing\ncomplex and inexplicable correspondence maps, our method exploits a simple yet\neffective deep feature modulation (DFM) module, which injects the color\nembeddings extracted from the reference image into the deep representations of\nthe input grayscale image. Such design is much more lightweight and\nintelligible, achieving appealing performance with real-time processing speed.\nMoreover, our model does not require multifarious loss functions and\nregularization terms like existing methods, but only two widely used loss\nfunctions. Codes and models will be available at\nhttps://github.com/zhaohengyuan1/Color2Style.",
          "link": "http://arxiv.org/abs/2106.08017",
          "publishedOn": "2021-06-16T01:21:06.233Z",
          "wordCount": 714,
          "title": "Color2Style: Real-Time Exemplar-Based Image Colorization with Self-Reference Learning and Deep Feature Modulation. (arXiv:2106.08017v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08320",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yazhe Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pogodin_R/0/1/0/all/0/1\">Roman Pogodin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1\">Danica J. Sutherland</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1\">Arthur Gretton</a>",
          "description": "We approach self-supervised learning of image representations from a\nstatistical dependence perspective, proposing Self-Supervised Learning with the\nHilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes\ndependence between representations of transformed versions of an image and the\nimage identity, while minimizing the kernelized variance of those features.\nThis self-supervised learning framework yields a new understanding of InfoNCE,\na variational lower bound on the mutual information (MI) between different\ntransformations. While the MI itself is known to have pathologies which can\nresult in meaningless representations being learned, its bound is much better\nbehaved: we show that it implicitly approximates SSL-HSIC (with a slightly\ndifferent regularizer). Our approach also gives us insight into BYOL, since\nSSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to\ndirectly optimize statistical dependence in time linear in the batch size,\nwithout restrictive data assumptions or indirect mutual information estimators.\nTrained with or without a target network, SSL-HSIC matches the current\nstate-of-the-art for standard linear evaluation on ImageNet, semi-supervised\nlearning and transfer to other classification and vision tasks such as semantic\nsegmentation, depth estimation and object recognition.",
          "link": "http://arxiv.org/abs/2106.08320",
          "publishedOn": "2021-06-16T01:21:06.223Z",
          "wordCount": 615,
          "title": "Self-Supervised Learning with Kernel Dependence Maximization. (arXiv:2106.08320v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blakeney_C/0/1/0/all/0/1\">Cody Blakeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huish_N/0/1/0/all/0/1\">Nathaniel Huish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Ziliang Zong</a>",
          "description": "In recent years the ubiquitous deployment of AI has posed great concerns in\nregards to algorithmic bias, discrimination, and fairness. Compared to\ntraditional forms of bias or discrimination caused by humans, algorithmic bias\ngenerated by AI is more abstract and unintuitive therefore more difficult to\nexplain and mitigate. A clear gap exists in the current literature on\nevaluating and mitigating bias in pruned neural networks. In this work, we\nstrive to tackle the challenging issues of evaluating, mitigating, and\nexplaining induced bias in pruned neural networks. Our paper makes three\ncontributions. First, we propose two simple yet effective metrics, Combined\nError Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively\nevaluate the induced bias prevention quality of pruned models. Second, we\ndemonstrate that knowledge distillation can mitigate induced bias in pruned\nneural networks, even with unbalanced datasets. Third, we reveal that model\nsimilarity has strong correlations with pruning induced bias, which provides a\npowerful method to explain why bias occurs in pruned neural networks. Our code\nis available at https://github.com/codestar12/pruning-distilation-bias",
          "link": "http://arxiv.org/abs/2106.07849",
          "publishedOn": "2021-06-16T01:21:06.215Z",
          "wordCount": 616,
          "title": "Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks with Knowledge Distillation. (arXiv:2106.07849v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1\">Luis Felipe Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1\">S&#xe9;rgio Colcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1\">Ruy Luiz Milidi&#xfa;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulcao_A/0/1/0/all/0/1\">Andr&#xe9; Bulc&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barros_P/0/1/0/all/0/1\">Pablo Barros</a>",
          "description": "Nowadays, subsurface salt body localization and delineation, also called\nsemantic segmentation of salt bodies, are among the most challenging\ngeophysicist tasks. Thus, identifying large salt bodies is notoriously tricky\nand is crucial for identifying hydrocarbon reservoirs and drill path planning.\nThis work proposes a Data Augmentation method based on training two generative\nmodels to augment the number of samples in a seismic image dataset for the\nsemantic segmentation of salt bodies. Our method uses deep learning models to\ngenerate pairs of seismic image patches and their respective salt masks for the\nData Augmentation. The first model is a Variational Autoencoder and is\nresponsible for generating patches of salt body masks. The second is a\nConditional Normalizing Flow model, which receives the generated masks as\ninputs and generates the associated seismic image patches. We evaluate the\nproposed method by comparing the performance of ten distinct state-of-the-art\nmodels for semantic segmentation, trained with and without the generated\naugmentations, in a dataset from two synthetic seismic images. The proposed\nmethodology yields an average improvement of 8.57% in the IoU metric across all\ncompared models. The best result is achieved by a DeeplabV3+ model variant,\nwhich presents an IoU score of 95.17% when trained with our augmentations.\nAdditionally, our proposal outperformed six selected data augmentation methods,\nand the most significant improvement in the comparison, of 9.77%, is achieved\nby composing our DA with augmentations from an elastic transformation. At last,\nwe show that the proposed method is adaptable for a larger context size by\nachieving results comparable to the obtained on the smaller context size.",
          "link": "http://arxiv.org/abs/2106.08269",
          "publishedOn": "2021-06-16T01:21:06.208Z",
          "wordCount": 719,
          "title": "Generating Data Augmentation samples for Semantic Segmentation of Salt Bodies in a Synthetic Seismic Image Dataset. (arXiv:2106.08269v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>",
          "description": "This paper presents our solution to the AVA-Kinetics Crossover Challenge of\nActivityNet workshop at CVPR 2021. Our solution utilizes multiple types of\nrelation modeling methods for spatio-temporal action detection and adopts a\ntraining strategy to integrate multiple relation modeling in end-to-end\ntraining over the two large-scale video datasets. Learning with memory bank and\nfinetuning for long-tailed distribution are also investigated to further\nimprove the performance. In this paper, we detail the implementations of our\nsolution and provide experiments results and corresponding discussions. We\nfinally achieve 40.67 mAP on the test set of AVA-Kinetics.",
          "link": "http://arxiv.org/abs/2106.08061",
          "publishedOn": "2021-06-16T01:21:06.201Z",
          "wordCount": 526,
          "title": "Relation Modeling in Spatio-Temporal Action Localization. (arXiv:2106.08061v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chanti_D/0/1/0/all/0/1\">Dawood Al Chanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mateus_D/0/1/0/all/0/1\">Diana Mateus</a>",
          "description": "This paper addresses the domain shift problem for segmentation. As a\nsolution, we propose OLVA, a novel and lightweight unsupervised domain\nadaptation method based on a Variational Auto-Encoder (VAE) and Optimal\nTransport (OT) theory. Thanks to the VAE, our model learns a shared\ncross-domain latent space that follows a normal distribution, which reduces the\ndomain shift. To guarantee valid segmentations, our shared latent space is\ndesigned to model the shape rather than the intensity variations. We further\nrely on an OT loss to match and align the remaining discrepancy between the two\ndomains in the latent space. We demonstrate OLVA's effectiveness for the\nsegmentation of multiple cardiac structures on the public Multi-Modality Whole\nHeart Segmentation (MM-WHS) dataset, where the source domain consists of\nannotated 3D MR images and the unlabelled target domain of 3D CTs. Our results\nshow remarkable improvements with an additional margin of 12.5\\% dice score\nover concurrent generative training approaches.",
          "link": "http://arxiv.org/abs/2106.08188",
          "publishedOn": "2021-06-16T01:21:06.181Z",
          "wordCount": 600,
          "title": "Optimal Latent Vector Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation. (arXiv:2106.08188v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "The complex nature of combining localization and classification in object\ndetection has resulted in the flourished development of methods. Previous works\ntried to improve the performance in various object detection heads but failed\nto present a unified view. In this paper, we present a novel dynamic head\nframework to unify object detection heads with attentions. By coherently\ncombining multiple self-attention mechanisms between feature levels for\nscale-awareness, among spatial locations for spatial-awareness, and within\noutput channels for task-awareness, the proposed approach significantly\nimproves the representation ability of object detection heads without any\ncomputational overhead. Further experiments demonstrate that the effectiveness\nand efficiency of the proposed dynamic head on the COCO benchmark. With a\nstandard ResNeXt-101-DCN backbone, we largely improve the performance over\npopular object detectors and achieve a new state-of-the-art at 54.0 AP.\nFurthermore, with latest transformer backbone and extra data, we can push\ncurrent best COCO result to a new record at 60.6 AP. The code will be released\nat https://github.com/microsoft/DynamicHead.",
          "link": "http://arxiv.org/abs/2106.08322",
          "publishedOn": "2021-06-16T01:21:06.174Z",
          "wordCount": 605,
          "title": "Dynamic Head: Unifying Object Detection Heads with Attentions. (arXiv:2106.08322v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songnan Li</a>",
          "description": "Compressing Deep Neural Network (DNN) models to alleviate the storage and\ncomputation requirements is essential for practical applications, especially\nfor resource limited devices. Although capable of reducing a reasonable amount\nof model parameters, previous unstructured or structured weight pruning methods\ncan hardly truly accelerate inference, either due to the poor hardware\ncompatibility of the unstructured sparsity or due to the low sparse rate of the\nstructurally pruned network. Aiming at reducing both storage and computation,\nas well as preserving the original task performance, we propose a generalized\nweight unification framework at a hardware compatible micro-structured level to\nachieve high amount of compression and acceleration. Weight coefficients of a\nselected micro-structured block are unified to reduce the storage and\ncomputation of the block without changing the neuron connections, which turns\nto a micro-structured pruning special case when all unified coefficients are\nset to zero, where neuron connections (hence storage and computation) are\ncompletely removed. In addition, we developed an effective training framework\nbased on the alternating direction method of multipliers (ADMM), which converts\nour complex constrained optimization into separately solvable subproblems.\nThrough iteratively optimizing the subproblems, the desired micro-structure can\nbe ensured with high compression ratio and low performance degradation. We\nextensively evaluated our method using a variety of benchmark models and\ndatasets for different applications. Experimental results demonstrate\nstate-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.08301",
          "publishedOn": "2021-06-16T01:21:06.167Z",
          "wordCount": 669,
          "title": "Efficient Micro-Structured Weight Unification for Neural Network Compression. (arXiv:2106.08301v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.01529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blumenkamp_J/0/1/0/all/0/1\">Jan Blumenkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baude_A/0/1/0/all/0/1\">Andreas Baude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laue_T/0/1/0/all/0/1\">Tim Laue</a>",
          "description": "Deep learning approaches have become the standard solution to many problems\nin computer vision and robotics, but obtaining sufficient training data in high\nenough quality is challenging, as human labor is error prone, time consuming,\nand expensive. Solutions based on simulation have become more popular in recent\nyears, but the gap between simulation and reality is still a major issue. In\nthis paper, we introduce a novel method for augmenting synthetic image data\nthrough unsupervised image-to-image translation by applying the style of real\nworld images to simulated images with open source frameworks. The generated\ndataset is combined with conventional augmentation methods and is then applied\nto a neural network model running in real-time on autonomous soccer robots. Our\nevaluation shows a significant improvement compared to models trained on images\ngenerated entirely in simulation.",
          "link": "http://arxiv.org/abs/1911.01529",
          "publishedOn": "2021-06-16T01:21:06.159Z",
          "wordCount": 599,
          "title": "Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation. (arXiv:1911.01529v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ordun_C/0/1/0/all/0/1\">Catherine Ordun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purushotham_S/0/1/0/all/0/1\">Sanjay Purushotham</a>",
          "description": "Thermal images reveal medically important physiological information about\nhuman stress, signs of inflammation, and emotional mood that cannot be seen on\nvisible images. Providing a method to generate thermal faces from visible\nimages would be highly valuable for the telemedicine community in order to show\nthis medical information. To the best of our knowledge, there are limited works\non visible-to-thermal (VT) face translation, and many current works go the\nopposite direction to generate visible faces from thermal surveillance images\n(TV) for law enforcement applications. As a result, we introduce favtGAN, a VT\nGAN which uses the pix2pix image translation model with an auxiliary sensor\nlabel prediction network for generating thermal faces from visible images.\nSince most TV methods are trained on only one data source drawn from one\nthermal sensor, we combine datasets from faces and cityscapes. These combined\ndata are captured from similar sensors in order to bootstrap the training and\ntransfer learning task, especially valuable because visible-thermal face\ndatasets are limited. Experiments on these combined datasets show that favtGAN\ndemonstrates an increase in SSIM and PSNR scores of generated thermal faces,\ncompared to training on a single face dataset alone.",
          "link": "http://arxiv.org/abs/2106.08091",
          "publishedOn": "2021-06-16T01:21:06.152Z",
          "wordCount": 628,
          "title": "Generating Thermal Human Faces for Physiological Assessment Using Thermal Sensor Auxiliary Labels. (arXiv:2106.08091v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1\">Tim Prangemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1\">Christoph Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1\">Christian Wildner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1\">Heinz Koeppl</a>",
          "description": "Time-lapse fluorescent microscopy (TLFM) combined with predictive\nmathematical modelling is a powerful tool to study the inherently dynamic\nprocesses of life on the single-cell level. Such experiments are costly,\ncomplex and labour intensive. A complimentary approach and a step towards\ncompletely in silico experiments, is to synthesise the imagery itself. Here, we\npropose Multi-StyleGAN as a descriptive approach to simulate time-lapse\nfluorescence microscopy imagery of living cells, based on a past experiment.\nThis novel generative adversarial network synthesises a multi-domain sequence\nof consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple\nlive yeast cells in microstructured environments and train on a dataset\nrecorded in our laboratory. The simulation captures underlying biophysical\nfactors and time dependencies, such as cell morphology, growth, physical\ninteractions, as well as the intensity of a fluorescent reporter protein. An\nimmediate application is to generate additional training and validation data\nfor feature extraction algorithms or to aid and expedite development of\nadvanced experimental techniques such as online monitoring or control of cells.\n\nCode and dataset is available at\nhttps://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.",
          "link": "http://arxiv.org/abs/2106.08285",
          "publishedOn": "2021-06-16T01:21:06.132Z",
          "wordCount": 640,
          "title": "Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.06837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1\">Nan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaochun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Duo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Dongrui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhimin Tang</a>",
          "description": "Many meta-learning methods are proposed for few-shot detection. However,\nprevious most methods have two main problems, poor detection APs, and strong\nbias because of imbalance and insufficient datasets. Previous works mainly\nalleviate these issues by additional datasets, multi-relation attention\nmechanisms and sub-modules. However, they require more cost. In this work, for\nmeta-learning, we find that the main challenges focus on related or irrelevant\nsemantic features between categories. Therefore, based on semantic features, we\npropose a Top-C classification loss (i.e., TCL-C) for classification task and a\ncategory-based grouping mechanism for category-based meta-features obtained by\nthe meta-model. The TCL-C exploits the true-label prediction and the most\nlikely C-1 false classification predictions to improve detection performance on\nfew-shot classes. According to similar appearance (i.e., visual appearance,\nshape, and limbs etc.) and environment in which objects often appear, the\ncategory-based grouping mechanism splits categories into disjoint groups to\nmake similar semantic features more compact between categories within a group\nand obtain more significant difference between groups, alleviating the strong\nbias problem and further improving detection APs. The whole training consists\nof the base model and the fine-tuning phases. According to grouping mechanism,\nwe group the meta-features vectors obtained by meta-model, so that the\ndistribution difference between groups is obvious, and the one within each\ngroup is less. Extensive experiments on Pascal VOC dataset demonstrate that\nours which combines the TCL-C with category-based grouping significantly\noutperforms previous state-of-the-art methods for few-shot detection. Compared\nwith previous competitive baseline, ours improves detection APs by almost 4%\nfor few-shot detection.",
          "link": "http://arxiv.org/abs/2007.06837",
          "publishedOn": "2021-06-16T01:21:06.125Z",
          "wordCount": 762,
          "title": "Top-Related Meta-Learning Method for Few-Shot Object Detection. (arXiv:2007.06837v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Edstedt_J/0/1/0/all/0/1\">Johan Edstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_J/0/1/0/all/0/1\">Johan Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benavente_F/0/1/0/all/0/1\">Francisca Benavente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_A/0/1/0/all/0/1\">Anette Novak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Amanda Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "Automatically identifying harmful content in video is an important task with\na wide range of applications. However, due to the difficulty of collecting\nhigh-quality labels as well as demanding computational requirements, the task\nhas not had a satisfying general approach. Typically, only small subsets of the\nproblem are considered, such as identifying violent content. In cases where the\ngeneral problem is tackled, rough approximations and simplifications are made\nto deal with the lack of labels and computational complexity. In this work, we\nidentify and tackle the two main obstacles. First, we create a dataset of\napproximately 4000 video clips, annotated by professionals in the field.\nSecondly, we demonstrate that advances in video recognition enable training\nmodels on our dataset that consider the full context of the scene. We conduct\nan in-depth study on our modeling choices and find that we greatly benefit from\ncombining the visual and audio modality and that pretraining on large-scale\nvideo recognition datasets and class balanced sampling further improves\nperformance. We additionally perform a qualitative study that reveals the\nheavily multi-modal nature of our dataset. Our dataset will be made available\nupon publication.",
          "link": "http://arxiv.org/abs/2106.08323",
          "publishedOn": "2021-06-16T01:21:06.118Z",
          "wordCount": 629,
          "title": "Is this Harmful? Learning to Predict Harmfulness Ratings from Video. (arXiv:2106.08323v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yibo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yiming Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiyang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haidi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1\">Wenhua Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>",
          "description": "Defect detection and classification technology has changed from traditional\nartificial visual inspection to current intelligent automated inspection, but\nmost of the current defect detection methods are training related detection\nmodels based on a data-driven approach, taking into account the difficulty of\ncollecting some sample data in the industrial field. We apply zero-shot\nlearning technology to the industrial field. Aiming at the problem of the\nexisting \"Latent Feature Guide Attribute Attention\" (LFGAA) zero-shot image\nclassification network, the output latent attributes and artificially defined\nattributes are different in the semantic space, which leads to the problem of\nmodel performance degradation, proposed an LGFAA network based on semantic\nfeedback, and improved model performance by constructing semantic embedded\nmodules and feedback mechanisms. At the same time, for the common domain shift\nproblem in zero-shot learning, based on the idea of co-training algorithm using\nthe difference information between different views of data to learn from each\nother, we propose an Ensemble Co-training algorithm, which adaptively reduces\nthe prediction error in image tag embedding from multiple angles. Various\nexperiments conducted on the zero-shot dataset and the cylinder liner dataset\nin the industrial field provide competitive results.",
          "link": "http://arxiv.org/abs/2106.07959",
          "publishedOn": "2021-06-16T01:21:06.111Z",
          "wordCount": 640,
          "title": "Zero-sample surface defect detection and classification based on semantic feedback neural network. (arXiv:2106.07959v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reul_C/0/1/0/all/0/1\">Christian Reul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wick_C/0/1/0/all/0/1\">Christoph Wick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noth_M/0/1/0/all/0/1\">Maximilian N&#xf6;th</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttner_A/0/1/0/all/0/1\">Andreas B&#xfc;ttner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehner_M/0/1/0/all/0/1\">Maximilian Wehner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Springmann_U/0/1/0/all/0/1\">Uwe Springmann</a>",
          "description": "In order to apply Optical Character Recognition (OCR) to historical printings\nof Latin script fully automatically, we report on our efforts to construct a\nwidely-applicable polyfont recognition model yielding text with a Character\nError Rate (CER) around 2% when applied out-of-the-box. Moreover, we show how\nthis model can be further finetuned to specific classes of printings with\nlittle manual and computational effort. The mixed or polyfont model is trained\non a wide variety of materials, in terms of age (from the 15th to the 19th\ncentury), typography (various types of Fraktur and Antiqua), and languages\n(among others, German, Latin, and French). To optimize the results we combined\nestablished techniques of OCR training like pretraining, data augmentation, and\nvoting. In addition, we used various preprocessing methods to enrich the\ntraining data and obtain more robust models. We also implemented a two-stage\napproach which first trains on all available, considerably unbalanced data and\nthen refines the output by training on a selected more balanced subset.\nEvaluations on 29 previously unseen books resulted in a CER of 1.73%,\noutperforming a widely used standard model with a CER of 2.84% by almost 40%.\nTraining a more specialized model for some unseen Early Modern Latin books\nstarting from our mixed model led to a CER of 1.47%, an improvement of up to\n50% compared to training from scratch and up to 30% compared to training from\nthe aforementioned standard model. Our new mixed model is made openly available\nto the community.",
          "link": "http://arxiv.org/abs/2106.07881",
          "publishedOn": "2021-06-16T01:21:06.103Z",
          "wordCount": 693,
          "title": "Mixed Model OCR Training on Historical Latin Script for Out-of-the-Box Recognition and Finetuning. (arXiv:2106.07881v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>",
          "description": "We present a new method, called MEsh TRansfOrmer (METRO), to reconstruct 3D\nhuman pose and mesh vertices from a single image. Our method uses a transformer\nencoder to jointly model vertex-vertex and vertex-joint interactions, and\noutputs 3D joint coordinates and mesh vertices simultaneously. Compared to\nexisting techniques that regress pose and shape parameters, METRO does not rely\non any parametric mesh models like SMPL, thus it can be easily extended to\nother objects such as hands. We further relax the mesh topology and allow the\ntransformer self-attention mechanism to freely attend between any two vertices,\nmaking it possible to learn non-local relationships among mesh vertices and\njoints. With the proposed masked vertex modeling, our method is more robust and\neffective in handling challenging situations like partial occlusions. METRO\ngenerates new state-of-the-art results for human mesh reconstruction on the\npublic Human3.6M and 3DPW datasets. Moreover, we demonstrate the\ngeneralizability of METRO to 3D hand reconstruction in the wild, outperforming\nexisting state-of-the-art methods on FreiHAND dataset. Code and pre-trained\nmodels are available at https://github.com/microsoft/MeshTransformer.",
          "link": "http://arxiv.org/abs/2012.09760",
          "publishedOn": "2021-06-16T01:21:06.083Z",
          "wordCount": 642,
          "title": "End-to-End Human Pose and Mesh Reconstruction with Transformers. (arXiv:2012.09760v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial robustness of deep neural networks is an extensively studied\nproblem in the literature and various methods have been proposed to defend\nagainst adversarial images. However, only a handful of defense methods have\nbeen developed for defending against attacked videos. In this paper, we propose\na novel Over-and-Under complete restoration network for Defending against\nadversarial videos (OUDefend). Most restoration networks adopt an\nencoder-decoder architecture that first shrinks spatial dimension then expands\nit back. This approach learns undercomplete representations, which have large\nreceptive fields to collect global information but overlooks local details. On\nthe other hand, overcomplete representations have opposite properties. Hence,\nOUDefend is designed to balance local and global features by learning those two\nrepresentations. We attach OUDefend to target video recognition models as a\nfeature restoration block and train the entire network end-to-end. Experimental\nresults show that the defenses focusing on images may be ineffective to videos,\nwhile OUDefend enhances robustness against different types of adversarial\nvideos, ranging from additive attacks, multiplicative attacks to physically\nrealizable attacks. Code: https://github.com/shaoyuanlo/OUDefend",
          "link": "http://arxiv.org/abs/2012.04262",
          "publishedOn": "2021-06-16T01:21:06.062Z",
          "wordCount": 647,
          "title": "Overcomplete Representations Against Adversarial Videos. (arXiv:2012.04262v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duboudin_T/0/1/0/all/0/1\">Thomas Duboudin</a> (imagine), <a href=\"http://arxiv.org/find/cs/1/au:+Dellandrea_E/0/1/0/all/0/1\">Emmanuel Dellandr&#xe9;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abgrall_C/0/1/0/all/0/1\">Corentin Abgrall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_G/0/1/0/all/0/1\">Gilles H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "Traditional deep learning algorithms often fail to generalize when they are\ntested outside of the domain of training data. Because data distributions can\nchange dynamically in real-life applications once a learned model is deployed,\nin this paper we are interested in single-source domain generalization (SDG)\nwhich aims to develop deep learning algorithms able to generalize from a single\ntraining domain where no information about the test domain is available at\ntraining time. Firstly, we design two simple MNISTbased SDG benchmarks, namely\nMNIST Color SDG-MP and MNIST Color SDG-UP, which highlight the two different\nfundamental SDG issues of increasing difficulties: 1) a class-correlated\npattern in the training domain is missing (SDG-MP), or 2) uncorrelated with the\nclass (SDG-UP), in the testing data domain. This is in sharp contrast with the\ncurrent domain generalization (DG) benchmarks which mix up different\ncorrelation and variation factors and thereby make hard to disentangle success\nor failure factors when benchmarking DG algorithms. We further evaluate several\nstate-of-the-art SDG algorithms through our simple benchmark, namely MNIST\nColor SDG-MP, and show that the issue SDG-MP is largely unsolved despite of a\ndecade of efforts in developing DG algorithms. Finally, we also propose a\npartially reversed contrastive loss to encourage intra-class diversity and find\nless strongly correlated patterns, to deal with SDG-MP and show that the\nproposed approach is very effective on our MNIST Color SDG-MP benchmark.",
          "link": "http://arxiv.org/abs/2106.07916",
          "publishedOn": "2021-06-16T01:21:06.047Z",
          "wordCount": 678,
          "title": "Encouraging Intra-Class Diversity Through a Reverse Contrastive Loss for Better Single-Source Domain Generalization. (arXiv:2106.07916v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Recently, different researchers have found that the gallery composition of a\nface database can induce performance differentials to facial identification\nsystems in which a probe image is compared against up to all stored reference\nimages to reach a biometric decision. This negative effect is referred to as\n\"watchlist imbalance effect\". In this work, we present a method to\ntheoretically estimate said effect for a biometric identification system given\nits verification performance across demographic groups and the composition of\nthe used gallery. Further, we report results for identification experiments on\ndifferently composed demographic subsets, i.e. females and males, of the public\nacademic MORPH database using the open-source ArcFace face recognition system.\nIt is shown that the database composition has a huge impact on performance\ndifferentials in biometric identification systems, even if performance\ndifferentials are less pronounced in the verification scenario. This study\nrepresents the first detailed analysis of the watchlist imbalance effect which\nis expected to be of high interest for future research in the field of facial\nrecognition.",
          "link": "http://arxiv.org/abs/2106.08049",
          "publishedOn": "2021-06-16T01:21:05.982Z",
          "wordCount": 608,
          "title": "Demographic Fairness in Face Identification: The Watchlist Imbalance Effect. (arXiv:2106.08049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08107",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Stucker_C/0/1/0/all/0/1\">Corinne Stucker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>",
          "description": "Modern optical satellite sensors enable high-resolution stereo reconstruction\nfrom space. But the challenging imaging conditions when observing the Earth\nfrom space push stereo matching to its limits. In practice, the resulting\ndigital surface models (DSMs) are fairly noisy and often do not attain the\naccuracy needed for high-resolution applications such as 3D city modeling.\nArguably, stereo correspondence based on low-level image similarity is\ninsufficient and should be complemented with a-priori knowledge about the\nexpected surface geometry beyond basic local smoothness. To that end, we\nintroduce ResDepth, a convolutional neural network that learns such an\nexpressive geometric prior from example data. ResDepth refines an initial, raw\nstereo DSM while conditioning the refinement on the images. I.e., it acts as a\nsmart, learned post-processing filter and can seamlessly complement any stereo\nmatching pipeline. In a series of experiments, we find that the proposed method\nconsistently improves stereo DSMs both quantitatively and qualitatively. We\nshow that the prior encoded in the network weights captures meaningful\ngeometric characteristics of urban design, which also generalize across\ndifferent districts and even from one city to another. Moreover, we demonstrate\nthat, by training on a variety of stereo pairs, ResDepth can acquire a\nsufficient degree of invariance against variations in imaging conditions and\nacquisition geometry.",
          "link": "http://arxiv.org/abs/2106.08107",
          "publishedOn": "2021-06-16T01:21:05.973Z",
          "wordCount": 651,
          "title": "ResDepth: A Deep Prior For 3D Reconstruction From High-resolution Satellite Images. (arXiv:2106.08107v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1\">Matthias Minderer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1\">Josip Djolonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1\">Rob Romijnders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubis_F/0/1/0/all/0/1\">Frances Hubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dustin Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lucic</a>",
          "description": "Accurate estimation of predictive uncertainty (model calibration) is\nessential for the safe application of neural networks. Many instances of\nmiscalibration in modern neural networks have been reported, suggesting a trend\nthat newer, more accurate models produce poorly calibrated predictions. Here,\nwe revisit this question for recent state-of-the-art image classification\nmodels. We systematically relate model calibration and accuracy, and find that\nthe most recent models, notably those not using convolutions, are among the\nbest calibrated. Trends observed in prior model generations, such as decay of\ncalibration with distribution shift or model size, are less pronounced in\nrecent architectures. We also show that model size and amount of pretraining do\nnot fully explain these differences, suggesting that architecture is a major\ndeterminant of calibration properties.",
          "link": "http://arxiv.org/abs/2106.07998",
          "publishedOn": "2021-06-16T01:21:05.905Z",
          "wordCount": 560,
          "title": "Revisiting the Calibration of Modern Neural Networks. (arXiv:2106.07998v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yanhao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Renwang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>",
          "description": "Non-parametric face modeling aims to reconstruct 3D face only from images\nwithout shape assumptions. While plausible facial details are predicted, the\nmodels tend to over-depend on local color appearance and suffer from ambiguous\nnoise. To address such problem, this paper presents a novel Learning to\nAggregate and Personalize (LAP) framework for unsupervised robust 3D face\nmodeling. Instead of using controlled environment, the proposed method\nimplicitly disentangles ID-consistent and scene-specific face from\nunconstrained photo set. Specifically, to learn ID-consistent face, LAP\nadaptively aggregates intrinsic face factors of an identity based on a novel\ncurriculum learning approach with relaxed consistency loss. To adapt the face\nfor a personalized scene, we propose a novel attribute-refining network to\nmodify ID-consistent face with target attribute and details. Based on the\nproposed method, we make unsupervised 3D face modeling benefit from meaningful\nimage facial structure and possibly higher resolutions. Extensive experiments\non benchmarks show LAP recovers superior or competitive face shape and texture,\ncompared with state-of-the-art (SOTA) methods with or without prior and\nsupervision.",
          "link": "http://arxiv.org/abs/2106.07852",
          "publishedOn": "2021-06-16T01:21:05.888Z",
          "wordCount": 626,
          "title": "Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection. (arXiv:2106.07852v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gondere_M/0/1/0/all/0/1\">Mesay Samuel Gondere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1\">Lars Schmidt-Thieme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Durga Prasad Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholz_R/0/1/0/all/0/1\">Randolf Scholz</a>",
          "description": "Handwritten digit recognition is one of the extensively studied area in\nmachine learning. Apart from the wider research on handwritten digit\nrecognition on MNIST dataset, there are many other research works on various\nscript recognition. However, it is not very common for multi-script digit\nrecognition which encourage the development of robust and multipurpose systems.\nAdditionally working on multi-script digit recognition enables multi-task\nlearning, considering the script classification as a related task for instance.\nIt is evident that multi-task learning improves model performance through\ninductive transfer using the information contained in related tasks. Therefore,\nin this study multi-script handwritten digit recognition using multi-task\nlearning will be investigated. As a specific case of demonstrating the solution\nto the problem, Amharic handwritten character recognition will also be\nexperimented. The handwritten digits of three scripts including Latin, Arabic\nand Kannada are studied to show that multi-task models with reformulation of\nthe individual tasks have shown promising results. In this study a novel way of\nusing the individual tasks predictions was proposed to help classification\nperformance and regularize the different loss for the purpose of the main task.\nThis finding has outperformed the baseline and the conventional multi-task\nlearning models. More importantly, it avoided the need for weighting the\ndifferent losses of the tasks, which is one of the challenges in multi-task\nlearning.",
          "link": "http://arxiv.org/abs/2106.08267",
          "publishedOn": "2021-06-16T01:21:05.878Z",
          "wordCount": 651,
          "title": "Multi-script Handwritten Digit Recognition Using Multi-task Learning. (arXiv:2106.08267v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bear_D/0/1/0/all/0/1\">Daniel M. Bear</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Elias Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mrowca_D/0/1/0/all/0/1\">Damian Mrowca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_F/0/1/0/all/0/1\">Felix J. Binder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1\">Hsiau-Yu Fish Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramod_R/0/1/0/all/0/1\">R.T. Pramod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holdaway_C/0/1/0/all/0/1\">Cameron Holdaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Sirui Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanwisher_N/0/1/0/all/0/1\">Nancy Kanwisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1\">Daniel L.K. Yamins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Judith E. Fan</a>",
          "description": "While machine learning algorithms excel at many challenging visual tasks, it\nis unclear that they can make predictions about commonplace real world physical\nevents. Here, we present a visual and physical prediction benchmark that\nprecisely measures this capability. In realistically simulating a wide variety\nof physical phenomena -- rigid and soft-body collisions, stable multi-object\nconfigurations, rolling and sliding, projectile motion -- our dataset presents\na more comprehensive challenge than existing benchmarks. Moreover, we have\ncollected human responses for our stimuli so that model predictions can be\ndirectly compared to human judgments. We compare an array of algorithms --\nvarying in their architecture, learning objective, input-output structure, and\ntraining data -- on their ability to make diverse physical predictions. We find\nthat graph neural networks with access to the physical state best capture human\nbehavior, whereas among models that receive only visual input, those with\nobject-centric representations or pretraining do best but fall far short of\nhuman accuracy. This suggests that extracting physically meaningful\nrepresentations of scenes is the main bottleneck to achieving human-like visual\nprediction. We thus demonstrate how our benchmark can identify areas for\nimprovement and measure progress on this key aspect of physical understanding.",
          "link": "http://arxiv.org/abs/2106.08261",
          "publishedOn": "2021-06-16T01:21:05.871Z",
          "wordCount": 664,
          "title": "Physion: Evaluating Physical Prediction from Vision in Humans and Machines. (arXiv:2106.08261v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07879",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arefeen_M/0/1/0/all/0/1\">Md Adnan Arefeen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nimi_S/0/1/0/all/0/1\">Sumaiya Tabassum Nimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uddin_M/0/1/0/all/0/1\">Md Yusuf Sarwar Uddin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhu Li</a>",
          "description": "In this paper, we propose a transfer-learning based model construction\ntechnique for the aerial scene classification problem. The core of our\ntechnique is a layer selection strategy, named ReLU-Based Feature Fusion\n(RBFF), that extracts feature maps from a pretrained CNN-based single-object\nimage classification model, namely MobileNetV2, and constructs a model for the\naerial scene classification task. RBFF stacks features extracted from the batch\nnormalization layer of a few selected blocks of MobileNetV2, where the\ncandidate blocks are selected based on the characteristics of the ReLU\nactivation layers present in those blocks. The feature vector is then\ncompressed into a low-dimensional feature space using dimension reduction\nalgorithms on which we train a low-cost SVM classifier for the classification\nof the aerial images. We validate our choice of selected features based on the\nsignificance of the extracted features with respect to our classification\npipeline. RBFF remarkably does not involve any training of the base CNN model\nexcept for a few parameters for the classifier, which makes the technique very\ncost-effective for practical deployments. The constructed model despite being\nlightweight outperforms several recently proposed models in terms of accuracy\nfor a number of aerial scene datasets.",
          "link": "http://arxiv.org/abs/2106.07879",
          "publishedOn": "2021-06-16T01:21:05.863Z",
          "wordCount": 647,
          "title": "A Lightweight ReLU-Based Feature Fusion for Aerial Scene Classification. (arXiv:2106.07879v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "We introduce a self-supervised vision representation model BEiT, which stands\nfor Bidirectional Encoder representation from Image Transformers. Following\nBERT developed in the natural language processing area, we propose a masked\nimage modeling task to pretrain vision Transformers. Specifically, each image\nhas two views in our pre-training, i.e, image patches (such as 16x16 pixels),\nand visual tokens (i.e., discrete tokens). We first \"tokenize\" the original\nimage into visual tokens. Then we randomly mask some image patches and fed them\ninto the backbone Transformer. The pre-training objective is to recover the\noriginal visual tokens based on the corrupted image patches. After pre-training\nBEiT, we directly fine-tune the model parameters on downstream tasks by\nappending task layers upon the pretrained encoder. Experimental results on\nimage classification and semantic segmentation show that our model achieves\ncompetitive results with previous pre-training methods. For example, base-size\nBEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming\nfrom-scratch DeiT training (81.8%) with the same setup. Moreover, large-size\nBEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with\nsupervised pre-training on ImageNet-22K (85.2%). The code and pretrained models\nare available at https://aka.ms/beit.",
          "link": "http://arxiv.org/abs/2106.08254",
          "publishedOn": "2021-06-16T01:21:05.844Z",
          "wordCount": 625,
          "title": "BEiT: BERT Pre-Training of Image Transformers. (arXiv:2106.08254v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1\">Arsenii Ashukha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1\">Andrei Atanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Averaging predictions over a set of models -- an ensemble -- is widely used\nto improve predictive performance and uncertainty estimation of deep learning\nmodels. At the same time, many machine learning systems, such as search,\nmatching, and recommendation systems, heavily rely on embeddings.\nUnfortunately, due to misalignment of features of independently trained models,\nembeddings, cannot be improved with a naive deep ensemble like approach. In\nthis work, we look at the ensembling of representations and propose mean\nembeddings with test-time augmentation (MeTTA) simple yet well-performing\nrecipe for ensembling representations. Empirically we demonstrate that MeTTA\nsignificantly boosts the quality of linear evaluation on ImageNet for both\nsupervised and self-supervised models. Even more exciting, we draw connections\nbetween MeTTA, image retrieval, and transformation invariant models. We believe\nthat spreading the success of ensembles to inference higher-quality\nrepresentations is the important step that will open many new applications of\nensembling.",
          "link": "http://arxiv.org/abs/2106.08038",
          "publishedOn": "2021-06-16T01:21:05.837Z",
          "wordCount": 580,
          "title": "Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaemoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1\">Changyeon Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1\">Jeongwoo Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungjoo Kang</a>",
          "description": "Out-of-distribution (OOD) detection is an important task in machine learning\nsystems for ensuring their reliability and safety. Deep probabilistic\ngenerative models facilitate OOD detection by estimating the likelihood of a\ndata sample. However, such models frequently assign a suspiciously high\nlikelihood to a specific outlier. Several recent works have addressed this\nissue by training a neural network with auxiliary outliers, which are generated\nby perturbing the input data. In this paper, we discover that these approaches\nfail for certain OOD datasets. Thus, we suggest a new detection metric that\noperates without outlier exposure. We observe that our metric is robust to\ndiverse variations of an image compared to the previous outlier-exposing\nmethods. Furthermore, our proposed score requires neither auxiliary models nor\nadditional training. Instead, this paper utilizes the likelihood ratio\nstatistic in a new perspective to extract genuine properties from the given\nsingle deep probabilistic generative model. We also apply a novel numerical\napproximation to enable fast implementation. Finally, we demonstrate\ncomprehensive experiments on various probabilistic generative models and show\nthat our method achieves state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.07903",
          "publishedOn": "2021-06-16T01:21:05.830Z",
          "wordCount": 612,
          "title": "Robust Out-of-Distribution Detection on Deep Probabilistic Generative Models. (arXiv:2106.07903v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1\">Karsten Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pemula_L/0/1/0/all/0/1\">Latha Pemula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zepeda_J/0/1/0/all/0/1\">Joaquin Zepeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>",
          "description": "Being able to spot defective parts is a critical component in large-scale\nindustrial manufacturing. A particular challenge that we address in this work\nis the cold-start problem: fit a model using nominal (non-defective) example\nimages only. While handcrafted solutions per class are possible, the goal is to\nbuild systems that work well simultaneously on many different tasks\nautomatically. The best peforming approaches combine embeddings from ImageNet\nmodels with an outlier detection model. In this paper, we extend on this line\nof work and propose PatchCore, which uses a maximally representative memory\nbank of nominal patch-features. PatchCore offers competitive inference times\nwhile achieving state-of-the-art performance for both detection and\nlocalization. On the standard dataset MVTec AD, PatchCore achieves an\nimage-level anomaly detection AUROC score of $99.1\\%$, more than halving the\nerror compared to the next best competitor. We further report competitive\nresults on two additional datasets and also find competitive results in the few\nsamples regime.",
          "link": "http://arxiv.org/abs/2106.08265",
          "publishedOn": "2021-06-16T01:21:05.822Z",
          "wordCount": 587,
          "title": "Towards Total Recall in Industrial Anomaly Detection. (arXiv:2106.08265v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hofer_T/0/1/0/all/0/1\">Timon H&#xf6;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsafar_F/0/1/0/all/0/1\">Faranak Shamsafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benbarka_N/0/1/0/all/0/1\">Nuri Benbarka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>",
          "description": "Bin picking is a core problem in industrial environments and robotics, with\nits main module as 6D pose estimation. However, industrial depth sensors have a\nlack of accuracy when it comes to small objects. Therefore, we propose a\nframework for pose estimation in highly cluttered scenes with small objects,\nwhich mainly relies on RGB data and makes use of depth information only for\npose refinement. In this work, we compare synthetic data generation approaches\nfor object detection and pose estimation and introduce a pose filtering\nalgorithm that determines the most accurate estimated poses. We will make our",
          "link": "http://arxiv.org/abs/2106.08045",
          "publishedOn": "2021-06-16T01:21:05.815Z",
          "wordCount": 545,
          "title": "Object detection and Autoencoder-based 6D pose estimation for highly cluttered Bin Picking. (arXiv:2106.08045v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08176",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wood_D/0/1/0/all/0/1\">David A. Wood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kafiabadi_S/0/1/0/all/0/1\">Sina Kafiabadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Busaidi_A/0/1/0/all/0/1\">Ayisha Al Busaidi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guilhem_E/0/1/0/all/0/1\">Emily Guilhem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montvila_A/0/1/0/all/0/1\">Antanas Montvila</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1\">Siddharth Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lynch_J/0/1/0/all/0/1\">Jeremy Lynch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Townend_M/0/1/0/all/0/1\">Matthew Townend</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barker_G/0/1/0/all/0/1\">Gareth Barker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cole_J/0/1/0/all/0/1\">James H. Cole</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Booth_T/0/1/0/all/0/1\">Thomas C. Booth</a>",
          "description": "The growing demand for head magnetic resonance imaging (MRI) examinations,\nalong with a global shortage of radiologists, has led to an increase in the\ntime taken to report head MRI scans around the world. For many neurological\nconditions, this delay can result in increased morbidity and mortality. An\nautomated triaging tool could reduce reporting times for abnormal examinations\nby identifying abnormalities at the time of imaging and prioritizing the\nreporting of these scans. In this work, we present a convolutional neural\nnetwork for detecting clinically-relevant abnormalities in\n$\\text{T}_2$-weighted head MRI scans. Using a validated neuroradiology report\nclassifier, we generated a labelled dataset of 43,754 scans from two large UK\nhospitals for model training, and demonstrate accurate classification (area\nunder the receiver operating curve (AUC) = 0.943) on a test set of 800 scans\nlabelled by a team of neuroradiologists. Importantly, when trained on scans\nfrom only a single hospital the model generalized to scans from the other\nhospital ($\\Delta$AUC $\\leq$ 0.02). A simulation study demonstrated that our\nmodel would reduce the mean reporting time for abnormal examinations from 28\ndays to 14 days and from 9 days to 5 days at the two hospitals, demonstrating\nfeasibility for use in a clinical triage environment.",
          "link": "http://arxiv.org/abs/2106.08176",
          "publishedOn": "2021-06-16T01:21:05.796Z",
          "wordCount": 680,
          "title": "Automated triaging of head MRI examinations using convolutional neural networks. (arXiv:2106.08176v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiangnan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zehua Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "Although much progress has been made recently in 3D face reconstruction, most\nprevious work has been devoted to predicting accurate and fine-grained 3D\nshapes. In contrast, relatively little work has focused on generating\nhigh-fidelity face textures. Compared with the prosperity of photo-realistic 2D\nface image generation, high-fidelity 3D face texture generation has yet to be\nstudied. In this paper, we proposed a novel UV map generation model that\npredicts the UV map from a single face image. The model consists of a UV\nsampler and a UV generator. By selectively sampling the input face image's\npixels and adjusting their relative locations, the UV sampler generates an\nincomplete UV map that could faithfully reconstruct the original face. Missing\ntextures in the incomplete UV map are further full-filled by the UV generator.\nThe training is based on pseudo ground truth blended by the 3DMM texture and\nthe input face texture, thus weakly supervised. To deal with the artifacts in\nthe imperfect pseudo UV map, multiple partial UV map discriminators are\nleveraged.",
          "link": "http://arxiv.org/abs/2106.08148",
          "publishedOn": "2021-06-16T01:21:05.789Z",
          "wordCount": 601,
          "title": "Weakly-Supervised Photo-realistic Texture Generation for 3D Face Reconstruction. (arXiv:2106.08148v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08174",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Avisdris_N/0/1/0/all/0/1\">Netanell Avisdris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yehuda_B/0/1/0/all/0/1\">Bossmat Yehuda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ben_Zvi_O/0/1/0/all/0/1\">Ori Ben-Zvi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Link_Sourani_D/0/1/0/all/0/1\">Daphna Link-Sourani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ben_Sira_L/0/1/0/all/0/1\">Liat Ben-Sira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miller_E/0/1/0/all/0/1\">Elka Miller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zharkov_E/0/1/0/all/0/1\">Elena Zharkov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bashat_D/0/1/0/all/0/1\">Dafna Ben Bashat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joskowicz_L/0/1/0/all/0/1\">Leo Joskowicz</a>",
          "description": "Timely, accurate and reliable assessment of fetal brain development is\nessential to reduce short and long-term risks to fetus and mother. Fetal MRI is\nincreasingly used for fetal brain assessment. Three key biometric linear\nmeasurements important for fetal brain evaluation are Cerebral Biparietal\nDiameter (CBD), Bone Biparietal Diameter (BBD), and Trans-Cerebellum Diameter\n(TCD), obtained manually by expert radiologists on reference slices, which is\ntime consuming and prone to human error. The aim of this study was to develop a\nfully automatic method computing the CBD, BBD and TCD measurements from fetal\nbrain MRI. The input is fetal brain MRI volumes which may include the fetal\nbody and the mother's abdomen. The outputs are the measurement values and\nreference slices on which the measurements were computed. The method, which\nfollows the manual measurements principle, consists of five stages: 1)\ncomputation of a Region Of Interest that includes the fetal brain with an\nanisotropic 3D U-Net classifier; 2) reference slice selection with a\nConvolutional Neural Network; 3) slice-wise fetal brain structures segmentation\nwith a multiclass U-Net classifier; 4) computation of the fetal brain\nmidsagittal line and fetal brain orientation, and; 5) computation of the\nmeasurements. Experimental results on 214 volumes for CBD, BBD and TCD\nmeasurements yielded a mean $L_1$ difference of 1.55mm, 1.45mm and 1.23mm\nrespectively, and a Bland-Altman 95% confidence interval ($CI_{95}$) of 3.92mm,\n3.98mm and 2.25mm respectively. These results are similar to the manual\ninter-observer variability. The proposed automatic method for computing\nbiometric linear measurements of the fetal brain from MR imaging achieves human\nlevel performance. It has the potential of being a useful method for the\nassessment of fetal brain biometry in normal and pathological cases, and of\nimproving routine clinical practice.",
          "link": "http://arxiv.org/abs/2106.08174",
          "publishedOn": "2021-06-16T01:21:05.781Z",
          "wordCount": 755,
          "title": "Automatic linear measurements of the fetal brain on MRI with deep neural networks. (arXiv:2106.08174v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1\">Mohit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrotra_P/0/1/0/all/0/1\">Pragyan Mehrotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rajesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>",
          "description": "Previous studies have demonstrated that commonly studied (vanilla)\ntouch-based continuous authentication systems (V-TCAS) are susceptible to\npopulation attack. This paper proposes a novel Generative Adversarial Network\nassisted TCAS (G-TCAS) framework, which showed more resilience to the\npopulation attack. G-TCAS framework was tested on a dataset of 117 users who\ninteracted with a smartphone and tablet pair. On average, the increase in the\nfalse accept rates (FARs) for V-TCAS was much higher (22%) than G-TCAS (13%)\nfor the smartphone. Likewise, the increase in the FARs for V-TCAS was 25%\ncompared to G-TCAS (6%) for the tablet.",
          "link": "http://arxiv.org/abs/2106.07867",
          "publishedOn": "2021-06-16T01:21:05.772Z",
          "wordCount": 554,
          "title": "Defending Touch-based Continuous Authentication Systems from Active Adversaries Using Generative Adversarial Networks. (arXiv:2106.07867v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Sen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yidan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiping Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jonathan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>",
          "description": "We present a novel direction-aware feature-level frequency decomposition\nnetwork for single image deraining. Compared with existing solutions, the\nproposed network has three compelling characteristics. First, unlike previous\nalgorithms, we propose to perform frequency decomposition at feature-level\ninstead of image-level, allowing both low-frequency maps containing structures\nand high-frequency maps containing details to be continuously refined during\nthe training procedure. Second, we further establish communication channels\nbetween low-frequency maps and high-frequency maps to interactively capture\nstructures from high-frequency maps and add them back to low-frequency maps\nand, simultaneously, extract details from low-frequency maps and send them back\nto high-frequency maps, thereby removing rain streaks while preserving more\ndelicate features in the input image. Third, different from existing algorithms\nusing convolutional filters consistent in all directions, we propose a\ndirection-aware filter to capture the direction of rain streaks in order to\nmore effectively and thoroughly purge the input images of rain streaks. We\nextensively evaluate the proposed approach in three representative datasets and\nexperimental results corroborate our approach consistently outperforms\nstate-of-the-art deraining algorithms.",
          "link": "http://arxiv.org/abs/2106.07941",
          "publishedOn": "2021-06-16T01:21:05.759Z",
          "wordCount": 607,
          "title": "Direction-aware Feature-level Frequency Decomposition for Single Image Deraining. (arXiv:2106.07941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingkun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Guang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>",
          "description": "Unsupervised person re-identification (Re-ID) aims to match pedestrian images\nfrom different camera views in unsupervised setting. Existing methods for\nunsupervised person Re-ID are usually built upon the pseudo labels from\nclustering. However, the quality of clustering depends heavily on the quality\nof the learned features, which are overwhelmingly dominated by the colors in\nimages especially in the unsupervised setting. In this paper, we propose a\nCluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised\nperson Re-ID, in which cluster structure is leveraged to guide the feature\nlearning in a properly designed asymmetric contrastive learning framework. To\nbe specific, we propose a novel cluster-level contrastive loss to help the\nsiamese network effectively mine the invariance in feature learning with\nrespect to the cluster structure within and between different data augmentation\nviews, respectively. Extensive experiments conducted on three benchmark\ndatasets demonstrate superior performance of our proposal.",
          "link": "http://arxiv.org/abs/2106.07846",
          "publishedOn": "2021-06-16T01:21:05.751Z",
          "wordCount": 570,
          "title": "Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification. (arXiv:2106.07846v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07991",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">Xuan Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoming Yuan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zeng_S/0/1/0/all/0/1\">Shangzhi Zeng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>",
          "description": "Bi-level optimization model is able to capture a wide range of complex\nlearning tasks with practical interest. Due to the witnessed efficiency in\nsolving bi-level programs, gradient-based methods have gained popularity in the\nmachine learning community. In this work, we propose a new gradient-based\nsolution scheme, namely, the Bi-level Value-Function-based Interior-point\nMethod (BVFIM). Following the main idea of the log-barrier interior-point\nscheme, we penalize the regularized value function of the lower level problem\ninto the upper level objective. By further solving a sequence of differentiable\nunconstrained approximation problems, we consequently derive a sequential\nprogramming scheme. The numerical advantage of our scheme relies on the fact\nthat, when gradient methods are applied to solve the approximation problem, we\nsuccessfully avoid computing any expensive Hessian-vector or Jacobian-vector\nproduct. We prove the convergence without requiring any convexity assumption on\neither the upper level or the lower level objective. Experiments demonstrate\nthe efficiency of the proposed BVFIM on non-convex bi-level problems.",
          "link": "http://arxiv.org/abs/2106.07991",
          "publishedOn": "2021-06-16T01:21:05.732Z",
          "wordCount": 600,
          "title": "A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization. (arXiv:2106.07991v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Prasen Kumar Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bisht_I/0/1/0/all/0/1\">Ira Bisht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sur_A/0/1/0/all/0/1\">Arijit Sur</a>",
          "description": "Underwater images, in general, suffer from low contrast and high color\ndistortions due to the non-uniform attenuation of the light as it propagates\nthrough the water. In addition, the degree of attenuation varies with the\nwavelength resulting in the asymmetric traversing of colors. Despite the\nprolific works for underwater image restoration (UIR) using deep learning, the\nabove asymmetricity has not been addressed in the respective network\nengineering. As the first novelty, this paper shows that attributing the right\nreceptive field size (context) based on the traversing range of the color\nchannel may lead to a substantial performance gain for the task of UIR.\nFurther, it is important to suppress the irrelevant multi-contextual features\nand increase the representational power of the model. Therefore, as a second\nnovelty, we have incorporated an attentive skip mechanism to adaptively refine\nthe learned multi-contextual features. The proposed framework, called Deep\nWaveNet, is optimized using the traditional pixel-wise and feature-based cost\nfunctions. An extensive set of experiments have been carried out to show the\nefficacy of the proposed scheme over existing best-published literature on\nbenchmark datasets. More importantly, we have demonstrated a comprehensive\nvalidation of enhanced images across various high-level vision tasks, e.g.,\nunderwater image semantic segmentation, and diver's 2D pose estimation. A\nsample video to exhibit our real-world performance is available at\n\\url{https://www.youtube.com/watch?v=8qtuegBdfac}.",
          "link": "http://arxiv.org/abs/2106.07910",
          "publishedOn": "2021-06-16T01:21:05.725Z",
          "wordCount": 687,
          "title": "Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration. (arXiv:2106.07910v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08208",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Adaptive gradient methods have shown excellent performance for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using specific adaptive learning rates. It\nis desired to design a universal framework for practical algorithms of adaptive\ngradients with theoretical guarantee to solve general problems. To fill this\ngap, we propose a faster and universal framework of adaptive gradients (i.e.,\nSUPER-ADAM) by introducing a universal adaptive matrix that includes most\nexisting adaptive gradient forms. Moreover, our framework can flexibly\nintegrates the momentum and variance reduced techniques. In particular, our\nnovel framework provides the convergence analysis support for adaptive gradient\nmethods under the nonconvex setting. In theoretical analysis, we prove that our\nnew algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms.",
          "link": "http://arxiv.org/abs/2106.08208",
          "publishedOn": "2021-06-16T01:21:05.665Z",
          "wordCount": 616,
          "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chengzeng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hau_Z/0/1/0/all/0/1\">Zhongyuan Hau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demetriou_S/0/1/0/all/0/1\">Soteris Demetriou</a>",
          "description": "LiDAR sensors are used widely in Autonomous Vehicles for better perceiving\nthe environment which enables safer driving decisions. Recent work has\ndemonstrated serious LiDAR spoofing attacks with alarming consequences. In\nparticular, model-level LiDAR spoofing attacks aim to inject fake depth\nmeasurements to elicit ghost objects that are erroneously detected by 3D Object\nDetectors, resulting in hazardous driving decisions. In this work, we explore\nthe use of motion as a physical invariant of genuine objects for detecting such\nattacks. Based on this, we propose a general methodology, 3D Temporal\nConsistency Check (3D-TC2), which leverages spatio-temporal information from\nmotion prediction to verify objects detected by 3D Object Detectors. Our\npreliminary design and implementation of a 3D-TC2 prototype demonstrates very\npromising performance, providing more than 98% attack detection rate with a\nrecall of 91% for detecting spoofed Vehicle (Car) objects, and is able to\nachieve real-time detection at 41Hz",
          "link": "http://arxiv.org/abs/2106.07833",
          "publishedOn": "2021-06-16T01:21:05.654Z",
          "wordCount": 600,
          "title": "Temporal Consistency Checks to Detect LiDAR Spoofing Attacks on Autonomous Vehicle Perception. (arXiv:2106.07833v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Czolbe_S/0/1/0/all/0/1\">Steffen Czolbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>",
          "description": "Geometric alignment appears in a variety of applications, ranging from domain\nadaptation, optimal transport, and normalizing flows in machine learning;\noptical flow and learned augmentation in computer vision and deformable\nregistration within biomedical imaging. A recurring challenge is the alignment\nof domains whose topology is not the same; a problem that is routinely ignored,\npotentially introducing bias in downstream analysis. As a first step towards\nsolving such alignment problems, we propose an unsupervised topological\ndifference detection algorithm. The model is based on a conditional variational\nauto-encoder and detects topological anomalies with regards to a reference\nalongside the registration step. We consider both a) topological changes in the\nimage under spatial variation and b) unexpected transformations. Our approach\nis validated on a proxy task of unsupervised anomaly detection in images.",
          "link": "http://arxiv.org/abs/2106.08233",
          "publishedOn": "2021-06-16T01:21:05.635Z",
          "wordCount": 573,
          "title": "Spot the Difference: Topological Anomaly Detection via Geometric Alignment. (arXiv:2106.08233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Efe_U/0/1/0/all/0/1\">Ufuk Efe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ince_K/0/1/0/all/0/1\">Kutalmis Gokalp Ince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1\">A. Aydin Alatan</a>",
          "description": "A novel image matching method is proposed that utilizes learned features\nextracted by an off-the-shelf deep neural network to obtain a promising\nperformance. The proposed method uses pre-trained VGG architecture as a feature\nextractor and does not require any additional training specific to improve\nmatching. Inspired by well-established concepts in the psychology area, such as\nthe Mental Rotation paradigm, an initial warping is performed as a result of a\npreliminary geometric transformation estimate. These estimates are simply based\non dense matching of nearest neighbors at the terminal layer of VGG network\noutputs of the images to be matched. After this initial alignment, the same\napproach is repeated again between reference and aligned images in a\nhierarchical manner to reach a good localization and matching performance. Our\nalgorithm achieves 0.57 and 0.80 overall scores in terms of Mean Matching\nAccuracy (MMA) for 1 pixel and 2 pixels thresholds respectively on Hpatches\ndataset, which indicates a better performance than the state-of-the-art.",
          "link": "http://arxiv.org/abs/2106.07791",
          "publishedOn": "2021-06-16T01:21:05.612Z",
          "wordCount": 603,
          "title": "DFM: A Performance Baseline for Deep Feature Matching. (arXiv:2106.07791v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1\">Dung Anh Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>",
          "description": "Virtually all aspects of modern life depend on space technology. Thanks to\nthe great advancement of computer vision in general and deep learning-based\ntechniques in particular, over the decades, the world witnessed the growing use\nof deep learning in solving problems for space applications, such as\nself-driving robot, tracers, insect-like robot on cosmos and health monitoring\nof spacecraft. These are just some prominent examples that has advanced space\nindustry with the help of deep learning. However, the success of deep learning\nmodels requires a lot of training data in order to have decent performance,\nwhile on the other hand, there are very limited amount of publicly available\nspace datasets for the training of deep learning models. Currently, there is no\npublic datasets for space-based object detection or instance segmentation,\npartly because manually annotating object segmentation masks is very time\nconsuming as they require pixel-level labelling, not to mention the challenge\nof obtaining images from space. In this paper, we aim to fill this gap by\nreleasing a dataset for spacecraft detection, instance segmentation and part\nrecognition. The main contribution of this work is the development of the\ndataset using images of space stations and satellites, with rich annotations\nincluding bounding boxes of spacecrafts and masks to the level of object parts,\nwhich are obtained with a mixture of automatic processes and manual efforts. We\nalso provide evaluations with state-of-the-art methods in object detection and\ninstance segmentation as a benchmark for the dataset. The link for downloading\nthe proposed dataset can be found on\nhttps://github.com/Yurushia1998/SatelliteDataset.",
          "link": "http://arxiv.org/abs/2106.08186",
          "publishedOn": "2021-06-16T01:21:05.585Z",
          "wordCount": 687,
          "title": "A Spacecraft Dataset for Detection, Segmentation and Parts Recognition. (arXiv:2106.08186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We study transfer learning in the presence of spurious correlations. We\nexperimentally demonstrate that directly transferring the stable feature\nextractor learned on the source task may not eliminate these biases for the\ntarget task. However, we hypothesize that the unstable features in the source\ntask and those in the target task are directly related. By explicitly informing\nthe target classifier of the source task's unstable features, we can regularize\nthe biases in the target task. Specifically, we derive a representation that\nencodes the unstable features by contrasting different data environments in the\nsource task. On the target task, we cluster data from this representation, and\nachieve robustness by minimizing the worst-case risk across all clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask, outperforming the best baseline by 22.9% in absolute accuracy across 12\ntransfer settings. Our code is available at https://github.com/YujiaBao/Tofu.",
          "link": "http://arxiv.org/abs/2106.07847",
          "publishedOn": "2021-06-16T01:21:05.578Z",
          "wordCount": 601,
          "title": "Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08147",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_D/0/1/0/all/0/1\">Di Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Afonso_M/0/1/0/all/0/1\">Mariana Afonso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1\">David R. Bull</a>",
          "description": "Spatial resolution adaptation is a technique which has often been employed in\nvideo compression to enhance coding efficiency. This approach encodes a lower\nresolution version of the input video and reconstructs the original resolution\nduring decoding. Instead of using conventional up-sampling filters, recent work\nhas employed advanced super-resolution methods based on convolutional neural\nnetworks (CNNs) to further improve reconstruction quality. These approaches are\nusually trained to minimise pixel-based losses such as Mean-Squared Error\n(MSE), despite the fact that this type of loss metric does not correlate well\nwith subjective opinions. In this paper, a perceptually-inspired\nsuper-resolution approach (M-SRGAN) is proposed for spatial up-sampling of\ncompressed video using a modified CNN model, which has been trained using a\ngenerative adversarial network (GAN) on compressed content with perceptual loss\nfunctions. The proposed method was integrated with HEVC HM 16.20, and has been\nevaluated on the JVET Common Test Conditions (UHD test sequences) using the\nRandom Access configuration. The results show evident perceptual quality\nimprovement over the original HM 16.20, with an average bitrate saving of 35.6%\n(Bj{\\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF.",
          "link": "http://arxiv.org/abs/2106.08147",
          "publishedOn": "2021-06-16T01:21:05.563Z",
          "wordCount": 624,
          "title": "Perceptually-inspired super-resolution of compressed videos. (arXiv:2106.08147v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiu-Shen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "One single instance could possess multiple portraits and reveal diverse\nrelationships with others according to different contexts. Those ambiguities\nincrease the difficulty of learning a generalizable model when there exists one\nconcept or mixed concepts in a task. We propose a general approach Learning to\nDecompose Network (LeadNet) for both two cases, which contextualizes a model\nthrough meta-learning multiple maps for concepts discovery -- the\nrepresentations of instances are decomposed and adapted conditioned on the\ncontexts. Through taking a holistic view over multiple latent components over\ninstances in a sampled pseudo task, LeadNet learns to automatically select the\nright concept via incorporating those rich semantics inside and between\nobjects. LeadNet demonstrates its superiority in various applications,\nincluding exploring multiple views of confusing tasks, out-of-distribution\nrecognition, and few-shot image classification.",
          "link": "http://arxiv.org/abs/2106.08112",
          "publishedOn": "2021-06-16T01:21:05.552Z",
          "wordCount": 562,
          "title": "Contextualizing Multiple Tasks via Learning to Decompose. (arXiv:2106.08112v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yufei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkhir_N/0/1/0/all/0/1\">Nacim Belkhir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angulo_J/0/1/0/all/0/1\">Jesus Angulo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>",
          "description": "Deep Neural Networks (DNNs) are generated by sequentially performing linear\nand non-linear processes. Using a combination of linear and non-linear\nprocedures is critical for generating a sufficiently deep feature space. The\nmajority of non-linear operators are derivations of activation functions or\npooling functions. Mathematical morphology is a branch of mathematics that\nprovides non-linear operators for a variety of image processing problems. We\ninvestigate the utility of integrating these operations in an end-to-end deep\nlearning framework in this paper. DNNs are designed to acquire a realistic\nrepresentation for a particular job. Morphological operators give topological\ndescriptors that convey salient information about the shapes of objects\ndepicted in images. We propose a method based on meta-learning to incorporate\nmorphological operators into DNNs. The learned architecture demonstrates how\nour novel morphological operations significantly increase DNN performance on\nvarious tasks, including picture classification and edge detection.",
          "link": "http://arxiv.org/abs/2106.07714",
          "publishedOn": "2021-06-16T01:21:05.545Z",
          "wordCount": 584,
          "title": "Learning Deep Morphological Networks with Neural Architecture Search. (arXiv:2106.07714v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McNeely_White_D/0/1/0/all/0/1\">David McNeely-White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattelberg_B/0/1/0/all/0/1\">Ben Sattelberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanchard_N/0/1/0/all/0/1\">Nathaniel Blanchard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beveridge_R/0/1/0/all/0/1\">Ross Beveridge</a>",
          "description": "We present evidence that many common convolutional neural networks (CNNs)\ntrained for face verification learn functions that are nearly equivalent under\nrotation. More specifically, we demonstrate that one face verification model's\nembeddings (i.e. last--layer activations) can be compared directly to another\nmodel's embeddings after only a rotation or linear transformation, with little\nperformance penalty. This finding is demonstrated using IJB-C 1:1 verification\nacross the combinations of ten modern off-the-shelf CNN-based face verification\nmodels which vary in training dataset, CNN architecture, way of using angular\nloss, or some combination of the 3, and achieve a mean true accept rate of 0.96\nat a false accept rate of 0.01. When instead evaluating embeddings generated\nfrom two CNNs, where one CNN's embeddings are mapped with a linear\ntransformation, the mean true accept rate drops to 0.95 using the same\nverification paradigm. Restricting these linear maps to only perform rotation\nproduces a mean true accept rate of 0.91. These mappings' existence suggests\nthat a common representation is learned by models with variation in training or\nstructure. A discovery such as this likely has broad implications, and we\nprovide an application in which face embeddings can be de-anonymized using a\nlimited number of samples.",
          "link": "http://arxiv.org/abs/2106.07822",
          "publishedOn": "2021-06-16T01:21:05.524Z",
          "wordCount": 630,
          "title": "Canonical Face Embeddings. (arXiv:2106.07822v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boney_R/0/1/0/all/0/1\">Rinu Boney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1\">Alexander Ilin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1\">Juho Kannala</a>",
          "description": "In many control problems that include vision, optimal controls can be\ninferred from the location of the objects in the scene. This information can be\nrepresented using keypoints, which is a list of spatial locations in the input\nimage. Previous works show that keypoint representations learned during\nunsupervised pre-training using encoder-decoder architectures can provide good\nfeatures for control tasks. In this paper, we show that it is possible to learn\nefficient keypoint representations end-to-end, without the need for\nunsupervised pre-training, decoders, or additional losses. Our proposed\narchitecture consists of a differentiable keypoint extractor that feeds the\ncoordinates of the estimated keypoints directly to a soft actor-critic agent.\nThe proposed algorithm yields performance competitive to the state-of-the art\non DeepMind Control Suite tasks.",
          "link": "http://arxiv.org/abs/2106.07995",
          "publishedOn": "2021-06-16T01:21:05.477Z",
          "wordCount": 558,
          "title": "End-to-End Learning of Keypoint Representations for Continuous Control from Images. (arXiv:2106.07995v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rizzo_M/0/1/0/all/0/1\">Matteo Rizzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conati_C/0/1/0/all/0/1\">Cristina Conati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1\">Daesik Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hui Hu</a>",
          "description": "Computational Colour Constancy (CCC) consists of estimating the colour of one\nor more illuminants in a scene and using them to remove unwanted chromatic\ndistortions. Much research has focused on illuminant estimation for CCC on\nsingle images, with few attempts of leveraging the temporal information\nintrinsic in sequences of correlated images (e.g., the frames in a video), a\ntask known as Temporal Colour Constancy (TCC). The state-of-the-art for TCC is\nTCCNet, a deep-learning architecture that uses a ConvLSTM for aggregating the\nencodings produced by CNN submodules for each image in a sequence. We extend\nthis architecture with different models obtained by (i) substituting the TCCNet\nsubmodules with C4, the state-of-the-art method for CCC targeting images; (ii)\nadding a cascading strategy to perform an iterative improvement of the estimate\nof the illuminant. We tested our models on the recently released TCC benchmark\nand achieved results that surpass the state-of-the-art. Analyzing the impact of\nthe number of frames involved in illuminant estimation on performance, we show\nthat it is possible to reduce inference time by training the models on few\nselected frames from the sequences while retaining comparable accuracy.",
          "link": "http://arxiv.org/abs/2106.07955",
          "publishedOn": "2021-06-16T01:21:05.462Z",
          "wordCount": 614,
          "title": "Cascading Convolutional Temporal Colour Constancy. (arXiv:2106.07955v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1\">Ashraful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radke_R/0/1/0/all/0/1\">Richard J. Radke</a>",
          "description": "Most existing works in few-shot learning rely on meta-learning the network on\na large base dataset which is typically from the same domain as the target\ndataset. We tackle the problem of cross-domain few-shot learning where there is\na large shift between the base and target domain. The problem of cross-domain\nfew-shot recognition with unlabeled target data is largely unaddressed in the\nliterature. STARTUP was the first method that tackles this problem using\nself-training. However, it uses a fixed teacher pretrained on a labeled base\ndataset to create soft labels for the unlabeled target samples. As the base\ndataset and unlabeled dataset are from different domains, projecting the target\nimages in the class-domain of the base dataset with a fixed pretrained model\nmight be sub-optimal. We propose a simple dynamic distillation-based approach\nto facilitate unlabeled images from the novel/base dataset. We impose\nconsistency regularization by calculating predictions from the weakly-augmented\nversions of the unlabeled images from a teacher network and matching it with\nthe strongly augmented versions of the same images from a student network. The\nparameters of the teacher network are updated as exponential moving average of\nthe parameters of the student network. We show that the proposed network learns\nrepresentation that can be easily adapted to the target domain even though it\nhas not been trained with target-specific classes during the pretraining phase.\nOur model outperforms the current state-of-the art method by 4.4% for 1-shot\nand 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows\ncompetitive performance on traditional in-domain few-shot learning task. Our\ncode will be available at: https://github.com/asrafulashiq/dynamic-cdfsl.",
          "link": "http://arxiv.org/abs/2106.07807",
          "publishedOn": "2021-06-16T01:21:05.443Z",
          "wordCount": 704,
          "title": "Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data. (arXiv:2106.07807v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1\">Ziheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>",
          "description": "Deep neural network (DNN) generally takes thousands of iterations to optimize\nvia gradient descent and thus has a slow convergence. In addition, softmax, as\na decision layer, may ignore the distribution information of the data during\nclassification. Aiming to tackle the referred problems, we propose a novel\nmanifold neural network based on non-gradient optimization, i.e., the\nclosed-form solutions. Considering that the activation function is generally\ninvertible, we reconstruct the network via forward ridge regression and low\nrank backward approximation, which achieve the rapid convergence. Moreover, by\nunifying the flexible Stiefel manifold and adaptive support vector machine, we\ndevise the novel decision layer which efficiently fits the manifold structure\nof the data and label information. Consequently, a jointly non-gradient\noptimization method is designed to generate the network with closed-form\nresults. Eventually, extensive experiments validate the superior performance of\nthe model.",
          "link": "http://arxiv.org/abs/2106.07905",
          "publishedOn": "2021-06-16T01:21:05.436Z",
          "wordCount": 566,
          "title": "Non-Gradient Manifold Neural Network. (arXiv:2106.07905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guangze Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fuling Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Fangqiang Ding</a>",
          "description": "Unmanned aerial vehicle (UAV) based visual tracking has been confronted with\nnumerous challenges, e.g., object motion and occlusion. These challenges\ngenerally introduce unexpected mutations of target appearance and result in\ntracking failure. However, prevalent discriminative correlation filter (DCF)\nbased trackers are insensitive to target mutations due to a predefined label,\nwhich concentrates on merely the centre of the training region. Meanwhile,\nappearance mutations caused by occlusion or similar objects usually lead to the\ninevitable learning of wrong information. To cope with appearance mutations,\nthis paper proposes a novel DCF-based method to enhance the sensitivity and\nresistance to mutations with an adaptive hybrid label, i.e., MSCF. The ideal\nlabel is optimized jointly with the correlation filter and remains temporal\nconsistency. Besides, a novel measurement of mutations called mutation threat\nfactor (MTF) is applied to correct the label dynamically. Considerable\nexperiments are conducted on widely used UAV benchmarks. The results indicate\nthat the performance of MSCF tracker surpasses other 26 state-of-the-art\nDCF-based and deep-based trackers. With a real-time speed of _38 frames/s, the\nproposed approach is sufficient for UAV tracking commissions.",
          "link": "http://arxiv.org/abs/2106.08073",
          "publishedOn": "2021-06-16T01:21:05.429Z",
          "wordCount": 631,
          "title": "Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label. (arXiv:2106.08073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akundi_P/0/1/0/all/0/1\">Prathyusha Akundi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gun_S/0/1/0/all/0/1\">Soumyasis Gun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivaswamy_J/0/1/0/all/0/1\">Jayanthi Sivaswamy</a>",
          "description": "Melanoma is a leading cause of deaths due to skin cancer deaths and hence,\nearly and effective diagnosis of melanoma is of interest. Current approaches\nfor automated diagnosis of melanoma either use pattern recognition or\nanalytical recognition like ABCDE (asymmetry, border, color, diameter and\nevolving) criterion. In practice however, a differential approach wherein\noutliers (ugly duckling) are detected and used to evaluate nevi/lesions.\nIncorporation of differential recognition in Computer Aided Diagnosis (CAD)\nsystems has not been explored but can be beneficial as it can provide a\nclinical justification for the derived decision. We present a method for\nidentifying and quantifying ugly ducklings by performing Intra-Patient\nComparative Analysis (IPCA) of neighboring nevi. This is then incorporated in a\nCAD system design for melanoma detection. This design ensures flexibility to\nhandle cases where IPCA is not possible. Our experiments on a public dataset\nshow that the outlier information helps boost the sensitivity of detection by\nat least 4.1 % and specificity by 4.0 % to 8.9 %, depending on the use of a\nstrong (EfficientNet) or moderately strong (VGG or ResNet) classifier.",
          "link": "http://arxiv.org/abs/2106.08021",
          "publishedOn": "2021-06-16T01:21:05.404Z",
          "wordCount": 620,
          "title": "A Clinically Inspired Approach for Melanoma classification. (arXiv:2106.08021v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alexander Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1\">Long Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>",
          "description": "We present an algorithm for searching image collections using free-hand\nsketches that describe the appearance and relative positions of multiple\nobjects. Sketch based image retrieval (SBIR) methods predominantly match\nqueries containing a single, dominant object invariant to its position within\nan image. Our work exploits drawings as a concise and intuitive representation\nfor specifying entire scene compositions. We train a convolutional neural\nnetwork (CNN) to encode masked visual features from sketched objects, pooling\nthese into a spatial descriptor encoding the spatial relationships and\nappearances of objects in the composition. Training the CNN backbone as a\nSiamese network under triplet loss yields a metric search embedding for\nmeasuring compositional similarity which may be efficiently leveraged for\nvisual search by applying product quantization.",
          "link": "http://arxiv.org/abs/2106.08009",
          "publishedOn": "2021-06-16T01:21:05.395Z",
          "wordCount": 548,
          "title": "Compositional Sketch Search. (arXiv:2106.08009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tseytlin_B/0/1/0/all/0/1\">Boris Tseytlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1\">Ilya Makarov</a>",
          "description": "We approach the problem of hotel recognition with deep metric learning. We\noverview the existing approaches and propose a modification to Contrastive loss\ncalled Contrastive-Triplet loss. We construct a robust pipeline for\nbenchmarking metric learning models and perform experiments on Hotels-50K and\nCUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval\non Hotels-50k. We open-source our code.",
          "link": "http://arxiv.org/abs/2106.08042",
          "publishedOn": "2021-06-16T01:21:05.385Z",
          "wordCount": 491,
          "title": "Hotel Recognition via Latent Image Embedding. (arXiv:2106.08042v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08151",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schneider_M/0/1/0/all/0/1\">Maja Schneider</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Broszeit_A/0/1/0/all/0/1\">Amelie Broszeit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korner_M/0/1/0/all/0/1\">Marco K&#xf6;rner</a>",
          "description": "We present EuroCrops, a dataset based on self-declared field annotations for\ntraining and evaluating methods for crop type classification and mapping,\ntogether with its process of acquisition and harmonisation. By this, we aim to\nenrich the research efforts and discussion for data-driven land cover\nclassification via Earth observation and remote sensing. Additionally, through\ninclusion of self-declarations gathered in the scope of subsidy control from\nall countries of the European Union (EU), this dataset highlights the\ndifficulties and pitfalls one comes across when operating on a transnational\nlevel. We, therefore, also introduce a new taxonomy scheme, HCAT-ID, that\naspires to capture all the aspects of reference data originating from\nadministrative and agency databases. To address researchers from both the\nremote sensing and the computer vision and machine learning communities, we\npublish the dataset in different formats and processing levels.",
          "link": "http://arxiv.org/abs/2106.08151",
          "publishedOn": "2021-06-16T01:21:05.378Z",
          "wordCount": 607,
          "title": "EuroCrops: A Pan-European Dataset for Time Series Crop Type Classification. (arXiv:2106.08151v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vonikakis_V/0/1/0/all/0/1\">Vassilios Vonikakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1\">Stefan Winkler</a>",
          "description": "Despite their continued popularity, categorical approaches to affect\nrecognition have limitations, especially in real-life situations. Dimensional\nmodels of affect offer important advantages for the recognition of subtle\nexpressions and more fine-grained analysis. We introduce a simple but effective\nfacial expression analysis (FEA) system for dimensional affect, solely based on\ngeometric features and Partial Least Squares (PLS) regression. The system\njointly learns to estimate Arousal and Valence ratings from a set of facial\nimages. The proposed approach is robust, efficient, and exhibits comparable\nperformance to contemporary deep learning models, while requiring a fraction of\nthe computational resources.",
          "link": "http://arxiv.org/abs/2106.07817",
          "publishedOn": "2021-06-16T01:21:05.366Z",
          "wordCount": 531,
          "title": "Efficient Facial Expression Analysis For Dimensional Affect Recognition Using Geometric Features. (arXiv:2106.07817v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08094",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wilde_B/0/1/0/all/0/1\">Bram de Wilde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Broek_R/0/1/0/all/0/1\">Richard P. G. ten Broek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huisman_H/0/1/0/all/0/1\">Henkjan Huisman</a>",
          "description": "Adhesions are an important cause of chronic pain following abdominal surgery.\nRecent developments in abdominal cine-MRI have enabled the non-invasive\ndiagnosis of adhesions. Adhesions are identified on cine-MRI by the absence of\nsliding motion during movement. Diagnosis and mapping of adhesions improves the\nmanagement of patients with pain. Detection of abdominal adhesions on cine-MRI\nis challenging from both a radiological and deep learning perspective. We focus\non classifying presence or absence of adhesions in sagittal abdominal cine-MRI\nseries. We experimented with spatio-temporal deep learning architectures\ncentered around a ConvGRU architecture. A hybrid architecture comprising a\nResNet followed by a ConvGRU model allows to classify a whole time-series.\nCompared to a stand-alone ResNet with a two time-point (inspiration/expiration)\ninput, we show an increase in classification performance (AUROC) from 0.74 to\n0.83 ($p<0.05$). Our full temporal classification approach adds only a small\namount (5%) of parameters to the entire architecture, which may be useful for\nother medical imaging problems with a temporal dimension.",
          "link": "http://arxiv.org/abs/2106.08094",
          "publishedOn": "2021-06-16T01:21:05.343Z",
          "wordCount": 612,
          "title": "Cine-MRI detection of abdominal adhesions with spatio-temporal deep learning. (arXiv:2106.08094v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lakshika_J/0/1/0/all/0/1\">Jayani P. G. Lakshika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talagala_T/0/1/0/all/0/1\">Thiyanga S. Talagala</a>",
          "description": "Plant species identification is time consuming, costly, and requires lots of\nefforts, and expertise knowledge. In recent, many researchers use deep learning\nmethods to classify plants directly using plant images. While deep learning\nmodels have achieved a great success, the lack of interpretability limit their\nwidespread application. To overcome this, we explore the use of interpretable,\nmeasurable and computer-aided features extracted from plant leaf images. Image\nprocessing is one of the most challenging, and crucial steps in\nfeature-extraction. The purpose of image processing is to improve the leaf\nimage by removing undesired distortion. The main image processing steps of our\nalgorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,\nii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove\nstalk, vi) Closing holes, and vii) Resize image. The next step after image\nprocessing is to extract features from plant leaf images. We introduced 52\ncomputationally efficient features to classify plant species. These features\nare mainly classified into four groups as: i) shape-based features, ii)\ncolor-based features, iii) texture-based features, and iv) scagnostic features.\nLength, width, area, texture correlation, monotonicity and scagnostics are to\nname few of them. We explore the ability of features to discriminate the\nclasses of interest under supervised learning and unsupervised learning\nsettings. For that, supervised dimensionality reduction technique, Linear\nDiscriminant Analysis (LDA), and unsupervised dimensionality reduction\ntechnique, Principal Component Analysis (PCA) are used to convert and visualize\nthe images from digital-image space to feature space. The results show that the\nfeatures are sufficient to discriminate the classes of interest under both\nsupervised and unsupervised learning settings.",
          "link": "http://arxiv.org/abs/2106.08077",
          "publishedOn": "2021-06-16T01:21:05.331Z",
          "wordCount": 700,
          "title": "Computer-aided Interpretable Features for Leaf Image Classification. (arXiv:2106.08077v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1\">Franziska Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_M/0/1/0/all/0/1\">Micah Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotnychenko_O/0/1/0/all/0/1\">Oleksandr Sotnychenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verschoor_M/0/1/0/all/0/1\">Mickeal Verschoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otaduy_M/0/1/0/all/0/1\">Miguel A. Otaduy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1\">Dan Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>",
          "description": "We present a novel method for real-time pose and shape reconstruction of two\nstrongly interacting hands. Our approach is the first two-hand tracking\nsolution that combines an extensive list of favorable properties, namely it is\nmarker-less, uses a single consumer-level depth camera, runs in real time,\nhandles inter- and intra-hand collisions, and automatically adjusts to the\nuser's hand shape. In order to achieve this, we embed a recent parametric hand\npose and shape model and a dense correspondence predictor based on a deep\nneural network into a suitable energy minimization framework. For training the\ncorrespondence prediction network, we synthesize a two-hand dataset based on\nphysical simulations that includes both hand pose and shape annotations while\nat the same time avoiding inter-hand penetrations. To achieve real-time rates,\nwe phrase the model fitting in terms of a nonlinear least-squares problem so\nthat the energy can be optimized based on a highly efficient GPU-based\nGauss-Newton optimizer. We show state-of-the-art results in scenes that exceed\nthe complexity level demonstrated by previous work, including tight two-hand\ngrasps, significant inter-hand occlusions, and gesture interaction.",
          "link": "http://arxiv.org/abs/2106.08059",
          "publishedOn": "2021-06-16T01:21:05.322Z",
          "wordCount": 639,
          "title": "Real-time Pose and Shape Reconstruction of Two Interacting Hands With a Single Depth Camera. (arXiv:2106.08059v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruf_B/0/1/0/all/0/1\">Boitumelo Ruf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohrs_J/0/1/0/all/0/1\">Jonas Mohrs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1\">Martin Weinmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinz_S/0/1/0/all/0/1\">Stefan Hinz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyerer_J/0/1/0/all/0/1\">J&#xfc;rgen Beyerer</a>",
          "description": "With the emergence of low-cost robotic systems, such as unmanned aerial\nvehicle, the importance of embedded high-performance image processing has\nincreased. For a long time, FPGAs were the only processing hardware that were\ncapable of high-performance computing, while at the same time preserving a low\npower consumption, essential for embedded systems. However, the recently\nincreasing availability of embedded GPU-based systems, such as the NVIDIA\nJetson series, comprised of an ARM CPU and a NVIDIA Tegra GPU, allows for\nmassively parallel embedded computing on graphics hardware. With this in mind,\nwe propose an approach for real-time embedded stereo processing on ARM and\nCUDA-enabled devices, which is based on the popular and widely used Semi-Global\nMatching algorithm. In this, we propose an optimization of the algorithm for\nembedded CUDA GPUs, by using massively parallel computing, as well as using the\nNEON intrinsics to optimize the algorithm for vectorized SIMD processing on\nembedded ARM CPUs. We have evaluated our approach with different configurations\non two public stereo benchmark datasets to demonstrate that they can reach an\nerror rate as low as 3.3%. Furthermore, our experiments show that the fastest\nconfiguration of our approach reaches up to 46 FPS on VGA image resolution.\nFinally, in a use-case specific qualitative evaluation, we have evaluated the\npower consumption of our approach and deployed it on the DJI Manifold 2-G\nattached to a DJI Matrix 210v2 RTK unmanned aerial vehicle (UAV), demonstrating\nits suitability for real-time stereo processing onboard a UAV.",
          "link": "http://arxiv.org/abs/2106.07927",
          "publishedOn": "2021-06-16T01:21:05.250Z",
          "wordCount": 693,
          "title": "ReS2tAC -- UAV-Borne Real-Time SGM Stereo Optimized for Embedded ARM and CUDA Devices. (arXiv:2106.07927v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jae Myung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Junsuk Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>",
          "description": "The class activation mapping, or CAM, has been the cornerstone of feature\nattribution methods for multiple vision tasks. Its simplicity and effectiveness\nhave led to wide applications in the explanation of visual predictions and\nweakly-supervised localization tasks. However, CAM has its own shortcomings.\nThe computation of attribution maps relies on ad-hoc calibration steps that are\nnot part of the training computational graph, making it difficult for us to\nunderstand the real meaning of the attribution values. In this paper, we\nimprove CAM by explicitly incorporating a latent variable encoding the location\nof the cue for recognition in the formulation, thereby subsuming the\nattribution map into the training computational graph. The resulting model,\nclass activation latent mapping, or CALM, is trained with the\nexpectation-maximization algorithm. Our experiments show that CALM identifies\ndiscriminative attributes for image classifiers more accurately than CAM and\nother visual attribution baselines. CALM also shows performance improvements\nover prior arts on the weakly-supervised object localization benchmarks. Our\ncode is available at https://github.com/naver-ai/calm.",
          "link": "http://arxiv.org/abs/2106.07861",
          "publishedOn": "2021-06-16T01:21:05.162Z",
          "wordCount": 602,
          "title": "Keep CALM and Improve Visual Feature Attribution. (arXiv:2106.07861v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi-Dong Shen</a>",
          "description": "Vision-language Navigation (VLN) tasks require an agent to navigate\nstep-by-step while perceiving the visual observations and comprehending a\nnatural language instruction. Large data bias, which is caused by the disparity\nratio between the small data scale and large navigation space, makes the VLN\ntask challenging. Previous works have proposed various data augmentation\nmethods to reduce data bias. However, these works do not explicitly reduce the\ndata bias across different house scenes. Therefore, the agent would overfit to\nthe seen scenes and achieve poor navigation performance in the unseen scenes.\nTo tackle this problem, we propose the Random Environmental Mixup (REM) method,\nwhich generates cross-connected house scenes as augmented data via mixuping\nenvironment. Specifically, we first select key viewpoints according to the room\nconnection graph for each scene. Then, we cross-connect the key views of\ndifferent scenes to construct augmented scenes. Finally, we generate augmented\ninstruction-path pairs in the cross-connected scenes. The experimental results\non benchmark datasets demonstrate that our augmentation data via REM help the\nagent reduce its performance gap between the seen and unseen environment and\nimprove the overall performance, making our model the best existing approach on\nthe standard VLN benchmark.",
          "link": "http://arxiv.org/abs/2106.07876",
          "publishedOn": "2021-06-16T01:21:05.154Z",
          "wordCount": 620,
          "title": "Vision-Language Navigation with Random Environmental Mixup. (arXiv:2106.07876v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1\">Amir Zandieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_I/0/1/0/all/0/1\">Insu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avron_H/0/1/0/all/0/1\">Haim Avron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoham_N/0/1/0/all/0/1\">Neta Shoham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chaewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide\nneural networks trained under least squares loss by gradient descent. Recent\nworks also report that NTK regression can outperform finitely-wide neural\nnetworks trained on small-scale datasets. However, the computational complexity\nof kernel methods has limited its use in large-scale learning tasks. To\naccelerate learning with NTK, we design a near input-sparsity time\napproximation algorithm for NTK, by sketching the polynomial expansions of\narc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK)\ncan transform any image using a linear runtime in the number of pixels.\nFurthermore, we prove a spectral approximation guarantee for the NTK matrix, by\ncombining random features (based on leverage score sampling) of the arc-cosine\nkernels with a sketching algorithm. We benchmark our methods on various\nlarge-scale regression and classification tasks and show that a linear\nregressor trained on our CNTK features matches the accuracy of exact CNTK on\nCIFAR-10 dataset while achieving 150x speedup.",
          "link": "http://arxiv.org/abs/2106.07880",
          "publishedOn": "2021-06-16T01:21:05.118Z",
          "wordCount": 609,
          "title": "Scaling Neural Tangent Kernels via Sketching and Random Features. (arXiv:2106.07880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongzhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "Benefit from large-scale training data, recent advances in Siamese-based\nobject tracking have achieved compelling results on the normal sequences.\nWhilst Siamese-based trackers assume training and test data follow an identical\ndistribution. Suppose there is a set of foggy or rainy test sequences, it\ncannot be guaranteed that the trackers trained on the normal images perform\nwell on the data belonging to other domains. The problem of domain shift among\ntraining and test data has already been discussed in object detection and\nsemantic segmentation areas, which, however, has not been investigated for\nvisual tracking. To this end, based on SiamRPN++, we introduce a Domain\nAdaptive SiamRPN++, namely DASiamRPN++, to improve the cross-domain\ntransferability and robustness of a tracker. Inspired by A-distance theory, we\npresent two domain adaptive modules, Pixel Domain Adaptation (PDA) and Semantic\nDomain Adaptation (SDA). The PDA module aligns the feature maps of template and\nsearch region images to eliminate the pixel-level domain shift caused by\nweather, illumination, etc. The SDA module aligns the feature representations\nof the tracking target's appearance to eliminate the semantic-level domain\nshift. PDA and SDA modules reduce the domain disparity by learning domain\nclassifiers in an adversarial training manner. The domain classifiers enforce\nthe network to learn domain-invariant feature representations. Extensive\nexperiments are performed on the standard datasets of two different domains,\nincluding synthetic foggy and TIR sequences, which demonstrate the\ntransferability and domain adaptability of the proposed tracker.",
          "link": "http://arxiv.org/abs/2106.07862",
          "publishedOn": "2021-06-16T01:21:05.109Z",
          "wordCount": 667,
          "title": "Domain Adaptive SiamRPN++ for Object Tracking in the Wild. (arXiv:2106.07862v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avram_R/0/1/0/all/0/1\">Robert Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olgin_J/0/1/0/all/0/1\">Jeffrey E. Olgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_A/0/1/0/all/0/1\">Alvin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zeeshan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verreault_Julien_L/0/1/0/all/0/1\">Louis Verreault-Julien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abreau_S/0/1/0/all/0/1\">Sean Abreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1\">Derek Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">Derek Y. So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_K/0/1/0/all/0/1\">Krishan Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tison_G/0/1/0/all/0/1\">Geoffrey H. Tison</a>",
          "description": "Coronary heart disease (CHD) is the leading cause of adult death in the\nUnited States and worldwide, and for which the coronary angiography procedure\nis the primary gateway for diagnosis and clinical management decisions. The\nstandard-of-care for interpretation of coronary angiograms depends upon ad-hoc\nvisual assessment by the physician operator. However, ad-hoc visual\ninterpretation of angiograms is poorly reproducible, highly variable and bias\nprone. Here we show for the first time that fully-automated angiogram\ninterpretation to estimate coronary artery stenosis is possible using a\nsequence of deep neural network algorithms. The algorithmic pipeline we\ndeveloped--called CathAI--achieves state-of-the art performance across the\nsequence of tasks required to accomplish automated interpretation of\nunselected, real-world angiograms. CathAI (Algorithms 1-2) demonstrated\npositive predictive value, sensitivity and F1 score of >=90% to identify the\nprojection angle overall and >=93% for left or right coronary artery angiogram\ndetection, the primary anatomic structures of interest. To predict obstructive\ncoronary artery stenosis (>=70% stenosis), CathAI (Algorithm 4) exhibited an\narea under the receiver operating characteristic curve (AUC) of 0.862 (95% CI:\n0.843-0.880). When externally validated in a healthcare system in another\ncountry, CathAI AUC was 0.869 (95% CI: 0.830-0.907) to predict obstructive\ncoronary artery stenosis. Our results demonstrate that multiple purpose-built\nneural networks can function in sequence to accomplish the complex series of\ntasks required for automated analysis of real-world angiograms. Deployment of\nCathAI may serve to increase standardization and reproducibility in coronary\nstenosis assessment, while providing a robust foundation to accomplish future\ntasks for algorithmic angiographic interpretation.",
          "link": "http://arxiv.org/abs/2106.07708",
          "publishedOn": "2021-06-16T01:21:05.088Z",
          "wordCount": 727,
          "title": "CathAI: Fully Automated Interpretation of Coronary Angiograms Using Neural Networks. (arXiv:2106.07708v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1\">Lin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zongyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Q/0/1/0/all/0/1\">Qianyan Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yehansen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lijing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihang Li</a>",
          "description": "RGB-Infrared (IR) person re-identification aims to retrieve\nperson-of-interest between heterogeneous modalities, suffering from large\nmodality discrepancy caused by different sensory devices. Existing methods\nmainly focus on global-level modality alignment, whereas neglect sample-level\nmodality divergence to some extent, leading to performance degradation. This\npaper attempts to find RGB-IR ReID solutions from tackling sample-level\nmodality difference, and presents a Geometry-Guided Dual-Alignment learning\nframework (G$^2$DA), which jointly enhances modality-invariance and reinforces\ndiscriminability with human topological structure in features to boost the\noverall matching performance. Specifically, G$^2$DA extracts accurate body part\nfeatures with a pose estimator, serving as a semantic bridge complementing the\nmissing local details in global descriptor. Based on extracted local and global\nfeatures, a novel distribution constraint derived from optimal transport is\nintroduced to mitigate the modality gap in a fine-grained sample-level manner.\nBeyond pair-wise relations across two modalities, it additionally measures the\nstructural similarity of different parts, thus both multi-level features and\ntheir relations are kept consistent in the common feature space. Considering\nthe inherent human-topology information, we further advance a geometry-guided\ngraph learning module to refine each part features, where relevant regions can\nbe emphasized while meaningless ones are suppressed, effectively facilitating\nrobust feature learning. Extensive experiments on two standard benchmark\ndatasets validate the superiority of our proposed method, yielding competitive\nperformance over the state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2106.07853",
          "publishedOn": "2021-06-16T01:21:05.074Z",
          "wordCount": 663,
          "title": "G$^2$DA: Geometry-Guided Dual-Alignment Learning for RGB-Infrared Person Re-Identification. (arXiv:2106.07853v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jing_J/0/1/0/all/0/1\">Junfeng Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weichuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changming Sun</a>",
          "description": "Interest point detection is one of the most fundamental and critical problems\nin computer vision and image processing. In this paper, we carry out a\ncomprehensive review on image feature information (IFI) extraction techniques\nfor interest point detection. To systematically introduce how the existing\ninterest point detection methods extract IFI from an input image, we propose a\ntaxonomy of the IFI extraction techniques for interest point detection.\nAccording to this taxonomy, we discuss different types of IFI extraction\ntechniques for interest point detection. Furthermore, we identify the main\nunresolved issues related to the existing IFI extraction techniques for\ninterest point detection and any interest point detection methods that have not\nbeen discussed before. The existing popular datasets and evaluation standards\nare provided and the performances for eighteen state-of-the-art approaches are\nevaluated and discussed. Moreover, future research directions on IFI extraction\ntechniques for interest point detection are elaborated.",
          "link": "http://arxiv.org/abs/2106.07929",
          "publishedOn": "2021-06-16T01:21:05.064Z",
          "wordCount": 585,
          "title": "Image Feature Information Extraction for Interest Point Detection: A Comprehensive Review. (arXiv:2106.07929v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiankun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xiaolan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chibiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yirong Wu</a>",
          "description": "At present, the Synthetic Aperture Radar (SAR) image classification method\nbased on convolution neural network (CNN) has faced some problems such as poor\nnoise resistance and generalization ability. Spiking neural network (SNN) is\none of the core components of brain-like intelligence and has good application\nprospects. This article constructs a complete SAR image classifier based on\nunsupervised and supervised learning of SNN by using spike sequences with\ncomplex spatio-temporal information. We firstly expound the spiking neuron\nmodel, the receptive field of SNN, and the construction of spike sequence. Then\nwe put forward an unsupervised learning algorithm based on STDP and a\nsupervised learning algorithm based on gradient descent. The average\nclassification accuracy of single layer and bilayer unsupervised learning SNN\nin three categories images on MSTAR dataset is 80.8\\% and 85.1\\%, respectively.\nFurthermore, the convergent output spike sequences of unsupervised learning can\nbe used as teaching signals. Based on the TensorFlow framework, a single layer\nsupervised learning SNN is built from the bottom, and the classification\naccuracy reaches 90.05\\%. By comparing noise resistance and model parameters\nbetween SNNs and CNNs, the effectiveness and outstanding advantages of SNN are\nverified. Code to reproduce our experiments is available at\n\\url{https://github.com/Jiankun-chen/Supervised-SNN-with-GD}.",
          "link": "http://arxiv.org/abs/2106.08005",
          "publishedOn": "2021-06-16T01:21:05.053Z",
          "wordCount": 643,
          "title": "SAR Image Classification Based on Spiking Neural Network through Spike-Time Dependent Plasticity and Gradient Descent. (arXiv:2106.08005v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Diana Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakara_A/0/1/0/all/0/1\">Akarsh Prabhakara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munir_S/0/1/0/all/0/1\">Sirajum Munir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aswin Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Swarun Kumar</a>",
          "description": "mmWave radars offer excellent depth resolution owing to their high bandwidth\nat mmWave radio frequencies. Yet, they suffer intrinsically from poor angular\nresolution, that is an order-of-magnitude worse than camera systems, and are\ntherefore not a capable 3-D imaging solution in isolation. We propose\nMetamoran, a system that combines the complimentary strengths of radar and\ncamera systems to obtain depth images at high azimuthal resolutions at\ndistances of several tens of meters with high accuracy, all from a single fixed\nvantage point. Metamoran enables rich long-range depth imaging outdoors with\napplications to roadside safety infrastructure, surveillance and wide-area\nmapping. Our key insight is to use the high azimuth resolution from cameras\nusing computer vision techniques, including image segmentation and monocular\ndepth estimation, to obtain object shapes and use these as priors for our novel\nspecular beamforming algorithm. We also design this algorithm to work in\ncluttered environments with weak reflections and in partially occluded\nscenarios. We perform a detailed evaluation of Metamoran's depth imaging and\nsensing capabilities in 200 diverse scenes at a major U.S. city. Our evaluation\nshows that Metamoran estimates the depth of an object up to 60~m away with a\nmedian error of 28~cm, an improvement of 13$\\times$ compared to a naive\nradar+camera baseline and 23$\\times$ compared to monocular depth estimation.",
          "link": "http://arxiv.org/abs/2106.07856",
          "publishedOn": "2021-06-16T01:21:05.043Z",
          "wordCount": 663,
          "title": "A Hybrid mmWave and Camera System for Long-Range Depth Imaging. (arXiv:2106.07856v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tatikonda_S/0/1/0/all/0/1\">Sinzith Tatikonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nambiar_A/0/1/0/all/0/1\">Athira Nambiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anurag Mittal</a>",
          "description": "Face is one of the predominant means of person recognition. In the process of\nageing, human face is prone to many factors such as time, attributes, weather\nand other subject specific variations. The impact of these factors were not\nwell studied in the literature of face aging. In this paper, we propose a novel\nholistic model in this regard viz., ``Face Age progression With Attribute\nManipulation (FAWAM)\", i.e. generating face images at different ages while\nsimultaneously varying attributes and other subject specific characteristics.\nWe address the task in a bottom-up manner, as two submodules i.e. face age\nprogression and face attribute manipulation. For face aging, we use an\nattribute-conscious face aging model with a pyramidal generative adversarial\nnetwork that can model age-specific facial changes while maintaining intrinsic\nsubject specific characteristics. For facial attribute manipulation, the age\nprocessed facial image is manipulated with desired attributes while preserving\nother details unchanged, leveraging an attribute generative adversarial network\narchitecture. We conduct extensive analysis in standard large scale datasets\nand our model achieves significant performance both quantitatively and\nqualitatively.",
          "link": "http://arxiv.org/abs/2106.07696",
          "publishedOn": "2021-06-16T01:21:04.986Z",
          "wordCount": 605,
          "title": "Face Age Progression With Attribute Manipulation. (arXiv:2106.07696v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asnani_V/0/1/0/all/0/1\">Vishal Asnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1\">Tal Hassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>",
          "description": "State-of-the-art (SOTA) Generative Models (GMs) can synthesize\nphoto-realistic images that are hard for humans to distinguish from genuine\nphotos. We propose to perform reverse engineering of GMs to infer the model\nhyperparameters from the images generated by these models. We define a novel\nproblem, \"model parsing\", as estimating GM network architectures and training\nloss functions by examining their generated images -- a task seemingly\nimpossible for human beings. To tackle this problem, we propose a framework\nwith two components: a Fingerprint Estimation Network (FEN), which estimates a\nGM fingerprint from a generated image by training with four constraints to\nencourage the fingerprint to have desired properties, and a Parsing Network\n(PN), which predicts network architecture and loss functions from the estimated\nfingerprints. To evaluate our approach, we collect a fake image dataset with\n$100$K images generated by $100$ GMs. Extensive experiments show encouraging\nresults in parsing the hyperparameters of the unseen models. Finally, our\nfingerprint estimation can be leveraged for deepfake detection and image\nattribution, as we show by reporting SOTA results on both the recent Celeb-DF\nand image attribution benchmarks.",
          "link": "http://arxiv.org/abs/2106.07873",
          "publishedOn": "2021-06-16T01:21:04.812Z",
          "wordCount": 624,
          "title": "Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images. (arXiv:2106.07873v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodford_O/0/1/0/all/0/1\">Oliver J. Woodford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1\">Kyle Olszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>",
          "description": "Human motion retargeting aims to transfer the motion of one person in a\n\"driving\" video or set of images to another person. Existing efforts leverage a\nlong training video from each target person to train a subject-specific motion\ntransfer model. However, the scalability of such methods is limited, as each\nmodel can only generate videos for the given target subject, and such training\nvideos are labor-intensive to acquire and process. Few-shot motion transfer\ntechniques, which only require one or a few images from a target, have recently\ndrawn considerable attention. Methods addressing this task generally use either\n2D or explicit 3D representations to transfer motion, and in doing so,\nsacrifice either accurate geometric modeling or the flexibility of an\nend-to-end learned representation. Inspired by the Transformable Bottleneck\nNetwork, which renders novel views and manipulations of rigid objects, we\npropose an approach based on an implicit volumetric representation of the image\ncontent, which can then be spatially manipulated using volumetric flow fields.\nWe address the challenging question of how to aggregate information across\ndifferent body poses, learning flow fields that allow for combining content\nfrom the appropriate regions of input images of highly non-rigid human subjects\nperforming complex motions into a single implicit volumetric representation.\nThis allows us to learn our 3D representation solely from videos of moving\npeople. Armed with both 3D object understanding and end-to-end learned\nrendering, this categorically novel representation delivers state-of-the-art\nimage generation quality, as shown by our quantitative and qualitative\nevaluations.",
          "link": "http://arxiv.org/abs/2106.07771",
          "publishedOn": "2021-06-16T01:21:04.784Z",
          "wordCount": 680,
          "title": "Flow Guided Transformable Bottleneck Networks for Motion Retargeting. (arXiv:2106.07771v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Butte_S/0/1/0/all/0/1\">Sujata Butte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakanski_A/0/1/0/all/0/1\">Aleksandar Vakanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duellman_K/0/1/0/all/0/1\">Kasia Duellman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirkouei_A/0/1/0/all/0/1\">Amin Mirkouei</a>",
          "description": "Recent research on the application of remote sensing and deep learning-based\nanalysis in precision agriculture demonstrated a potential for improved crop\nmanagement and reduced environmental impacts of agricultural production.\nDespite the promising results, the practical relevance of these technologies\nfor actual field deployment requires novel algorithms that are customized for\nanalysis of agricultural images and robust to implementation on natural field\nimagery. The paper presents an approach for analyzing aerial images of a potato\ncrop using deep neural networks. The main objective is to demonstrate automated\nspatial recognition of a healthy versus stressed crop at a plant level.\nSpecifically, we examine premature plant senescence resulting in drought stress\non Russet Burbank potato plants. The proposed deep learning model, named\nRetina-UNet-Ag, is a variant of Retina-UNet (Jaeger et al., 2018) and includes\nconnections from low-level semantic dense representation maps to the feature\npyramid network. The paper also introduces a dataset of field images acquired\nwith a Parrot Sequoia camera carried by a Solo unmanned aerial vehicle.\nExperimental validation demonstrated the ability for distinguishing healthy and\nstressed plants in field images, achieving an average Dice score coefficient of\n0.74. A comparison to related state-of-the-art deep learning models for object\ndetection revealed that the presented approach is effective for the task at\nhand. The method applied here is conducive toward the assessment and\nrecognition of potato crop stress (early plant senescence resulting from\ndrought stress in this case) in natural aerial field images collected under\nreal conditions.",
          "link": "http://arxiv.org/abs/2106.07770",
          "publishedOn": "2021-06-16T01:21:04.759Z",
          "wordCount": 692,
          "title": "Potato Crop Stress Identification in Aerial Images using Deep Learning-based Object Detection. (arXiv:2106.07770v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>",
          "description": "Reverberation from audio reflecting off surfaces and objects in the\nenvironment not only degrades the quality of speech for human perception, but\nalso severely impacts the accuracy of automatic speech recognition. Prior work\nattempts to remove reverberation based on the audio modality only. Our idea is\nto learn to dereverberate speech from audio-visual observations. The visual\nenvironment surrounding a human speaker reveals important cues about the room\ngeometry, materials, and speaker location, all of which influence the precise\nreverberation effects in the audio stream. We introduce Visually-Informed\nDereverberation of Audio (VIDA), an end-to-end approach that learns to remove\nreverberation based on both the observed sounds and visual scene. In support of\nthis new task, we develop a large-scale dataset that uses realistic acoustic\nrenderings of speech in real-world 3D scans of homes offering a variety of room\nacoustics. Demonstrating our approach on both simulated and real imagery for\nspeech enhancement, speech recognition, and speaker identification, we show it\nachieves state-of-the-art performance and substantially improves over\ntraditional audio-only methods. Project page:\nthis http URL",
          "link": "http://arxiv.org/abs/2106.07732",
          "publishedOn": "2021-06-16T01:21:04.744Z",
          "wordCount": 602,
          "title": "Learning Audio-Visual Dereverberation. (arXiv:2106.07732v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07806",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bridge_C/0/1/0/all/0/1\">Christopher P. Bridge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorman_C/0/1/0/all/0/1\">Chris Gorman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pieper_S/0/1/0/all/0/1\">Steven Pieper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doyle_S/0/1/0/all/0/1\">Sean W. Doyle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lennerz_J/0/1/0/all/0/1\">Jochen K. Lennerz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clunie_D/0/1/0/all/0/1\">David A. Clunie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fedorov_A/0/1/0/all/0/1\">Andriy Y. Fedorov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herrmann_M/0/1/0/all/0/1\">Markus D. Herrmann</a>",
          "description": "Machine learning is revolutionizing image-based diagnostics in pathology and\nradiology. ML models have shown promising results in research settings, but\ntheir lack of interoperability has been a major barrier for clinical\nintegration and evaluation. The DICOM a standard specifies Information Object\nDefinitions and Services for the representation and communication of digital\nimages and related information, including image-derived annotations and\nanalysis results. However, the complexity of the standard represents an\nobstacle for its adoption in the ML community and creates a need for software\nlibraries and tools that simplify working with data sets in DICOM format. Here\nwe present the highdicom library, which provides a high-level application\nprogramming interface for the Python programming language that abstracts\nlow-level details of the standard and enables encoding and decoding of\nimage-derived information in DICOM format in a few lines of Python code. The\nhighdicom library ties into the extensive Python ecosystem for image processing\nand machine learning. Simultaneously, by simplifying creation and parsing of\nDICOM-compliant files, highdicom achieves interoperability with the medical\nimaging systems that hold the data used to train and run ML models, and\nultimately communicate and store model outputs for clinical use. We demonstrate\nthrough experiments with slide microscopy and computed tomography imaging,\nthat, by bridging these two ecosystems, highdicom enables developers to train\nand evaluate state-of-the-art ML models in pathology and radiology while\nremaining compliant with the DICOM standard and interoperable with clinical\nsystems at all stages. To promote standardization of ML research and streamline\nthe ML model development and deployment process, we made the library available\nfree and open-source.",
          "link": "http://arxiv.org/abs/2106.07806",
          "publishedOn": "2021-06-16T01:21:04.724Z",
          "wordCount": 740,
          "title": "Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Ruizhi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gaochang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuemei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>",
          "description": "Cross-resolution image alignment is a key problem in multiscale gigapixel\nphotography, which requires to estimate homography matrix using images with\nlarge resolution gap. Existing deep homography methods concatenate the input\nimages or features, neglecting the explicit formulation of correspondences\nbetween them, which leads to degraded accuracy in cross-resolution challenges.\nIn this paper, we consider the cross-resolution homography estimation as a\nmultimodal problem, and propose a local transformer network embedded within a\nmultiscale structure to explicitly learn correspondences between the multimodal\ninputs, namely, input images with different resolutions. The proposed local\ntransformer adopts a local attention map specifically for each position in the\nfeature. By combining the local transformer with the multiscale structure, the\nnetwork is able to capture long-short range correspondences efficiently and\naccurately. Experiments on both the MS-COCO dataset and the real-captured\ncross-resolution dataset show that the proposed network outperforms existing\nstate-of-the-art feature-based and deep-learning-based homography estimation\nmethods, and is able to accurately align images under $10\\times$ resolution\ngap.",
          "link": "http://arxiv.org/abs/2106.04067",
          "publishedOn": "2021-06-15T22:41:25.275Z",
          "wordCount": 613,
          "title": "LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution Homography Estimation. (arXiv:2106.04067v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junfu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bi Wang</a>",
          "description": "Working memory (WM) is a basic part of human cognition, which plays an\nimportant role in the study of human cognitive load. Among various brain\nimaging techniques, electroencephalography has shown its advantage on easy\naccess and reliability. However, one of the critical challenges is that\nindividual difference may cause the ineffective results, especially when the\nestablished model meets an unfamiliar subject. In this work, we propose a\ncross-subject deep adaptation model with spatial attention (CS-DASA) to\ngeneralize the workload classifications across subjects. First, we transform\ntime-series EEG data into multi-frame EEG images incorporating more\nspatio-temporal information. First, the subject-shared module in CS-DASA\nreceives multi-frame EEG image data from both source and target subjects and\nlearns the common feature representations. Then, in subject-specific module,\nthe maximum mean discrepancy is implemented to measure the domain distribution\ndivergence in a reproducing kernel Hilbert space, which can add an effective\npenalty loss for domain adaptation. Additionally, the subject-to-subject\nspatial attention mechanism is employed to focus on the most discriminative\nspatial feature in EEG image data. Experiments conducted on a public WM EEG\ndataset containing 13 subjects show that the proposed model is capable of\nachieve better performance than existing state-of-the art methods.",
          "link": "http://arxiv.org/abs/2106.06769",
          "publishedOn": "2021-06-15T22:07:49.048Z",
          "wordCount": 632,
          "title": "Cross-Subject Domain Adaptation for Multi-Frame EEG Images. (arXiv:2106.06769v1 [cs.LG])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2010.07904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zixin Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1\">Wang Chi Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>",
          "description": "We consider a best arm identification (BAI) problem for stochastic bandits\nwith adversarial corruptions in the fixed-budget setting of T steps. We design\na novel randomized algorithm, Probabilistic Sequential Shrinking($u$)\n(PSS($u$)), which is agnostic to the amount of corruptions. When the amount of\ncorruptions per step (CPS) is below a threshold, PSS($u$) identifies the best\narm or item with probability tending to $1$ as $T\\rightarrow \\infty$.\nOtherwise, the optimality gap of the identified item degrades gracefully with\nthe CPS.We argue that such a bifurcation is necessary. In PSS($u$), the\nparameter $u$ serves to balance between the optimality gap and success\nprobability. The injection of randomization is shown to be essential to\nmitigate the impact of corruptions. To demonstrate this, we design two attack\nstrategies that are applicable to any algorithm. We apply one of them to a\ndeterministic analogue of PSS($u$) known as Successive Halving (SH) by Karnin\net al. (2013). The attack strategy results in a high failure probability for\nSH, but PSS($u$) remains robust. In the absence of corruptions, PSS($2$)'s\nperformance guarantee matches SH's. We show that when the CPS is sufficiently\nlarge, no algorithm can achieve a BAI probability tending to $1$ as\n$T\\rightarrow \\infty$. Numerical experiments corroborate our theoretical\nfindings.",
          "link": "http://arxiv.org/abs/2010.07904",
          "publishedOn": "2021-06-21T02:07:41.330Z",
          "wordCount": 705,
          "title": "Probabilistic Sequential Shrinking: A Best Arm Identification Algorithm for Stochastic Bandits with Corruptions. (arXiv:2010.07904v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yicheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filieri_A/0/1/0/all/0/1\">Antonio Filieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>",
          "description": "Probabilistic software analysis aims at quantifying the probability of a\ntarget event occurring during the execution of a program processing uncertain\nincoming data or written itself using probabilistic programming constructs.\nRecent techniques combine symbolic execution with model counting or solution\nspace quantification methods to obtain accurate estimates of the occurrence\nprobability of rare target events, such as failures in a mission-critical\nsystem. However, they face several scalability and applicability limitations\nwhen analyzing software processing with high-dimensional and correlated\nmultivariate input distributions. In this paper, we present SYMbolic Parallel\nAdaptive Importance Sampling (SYMPAIS), a new inference method tailored to\nanalyze path conditions generated from the symbolic execution of programs with\nhigh-dimensional, correlated input distributions. SYMPAIS combines results from\nimportance sampling and constraint solving to produce accurate estimates of the\nsatisfaction probability for a broad class of constraints that cannot be\nanalyzed by current solution space quantification methods. We demonstrate\nSYMPAIS's generality and performance compared with state-of-the-art\nalternatives on a set of problems from different application domains.",
          "link": "http://arxiv.org/abs/2010.05050",
          "publishedOn": "2021-06-21T02:07:41.323Z",
          "wordCount": 640,
          "title": "Symbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis. (arXiv:2010.05050v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1\">Shufeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guevarra_D/0/1/0/all/0/1\">Dan Guevarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1\">Carla P. Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregoire_J/0/1/0/all/0/1\">John M. Gregoire</a>",
          "description": "The adoption of machine learning in materials science has rapidly transformed\nmaterials property prediction. Hurdles limiting full capitalization of recent\nadvancements in machine learning include the limited development of methods to\nlearn the underlying interactions of multiple elements, as well as the\nrelationships among multiple properties, to facilitate property prediction in\nnew composition spaces. To address these issues, we introduce the Hierarchical\nCorrelation Learning for Multi-property Prediction (H-CLMP) framework that\nseamlessly integrates (i) prediction using only a material's composition, (ii)\nlearning and exploitation of correlations among target properties in\nmulti-target regression, and (iii) leveraging training data from tangential\ndomains via generative transfer learning. The model is demonstrated for\nprediction of spectral optical absorption of complex metal oxides spanning 69\n3-cation metal oxide composition spaces. H-CLMP accurately predicts non-linear\ncomposition-property relationships in composition spaces for which no training\ndata is available, which broadens the purview of machine learning to the\ndiscovery of materials with exceptional properties. This achievement results\nfrom the principled integration of latent embedding learning, property\ncorrelation learning, generative transfer learning, and attention models. The\nbest performance is obtained using H-CLMP with Transfer learning (H-CLMP(T))\nwherein a generative adversarial network is trained on computational density of\nstates data and deployed in the target domain to augment prediction of optical\nabsorption from composition. H-CLMP(T) aggregates multiple knowledge sources\nwith a framework that is well-suited for multi-target regression across the\nphysical sciences.",
          "link": "http://arxiv.org/abs/2106.02225",
          "publishedOn": "2021-06-21T02:07:41.307Z",
          "wordCount": 708,
          "title": "Materials Representation and Transfer Learning for Multi-Property Prediction. (arXiv:2106.02225v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1\">Jean Kaddour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1\">Matt J. Kusner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1\">Ricardo Silva</a>",
          "description": "We address the estimation of conditional average treatment effects (CATEs)\nwhen treatments are graph-structured (e.g., molecular graphs of drugs). Given a\nweak condition on the effect, we propose a plug-in estimator that decomposes\nCATE estimation into separate, simpler optimization problems. Our estimator (a)\nisolates the causal estimands (reducing regularization bias), and (b) allows\none to plug in arbitrary models for learning. In experiments with small-world\nand molecular graphs, we show that our approach outperforms prior approaches\nand is robust to varying selection biases. Our implementation is online.",
          "link": "http://arxiv.org/abs/2106.01939",
          "publishedOn": "2021-06-21T02:07:41.301Z",
          "wordCount": 540,
          "title": "Graph Intervention Networks for Causal Effect Estimation. (arXiv:2106.01939v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_O/0/1/0/all/0/1\">Ou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Weiyao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yingjun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haixiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qinghu Hou</a>",
          "description": "A common assumption in machine learning is that samples are independently and\nidentically distributed (i.i.d). However, the contributions of different\nsamples are not identical in training. Some samples are difficult to learn and\nsome samples are noisy. The unequal contributions of samples has a considerable\neffect on training performances. Studies focusing on unequal sample\ncontributions (e.g., easy, hard, noisy) in learning usually refer to these\ncontributions as robust machine learning (RML). Weighing and regularization are\ntwo common techniques in RML. Numerous learning algorithms have been proposed\nbut the strategies for dealing with easy/hard/noisy samples differ or even\ncontradict with different learning algorithms. For example, some strategies\ntake the hard samples first, whereas some strategies take easy first.\nConducting a clear comparison for existing RML algorithms in dealing with\ndifferent samples is difficult due to lack of a unified theoretical framework\nfor RML. This study attempts to construct a mathematical foundation for RML\nbased on the bias-variance trade-off theory. A series of definitions and\nproperties are presented and proved. Several classical learning algorithms are\nalso explained and compared. Improvements of existing methods are obtained\nbased on the comparison. A unified method that combines two classical learning\nstrategies is proposed.",
          "link": "http://arxiv.org/abs/2106.05522",
          "publishedOn": "2021-06-21T02:07:41.280Z",
          "wordCount": 663,
          "title": "A Mathematical Foundation for Robust Machine Learning based on Bias-Variance Trade-off. (arXiv:2106.05522v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1\">Wang Yifan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1\">Lukas Rahmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1\">Olga Sorkine-Hornung</a>",
          "description": "We present implicit displacement fields, a novel representation for detailed\n3D geometry. Inspired by a classic surface deformation technique, displacement\nmapping, our method represents a complex surface as a smooth base surface plus\na displacement along the base's normal directions, resulting in a\nfrequency-based shape decomposition, where the high frequency signal is\nconstrained geometrically by the low frequency signal. Importantly, this\ndisentanglement is unsupervised thanks to a tailored architectural design that\nhas an innate frequency hierarchy by construction. We explore implicit\ndisplacement field surface reconstruction and detail transfer and demonstrate\nsuperior representational power, training stability and generalizability.",
          "link": "http://arxiv.org/abs/2106.05187",
          "publishedOn": "2021-06-21T02:07:41.262Z",
          "wordCount": 567,
          "title": "Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1\">Arturs Backurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1\">Piotr Indyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Cameron Musco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1\">Tal Wagner</a>",
          "description": "We study fast algorithms for computing fundamental properties of a positive\nsemidefinite kernel matrix $K \\in \\mathbb{R}^{n \\times n}$ corresponding to $n$\npoints $x_1,\\ldots,x_n \\in \\mathbb{R}^d$. In particular, we consider estimating\nthe sum of kernel matrix entries, along with its top eigenvalue and\neigenvector.\n\nWe show that the sum of matrix entries can be estimated to $1+\\epsilon$\nrelative error in time $sublinear$ in $n$ and linear in $d$ for many popular\nkernels, including the Gaussian, exponential, and rational quadratic kernels.\nFor these kernels, we also show that the top eigenvalue (and an approximate\neigenvector) can be approximated to $1+\\epsilon$ relative error in time\n$subquadratic$ in $n$ and linear in $d$.\n\nOur algorithms represent significant advances in the best known runtimes for\nthese problems. They leverage the positive definiteness of the kernel matrix,\nalong with a recent line of work on efficient kernel density estimation.",
          "link": "http://arxiv.org/abs/2102.08341",
          "publishedOn": "2021-06-21T02:07:41.256Z",
          "wordCount": 614,
          "title": "Faster Kernel Matrix Algebra via Density Estimation. (arXiv:2102.08341v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Vinicius L. S. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heaney_C/0/1/0/all/0/1\">Claire E. Heaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pain_C/0/1/0/all/0/1\">Christopher C. Pain</a>",
          "description": "We propose a new method in which a generative adversarial network (GAN) is\nused to quantify the uncertainty of forward simulations in the presence of\nobserved data. Previously, a method has been developed which enables GANs to\nmake time series predictions and data assimilation by training a GAN with\nunconditional simulations of a high-fidelity numerical model. After training,\nthe GAN can be used to predict the evolution of the spatial distribution of the\nsimulation states and observed data is assimilated. In this paper, we describe\nthe process required in order to quantify uncertainty, during which no\nadditional simulations of the high-fidelity numerical model are required. These\nmethods take advantage of the adjoint-like capabilities of generative models\nand the ability to simulate forwards and backwards in time. Set within a\nreduced-order model framework for efficiency, we apply these methods to a\ncompartmental model in epidemiology to predict the spread of COVID-19 in an\nidealised town. The results show that the proposed method can efficiently\nquantify uncertainty in the presence of measurements using only unconditional\nsimulations of the high-fidelity numerical model.",
          "link": "http://arxiv.org/abs/2105.13859",
          "publishedOn": "2021-06-21T02:07:41.248Z",
          "wordCount": 702,
          "title": "GAN for time series prediction, data assimilation and uncertainty quantification. (arXiv:2105.13859v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00075",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Moretti_A/0/1/0/all/0/1\">Antonio Khalil Moretti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1\">Liyi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Naesseth_C/0/1/0/all/0/1\">Christian A. Naesseth</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Venner_H/0/1/0/all/0/1\">Hadiah Venner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1\">David Blei</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Peer_I/0/1/0/all/0/1\">Itsik Pe&#x27;er</a>",
          "description": "Bayesian phylogenetic inference is often conducted via local or sequential\nsearch over topologies and branch lengths using algorithms such as random-walk\nMarkov chain Monte Carlo (MCMC) or Combinatorial Sequential Monte Carlo (CSMC).\nHowever, when MCMC is used for evolutionary parameter learning, convergence\nrequires long runs with inefficient exploration of the state space. We\nintroduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a powerful\nframework that establishes variational sequential search to learn distributions\nover intricate combinatorial structures. We then develop nested CSMC, an\nefficient proposal distribution for CSMC and prove that nested CSMC is an exact\napproximation to the (intractable) locally optimal proposal. We use nested CSMC\nto define a second objective, VNCSMC which yields tighter lower bounds than\nVCSMC. We show that VCSMC and VNCSMC are computationally efficient and explore\nhigher probability spaces than existing methods on a range of tasks.",
          "link": "http://arxiv.org/abs/2106.00075",
          "publishedOn": "2021-06-21T02:07:41.236Z",
          "wordCount": 608,
          "title": "Variational Combinatorial Sequential Monte Carlo Methods for Bayesian Phylogenetic Inference. (arXiv:2106.00075v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_T/0/1/0/all/0/1\">Tal Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peake_A/0/1/0/all/0/1\">Ashley Peake</a>",
          "description": "Machine Learning models have been deployed across many different aspects of\nsociety, often in situations that affect social welfare. Although these models\noffer streamlined solutions to large problems, they may contain biases and\ntreat groups or individuals unfairly based on protected attributes such as\ngender. In this paper, we introduce several examples of machine learning gender\nbias in practice followed by formalizations of fairness. We provide a survey of\nfairness research by detailing influential pre-processing, in-processing, and\npost-processing bias mitigation algorithms. We then propose an\n\\textup{end-to-end bias mitigation} framework, which employs a fusion of pre-,\nin-, and post-processing methods to leverage the strengths of each individual\ntechnique. We test this method, along with the standard techniques we review,\non a deep neural network to analyze bias mitigation in a deep learning setting.\nWe find that our end-to-end bias mitigation framework outperforms the baselines\nwith respect to several fairness metrics, suggesting its promise as a method\nfor improving fairness. As society increasingly relies on artificial\nintelligence to help in decision-making, addressing gender biases present in\ndeep learning models is imperative. To provide readers with the tools to assess\nthe fairness of machine learning models and mitigate the biases present in\nthem, we discuss multiple open source packages for fairness in AI.",
          "link": "http://arxiv.org/abs/2104.02532",
          "publishedOn": "2021-06-21T02:07:41.229Z",
          "wordCount": 676,
          "title": "End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning. (arXiv:2104.02532v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identifying\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-06-21T02:07:41.212Z",
          "wordCount": 617,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guangchun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Finding a suitable data representation for a specific task has been shown to\nbe crucial in many applications. The success of subspace clustering depends on\nthe assumption that the data can be separated into different subspaces.\nHowever, this simple assumption does not always hold since the raw data might\nnot be separable into subspaces. To recover the ``clustering-friendly''\nrepresentation and facilitate the subsequent clustering, we propose a graph\nfiltering approach by which a smooth representation is achieved. Specifically,\nit injects graph similarity into data features by applying a low-pass filter to\nextract useful data representations for clustering. Extensive experiments on\nimage and document clustering datasets demonstrate that our method improves\nupon state-of-the-art subspace clustering techniques. Especially, its\ncomparable performance with deep learning methods emphasizes the effectiveness\nof the simple graph filtering scheme for many real-world applications. An\nablation study shows that graph filtering can remove noise, preserve structure\nin the image, and increase the separability of classes.",
          "link": "http://arxiv.org/abs/2106.09874",
          "publishedOn": "2021-06-21T02:07:41.205Z",
          "wordCount": 606,
          "title": "Towards Clustering-friendly Representations: Subspace Clustering via Graph Filtering. (arXiv:2106.09874v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yulin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuni Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaifa Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiapu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingquan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kai Zhou</a>",
          "description": "Graph-based Anomaly Detection (GAD) is becoming prevalent due to the powerful\nrepresentation abilities of graphs as well as recent advances in graph mining\ntechniques. These GAD tools, however, expose a new attacking surface,\nironically due to their unique advantage of being able to exploit the relations\namong data. That is, attackers now can manipulate those relations (i.e., the\nstructure of the graph) to allow some target nodes to evade detection. In this\npaper, we exploit this vulnerability by designing a new type of targeted\nstructural poisoning attacks to a representative regression-based GAD system\ntermed OddBall. Specially, we formulate the attack against OddBall as a\nbi-level optimization problem, where the key technical challenge is to\nefficiently solve the problem in a discrete domain. We propose a novel attack\nmethod termed BinarizedAttack based on gradient descent. Comparing to prior\narts, BinarizedAttack can better use the gradient information, making it\nparticularly suitable for solving combinatorial optimization problems.\nFurthermore, we investigate the attack transferability of BinarizedAttack by\nemploying it to attack other representation-learning-based GAD systems. Our\ncomprehensive experiments demonstrate that BinarizedAttack is very effective in\nenabling target nodes to evade graph-based anomaly detection tools with limited\nattackers' budget, and in the black-box transfer attack setting,\nBinarizedAttack is also tested effective and in particular, can significantly\nchange the node embeddings learned by the GAD systems. Our research thus opens\nthe door to studying a new type of attack against security analytic tools that\nrely on graph data.",
          "link": "http://arxiv.org/abs/2106.09989",
          "publishedOn": "2021-06-21T02:07:41.197Z",
          "wordCount": 682,
          "title": "BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection. (arXiv:2106.09989v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09474",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Aylett_Bullock_J/0/1/0/all/0/1\">Joseph Aylett-Bullock</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Badger_S/0/1/0/all/0/1\">Simon Badger</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Moodie_R/0/1/0/all/0/1\">Ryan Moodie</a>",
          "description": "Machine learning technology has the potential to dramatically optimise event\ngeneration and simulations. We continue to investigate the use of neural\nnetworks to approximate matrix elements for high-multiplicity scattering\nprocesses. We focus on the case of loop-induced diphoton production through\ngluon fusion and develop a realistic simulation method that can be applied to\nhadron collider observables. Neural networks are trained using the one-loop\namplitudes implemented in the NJet C++ library and interfaced to the Sherpa\nMonte Carlo event generator where we perform a detailed study for $2\\to3$ and\n$2\\to4$ scattering problems. We also consider how the trained networks perform\nwhen varying the kinematic cuts effecting the phase space and the reliability\nof the neural network simulations.",
          "link": "http://arxiv.org/abs/2106.09474",
          "publishedOn": "2021-06-21T02:07:41.142Z",
          "wordCount": 571,
          "title": "Optimising simulations for diphoton production at hadron colliders using amplitude neural networks. (arXiv:2106.09474v1 [hep-ph] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1\">Tung Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anup B. Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Cameron Musco</a>",
          "description": "We give relative error coresets for training linear classifiers with a broad\nclass of loss functions, including the logistic loss and hinge loss. Our\nconstruction achieves $(1\\pm \\epsilon)$ relative error with $\\tilde O(d \\cdot\n\\mu_y(X)^2/\\epsilon^2)$ points, where $\\mu_y(X)$ is a natural complexity\nmeasure of the data matrix $X \\in \\mathbb{R}^{n \\times d}$ and label vector $y\n\\in \\{-1,1\\}^n$, introduced in by Munteanu et al. 2018. Our result is based on\nsubsampling data points with probabilities proportional to their $\\ell_1$\n$Lewis$ $weights$. It significantly improves on existing theoretical bounds and\nperforms well in practice, outperforming uniform subsampling along with other\nimportance sampling methods. Our sampling distribution does not depend on the\nlabels, so can be used for active learning. It also does not depend on the\nspecific loss function, so a single coreset can be used in multiple training\nscenarios.",
          "link": "http://arxiv.org/abs/2106.04254",
          "publishedOn": "2021-06-21T02:07:41.136Z",
          "wordCount": 589,
          "title": "Coresets for Classification -- Simplified and Strengthened. (arXiv:2106.04254v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fakoor_R/0/1/0/all/0/1\">Rasool Fakoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadi_K/0/1/0/all/0/1\">Kavosh Asadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1\">Pratik Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "Reliant on too many experiments to learn good actions, current Reinforcement\nLearning (RL) algorithms have limited applicability in real-world settings,\nwhich can be too expensive to allow exploration. We propose an algorithm for\nbatch RL, where effective policies are learned using only a fixed offline\ndataset instead of online interactions with the environment. The limited data\nin batch RL produces inherent uncertainty in value estimates of states/actions\nthat were insufficiently represented in the training data. This leads to\nparticularly severe extrapolation when our candidate policies diverge from one\nthat generated the data. We propose to mitigate this issue via two\nstraightforward penalties: a policy-constraint to reduce this divergence and a\nvalue-constraint that discourages overly optimistic estimates. Over a\ncomprehensive set of 32 continuous-action batch RL benchmarks, our approach\ncompares favorably to state-of-the-art methods, regardless of how the offline\ndata were collected.",
          "link": "http://arxiv.org/abs/2102.09225",
          "publishedOn": "2021-06-21T02:07:41.077Z",
          "wordCount": 614,
          "title": "Continuous Doubly Constrained Batch Reinforcement Learning. (arXiv:2102.09225v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafarpour_S/0/1/0/all/0/1\">Saber Jafarpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davydov_A/0/1/0/all/0/1\">Alexander Davydov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proskurnikov_A/0/1/0/all/0/1\">Anton V. Proskurnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bullo_F/0/1/0/all/0/1\">Francesco Bullo</a>",
          "description": "Implicit neural networks, a.k.a., deep equilibrium networks, are a class of\nimplicit-depth learning models where function evaluation is performed by\nsolving a fixed point equation. They generalize classic feedforward models and\nare equivalent to infinite-depth weight-tied feedforward networks. While\nimplicit models show improved accuracy and significant reduction in memory\nconsumption, they can suffer from ill-posedness and convergence instability.\n\nThis paper provides a new framework to design well-posed and robust implicit\nneural networks based upon contraction theory for the non-Euclidean norm\n$\\ell_\\infty$. Our framework includes (i) a novel condition for well-posedness\nbased on one-sided Lipschitz constants, (ii) an average iteration for computing\nfixed-points, and (iii) explicit estimates on input-output Lipschitz constants.\nAdditionally, we design a training problem with the well-posedness condition\nand the average iteration as constraints and, to achieve robust models, with\nthe input-output Lipschitz constant as a regularizer. Our $\\ell_\\infty$\nwell-posedness condition leads to a larger polytopic training search space than\nexisting conditions and our average iteration enjoys accelerated convergence.\nFinally, we perform several numerical experiments for function estimation and\ndigit classification through the MNIST data set. Our numerical results\ndemonstrate improved accuracy and robustness of the implicit models with\nsmaller input-output Lipschitz bounds.",
          "link": "http://arxiv.org/abs/2106.03194",
          "publishedOn": "2021-06-21T02:07:41.070Z",
          "wordCount": 657,
          "title": "Robust Implicit Networks via Non-Euclidean Contractions. (arXiv:2106.03194v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sumon Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1\">Hridesh Rajan</a>",
          "description": "In recent years, many incidents have been reported where machine learning\nmodels exhibited discrimination among people based on race, sex, age, etc.\nResearch has been conducted to measure and mitigate unfairness in machine\nlearning models. For a machine learning task, it is a common practice to build\na pipeline that includes an ordered set of data preprocessing stages followed\nby a classifier. However, most of the research on fairness has considered a\nsingle classifier based prediction task. What are the fairness impacts of the\npreprocessing stages in machine learning pipeline? Furthermore, studies showed\nthat often the root cause of unfairness is ingrained in the data itself, rather\nthan the model. But no research has been conducted to measure the unfairness\ncaused by a specific transformation made in the data preprocessing stage. In\nthis paper, we introduced the causal method of fairness to reason about the\nfairness impact of data preprocessing stages in ML pipeline. We leveraged\nexisting metrics to define the fairness measures of the stages. Then we\nconducted a detailed fairness evaluation of the preprocessing stages in 37\npipelines collected from three different sources. Our results show that certain\ndata transformers are causing the model to exhibit unfairness. We identified a\nnumber of fairness patterns in several categories of data transformers.\nFinally, we showed how the local fairness of a preprocessing stage composes in\nthe global fairness of the pipeline. We used the fairness composition to choose\nappropriate downstream transformer that mitigates unfairness in the machine\nlearning pipeline.",
          "link": "http://arxiv.org/abs/2106.06054",
          "publishedOn": "2021-06-21T02:07:41.062Z",
          "wordCount": 746,
          "title": "Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11784",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Zwolak_J/0/1/0/all/0/1\">Justyna P. Zwolak</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+McJunkin_T/0/1/0/all/0/1\">Thomas McJunkin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kalantre_S/0/1/0/all/0/1\">Sandesh S. Kalantre</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Neyens_S/0/1/0/all/0/1\">Samuel F. Neyens</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+MacQuarrie_E/0/1/0/all/0/1\">E. R. MacQuarrie</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Eriksson_M/0/1/0/all/0/1\">Mark A. Eriksson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Taylor_J/0/1/0/all/0/1\">Jacob M. Taylor</a>",
          "description": "Quantum dots (QDs) defined with electrostatic gates are a leading platform\nfor a scalable quantum computing implementation. However, with increasing\nnumbers of qubits, the complexity of the control parameter space also grows.\nTraditional measurement techniques, relying on complete or near-complete\nexploration via two-parameter scans (images) of the device response, quickly\nbecome impractical with increasing numbers of gates. Here we propose to\ncircumvent this challenge by introducing a measurement technique relying on\none-dimensional projections of the device response in the multidimensional\nparameter space. Dubbed the ``ray-based classification (RBC) framework,'' we\nuse this machine learning approach to implement a classifier for QD states,\nenabling automated recognition of qubit-relevant parameter regimes. We show\nthat RBC surpasses the 82 % accuracy benchmark from the experimental\nimplementation of image-based classification techniques from prior work while\nreducing the number of measurement points needed by up to 70 %. The reduction\nin measurement cost is a significant gain for time-intensive QD measurements\nand is a step forward toward the scalability of these devices. We also discuss\nhow the RBC-based optimizer, which tunes the device to a multiqubit regime,\nperforms when tuning in the two-dimensional and three-dimensional parameter\nspaces defined by plunger and barrier gates that control the QDs.This work\nprovides experimental validation of both efficient state identification and\noptimization with machine learning techniques for non-traditional measurements\nin quantum systems with high-dimensional parameter spaces and time-intensive\nmeasurements.",
          "link": "http://arxiv.org/abs/2102.11784",
          "publishedOn": "2021-06-21T02:07:41.054Z",
          "wordCount": 708,
          "title": "Ray-based framework for state identification in quantum dot devices. (arXiv:2102.11784v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13727",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wood_K/0/1/0/all/0/1\">Kieran Wood</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zohren_S/0/1/0/all/0/1\">Stefan Zohren</a>",
          "description": "Momentum strategies are an important part of alternative investments and are\nat the heart of commodity trading advisors (CTAs). These strategies have\nhowever been found to have difficulties adjusting to rapid changes in market\nconditions, such as during the 2020 market crash. In particular, immediately\nafter momentum turning points, where a trend reverses from an uptrend\n(downtrend) to a downtrend (uptrend), time-series momentum (TSMOM) strategies\nare prone to making bad bets. To improve the response to regime change, we\nintroduce a novel approach, where we insert an online change-point detection\n(CPD) module into a Deep Momentum Network (DMN) [1904.04912] pipeline, which\nuses an LSTM deep-learning architecture to simultaneously learn both trend\nestimation and position sizing. Furthermore, our model is able to optimise the\nway in which it balances 1) a slow momentum strategy which exploits persisting\ntrends, but does not overreact to localised price moves, and 2) a fast\nmean-reversion strategy regime by quickly flipping its position, then swapping\nit back again to exploit localised price moves. Our CPD module outputs a\nchangepoint location and severity score, allowing our model to learn to respond\nto varying degrees of disequilibrium, or smaller and more localised\nchangepoints, in a data driven manner. Using a portfolio of 50, liquid,\ncontinuous futures contracts over the period 1990-2020, the addition of the CPD\nmodule leads to an improvement in Sharpe ratio of one-third. Even more notably,\nthis module is especially beneficial in periods of significant nonstationarity,\nand in particular, over the most recent years tested (2015-2020) the\nperformance boost is approximately two-thirds. This is especially interesting\nas traditional momentum strategies have been underperforming in this period.",
          "link": "http://arxiv.org/abs/2105.13727",
          "publishedOn": "2021-06-21T02:07:41.037Z",
          "wordCount": 752,
          "title": "Slow Momentum with Fast Reversion: A Trading Strategy Using Deep Learning and Changepoint Detection. (arXiv:2105.13727v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.15069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nowak_Vila_A/0/1/0/all/0/1\">Alex Nowak-Vila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>",
          "description": "The foundational concept of Max-Margin in machine learning is ill-posed for\noutput spaces with more than two labels such as in structured prediction. In\nthis paper, we show that the Max-Margin loss can only be consistent to the\nclassification task under highly restrictive assumptions on the discrete loss\nmeasuring the error between outputs. These conditions are satisfied by\ndistances defined in tree graphs, for which we prove consistency, thus being\nthe first losses shown to be consistent for Max-Margin beyond the binary\nsetting. We finally address these limitations by correcting the concept of\nMax-Margin and introducing the Restricted-Max-Margin, where the maximization of\nthe loss-augmented scores is maintained, but performed over a subset of the\noriginal domain. The resulting loss is also a generalization of the binary\nsupport vector machine and it is consistent under milder conditions on the\ndiscrete loss.",
          "link": "http://arxiv.org/abs/2105.15069",
          "publishedOn": "2021-06-21T02:07:41.028Z",
          "wordCount": 591,
          "title": "Max-Margin is Dead, Long Live Max-Margin!. (arXiv:2105.15069v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_Z/0/1/0/all/0/1\">Zeshan Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1\">Rahul G. Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>",
          "description": "Modeling the time-series of high-dimensional, longitudinal data is important\nfor predicting patient disease progression. However, existing neural network\nbased approaches that learn representations of patient state, while very\nflexible, are susceptible to overfitting. We propose a deep generative model\nthat makes use of a novel attention-based neural architecture inspired by the\nphysics of how treatments affect disease state. The result is a scalable and\naccurate model of high-dimensional patient biomarkers as they vary over time.\nOur proposed model yields significant improvements in generalization and, on\nreal-world clinical data, provides interpretable insights into the dynamics of\ncancer progression.",
          "link": "http://arxiv.org/abs/2102.11218",
          "publishedOn": "2021-06-21T02:07:41.022Z",
          "wordCount": 570,
          "title": "Neural Pharmacodynamic State Space Modeling. (arXiv:2102.11218v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "In adversarial training (AT), the main focus has been the objective and\noptimizer while the model has been less studied, so that the models being used\nare still those classic ones in standard training (ST). Classic network\narchitectures (NAs) are generally worse than searched NAs in ST, which should\nbe the same in AT. In this paper, we argue that NA and AT cannot be handled\nindependently, since given a dataset, the optimal NA in ST would be no longer\noptimal in AT. That being said, AT is time-consuming itself; if we directly\nsearch NAs in AT over large search spaces, the computation will be practically\ninfeasible. Thus, we propose a diverse-structured network (DS-Net), to\nsignificantly reduce the size of the search space: instead of low-level\noperations, we only consider predefined atomic blocks, where an atomic block is\na time-tested building block like the residual block. There are only a few\natomic blocks and thus we can weight all atomic blocks rather than find the\nbest one in a searched block of DS-Net, which is an essential trade-off between\nexploring diverse structures and exploiting the best structures. Empirical\nresults demonstrate the advantages of DS-Net, i.e., weighting the atomic\nblocks.",
          "link": "http://arxiv.org/abs/2102.01886",
          "publishedOn": "2021-06-21T02:07:41.014Z",
          "wordCount": 695,
          "title": "Learning Diverse-Structured Networks for Adversarial Robustness. (arXiv:2102.01886v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We present a plug-in replacement for batch normalization (BN) called\nexponential moving average normalization (EMAN), which improves the performance\nof existing student-teacher based self- and semi-supervised learning\ntechniques. Unlike the standard BN, where the statistics are computed within\neach batch, EMAN, used in the teacher, updates its statistics by exponential\nmoving average from the BN statistics of the student. This design reduces the\nintrinsic cross-sample dependency of BN and enhances the generalization of the\nteacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2\npoints and semi-supervised learning by about 7/2 points, when 1%/10% supervised\nlabels are available on ImageNet. These improvements are consistent across\nmethods, network architectures, training duration, and datasets, demonstrating\nthe general effectiveness of this technique. The code is available at\nhttps://github.com/amazon-research/exponential-moving-average-normalization.",
          "link": "http://arxiv.org/abs/2101.08482",
          "publishedOn": "2021-06-21T02:07:41.007Z",
          "wordCount": 614,
          "title": "Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning. (arXiv:2101.08482v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07729",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Vinicius L. S. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heaney_C/0/1/0/all/0/1\">Claire E. Heaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pain_C/0/1/0/all/0/1\">Christopher C. Pain</a>",
          "description": "We propose the novel use of a generative adversarial network (GAN) (i) to\nmake predictions in time (PredGAN) and (ii) to assimilate measurements\n(DA-PredGAN). In the latter case, we take advantage of the natural adjoint-like\nproperties of generative models and the ability to simulate forwards and\nbackwards in time. GANs have received much attention recently, after achieving\nexcellent results for their generation of realistic-looking images. We wish to\nexplore how this property translates to new applications in computational\nmodelling and to exploit the adjoint-like properties for efficient data\nassimilation. To predict the spread of COVID-19 in an idealised town, we apply\nthese methods to a compartmental model in epidemiology that is able to model\nspace and time variations. To do this, the GAN is set within a reduced-order\nmodel (ROM), which uses a low-dimensional space for the spatial distribution of\nthe simulation states. Then the GAN learns the evolution of the low-dimensional\nstates over time. The results show that the proposed methods can accurately\npredict the evolution of the high-fidelity numerical simulation, and can\nefficiently assimilate observed data and determine the corresponding model\nparameters.",
          "link": "http://arxiv.org/abs/2105.07729",
          "publishedOn": "2021-06-21T02:07:41.000Z",
          "wordCount": 705,
          "title": "Data Assimilation Predictive GAN (DA-PredGAN): applied to determine the spread of COVID-19. (arXiv:2105.07729v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1\">Sai Praneeth Karimireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Decentralized training of deep learning models is a key element for enabling\ndata privacy and on-device learning over networks. In realistic learning\nscenarios, the presence of heterogeneity across different clients' local\ndatasets poses an optimization challenge and may severely deteriorate the\ngeneralization performance. In this paper, we investigate and identify the\nlimitation of several decentralized optimization algorithms for different\ndegrees of data heterogeneity. We propose a novel momentum-based method to\nmitigate this decentralized training difficulty. We show in extensive empirical\nexperiments on various CV/NLP datasets (CIFAR-10, ImageNet, and AG News) and\nseveral network topologies (Ring and Social Network) that our method is much\nmore robust to the heterogeneity of clients' data than other existing methods,\nby a significant improvement in test performance ($1\\% \\!-\\! 20\\%$). Our code\nis publicly available.",
          "link": "http://arxiv.org/abs/2102.04761",
          "publishedOn": "2021-06-21T02:07:40.983Z",
          "wordCount": 594,
          "title": "Quasi-Global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous Data. (arXiv:2102.04761v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1\">Son Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Than_K/0/1/0/all/0/1\">Khoat Than</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_H/0/1/0/all/0/1\">Hung Bui</a>",
          "description": "Approximate inference in deep Bayesian networks exhibits a dilemma of how to\nyield high fidelity posterior approximations while maintaining computational\nefficiency and scalability. We tackle this challenge by introducing a novel\nvariational structured approximation inspired by the Bayesian interpretation of\nDropout regularization. Concretely, we focus on the inflexibility of the\nfactorized structure in Dropout posterior and then propose an improved method\ncalled Variational Structured Dropout (VSD). VSD employs an orthogonal\ntransformation to learn a structured representation on the variational noise\nand consequently induces statistical dependencies in the approximate posterior.\nTheoretically, VSD successfully addresses the pathologies of previous\nVariational Dropout methods and thus offers a standard Bayesian justification.\nWe further show that VSD induces an adaptive regularization term with several\ndesirable properties which contribute to better generalization. Finally, we\nconduct extensive experiments on standard benchmarks to demonstrate the\neffectiveness of VSD over state-of-the-art variational methods on predictive\naccuracy, uncertainty estimation, and out-of-distribution detection.",
          "link": "http://arxiv.org/abs/2102.07927",
          "publishedOn": "2021-06-21T02:07:40.975Z",
          "wordCount": 627,
          "title": "Structured Dropout Variational Inference for Bayesian Neural Networks. (arXiv:2102.07927v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15761",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Stanziola_A/0/1/0/all/0/1\">Antonio Stanziola</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Arridge_S/0/1/0/all/0/1\">Simon R. Arridge</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cox_B/0/1/0/all/0/1\">Ben T. Cox</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Treeby_B/0/1/0/all/0/1\">Bradley E. Treeby</a>",
          "description": "Transcranial ultrasound therapy is increasingly used for the non-invasive\ntreatment of brain disorders. However, conventional numerical wave solvers are\ncurrently too computationally expensive to be used online during treatments to\npredict the acoustic field passing through the skull (e.g., to account for\nsubject-specific dose and targeting variations). As a step towards real-time\npredictions, in the current work, a fast iterative solver for the heterogeneous\nHelmholtz equation in 2D is developed using a fully-learned optimizer. The\nlightweight network architecture is based on a modified UNet that includes a\nlearned hidden state. The network is trained using a physics-based loss\nfunction and a set of idealized sound speed distributions with fully\nunsupervised training (no knowledge of the true solution is required). The\nlearned optimizer shows excellent performance on the test set, and is capable\nof generalization well outside the training examples, including to much larger\ncomputational domains, and more complex source and sound speed distributions,\nfor example, those derived from x-ray computed tomography images of the skull.",
          "link": "http://arxiv.org/abs/2010.15761",
          "publishedOn": "2021-06-21T02:07:40.968Z",
          "wordCount": 652,
          "title": "A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound. (arXiv:2010.15761v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pfaff_T/0/1/0/all/0/1\">Tobias Pfaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortunato_M/0/1/0/all/0/1\">Meire Fortunato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1\">Alvaro Sanchez-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1\">Peter W. Battaglia</a>",
          "description": "Mesh-based simulations are central to modeling complex physical systems in\nmany disciplines across science and engineering. Mesh representations support\npowerful numerical integration methods and their resolution can be adapted to\nstrike favorable trade-offs between accuracy and efficiency. However,\nhigh-dimensional scientific simulations are very expensive to run, and solvers\nand parameters must often be tuned individually to each system studied. Here we\nintroduce MeshGraphNets, a framework for learning mesh-based simulations using\ngraph neural networks. Our model can be trained to pass messages on a mesh\ngraph and to adapt the mesh discretization during forward simulation. Our\nresults show it can accurately predict the dynamics of a wide range of physical\nsystems, including aerodynamics, structural mechanics, and cloth. The model's\nadaptivity supports learning resolution-independent dynamics and can scale to\nmore complex state spaces at test time. Our method is also highly efficient,\nrunning 1-2 orders of magnitude faster than the simulation on which it is\ntrained. Our approach broadens the range of problems on which neural network\nsimulators can operate and promises to improve the efficiency of complex,\nscientific modeling tasks.",
          "link": "http://arxiv.org/abs/2010.03409",
          "publishedOn": "2021-06-21T02:07:40.961Z",
          "wordCount": 670,
          "title": "Learning Mesh-Based Simulation with Graph Networks. (arXiv:2010.03409v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12696",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Giollo_M/0/1/0/all/0/1\">Manuel Giollo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunceler_D/0/1/0/all/0/1\">Deniz Gunceler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yulan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Willett_D/0/1/0/all/0/1\">Daniel Willett</a>",
          "description": "Bootstrapping speech recognition on limited data resources has been an area\nof active research for long. The recent transition to all-neural models and\nend-to-end (E2E) training brought along particular challenges as these models\nare known to be data hungry, but also came with opportunities around\nlanguage-agnostic representations derived from multilingual data as well as\nshared word-piece output representations across languages that share script and\nroots. We investigate here the effectiveness of different strategies to\nbootstrap an RNN-Transducer (RNN-T) based automatic speech recognition (ASR)\nsystem in the low resource regime, while exploiting the abundant resources\navailable in other languages as well as the synthetic audio from a\ntext-to-speech (TTS) engine. Our experiments demonstrate that transfer learning\nfrom a multilingual model, using a post-ASR text-to-text mapping and synthetic\naudio deliver additive improvements, allowing us to bootstrap a model for a new\nlanguage with a fraction of the data that would otherwise be needed. The best\nsystem achieved a 46% relative word error rate (WER) reduction compared to the\nmonolingual baseline, among which 25% relative WER improvement is attributed to\nthe post-ASR text-to-text mappings and the TTS synthetic data.",
          "link": "http://arxiv.org/abs/2011.12696",
          "publishedOn": "2021-06-21T02:07:40.944Z",
          "wordCount": 659,
          "title": "Bootstrap an end-to-end ASR system by multilingual training, transfer learning, text-to-text mapping and synthetic audio. (arXiv:2011.12696v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Armandpour_M/0/1/0/all/0/1\">Mohammadreza Armandpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghian_A/0/1/0/all/0/1\">Ali Sadeghian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "Despite the success of Generative Adversarial Networks (GANs), their training\nsuffers from several well-known problems, including mode collapse and\ndifficulties learning a disconnected set of manifolds. In this paper, we break\ndown the challenging task of learning complex high dimensional distributions,\nsupporting diverse data samples, to simpler sub-tasks. Our solution relies on\ndesigning a partitioner that breaks the space into smaller regions, each having\na simpler distribution, and training a different generator for each partition.\nThis is done in an unsupervised manner without requiring any labels.\n\nWe formulate two desired criteria for the space partitioner that aid the\ntraining of our mixture of generators: 1) to produce connected partitions and\n2) provide a proxy of distance between partitions and data samples, along with\na direction for reducing that distance. These criteria are developed to avoid\nproducing samples from places with non-existent data density, and also\nfacilitate training by providing additional direction to the generators. We\ndevelop theoretical constraints for a space partitioner to satisfy the above\ncriteria. Guided by our theoretical analysis, we design an effective neural\narchitecture for the space partitioner that empirically assures these\nconditions. Experimental results on various standard benchmarks show that the\nproposed unsupervised model outperforms several recent methods.",
          "link": "http://arxiv.org/abs/2104.00816",
          "publishedOn": "2021-06-21T02:07:40.937Z",
          "wordCount": 662,
          "title": "Partition-Guided GANs. (arXiv:2104.00816v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.01420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Elhamod_M/0/1/0/all/0/1\">Mohannad Elhamod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1\">Jie Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Christopher Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redell_M/0/1/0/all/0/1\">Matthew Redell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Abantika Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podolskiy_V/0/1/0/all/0/1\">Viktor Podolskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wei-Cheng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpatne_A/0/1/0/all/0/1\">Anuj Karpatne</a>",
          "description": "Physics-guided Neural Networks (PGNNs) represent an emerging class of neural\nnetworks that are trained using physics-guided (PG) loss functions (capturing\nviolations in network outputs with known physics), along with the supervision\ncontained in data. Existing work in PGNNs have demonstrated the efficacy of\nadding single PG loss functions in the neural network objectives, using\nconstant trade-off parameters, to ensure better generalizability. However, in\nthe presence of multiple physics loss functions with competing gradient\ndirections, there is a need to adaptively tune the contribution of competing PG\nloss functions during the course of training to arrive at generalizable\nsolutions. We demonstrate the presence of competing PG losses in the generic\nneural network problem of solving for the lowest (or highest) eigenvector of a\nphysics-based eigenvalue equation, common to many scientific problems. We\npresent a novel approach to handle competing PG losses and demonstrate its\nefficacy in learning generalizable solutions in two motivating applications of\nquantum mechanics and electromagnetic propagation. All the code and data used\nin this work is available at https://github.com/jayroxis/Cophy-PGNN.",
          "link": "http://arxiv.org/abs/2007.01420",
          "publishedOn": "2021-06-21T02:07:40.929Z",
          "wordCount": 697,
          "title": "CoPhy-PGNN: Learning Physics-guided Neural Networks with Competing Loss Functions for Solving Eigenvalue Problems. (arXiv:2007.01420v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Fenglei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>",
          "description": "Despite its best performance in image denoising, the supervised deep\ndenoising methods require paired noise-clean data, which are often unavailable.\nTo address this challenge, Noise2Noise was designed based on the fact that\npaired noise-clean images can be replaced by paired noise-noise images that are\neasier to collect. However, in many scenarios the collection of paired\nnoise-noise images is still impractical. To bypass labeled images, Noise2Void\nmethods predict masked pixels from their surroundings with single noisy images\nonly and give improved denoising results that still need improvements. An\nobservation on classic denoising methods is that non-local mean (NLM) outcomes\nare typically superior to locally denoised results. In contrast, Noise2Void and\nits variants do not utilize self-similarities in an image as the NLM-based\nmethods do. Here we propose Noise2Sim, an NLM-inspired self-learning method for\nimage denoising. Specifically, Noise2Sim leverages the self-similarity of image\npixels to train the denoising network, requiring single noisy images only. Our\ntheoretical analysis shows that Noise2Sim tends to be equivalent to Noise2Noise\nunder mild conditions. To efficiently manage the computational burden for\nglobally searching similar pixels, we design a two-step procedure to provide\ndata for Noise2Sim training. Extensive experiments demonstrate the superiority\nof Noise2Sim on common benchmark datasets.",
          "link": "http://arxiv.org/abs/2011.03384",
          "publishedOn": "2021-06-21T02:07:40.920Z",
          "wordCount": 687,
          "title": "Noise2Sim -- Similarity-based Self-Learning for Image Denoising. (arXiv:2011.03384v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.10783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiancan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1\">Jianxun Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "Representation learning on user-item graph for recommendation has evolved\nfrom using single ID or interaction history to exploiting higher-order\nneighbors. This leads to the success of graph convolution networks (GCNs) for\nrecommendation such as PinSage and LightGCN. Despite effectiveness, we argue\nthat they suffer from two limitations: (1) high-degree nodes exert larger\nimpact on the representation learning, deteriorating the recommendations of\nlow-degree (long-tail) items; and (2) representations are vulnerable to noisy\ninteractions, as the neighborhood aggregation scheme further enlarges the\nimpact of observed edges.\n\nIn this work, we explore self-supervised learning on user-item graph, so as\nto improve the accuracy and robustness of GCNs for recommendation. The idea is\nto supplement the classical supervised task of recommendation with an auxiliary\nself-supervised task, which reinforces node representation learning via\nself-discrimination. Specifically, we generate multiple views of a node,\nmaximizing the agreement between different views of the same node compared to\nthat of other nodes. We devise three operators to generate the views -- node\ndropout, edge dropout, and random walk -- that change the graph structure in\ndifferent manners. We term this new learning paradigm as\n\\textit{Self-supervised Graph Learning} (SGL), implementing it on the\nstate-of-the-art model LightGCN. Through theoretical analyses, we find that SGL\nhas the ability of automatically mining hard negatives. Empirical studies on\nthree benchmark datasets demonstrate the effectiveness of SGL, which improves\nthe recommendation accuracy, especially on long-tail items, and the robustness\nagainst interaction noises. Our implementations are available at\n\\url{https://github.com/wujcan/SGL}.",
          "link": "http://arxiv.org/abs/2010.10783",
          "publishedOn": "2021-06-21T02:07:40.911Z",
          "wordCount": 740,
          "title": "Self-supervised Graph Learning for Recommendation. (arXiv:2010.10783v4 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02452",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guillot_A/0/1/0/all/0/1\">Antoine Guillot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thorey_V/0/1/0/all/0/1\">Valentin Thorey</a>",
          "description": "Sleep disorder diagnosis relies on the analysis of polysomnography (PSG)\nrecords. As a preliminary step of this examination, sleep stages are\nsystematically determined. In practice, sleep stage classification relies on\nthe visual inspection of 30-second epochs of polysomnography signals. Numerous\nautomatic approaches have been developed to replace this tedious and expensive\ntask. Although these methods demonstrated better performance than human sleep\nexperts on specific datasets, they remain largely unused in sleep clinics. The\nmain reason is that each sleep clinic uses a specific PSG montage that most\nautomatic approaches cannot handle out-of-the-box. Moreover, even when the PSG\nmontage is compatible, publications have shown that automatic approaches\nperform poorly on unseen data with different demographics. To address these\nissues, we introduce RobustSleepNet, a deep learning model for automatic sleep\nstage classification able to handle arbitrary PSG montages. We trained and\nevaluated this model in a leave-one-out-dataset fashion on a large corpus of 8\nheterogeneous sleep staging datasets to make it robust to demographic changes.\nWhen evaluated on an unseen dataset, RobustSleepNet reaches 97% of the F1 of a\nmodel explicitly trained on this dataset. Hence, RobustSleepNet unlocks the\npossibility to perform high-quality out-of-the-box automatic sleep staging with\nany clinical setup. We further show that finetuning RobustSleepNet, using a\npart of the unseen dataset, increases the F1 by 2% when compared to a model\ntrained specifically for this dataset. Therefore, finetuning might be used to\nreach a state-of-the-art level of performance on a specific population.",
          "link": "http://arxiv.org/abs/2101.02452",
          "publishedOn": "2021-06-21T02:07:40.904Z",
          "wordCount": 697,
          "title": "RobustSleepNet: Transfer learning for automated sleep staging at scale. (arXiv:2101.02452v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhiyan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_S/0/1/0/all/0/1\">Stephen Wright</a>",
          "description": "Finding parameters in a deep neural network (NN) that fit training data is a\nnonconvex optimization problem, but a basic first-order optimization method\n(gradient descent) finds a global solution with perfect fit in many practical\nsituations. We examine this phenomenon for the case of Residual Neural Networks\n(ResNet) with smooth activation functions in a limiting regime in which both\nthe number of layers (depth) and the number of neurons in each layer (width) go\nto infinity. First, we use a mean-field-limit argument to prove that the\ngradient descent for parameter training becomes a partial differential equation\n(PDE) that characterizes gradient flow for a probability distribution in the\nlarge-NN limit. Next, we show that the solution to the PDE converges in the\ntraining time to a zero-loss solution. Together, these results imply that\ntraining of the ResNet also gives a near-zero loss if the Resnet is large\nenough. We give estimates of the depth and width needed to reduce the loss\nbelow a given threshold, with high probability.",
          "link": "http://arxiv.org/abs/2105.14417",
          "publishedOn": "2021-06-21T02:07:40.897Z",
          "wordCount": 627,
          "title": "Overparameterization of deep ResNet: zero loss and mean-field analysis. (arXiv:2105.14417v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diao_C/0/1/0/all/0/1\">Cameron Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1\">Denis Kleyko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabaey_J/0/1/0/all/0/1\">Jan M. Rabaey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olshausen_B/0/1/0/all/0/1\">Bruno A. Olshausen</a>",
          "description": "Machine learning algorithms deployed on edge devices must meet certain\nresource constraints and efficiency requirements. Random Vector Functional Link\n(RVFL) networks are favored for such applications due to their simple design\nand training efficiency. We propose a modified RVFL network that avoids\ncomputationally expensive matrix operations during training, thus expanding the\nnetwork's range of potential applications. Our modification replaces the\nleast-squares classifier with the Generalized Learning Vector Quantization\n(GLVQ) classifier, which only employs simple vector and distance calculations.\nThe GLVQ classifier can also be considered an improvement upon certain\nclassification algorithms popularly used in the area of Hyperdimensional\nComputing. The proposed approach achieved state-of-the-art accuracy on a\ncollection of datasets from the UCI Machine Learning Repository - higher than\npreviously proposed RVFL networks. We further demonstrate that our approach\nstill achieves high accuracy while severely limited in training iterations\n(using on average only 21% of the least-squares classifier computational\ncosts).",
          "link": "http://arxiv.org/abs/2106.09821",
          "publishedOn": "2021-06-21T02:07:40.879Z",
          "wordCount": 597,
          "title": "Generalized Learning Vector Quantization for Classification in Randomized Neural Networks and Hyperdimensional Computing. (arXiv:2106.09821v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1\">Emre Ozfatura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozfatura_K/0/1/0/all/0/1\">Kerem Ozfatura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>",
          "description": "Federated learning (FL) has become de facto framework for collaborative\nlearning among edge devices with privacy concern. The core of the FL strategy\nis the use of stochastic gradient descent (SGD) in a distributed manner. Large\nscale implementation of FL brings new challenges, such as the incorporation of\nacceleration techniques designed for SGD into the distributed setting, and\nmitigation of the drift problem due to non-homogeneous distribution of local\ndatasets. These two problems have been separately studied in the literature;\nwhereas, in this paper, we show that it is possible to address both problems\nusing a single strategy without any major alteration to the FL framework, or\nintroducing additional computation and communication load. To achieve this\ngoal, we propose FedADC, which is an accelerated FL algorithm with drift\ncontrol. We empirically illustrate the advantages of FedADC.",
          "link": "http://arxiv.org/abs/2012.09102",
          "publishedOn": "2021-06-21T02:07:40.850Z",
          "wordCount": 607,
          "title": "FedADC: Accelerated Federated Learning with Drift Control. (arXiv:2012.09102v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yoojin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Khamy_M/0/1/0/all/0/1\">Mostafa El-Khamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungwon Lee</a>",
          "description": "This paper proposes two novel knowledge transfer techniques for\nclass-incremental learning (CIL). First, we propose data-free generative replay\n(DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples\nfrom a generative model. In the conventional generative replay, the generative\nmodel is pre-trained for old data and shared in extra memory for later\nincremental learning. In our proposed DF-GR, we train a generative model from\nscratch without using any training data, based on the pre-trained\nclassification model from the past, so we curtail the cost of sharing\npre-trained generative models. Second, we introduce dual-teacher information\ndistillation (DT-ID) for knowledge distillation from two teachers to one\nstudent. In CIL, we use DT-ID to learn new classes incrementally based on the\npre-trained model for old classes and another model (pre-)trained on the new\ndata for new classes. We implemented the proposed schemes on top of one of the\nstate-of-the-art CIL methods and showed the performance improvement on\nCIFAR-100 and ImageNet datasets.",
          "link": "http://arxiv.org/abs/2106.09835",
          "publishedOn": "2021-06-21T02:07:40.839Z",
          "wordCount": 612,
          "title": "Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay. (arXiv:2106.09835v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.02373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Sin Kit Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1\">Hye-young Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>",
          "description": "Federated learning has received fast-growing interests from academia and\nindustry to tackle the challenges of data hungriness and privacy in machine\nlearning. A federated learning system can be viewed as a large-scale\ndistributed system with different components and stakeholders as numerous\nclient devices participate in federated learning. Designing a federated\nlearning system requires software system design thinking apart from machine\nlearning knowledge. Although much effort has been put into federated learning\nfrom the machine learning technique aspects, the software architecture design\nconcerns in building federated learning systems have been largely ignored.\nTherefore, in this paper, we present a collection of architectural patterns to\ndeal with the design challenges of federated learning systems. Architectural\npatterns present reusable solutions to a commonly occurring problem within a\ngiven context during software architecture design. The presented patterns are\nbased on the results of a systematic literature review and include three client\nmanagement patterns, four model management patterns, three model training\npatterns, and four model aggregation patterns. The patterns are associated to\nthe particular state transitions in a federated learning model lifecycle,\nserving as a guidance for effective use of the patterns in the design of\nfederated learning systems.",
          "link": "http://arxiv.org/abs/2101.02373",
          "publishedOn": "2021-06-21T02:07:40.830Z",
          "wordCount": 702,
          "title": "Architectural Patterns for the Design of Federated Learning Systems. (arXiv:2101.02373v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Willard_J/0/1/0/all/0/1\">Jared D. Willard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1\">Jordan S. Read</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appling_A/0/1/0/all/0/1\">Alison P. Appling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliver_S/0/1/0/all/0/1\">Samantha K. Oliver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vipin Kumar</a>",
          "description": "Most environmental data come from a minority of well-monitored sites. An\nongoing challenge in the environmental sciences is transferring knowledge from\nmonitored sites to unmonitored sites. Here, we demonstrate a novel transfer\nlearning framework that accurately predicts depth-specific temperature in\nunmonitored lakes (targets) by borrowing models from well-monitored lakes\n(sources). This method, Meta Transfer Learning (MTL), builds a meta-learning\nmodel to predict transfer performance from candidate source models to targets\nusing lake attributes and candidates' past performance. We constructed source\nmodels at 145 well-monitored lakes using calibrated process-based modeling (PB)\nand a recently developed approach called process-guided deep learning (PGDL).\nWe applied MTL to either PB or PGDL source models (PB-MTL or PGDL-MTL,\nrespectively) to predict temperatures in 305 target lakes treated as\nunmonitored in the Upper Midwestern United States. We show significantly\nimproved performance relative to the uncalibrated process-based General Lake\nModel, where the median RMSE for the target lakes is $2.52^{\\circ}C$. PB-MTL\nyielded a median RMSE of $2.43^{\\circ}C$; PGDL-MTL yielded $2.16^{\\circ}C$; and\na PGDL-MTL ensemble of nine sources per target yielded $1.88^{\\circ}C$. For\nsparsely monitored target lakes, PGDL-MTL often outperformed PGDL models\ntrained on the target lakes themselves. Differences in maximum depth between\nthe source and target were consistently the most important predictors. Our\napproach readily scales to thousands of lakes in the Midwestern United States,\ndemonstrating that MTL with meaningful predictor variables and high-quality\nsource models is a promising approach for many kinds of unmonitored systems and\nenvironmental variables.",
          "link": "http://arxiv.org/abs/2011.05369",
          "publishedOn": "2021-06-21T02:07:40.818Z",
          "wordCount": 723,
          "title": "Predicting Water Temperature Dynamics of Unmonitored Lakes with Meta Transfer Learning. (arXiv:2011.05369v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06231",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kimura_M/0/1/0/all/0/1\">Masanari Kimura</a>",
          "description": "Machine learning techniques are used in a wide range of domains. However,\nmachine learning models often suffer from the problem of over-fitting. Many\ndata augmentation methods have been proposed to tackle such a problem, and one\nof them is called mixup. Mixup is a recently proposed regularization procedure,\nwhich linearly interpolates a random pair of training examples. This\nregularization method works very well experimentally, but its theoretical\nguarantee is not adequately discussed. In this study, we aim to discover why\nmixup works well from the aspect of the statistical learning theory.",
          "link": "http://arxiv.org/abs/2006.06231",
          "publishedOn": "2021-06-21T02:07:40.801Z",
          "wordCount": 550,
          "title": "Why Mixup Improves the Model Performance. (arXiv:2006.06231v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.14162",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gedon_D/0/1/0/all/0/1\">Daniel Gedon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wahlstrom_N/0/1/0/all/0/1\">Niklas Wahlstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schon_T/0/1/0/all/0/1\">Thomas B. Sch&#xf6;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ljung_L/0/1/0/all/0/1\">Lennart Ljung</a>",
          "description": "Deep state space models (SSMs) are an actively researched model class for\ntemporal models developed in the deep learning community which have a close\nconnection to classic SSMs. The use of deep SSMs as a black-box identification\nmodel can describe a wide range of dynamics due to the flexibility of deep\nneural networks. Additionally, the probabilistic nature of the model class\nallows the uncertainty of the system to be modelled. In this work a deep SSM\nclass and its parameter learning algorithm are explained in an effort to extend\nthe toolbox of nonlinear identification methods with a deep learning based\nmethod. Six recent deep SSMs are evaluated in a first unified implementation on\nnonlinear system identification benchmarks.",
          "link": "http://arxiv.org/abs/2003.14162",
          "publishedOn": "2021-06-21T02:07:40.774Z",
          "wordCount": 590,
          "title": "Deep State Space Models for Nonlinear System Identification. (arXiv:2003.14162v3 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Aditya Ojha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1\">Daniel Neider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "Despite the fact that deep reinforcement learning (RL) has surpassed\nhuman-level performances in various tasks, it still has several fundamental\nchallenges. First, most RL methods require intensive data from the exploration\nof the environment to achieve satisfactory performance. Second, the use of\nneural networks in RL renders it hard to interpret the internals of the system\nin a way that humans can understand. To address these two challenges, we\npropose a framework that enables an RL agent to reason over its exploration\nprocess and distill high-level knowledge for effectively guiding its future\nexplorations. Specifically, we propose a novel RL algorithm that learns\nhigh-level knowledge in the form of a finite reward automaton by using the L*\nlearning algorithm. We prove that in episodic RL, a finite reward automaton can\nexpress any non-Markovian bounded reward functions with finitely many reward\nvalues and approximate any non-Markovian bounded reward function (with\ninfinitely many reward values) with arbitrary precision. We also provide a\nlower bound for the episode length such that the proposed RL approach almost\nsurely converges to an optimal policy in the limit. We test this approach on\ntwo RL environments with non-Markovian reward functions, choosing a variety of\ntasks with increasing complexity for each environment. We compare our algorithm\nwith the state-of-the-art RL algorithms for non-Markovian reward functions,\nsuch as Joint Inference of Reward machines and Policies for RL (JIRP), Learning\nReward Machine (LRM), and Proximal Policy Optimization (PPO2). Our results show\nthat our algorithm converges to an optimal policy faster than other baseline\nmethods.",
          "link": "http://arxiv.org/abs/2006.15714",
          "publishedOn": "2021-06-21T02:07:40.766Z",
          "wordCount": 736,
          "title": "Active Finite Reward Automaton Inference and Reinforcement Learning Using Queries and Counterexamples. (arXiv:2006.15714v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.01496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shuo Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>",
          "description": "Despite the great progress made by deep neural networks in the semantic\nsegmentation task, traditional neural-networkbased methods typically suffer\nfrom a shortage of large amounts of pixel-level annotations. Recent progress in\nfewshot semantic segmentation tackles the issue by only a few pixel-level\nannotated examples. However, these few-shot approaches cannot easily be applied\nto multi-way or weak annotation settings. In this paper, we advance the\nfew-shot segmentation paradigm towards a scenario where image-level annotations\nare available to help the training process of a few pixel-level annotations.\nOur key idea is to learn a better prototype representation of the class by\nfusing the knowledge from the image-level labeled data. Specifically, we\npropose a new framework, called PAIA, to learn the class prototype\nrepresentation in a metric space by integrating image-level annotations.\nFurthermore, by considering the uncertainty of pseudo-masks, a distilled soft\nmasked average pooling strategy is designed to handle distractions in\nimage-level annotations. Extensive empirical results on two datasets show\nsuperior performance of PAIA.",
          "link": "http://arxiv.org/abs/2007.01496",
          "publishedOn": "2021-06-21T02:07:40.759Z",
          "wordCount": 644,
          "title": "Few-Shot Semantic Segmentation Augmented with Image-Level Weak Annotations. (arXiv:2007.01496v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14610",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Garnier_R/0/1/0/all/0/1\">R&#xe9;my Garnier</a>",
          "description": "Competition between times series often arises in sales prediction, when\nsimilar products are on sale on a marketplace. This article provides a model of\nthe presence of cannibalization between times series. This model creates a\n\"competitiveness\" function that depends on external features such as price and\nmargin. It also provides a theoretical guaranty on the error of the model under\nsome reasonable conditions, and implement this model using a neural network to\ncompute this competitiveness function. This implementation outperforms other\ntraditional time series methods and classical neural networks for market share\nprediction on a real-world data set.",
          "link": "http://arxiv.org/abs/2009.14610",
          "publishedOn": "2021-06-21T02:07:40.753Z",
          "wordCount": 551,
          "title": "Concurrent Neural Network : A model of competition between times series. (arXiv:2009.14610v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papoudakis_G/0/1/0/all/0/1\">Georgios Papoudakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1\">Filippos Christianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1\">Stefano V. Albrecht</a>",
          "description": "Modelling the behaviours of other agents is essential for understanding how\nagents interact and making effective decisions. Existing methods for agent\nmodelling commonly assume knowledge of the local observations and chosen\nactions of the modelled agents during execution. To eliminate this assumption,\nwe extract representations from the local information of the controlled agent\nusing encoder-decoder architectures. Using the observations and actions of the\nmodelled agents during training, our models learn to extract representations\nabout the modelled agents conditioned only on the local observations of the\ncontrolled agent. The representations are used to augment the controlled\nagent's decision policy which is trained via deep reinforcement learning; thus,\nduring execution, the policy does not require access to other agents'\ninformation. We provide a comprehensive evaluation and ablations studies in\ncooperative, competitive and mixed multi-agent environments, showing that our\nmethod achieves significantly higher returns than baseline methods which do not\nuse the learned representations.",
          "link": "http://arxiv.org/abs/2006.09447",
          "publishedOn": "2021-06-21T02:07:40.735Z",
          "wordCount": 625,
          "title": "Local Information Agent Modelling in Partially-Observable Environments. (arXiv:2006.09447v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ebert_Uphoff_I/0/1/0/all/0/1\">Imme Ebert-Uphoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagerquist_R/0/1/0/all/0/1\">Ryan Lagerquist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilburn_K/0/1/0/all/0/1\">Kyle Hilburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoonjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haynes_K/0/1/0/all/0/1\">Katherine Haynes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stock_J/0/1/0/all/0/1\">Jason Stock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumler_C/0/1/0/all/0/1\">Christina Kumler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_J/0/1/0/all/0/1\">Jebb Q. Stewart</a>",
          "description": "Neural networks are increasingly used in environmental science applications.\nFurthermore, neural network models are trained by minimizing a loss function,\nand it is crucial to choose the loss function very carefully for environmental\nscience applications, as it determines what exactly is being optimized.\nStandard loss functions do not cover all the needs of the environmental\nsciences, which makes it important for scientists to be able to develop their\nown custom loss functions so that they can implement many of the classic\nperformance measures already developed in environmental science, including\nmeasures developed for spatial model verification. However, there are very few\nresources available that cover the basics of custom loss function development\ncomprehensively, and to the best of our knowledge none that focus on the needs\nof environmental scientists. This document seeks to fill this gap by providing\na guide on how to write custom loss functions targeted toward environmental\nscience applications. Topics include the basics of writing custom loss\nfunctions, common pitfalls, functions to use in loss functions, examples such\nas fractions skill score as loss function, how to incorporate physical\nconstraints, discrete and soft discretization, and concepts such as focal,\nrobust, and adaptive loss. While examples are currently provided in this guide\nfor Python with Keras and the TensorFlow backend, the basic concepts also apply\nto other environments, such as Python with PyTorch. Similarly, while the sample\nloss functions provided here are from meteorology, these are just examples of\nhow to create custom loss functions. Other fields in the environmental sciences\nhave very similar needs for custom loss functions, e.g., for evaluating spatial\nforecasts effectively, and the concepts discussed here can be applied there as\nwell. All code samples are provided in a GitHub repository.",
          "link": "http://arxiv.org/abs/2106.09757",
          "publishedOn": "2021-06-21T02:07:40.721Z",
          "wordCount": 752,
          "title": "CIRA Guide to Custom Loss Functions for Neural Networks in Environmental Sciences -- Version 1. (arXiv:2106.09757v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1\">Maxim Kodryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kropotov_D/0/1/0/all/0/1\">Dmitry Kropotov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Tensor decomposition methods are known to be efficient for compressing and\naccelerating neural networks. However, the problem of optimal decomposition\nstructure determination is still not well studied while being quite important.\nSpecifically, decomposition ranks present the crucial parameter controlling the\ncompression-accuracy trade-off. In this paper, we introduce MARS -- a new\nefficient method for the automatic selection of ranks in general tensor\ndecompositions. During training, the procedure learns binary masks over\ndecomposition cores that \"select\" the optimal tensor structure. The learning is\nperformed via relaxed maximum a posteriori (MAP) estimation in a specific\nBayesian model. The proposed method achieves better results compared to\nprevious works in various tasks.",
          "link": "http://arxiv.org/abs/2006.10859",
          "publishedOn": "2021-06-21T02:07:40.714Z",
          "wordCount": 569,
          "title": "MARS: Masked Automatic Ranks Selection in Tensor Decompositions. (arXiv:2006.10859v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.10847",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_T/0/1/0/all/0/1\">Tianyi Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1\">Yuejiao Sun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1\">Wotao Yin</a>",
          "description": "Stochastic compositional optimization generalizes classic (non-compositional)\nstochastic optimization to the minimization of compositions of functions. Each\ncomposition may introduce an additional expectation. The series of expectations\nmay be nested. Stochastic compositional optimization is gaining popularity in\napplications such as reinforcement learning and meta learning. This paper\npresents a new Stochastically Corrected Stochastic Compositional gradient\nmethod (SCSC). SCSC runs in a single-time scale with a single loop, uses a\nfixed batch size, and guarantees to converge at the same rate as the stochastic\ngradient descent (SGD) method for non-compositional stochastic optimization.\nThis is achieved by making a careful improvement to a popular stochastic\ncompositional gradient method. It is easy to apply SGD-improvement techniques\nto accelerate SCSC. This helps SCSC achieve state-of-the-art performance for\nstochastic compositional optimization. In particular, we apply Adam to SCSC,\nand the exhibited rate of convergence matches that of the original Adam on\nnon-compositional stochastic optimization. We test SCSC using the portfolio\nmanagement and model-agnostic meta-learning tasks.",
          "link": "http://arxiv.org/abs/2008.10847",
          "publishedOn": "2021-06-21T02:07:40.707Z",
          "wordCount": 651,
          "title": "Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization. (arXiv:2008.10847v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.06511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haiyun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiaosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>",
          "description": "This paper investigates a novel offline change-point detection problem from\nan information-theoretic perspective. In contrast to most related works, we\nassume that the knowledge of the underlying pre- and post-change distributions\nare not known and can only be learned from the training sequences which are\navailable. We further require the probability of the \\emph{estimation error} to\ndecay either exponentially or sub-exponentially fast (corresponding\nrespectively to the large and moderate deviations regimes in information theory\nparlance). Based on the training sequences as well as the test sequence\nconsisting of a single change-point, we design a change-point estimator and\nfurther show that this estimator is optimal by establishing matching (strong)\nconverses. This leads to a full characterization of the optimal confidence\nwidth (i.e., half the width of the confidence interval within which the true\nchange-point is located at with high probability) as a function of the\nundetected error, under both the large and moderate deviations regimes.",
          "link": "http://arxiv.org/abs/2003.06511",
          "publishedOn": "2021-06-21T02:07:40.688Z",
          "wordCount": 655,
          "title": "Optimal Change-Point Detection with Training Sequences in the Large and Moderate Deviations Regimes. (arXiv:2003.06511v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.09021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amir_T/0/1/0/all/0/1\">Tal Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1\">Ronen Basri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadler_B/0/1/0/all/0/1\">Boaz Nadler</a>",
          "description": "We present a new approach to solve the sparse approximation or best subset\nselection problem, namely find a $k$-sparse vector ${\\bf x}\\in\\mathbb{R}^d$\nthat minimizes the $\\ell_2$ residual $\\lVert A{\\bf x}-{\\bf y} \\rVert_2$. We\nconsider a regularized approach, whereby this residual is penalized by the\nnon-convex $\\textit{trimmed lasso}$, defined as the $\\ell_1$-norm of ${\\bf x}$\nexcluding its $k$ largest-magnitude entries. We prove that the trimmed lasso\nhas several appealing theoretical properties, and in particular derive sparse\nrecovery guarantees assuming successful optimization of the penalized\nobjective. Next, we show empirically that directly optimizing this objective\ncan be quite challenging. Instead, we propose a surrogate for the trimmed\nlasso, called the $\\textit{generalized soft-min}$. This penalty smoothly\ninterpolates between the classical lasso and the trimmed lasso, while taking\ninto account all possible $k$-sparse patterns. The generalized soft-min penalty\ninvolves summation over $\\binom{d}{k}$ terms, yet we derive a polynomial-time\nalgorithm to compute it. This, in turn, yields a practical method for the\noriginal sparse approximation problem. Via simulations, we demonstrate its\ncompetitive performance compared to current state of the art.",
          "link": "http://arxiv.org/abs/2005.09021",
          "publishedOn": "2021-06-21T02:07:40.681Z",
          "wordCount": 683,
          "title": "The Trimmed Lasso: Sparse Recovery Guarantees and Practical Optimization by the Generalized Soft-Min Penalty. (arXiv:2005.09021v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1\">Shruti Tople</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1\">Olga Ohrimenko</a>",
          "description": "Secure multi-party machine learning allows several parties to build a model\non their pooled data to increase utility while not explicitly sharing data with\neach other. We show that such multi-party computation can cause leakage of\nglobal dataset properties between the parties even when parties obtain only\nblack-box access to the final model. In particular, a ``curious'' party can\ninfer the distribution of sensitive attributes in other parties' data with high\naccuracy. This raises concerns regarding the confidentiality of properties\npertaining to the whole dataset as opposed to individual data records. We show\nthat our attack can leak population-level properties in datasets of different\ntypes, including tabular, text, and graph data. To understand and measure the\nsource of leakage, we consider several models of correlation between a\nsensitive attribute and the rest of the data. Using multiple machine learning\nmodels, we show that leakage occurs even if the sensitive attribute is not\nincluded in the training data and has a low correlation with other attributes\nor the target variable.",
          "link": "http://arxiv.org/abs/2006.07267",
          "publishedOn": "2021-06-21T02:07:40.662Z",
          "wordCount": 651,
          "title": "Leakage of Dataset Properties in Multi-Party Machine Learning. (arXiv:2006.07267v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1\">Kirill Bykov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedstrom_A/0/1/0/all/0/1\">Anna Hedstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1\">Shinichi Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1\">Marina M.-C. H&#xf6;hne</a>",
          "description": "Attribution methods remain a practical instrument that is used in real-world\napplications to explain the decision-making process of complex learning\nmachines. It has been shown that a simple method called SmoothGrad can\neffectively reduce the visual diffusion of gradient-based attribution methods\nand has established itself among both researchers and practitioners. What\nremains unexplored in research, however, is how explanations can be improved by\nintroducing stochasticity to the model weights. In the light of this, we\nintroduce - NoiseGrad - a stochastic, method-agnostic explanation-enhancing\nmethod that adds noise to the weights instead of the input data. We investigate\nour proposed method through various experiments including different datasets,\nexplanation methods and network architectures and conclude that NoiseGrad (and\nits extension NoiseGrad++) with multiplicative Gaussian noise offers a clear\nadvantage compared to SmoothGrad on several evaluation criteria. We connect our\nproposed method to Bayesian Learning and provide the user with a heuristic for\nchoosing hyperparameters.",
          "link": "http://arxiv.org/abs/2106.10185",
          "publishedOn": "2021-06-21T02:07:40.252Z",
          "wordCount": 590,
          "title": "NoiseGrad: enhancing explanations by introducing stochasticity to model weights. (arXiv:2106.10185v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yi-Ling Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yu-Che Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng-Te Li</a>",
          "description": "Financial technology (FinTech) has drawn much attention among investors and\ncompanies. While conventional stock analysis in FinTech targets at predicting\nstock prices, less effort is made for profitable stock recommendation. Besides,\nin existing approaches on modeling time series of stock prices, the\nrelationships among stocks and sectors (i.e., categories of stocks) are either\nneglected or pre-defined. Ignoring stock relationships will miss the\ninformation shared between stocks while using pre-defined relationships cannot\ndepict the latent interactions or influence of stock prices between stocks. In\nthis work, we aim at recommending the top-K profitable stocks in terms of\nreturn ratio using time series of stock prices and sector information. We\npropose a novel deep learning-based model, Financial Graph Attention Networks\n(FinGAT), to tackle the task under the setting that no pre-defined\nrelationships between stocks are given. The idea of FinGAT is three-fold.\nFirst, we devise a hierarchical learning component to learn short-term and\nlong-term sequential patterns from stock time series. Second, a fully-connected\ngraph between stocks and a fully-connected graph between sectors are\nconstructed, along with graph attention networks, to learn the latent\ninteractions among stocks and sectors. Third, a multi-task objective is devised\nto jointly recommend the profitable stocks and predict the stock movement.\nExperiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit\nremarkable recommendation performance of our FinGAT, comparing to\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.10159",
          "publishedOn": "2021-06-21T02:07:40.210Z",
          "wordCount": 695,
          "title": "FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks. (arXiv:2106.10159v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09963",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinhan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunzheng Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_R/0/1/0/all/0/1\">Ruchao Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alwan_A/0/1/0/all/0/1\">Abeer Alwan</a>",
          "description": "This paper describes the SPAPL system for the INTERSPEECH 2021 Challenge:\nShared Task on Automatic Speech Recognition for Non-Native Children's Speech in\nGerman. ~ 5 hours of transcribed data and ~ 60 hours of untranscribed data are\nprovided to develop a German ASR system for children. For the training of the\ntranscribed data, we propose a non-speech state discriminative loss (NSDL) to\nmitigate the influence of long-duration non-speech segments within speech\nutterances. In order to explore the use of the untranscribed data, various\napproaches are implemented and combined together to incrementally improve the\nsystem performance. First, bidirectional autoregressive predictive coding\n(Bi-APC) is used to learn initial parameters for acoustic modelling using the\nprovided untranscribed data. Second, incremental semi-supervised learning is\nfurther used to iteratively generate pseudo-transcribed data. Third, different\ndata augmentation schemes are used at different training stages to increase the\nvariability and size of the training data. Finally, a recurrent neural network\nlanguage model (RNNLM) is used for rescoring. Our system achieves a word error\nrate (WER) of 39.68% on the evaluation data, an approximately 12% relative\nimprovement over the official baseline (45.21%).",
          "link": "http://arxiv.org/abs/2106.09963",
          "publishedOn": "2021-06-21T02:07:40.192Z",
          "wordCount": 653,
          "title": "Low Resource German ASR with Untranscribed Data Spoken by Non-native Children -- INTERSPEECH 2021 Shared Task SPAPL System. (arXiv:2106.09963v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1\">Marc Brockschmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allamanis_M/0/1/0/all/0/1\">Miltiadis Allamanis</a>",
          "description": "Traditional generative models are limited to predicting sequences of terminal\ntokens. However, ambiguities in the generation task may lead to incorrect\noutputs. Towards addressing this, we introduce Grammformers, transformer-based\ngrammar-guided models that learn (without explicit supervision) to generate\nsketches -- sequences of tokens with holes. Through reinforcement learning,\nGrammformers learn to introduce holes avoiding the generation of incorrect\ntokens where there is ambiguity in the target task.\n\nWe train Grammformers for statement-level source code completion, i.e., the\ngeneration of code snippets given an ambiguous user intent, such as a partial\ncode context. We evaluate Grammformers on code completion for C# and Python and\nshow that it generates 10-50% more accurate sketches compared to traditional\ngenerative models and 37-50% longer sketches compared to sketch-generating\nbaselines trained with similar techniques.",
          "link": "http://arxiv.org/abs/2106.10158",
          "publishedOn": "2021-06-21T02:07:40.185Z",
          "wordCount": 560,
          "title": "Learning to Generate Code Sketches. (arXiv:2106.10158v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1\">Jordan T. Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Surbhi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Akshay Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1\">Dipendra Misra</a>",
          "description": "Noise contrastive learning is a popular technique for unsupervised\nrepresentation learning. In this approach, a representation is obtained via\nreduction to supervised learning, where given a notion of semantic similarity,\nthe learner tries to distinguish a similar (positive) example from a collection\nof random (negative) examples. The success of modern contrastive learning\npipelines relies on many parameters such as the choice of data augmentation,\nthe number of negative examples, and the batch size; however, there is limited\nunderstanding as to how these parameters interact and affect downstream\nperformance. We focus on disambiguating the role of one of these parameters:\nthe number of negative examples. Theoretically, we show the existence of a\ncollision-coverage trade-off suggesting that the optimal number of negative\nexamples should scale with the number of underlying concepts in the data.\nEmpirically, we scrutinize the role of the number of negatives in both NLP and\nvision tasks. In the NLP task, we find that the results broadly agree with our\ntheory, while our vision experiments are murkier with performance sometimes\neven being insensitive to the number of negatives. We discuss plausible\nexplanations for this behavior and suggest future directions to better align\ntheory and practice.",
          "link": "http://arxiv.org/abs/2106.09943",
          "publishedOn": "2021-06-21T02:07:40.158Z",
          "wordCount": 639,
          "title": "Investigating the Role of Negatives in Contrastive Representation Learning. (arXiv:2106.09943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Awiszus_M/0/1/0/all/0/1\">Maren Awiszus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1\">Frederik Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>",
          "description": "This work introduces World-GAN, the first method to perform data-driven\nProcedural Content Generation via Machine Learning in Minecraft from a single\nexample. Based on a 3D Generative Adversarial Network (GAN) architecture, we\nare able to create arbitrarily sized world snippets from a given sample. We\nevaluate our approach on creations from the community as well as structures\ngenerated with the Minecraft World Generator. Our method is motivated by the\ndense representations used in Natural Language Processing (NLP) introduced with\nword2vec [1]. The proposed block2vec representations make World-GAN independent\nfrom the number of different blocks, which can vary a lot in Minecraft, and\nenable the generation of larger levels. Finally, we demonstrate that changing\nthis new representation space allows us to change the generated style of an\nalready trained generator. World-GAN enables its users to generate Minecraft\nworlds based on parts of their creations.",
          "link": "http://arxiv.org/abs/2106.10155",
          "publishedOn": "2021-06-21T02:07:40.152Z",
          "wordCount": 593,
          "title": "World-GAN: a Generative Model for Minecraft Worlds. (arXiv:2106.10155v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10052",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mathieu_E/0/1/0/all/0/1\">Emile Mathieu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1\">Adam Foster</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>",
          "description": "Learning representations of stochastic processes is an emerging problem in\nmachine learning with applications from meta-learning to physical object models\nto time series. Typical methods rely on exact reconstruction of observations,\nbut this approach breaks down as observations become high-dimensional or noise\ndistributions become complex. To address this, we propose a unifying framework\nfor learning contrastive representations of stochastic processes (CRESP) that\ndoes away with exact reconstruction. We dissect potential use cases for\nstochastic process representations, and propose methods that accommodate each.\nEmpirically, we show that our methods are effective for learning\nrepresentations of periodic functions, 3D objects and dynamical processes. Our\nmethods tolerate noisy high-dimensional observations better than traditional\napproaches, and the learned representations transfer to a range of downstream\ntasks.",
          "link": "http://arxiv.org/abs/2106.10052",
          "publishedOn": "2021-06-21T02:07:40.145Z",
          "wordCount": 551,
          "title": "On Contrastive Representations of Stochastic Processes. (arXiv:2106.10052v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09798",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Clerico_E/0/1/0/all/0/1\">Eugenio Clerico</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1\">George Deligiannidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>",
          "description": "The limit of infinite width allows for substantial simplifications in the\nanalytical study of overparameterized neural networks. With a suitable random\ninitialization, an extremely large network is well approximated by a Gaussian\nprocess, both before and during training. In the present work, we establish a\nsimilar result for a simple stochastic architecture whose parameters are random\nvariables. The explicit evaluation of the output distribution allows for a\nPAC-Bayesian training procedure that directly optimizes the generalization\nbound. For a large but finite-width network, we show empirically on MNIST that\nthis training approach can outperform standard PAC-Bayesian methods.",
          "link": "http://arxiv.org/abs/2106.09798",
          "publishedOn": "2021-06-21T02:07:40.031Z",
          "wordCount": 532,
          "title": "Wide stochastic networks: Gaussian limit and PAC-Bayesian training. (arXiv:2106.09798v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10241",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pereira_M/0/1/0/all/0/1\">Mayana Pereira</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kshirsagar_M/0/1/0/all/0/1\">Meghana Kshirsagar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sumit Mukherjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>",
          "description": "Diferentially private (DP) synthetic datasets are a powerful approach for\ntraining machine learning models while respecting the privacy of individual\ndata providers. The effect of DP on the fairness of the resulting trained\nmodels is not yet well understood. In this contribution, we systematically\nstudy the effects of differentially private synthetic data generation on\nclassification. We analyze disparities in model utility and bias caused by the\nsynthetic dataset, measured through algorithmic fairness metrics. Our first set\nof results show that although there seems to be a clear negative correlation\nbetween privacy and utility (the more private, the less accurate) across all\ndata synthesizers we evaluated, more privacy does not necessarily imply more\nbias. Additionally, we assess the effects of utilizing synthetic datasets for\nmodel training and model evaluation. We show that results obtained on synthetic\ndata can misestimate the actual model performance when it is deployed on real\ndata. We hence advocate on the need for defining proper testing protocols in\nscenarios where differentially private synthetic datasets are utilized for\nmodel training and evaluation.",
          "link": "http://arxiv.org/abs/2106.10241",
          "publishedOn": "2021-06-21T02:07:40.024Z",
          "wordCount": 629,
          "title": "An Analysis of the Deployment of Models Trained on Private Tabular Synthetic Data: Unexpected Surprises. (arXiv:2106.10241v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2005.11115",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Steland_A/0/1/0/all/0/1\">Ansgar Steland</a>",
          "description": "Supervised learning by extreme learning machines resp. neural networks with\nrandom weights is studied under a non-stationary spatial-temporal sampling\ndesign which especially addresses settings where an autonomous object moving in\na non-stationary spatial environment collects and analyzes data. The stochastic\nmodel especially allows for spatial heterogeneity and weak dependence. As\nefficient and computationally cheap learning methods (unconstrained) least\nsquares, ridge regression and $\\ell_s$-penalized least squares (including the\nLASSO) are studied. Consistency and asymptotic normality of the least squares\nand ridge regression estimates as well as corresponding consistency results for\nthe $\\ell_s$-penalty are shown under weak conditions. The resuts also cover\nbounds for the sample squared predicition error.",
          "link": "http://arxiv.org/abs/2005.11115",
          "publishedOn": "2021-06-21T02:07:40.017Z",
          "wordCount": 581,
          "title": "Consistency of Extreme Learning Machines and Regression under Non-Stationarity and Dependence for ML-Enhanced Moving Objects. (arXiv:2005.11115v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.03180",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bhattacharya_K/0/1/0/all/0/1\">Kaushik Bhattacharya</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hosseini_B/0/1/0/all/0/1\">Bamdad Hosseini</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kovachki_N/0/1/0/all/0/1\">Nikola B. Kovachki</a>, <a href=\"http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1\">Andrew M. Stuart</a>",
          "description": "We develop a general framework for data-driven approximation of input-output\nmaps between infinite-dimensional spaces. The proposed approach is motivated by\nthe recent successes of neural networks and deep learning, in combination with\nideas from model reduction. This combination results in a neural network\napproximation which, in principle, is defined on infinite-dimensional spaces\nand, in practice, is robust to the dimension of finite-dimensional\napproximations of these spaces required for computation. For a class of\ninput-output maps, and suitably chosen probability measures on the inputs, we\nprove convergence of the proposed approximation methodology. We also include\nnumerical experiments which demonstrate the effectiveness of the method,\nshowing convergence and robustness of the approximation scheme with respect to\nthe size of the discretization, and compare it with existing algorithms from\nthe literature; our examples include the mapping from coefficient to solution\nin a divergence form elliptic partial differential equation (PDE) problem, and\nthe solution operator for viscous Burgers' equation.",
          "link": "http://arxiv.org/abs/2005.03180",
          "publishedOn": "2021-06-21T02:07:40.010Z",
          "wordCount": 626,
          "title": "Model Reduction and Neural Networks for Parametric PDEs. (arXiv:2005.03180v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aboutalebi_H/0/1/0/all/0/1\">Hossein Aboutalebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1\">Mohammad Javad Shafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karg_M/0/1/0/all/0/1\">Michelle Karg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharfenberger_C/0/1/0/all/0/1\">Christian Scharfenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>",
          "description": "Despite the significant advances in deep learning over the past decade, a\nmajor challenge that limits the wide-spread adoption of deep learning has been\ntheir fragility to adversarial attacks. This sensitivity to making erroneous\npredictions in the presence of adversarially perturbed data makes deep neural\nnetworks difficult to adopt for certain real-world, mission-critical\napplications. While much of the research focus has revolved around adversarial\nexample creation and adversarial hardening, the area of performance measures\nfor assessing adversarial robustness is not well explored. Motivated by this,\nthis study presents the concept of residual error, a new performance measure\nfor not only assessing the adversarial robustness of a deep neural network at\nthe individual sample level, but also can be used to differentiate between\nadversarial and non-adversarial examples to facilitate for adversarial example\ndetection. Furthermore, we introduce a hybrid model for approximating the\nresidual error in a tractable manner. Experimental results using the case of\nimage classification demonstrates the effectiveness and efficacy of the\nproposed residual error metric for assessing several well-known deep neural\nnetwork architectures. These results thus illustrate that the proposed measure\ncould be a useful tool for not only assessing the robustness of deep neural\nnetworks used in mission-critical scenarios, but also in the design of\nadversarially robust models.",
          "link": "http://arxiv.org/abs/2106.10212",
          "publishedOn": "2021-06-21T02:07:40.004Z",
          "wordCount": 657,
          "title": "Residual Error: a New Performance Measure for Adversarial Robustness. (arXiv:2106.10212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2001.09136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Byerly_A/0/1/0/all/0/1\">Adam Byerly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalganova_T/0/1/0/all/0/1\">Tatiana Kalganova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dear_I/0/1/0/all/0/1\">Ian Dear</a>",
          "description": "Most capsule network designs rely on traditional matrix multiplication\nbetween capsule layers and computationally expensive routing mechanisms to deal\nwith the capsule dimensional entanglement that the matrix multiplication\nintroduces. By using Homogeneous Vector Capsules (HVCs), which use element-wise\nmultiplication rather than matrix multiplication, the dimensions of the\ncapsules remain unentangled. In this work, we study HVCs as applied to the\nhighly structured MNIST dataset in order to produce a direct comparison to the\ncapsule research direction of Geoffrey Hinton, et al. In our study, we show\nthat a simple convolutional neural network using HVCs performs as well as the\nprior best performing capsule network on MNIST using 5.5x fewer parameters, 4x\nfewer training epochs, no reconstruction sub-network, and requiring no routing\nmechanism. The addition of multiple classification branches to the network\nestablishes a new state of the art for the MNIST dataset with an accuracy of\n99.87% for an ensemble of these models, as well as establishing a new state of\nthe art for a single model (99.83% accurate).",
          "link": "http://arxiv.org/abs/2001.09136",
          "publishedOn": "2021-06-21T02:07:39.984Z",
          "wordCount": 671,
          "title": "No Routing Needed Between Capsules. (arXiv:2001.09136v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sajadmanesh_S/0/1/0/all/0/1\">Sina Sajadmanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatica_Perez_D/0/1/0/all/0/1\">Daniel Gatica-Perez</a>",
          "description": "Graph Neural Networks (GNNs) have demonstrated superior performance in\nlearning node representations for various graph inference tasks. However,\nlearning over graph data can raise privacy concerns when nodes represent people\nor human-related variables that involve sensitive or personal information.\nWhile numerous techniques have been proposed for privacy-preserving deep\nlearning over non-relational data, there is less work addressing the privacy\nissues pertained to applying deep learning algorithms on graphs. In this paper,\nwe study the problem of node data privacy, where graph nodes have potentially\nsensitive data that is kept private, but they could be beneficial for a central\nserver for training a GNN over the graph. To address this problem, we develop a\nprivacy-preserving, architecture-agnostic GNN learning algorithm with formal\nprivacy guarantees based on Local Differential Privacy (LDP). Specifically, we\npropose an LDP encoder and an unbiased rectifier, by which the server can\ncommunicate with the graph nodes to privately collect their data and\napproximate the GNN's first layer. To further reduce the effect of the injected\nnoise, we propose to prepend a simple graph convolution layer, called KProp,\nwhich is based on the multi-hop aggregation of the nodes' features acting as a\ndenoising mechanism. Finally, we propose a robust training framework, in which\nwe benefit from KProp's denoising capability to increase the accuracy of\ninference in the presence of noisy labels. Extensive experiments conducted over\nreal-world datasets demonstrate that our method can maintain a satisfying level\nof accuracy with low privacy loss.",
          "link": "http://arxiv.org/abs/2006.05535",
          "publishedOn": "2021-06-21T02:07:39.978Z",
          "wordCount": 759,
          "title": "Locally Private Graph Neural Networks. (arXiv:2006.05535v8 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tebbutt_W/0/1/0/all/0/1\">Will Tebbutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1\">Arno Solin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1\">Richard E. Turner</a>",
          "description": "Gaussian processes (GPs) are important probabilistic tools for inference and\nlearning in spatio-temporal modelling problems such as those in climate science\nand epidemiology. However, existing GP approximations do not simultaneously\nsupport large numbers of off-the-grid spatial data-points and long time-series\nwhich is a hallmark of many applications.\n\nPseudo-point approximations, one of the gold-standard methods for scaling GPs\nto large data sets, are well suited for handling off-the-grid spatial data.\nHowever, they cannot handle long temporal observation horizons effectively\nreverting to cubic computational scaling in the time dimension. State space GP\napproximations are well suited to handling temporal data, if the temporal GP\nprior admits a Markov form, leading to linear complexity in the number of\ntemporal observations, but have a cubic spatial cost and cannot handle\noff-the-grid spatial data.\n\nIn this work we show that there is a simple and elegant way to combine\npseudo-point methods with the state space GP approximation framework to get the\nbest of both worlds. The approach hinges on a surprising conditional\nindependence property which applies to space--time separable GPs. We\ndemonstrate empirically that the combined approach is more scalable and\napplicable to a greater range of spatio-temporal problems than either method on\nits own.",
          "link": "http://arxiv.org/abs/2106.10210",
          "publishedOn": "2021-06-21T02:07:39.970Z",
          "wordCount": 638,
          "title": "Combining Pseudo-Point and State Space Approximations for Sum-Separable Gaussian Processes. (arXiv:2106.10210v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mak_C/0/1/0/all/0/1\">Carol Mak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiser_F/0/1/0/all/0/1\">Fabian Zaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_L/0/1/0/all/0/1\">Luke Ong</a>",
          "description": "Probabilistic programming uses programs to express generative models whose\nposterior probability is then computed by built-in inference engines. A\nchallenging goal is to develop general purpose inference algorithms that work\nout-of-the-box for arbitrary programs in a universal probabilistic programming\nlanguage (PPL). The densities defined by such programs, which may use\nstochastic branching and recursion, are (in general) nonparametric, in the\nsense that they correspond to models on an infinite-dimensional parameter\nspace. However standard inference algorithms, such as the Hamiltonian Monte\nCarlo (HMC) algorithm, target distributions with a fixed number of parameters.\nThis paper introduces the Nonparametric Hamiltonian Monte Carlo (NP-HMC)\nalgorithm which generalises HMC to nonparametric models. Inputs to NP-HMC are a\nnew class of measurable functions called \"tree representable\", which serve as a\nlanguage-independent representation of the density functions of probabilistic\nprograms in a universal PPL. We provide a correctness proof of NP-HMC, and\nempirically demonstrate significant performance improvements over existing\napproaches on several nonparametric examples.",
          "link": "http://arxiv.org/abs/2106.10238",
          "publishedOn": "2021-06-21T02:07:39.963Z",
          "wordCount": 608,
          "title": "Nonparametric Hamiltonian Monte Carlo. (arXiv:2106.10238v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.02682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biau_G/0/1/0/all/0/1\">G&#xe9;rard Biau</a> (LPSM (UMR\\_8001)), <a href=\"http://arxiv.org/find/cs/1/au:+Sangnier_M/0/1/0/all/0/1\">Maxime Sangnier</a> (LPSM (UMR\\_8001)), <a href=\"http://arxiv.org/find/cs/1/au:+Tanielian_U/0/1/0/all/0/1\">Ugo Tanielian</a> (LPSM (UMR\\_8001))",
          "description": "Generative Adversarial Networks (GANs) have been successful in producing\noutstanding results in areas as diverse as image, video, and text generation.\nBuilding on these successes, a large number of empirical studies have validated\nthe benefits of the cousin approach called Wasserstein GANs (WGANs), which\nbrings stabilization in the training process. In the present paper, we add a\nnew stone to the edifice by proposing some theoretical advances in the\nproperties of WGANs. First, we properly define the architecture of WGANs in the\ncontext of integral probability metrics parameterized by neural networks and\nhighlight some of their basic mathematical features. We stress in particular\ninteresting optimization properties arising from the use of a parametric\n1-Lipschitz discriminator. Then, in a statistically-driven approach, we study\nthe convergence of empirical WGANs as the sample size tends to infinity, and\nclarify the adversarial effects of the generator and the discriminator by\nunderlining some trade-off properties. These features are finally illustrated\nwith experiments using both synthetic and real-world datasets.",
          "link": "http://arxiv.org/abs/2006.02682",
          "publishedOn": "2021-06-21T02:07:39.956Z",
          "wordCount": 639,
          "title": "Some Theoretical Insights into Wasserstein GANs. (arXiv:2006.02682v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.05006",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Miyazawa_S/0/1/0/all/0/1\">Sanzo Miyazawa</a>",
          "description": "The inverse Potts problem to infer a Boltzmann distribution for homologous\nprotein sequences from their single-site and pairwise amino acid frequencies\nrecently attracts a great deal of attention in the studies of protein structure\nand evolution. We study regularization and learning methods and how to tune\nregularization parameters to correctly infer interactions in Boltzmann machine\nlearning. Using $L_2$ regularization for fields, group $L_1$ for couplings is\nshown to be very effective for sparse couplings in comparison with $L_2$ and\n$L_1$. Two regularization parameters are tuned to yield equal values for both\nthe sample and ensemble averages of evolutionary energy. Both averages smoothly\nchange and converge, but their learning profiles are very different between\nlearning methods. The Adam method is modified to make stepsize proportional to\nthe gradient for sparse couplings and to use a soft-thresholding function for\ngroup $L_1$. It is shown by first inferring interactions from protein sequences\nand then from Monte Carlo samples that the fields and couplings can be well\nrecovered, but that recovering the pairwise correlations in the resolution of a\ntotal energy is harder for the natural proteins than for the protein-like\nsequences. Selective temperature for folding/structural constrains in protein\nevolution is also estimated.",
          "link": "http://arxiv.org/abs/1909.05006",
          "publishedOn": "2021-06-21T02:07:39.936Z",
          "wordCount": 754,
          "title": "Boltzmann machine learning and regularization methods for inferring evolutionary fields and couplings from a multiple sequence alignment. (arXiv:1909.05006v3 [q-bio.PE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10259",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cattiau_J/0/1/0/all/0/1\">Julie Cattiau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1\">Angad Chandorkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>",
          "description": "While current state-of-the-art Automatic Speech Recognition (ASR) systems\nachieve high accuracy on typical speech, they suffer from significant\nperformance degradation on disordered speech and other atypical speech\npatterns. Personalization of ASR models, a commonly applied solution to this\nproblem, is usually performed in a server-based training environment posing\nproblems around data privacy, delayed model-update times, and communication\ncost for copying data and models between mobile device and server\ninfrastructure. In this paper, we present an approach to on-device based ASR\npersonalization with very small amounts of speaker-specific data. We test our\napproach on a diverse set of 100 speakers with disordered speech and find\nmedian relative word error rate improvement of 71% with only 50 short\nutterances required per speaker. When tested on a voice-controlled home\nautomation platform, on-device personalized models show a median task success\nrate of 81%, compared to only 40% of the unadapted models.",
          "link": "http://arxiv.org/abs/2106.10259",
          "publishedOn": "2021-06-21T02:07:39.929Z",
          "wordCount": 604,
          "title": "On-Device Personalization of Automatic Speech Recognition Models for Disordered Speech. (arXiv:2106.10259v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/1910.01210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1\">Mihir Prabhudesai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1\">Hsiao-Yu Fish Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Syed Ashar Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sieb_M/0/1/0/all/0/1\">Maximilian Sieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1\">Adam W. Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>",
          "description": "We propose associating language utterances to 3D visual abstractions of the\nscene they describe. The 3D visual abstractions are encoded as 3-dimensional\nvisual feature maps. We infer these 3D visual scene feature maps from RGB\nimages of the scene via view prediction: when the generated 3D scene feature\nmap is neurally projected from a camera viewpoint, it should match the\ncorresponding RGB image. We present generative models that condition on the\ndependency tree of an utterance and generate a corresponding visual 3D feature\nmap as well as reason about its plausibility, and detector models that\ncondition on both the dependency tree of an utterance and a related image and\nlocalize the object referents in the 3D feature map inferred from the image.\nOur model outperforms models of language and vision that associate language\nwith 2D CNN activations or 2D images by a large margin in a variety of tasks,\nsuch as, classifying plausibility of utterances, detecting referential\nexpressions, and supplying rewards for trajectory optimization of object\nplacement policies from language instructions. We perform numerous ablations\nand show the improved performance of our detectors is due to its better\ngeneralization across camera viewpoints and lack of object interferences in the\ninferred 3D feature space, and the improved performance of our generators is\ndue to their ability to spatially reason about objects and their configurations\nin 3D when mapping from language to scenes.",
          "link": "http://arxiv.org/abs/1910.01210",
          "publishedOn": "2021-06-21T02:07:39.922Z",
          "wordCount": 735,
          "title": "Embodied Language Grounding with 3D Visual Feature Representations. (arXiv:1910.01210v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xinjie Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barner_K/0/1/0/all/0/1\">Kenneth Barner</a>",
          "description": "Recently, Mutual Information (MI) has attracted attention in bounding the\ngeneralization error of Deep Neural Networks (DNNs). However, it is intractable\nto accurately estimate the MI in DNNs, thus most previous works have to relax\nthe MI bound, which in turn weakens the information theoretic explanation for\ngeneralization. To address the limitation, this paper introduces a\nprobabilistic representation of DNNs for accurately estimating the MI.\nLeveraging the proposed MI estimator, we validate the information theoretic\nexplanation for generalization, and derive a tighter generalization bound than\nthe state-of-the-art relaxations.",
          "link": "http://arxiv.org/abs/2106.10262",
          "publishedOn": "2021-06-21T02:07:39.915Z",
          "wordCount": 540,
          "title": "A Probabilistic Representation of DNNs: Bridging Mutual Information and Generalization. (arXiv:2106.10262v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1908.03840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajapaksha_D/0/1/0/all/0/1\">Dilini Rajapaksha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1\">Christoph Bergmeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>",
          "description": "As we rely more and more on machine learning models for real-life\ndecision-making, being able to understand and trust the predictions becomes\never more important. Local explainer models have recently been introduced to\nexplain the predictions of complex machine learning models at the instance\nlevel. In this paper, we propose Local Rule-based Model Interpretability with\nk-optimal Associations (LoRMIkA), a novel model-agnostic approach that obtains\nk-optimal association rules from a neighbourhood of the instance to be\nexplained. Compared with other rule-based approaches in the literature, we\nargue that the most predictive rules are not necessarily the rules that provide\nthe best explanations. Consequently, the LoRMIkA framework provides a flexible\nway to obtain predictive and interesting rules. It uses an efficient search\nalgorithm guaranteed to find the k-optimal rules with respect to objectives\nsuch as confidence, lift, leverage, coverage, and support. It also provides\nmultiple rules which explain the decision and counterfactual rules, which give\nindications for potential changes to obtain different outputs for given\ninstances. We compare our approach to other state-of-the-art approaches in\nlocal model interpretability on three different datasets and achieve\ncompetitive results in terms of local accuracy and interpretability.",
          "link": "http://arxiv.org/abs/1908.03840",
          "publishedOn": "2021-06-21T02:07:39.909Z",
          "wordCount": 668,
          "title": "LoRMIkA: Local rule-based model interpretability with k-optimal associations. (arXiv:1908.03840v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_F/0/1/0/all/0/1\">Florian Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1\">Johannes F&#xfc;rnkranz</a>",
          "description": "Inductive rule learning is arguably among the most traditional paradigms in\nmachine learning. Although we have seen considerable progress over the years in\nlearning rule-based theories, all state-of-the-art learners still learn\ndescriptions that directly relate the input features to the target concept. In\nthe simplest case, concept learning, this is a disjunctive normal form (DNF)\ndescription of the positive class. While it is clear that this is sufficient\nfrom a logical point of view because every logical expression can be reduced to\nan equivalent DNF expression, it could nevertheless be the case that more\nstructured representations, which form deep theories by forming intermediate\nconcepts, could be easier to learn, in very much the same way as deep neural\nnetworks are able to outperform shallow networks, even though the latter are\nalso universal function approximators. In this paper, we empirically compare\ndeep and shallow rule learning with a uniform general algorithm, which relies\non greedy mini-batch based optimization. Our experiments on both artificial and\nreal-world benchmark data indicate that deep rule networks outperform shallow\nnetworks.",
          "link": "http://arxiv.org/abs/2106.10254",
          "publishedOn": "2021-06-21T02:07:39.890Z",
          "wordCount": 605,
          "title": "An Empirical Investigation into Deep and Shallow Rule Learning. (arXiv:2106.10254v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Samuel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1\">Brandon Amos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1\">Yaron Lipman</a>",
          "description": "Modeling distributions on Riemannian manifolds is a crucial component in\nunderstanding non-Euclidean data that arises, e.g., in physics and geology. The\nbudding approaches in this space are limited by representational and\ncomputational tradeoffs. We propose and study a class of flows that uses convex\npotentials from Riemannian optimal transport. These are universal and can model\ndistributions on any compact Riemannian manifold without requiring domain\nknowledge of the manifold to be integrated into the architecture. We\ndemonstrate that these flows can model standard distributions on spheres, and\ntori, on synthetic and geological data. Our source code is freely available\nonline at this http URL",
          "link": "http://arxiv.org/abs/2106.10272",
          "publishedOn": "2021-06-21T02:07:39.884Z",
          "wordCount": 530,
          "title": "Riemannian Convex Potential Maps. (arXiv:2106.10272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10234",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_J/0/1/0/all/0/1\">Jinhua Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Inspired by its success in natural language processing and computer vision,\npre-training has attracted substantial attention in cheminformatics and\nbioinformatics, especially for molecule based tasks. A molecule can be\nrepresented by either a graph (where atoms are connected by bonds) or a SMILES\nsequence (where depth-first-search is applied to the molecular graph with\nspecific rules). Existing works on molecule pre-training use either graph\nrepresentations only or SMILES representations only. In this work, we propose\nto leverage both the representations and design a new pre-training algorithm,\ndual-view molecule pre-training (briefly, DMP), that can effectively combine\nthe strengths of both types of molecule representations. The model of DMP\nconsists of two branches: a Transformer branch that takes the SMILES sequence\nof a molecule as input, and a GNN branch that takes a molecular graph as input.\nThe training of DMP contains three tasks: (1) predicting masked tokens in a\nSMILES sequence by the Transformer branch, (2) predicting masked atoms in a\nmolecular graph by the GNN branch, and (3) maximizing the consistency between\nthe two high-level representations output by the Transformer and GNN branches\nseparately. After pre-training, we can use either the Transformer branch (this\none is recommended according to empirical results), the GNN branch, or both for\ndownstream tasks. DMP is tested on nine molecular property prediction tasks and\nachieves state-of-the-art performances on seven of them. Furthermore, we test\nDMP on three retrosynthesis tasks and achieve state-of-the-result on the\nUSPTO-full dataset. Our code will be released soon.",
          "link": "http://arxiv.org/abs/2106.10234",
          "publishedOn": "2021-06-21T02:07:39.878Z",
          "wordCount": 678,
          "title": "Dual-view Molecule Pre-training. (arXiv:2106.10234v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10188",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Neklyudov_K/0/1/0/all/0/1\">Kirill Neklyudov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bondesan_R/0/1/0/all/0/1\">Roberto Bondesan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>",
          "description": "Deterministic dynamics is an essential part of many MCMC algorithms, e.g.\nHybrid Monte Carlo or samplers utilizing normalizing flows. This paper presents\na general construction of deterministic measure-preserving dynamics using\nautonomous ODEs and tools from differential geometry. We show how Hybrid Monte\nCarlo and other deterministic samplers follow as special cases of our theory.\nWe then demonstrate the utility of our approach by constructing a continuous\nnon-sequential version of Gibbs sampling in terms of an ODE flow and extending\nit to discrete state spaces. We find that our deterministic samplers are more\nsample efficient than stochastic counterparts, even if the latter generate\nindependent samples.",
          "link": "http://arxiv.org/abs/2106.10188",
          "publishedOn": "2021-06-21T02:07:39.870Z",
          "wordCount": 532,
          "title": "Deterministic Gibbs Sampling via Ordinary Differential Equations. (arXiv:2106.10188v1 [stat.CO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diskin_M/0/1/0/all/0/1\">Michael Diskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukhtiyarov_A/0/1/0/all/0/1\">Alexey Bukhtiyarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1\">Max Ryabinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saulnier_L/0/1/0/all/0/1\">Lucile Saulnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoest_Q/0/1/0/all/0/1\">Quentin Lhoest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinitsin_A/0/1/0/all/0/1\">Anton Sinitsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popov_D/0/1/0/all/0/1\">Dmitry Popov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyrkin_D/0/1/0/all/0/1\">Dmitry Pyrkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashirin_M/0/1/0/all/0/1\">Maxim Kashirin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borzunov_A/0/1/0/all/0/1\">Alexander Borzunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moral_A/0/1/0/all/0/1\">Albert Villanova del Moral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazur_D/0/1/0/all/0/1\">Denis Mazur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobelev_I/0/1/0/all/0/1\">Ilia Kobelev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1\">Thomas Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pekhimenko_G/0/1/0/all/0/1\">Gennady Pekhimenko</a>",
          "description": "Modern deep learning applications require increasingly more compute to train\nstate-of-the-art models. To address this demand, large corporations and\ninstitutions use dedicated High-Performance Computing clusters, whose\nconstruction and maintenance are both environmentally costly and well beyond\nthe budget of most organizations. As a result, some research directions become\nthe exclusive domain of a few large industrial and even fewer academic actors.\nTo alleviate this disparity, smaller groups may pool their computational\nresources and run collaborative experiments that benefit all participants. This\nparadigm, known as grid- or volunteer computing, has seen successful\napplications in numerous scientific areas. However, using this approach for\nmachine learning is difficult due to high latency, asymmetric bandwidth, and\nseveral challenges unique to volunteer computing. In this work, we carefully\nanalyze these constraints and propose a novel algorithmic framework designed\nspecifically for collaborative training. We demonstrate the effectiveness of\nour approach for SwAV and ALBERT pretraining in realistic conditions and\nachieve performance comparable to traditional setups at a fraction of the cost.\nFinally, we provide a detailed report of successful collaborative language\nmodel pretraining with 40 participants.",
          "link": "http://arxiv.org/abs/2106.10207",
          "publishedOn": "2021-06-21T02:07:39.863Z",
          "wordCount": 648,
          "title": "Distributed Deep Learning in Open Collaborations. (arXiv:2106.10207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.05551",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rothfuss_J/0/1/0/all/0/1\">Jonas Rothfuss</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Josifoski_M/0/1/0/all/0/1\">Martin Josifoski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>",
          "description": "Meta-learning can successfully acquire useful inductive biases from data.\nYet, its generalization properties to unseen learning tasks are poorly\nunderstood. Particularly if the number of meta-training tasks is small, this\nraises concerns about overfitting. We provide a theoretical analysis using the\nPAC-Bayesian framework and derive novel generalization bounds for\nmeta-learning. Using these bounds, we develop a class of PAC-optimal\nmeta-learning algorithms with performance guarantees and a principled\nmeta-level regularization. Unlike previous PAC-Bayesian meta-learners, our\nmethod results in a standard stochastic optimization problem which can be\nsolved efficiently and scales well. When instantiating our PAC-optimal\nhyper-posterior (PACOH) with Gaussian processes and Bayesian Neural Networks as\nbase learners, the resulting methods yield state-of-the-art performance, both\nin terms of predictive accuracy and the quality of uncertainty estimates.\nThanks to their principled treatment of uncertainty, our meta-learners can also\nbe successfully employed for sequential decision problems.",
          "link": "http://arxiv.org/abs/2002.05551",
          "publishedOn": "2021-06-21T02:07:39.845Z",
          "wordCount": 624,
          "title": "PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees. (arXiv:2002.05551v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10229",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Karanasou_P/0/1/0/all/0/1\">Penny Karanasou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karlapati_S/0/1/0/all/0/1\">Sri Karlapati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1\">Alexis Moinet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1\">Arnaud Joly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbas_A/0/1/0/all/0/1\">Ammar Abbas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slangen_S/0/1/0/all/0/1\">Simon Slangen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trueba_J/0/1/0/all/0/1\">Jaime Lorenzo Trueba</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1\">Thomas Drugman</a>",
          "description": "Many factors influence speech yielding different renditions of a given\nsentence. Generative models, such as variational autoencoders (VAEs), capture\nthis variability and allow multiple renditions of the same sentence via\nsampling. The degree of prosodic variability depends heavily on the prior that\nis used when sampling. In this paper, we propose a novel method to compute an\ninformative prior for the VAE latent space of a neural text-to-speech (TTS)\nsystem. By doing so, we aim to sample with more prosodic variability, while\ngaining controllability over the latent space's structure.\n\nBy using as prior the posterior distribution of a secondary VAE, which we\ncondition on a speaker vector, we can sample from the primary VAE taking\nexplicitly the conditioning into account and resulting in samples from a\nspecific region of the latent space for each condition (i.e. speaker). A formal\npreference test demonstrates significant preference of the proposed approach\nover standard Conditional VAE. We also provide visualisations of the latent\nspace where well-separated condition-specific clusters appear, as well as\nablation studies to better understand the behaviour of the system.",
          "link": "http://arxiv.org/abs/2106.10229",
          "publishedOn": "2021-06-21T02:07:39.838Z",
          "wordCount": 648,
          "title": "A learned conditional prior for the VAE acoustic space of a TTS system. (arXiv:2106.10229v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1\">Kailas Vodrahalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>",
          "description": "Transfer learning aims to leverage models pre-trained on source data to\nefficiently adapt to target setting, where only limited data are available for\nmodel fine-tuning. Recent works empirically demonstrate that adversarial\ntraining in the source data can improve the ability of models to transfer to\nnew domains. However, why this happens is not known. In this paper, we provide\na theoretical model to rigorously analyze how adversarial training helps\ntransfer learning. We show that adversarial training in the source data\ngenerates provably better representations, so fine-tuning on top of this\nrepresentation leads to a more accurate predictor of the target data. We\nfurther demonstrate both theoretically and empirically that semi-supervised\nlearning in the source data can also improve transfer learning by similarly\nimproving the representation. Moreover, performing adversarial training on top\nof semi-supervised learning can further improve transferability, suggesting\nthat the two approaches have complementary benefits on representations. We\nsupport our theories with experiments on popular data sets and deep learning\narchitectures.",
          "link": "http://arxiv.org/abs/2106.10189",
          "publishedOn": "2021-06-21T02:07:39.830Z",
          "wordCount": 593,
          "title": "Adversarial Training Helps Transfer Learning via Better Representations. (arXiv:2106.10189v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10236",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Deo_A/0/1/0/all/0/1\">Anand Deo</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Murthy_K/0/1/0/all/0/1\">Karthyek Murthy</a>",
          "description": "This paper considers Importance Sampling (IS) for the estimation of tail\nrisks of a loss defined in terms of a sophisticated object such as a machine\nlearning feature map or a mixed integer linear optimisation formulation.\nAssuming only black-box access to the loss and the distribution of the\nunderlying random vector, the paper presents an efficient IS algorithm for\nestimating the Value at Risk and Conditional Value at Risk. The key challenge\nin any IS procedure, namely, identifying an appropriate change-of-measure, is\nautomated with a self-structuring IS transformation that learns and replicates\nthe concentration properties of the conditional excess from less rare samples.\nThe resulting estimators enjoy asymptotically optimal variance reduction when\nviewed in the logarithmic scale. Simulation experiments highlight the efficacy\nand practicality of the proposed scheme",
          "link": "http://arxiv.org/abs/2106.10236",
          "publishedOn": "2021-06-21T02:07:39.822Z",
          "wordCount": 572,
          "title": "Efficient Black-Box Importance Sampling for VaR and CVaR Estimation. (arXiv:2106.10236v1 [q-fin.RM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaken_E/0/1/0/all/0/1\">Elad Ben Zaken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>",
          "description": "We show that with small-to-medium training data, fine-tuning only the bias\nterms (or a subset of the bias terms) of pre-trained BERT models is competitive\nwith (and sometimes better than) fine-tuning the entire model. For larger data,\nbias-only fine-tuning is competitive with other sparse fine-tuning methods.\nBesides their practical utility, these findings are relevant for the question\nof understanding the commonly-used process of finetuning: they support the\nhypothesis that finetuning is mainly about exposing knowledge induced by\nlanguage-modeling training, rather than learning new task-specific linguistic\nknowledge.",
          "link": "http://arxiv.org/abs/2106.10199",
          "publishedOn": "2021-06-21T02:07:39.815Z",
          "wordCount": 520,
          "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashidinejad_P/0/1/0/all/0/1\">Paria Rashidinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jiantao Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1\">Stuart Russell</a>",
          "description": "In online reinforcement learning (RL), efficient exploration remains\nparticularly challenging in high-dimensional environments with sparse rewards.\nIn low-dimensional environments, where tabular parameterization is possible,\ncount-based upper confidence bound (UCB) exploration methods achieve minimax\nnear-optimal rates. However, it remains unclear how to efficiently implement\nUCB in realistic RL tasks that involve non-linear function approximation. To\naddress this, we propose a new exploration approach via \\textit{maximizing} the\ndeviation of the occupancy of the next policy from the explored regions. We add\nthis term as an adaptive regularizer to the standard RL objective to balance\nexploration vs. exploitation. We pair the new objective with a provably\nconvergent algorithm, giving rise to a new intrinsic reward that adjusts\nexisting bonuses. The proposed intrinsic reward is easy to implement and\ncombine with other existing RL algorithms to conduct exploration. As a proof of\nconcept, we evaluate the new intrinsic reward on tabular examples across a\nvariety of model-based and model-free algorithms, showing improvements over\ncount-only exploration strategies. When tested on navigation and locomotion\ntasks from MiniGrid and DeepMind Control Suite benchmarks, our approach\nsignificantly improves sample efficiency over state-of-the-art methods. Our\ncode is available at https://github.com/tianjunz/MADE.",
          "link": "http://arxiv.org/abs/2106.10268",
          "publishedOn": "2021-06-21T02:07:39.809Z",
          "wordCount": 641,
          "title": "MADE: Exploration via Maximizing Deviation from Explored Regions. (arXiv:2106.10268v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruirui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Chelsea J.-T. Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Hongda Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elibol_O/0/1/0/all/0/1\">Oguz Elibol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>",
          "description": "By implicitly recognizing a user based on his/her speech input, speaker\nidentification enables many downstream applications, such as personalized\nsystem behavior and expedited shopping checkouts. Based on whether the speech\ncontent is constrained or not, both text-dependent (TD) and text-independent\n(TI) speaker recognition models may be used. We wish to combine the advantages\nof both types of models through an ensemble system to make more reliable\npredictions. However, any such combined approach has to be robust to incomplete\ninputs, i.e., when either TD or TI input is missing. As a solution we propose a\nfusion of embeddings network foenet architecture, combining joint learning with\nneural attention. We compare foenet with four competitive baseline methods on a\ndataset of voice assistant inputs, and show that it achieves higher accuracy\nthan the baseline and score fusion methods, especially in the presence of\nincomplete inputs.",
          "link": "http://arxiv.org/abs/2106.10169",
          "publishedOn": "2021-06-21T02:07:39.790Z",
          "wordCount": 603,
          "title": "Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition. (arXiv:2106.10169v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fengyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runchuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng Zhong</a>",
          "description": "In recent years, phishing scams have become the crime type with the largest\nmoney involved on Ethereum, the second-largest blockchain platform. Meanwhile,\ngraph neural network (GNN) has shown promising performance in various node\nclassification tasks. However, for Ethereum transaction data, which could be\nnaturally abstracted to a real-world complex graph, the scarcity of labels and\nthe huge volume of transaction data make it difficult to take advantage of GNN\nmethods. Here in this paper, to address the two challenges, we propose a\nSelf-supervised Incremental deep Graph learning model (SIEGE), for the phishing\nscam detection problem on Ethereum. In our model, two pretext tasks designed\nfrom spatial and temporal perspectives help us effectively learn useful node\nembedding from the huge amount of unlabelled transaction data. And the\nincremental paradigm allows us to efficiently handle large-scale transaction\ndata and help the model maintain good performance when the data distribution is\ndrastically changing. We collect transaction records about half a year from\nEthereum and our extensive experiments show that our model consistently\noutperforms strong baselines in both transductive and inductive settings.",
          "link": "http://arxiv.org/abs/2106.10176",
          "publishedOn": "2021-06-21T02:07:39.783Z",
          "wordCount": 616,
          "title": "Self-supervised Incremental Deep Graph Learning for Ethereum Phishing Scam Detection. (arXiv:2106.10176v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1\">Emre Ozfatura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hameed_M/0/1/0/all/0/1\">Muhammad Zaid Hameed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozfatura_K/0/1/0/all/0/1\">Kerem Ozfatura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>",
          "description": "A common observation regarding adversarial attacks is that they mostly give\nrise to false activation at the penultimate layer to fool the classifier.\nAssuming that these activation values correspond to certain features of the\ninput, the objective becomes choosing the features that are most useful for\nclassification. Hence, we propose a novel approach to identify the important\nfeatures by employing counter-adversarial attacks, which highlights the\nconsistency at the penultimate layer with respect to perturbations on input\nsamples. First, we empirically show that there exist a subset of features,\nclassification based in which bridge the gap between the clean and robust\naccuracy. Second, we propose a simple yet efficient mechanism to identify those\nfeatures by searching the neighborhood of input sample. We then select features\nby observing the consistency of the activation values at the penultimate layer.",
          "link": "http://arxiv.org/abs/2106.10252",
          "publishedOn": "2021-06-21T02:07:39.776Z",
          "wordCount": 585,
          "title": "Less is More: Feature Selection for Adversarial Robustness with Compressive Counter-Adversarial Attacks. (arXiv:2106.10252v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zejiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Kuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>",
          "description": "Deep neural networks (DNNs) are effective in solving many real-world\nproblems. Larger DNN models usually exhibit better quality (e.g., accuracy) but\ntheir excessive computation results in long training and inference time. Model\nsparsification can reduce the computation and memory cost while maintaining\nmodel quality. Most existing sparsification algorithms unidirectionally remove\nweights, while others randomly or greedily explore a small subset of weights in\neach layer. The inefficiency of the algorithms reduces the achievable sparsity\nlevel. In addition, many algorithms still require pre-trained dense models and\nthus suffer from large memory footprint and long training time. In this paper,\nwe propose a novel scheduled grow-and-prune (GaP) methodology without\npre-training the dense models. It addresses the shortcomings of the previous\nworks by repeatedly growing a subset of layers to dense and then pruning back\nto sparse after some training. Experiments have shown that such models can\nmatch or beat the quality of highly optimized dense models at 80% sparsity on a\nvariety of tasks, such as image classification, objective detection, 3D object\npart segmentation, and translation. They also outperform other state-of-the-art\n(SOTA) pruning methods, including pruning from pre-trained dense models. As an\nexample, a 90% sparse ResNet-50 obtained via GaP achieves 77.9% top-1 accuracy\non ImageNet, improving the SOTA results by 1.5%.",
          "link": "http://arxiv.org/abs/2106.09857",
          "publishedOn": "2021-06-21T02:07:39.768Z",
          "wordCount": 669,
          "title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods. (arXiv:2106.09857v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Junyuan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiayu Zhou</a>",
          "description": "Federated learning (FL) emerges as a popular distributed learning schema that\nlearns a model from a set of participating users without requiring raw data to\nbe shared. One major challenge of FL comes from heterogeneity in users, which\nmay have distributionally different (or non-iid) data and varying computation\nresources. Just like in centralized learning, FL users also desire model\nrobustness against malicious attackers at test time. Whereas adversarial\ntraining (AT) provides a sound solution for centralized learning, extending its\nusage for FL users has imposed significant challenges, as many users may have\nvery limited training data as well as tight computational budgets, to afford\nthe data-hungry and costly AT. In this paper, we study a novel learning setting\nthat propagates adversarial robustness from high-resource users that can afford\nAT, to those low-resource users that cannot afford it, during the FL process.\nWe show that existing FL techniques cannot effectively propagate adversarial\nrobustness among non-iid users, and propose a simple yet effective propagation\napproach that transfers robustness through carefully designed\nbatch-normalization statistics. We demonstrate the rationality and\neffectiveness of our method through extensive experiments. Especially, the\nproposed method is shown to grant FL remarkable robustness even when only a\nsmall portion of users afford AT during learning. Codes will be published upon\nacceptance.",
          "link": "http://arxiv.org/abs/2106.10196",
          "publishedOn": "2021-06-21T02:07:39.730Z",
          "wordCount": 656,
          "title": "Federated Robustness Propagation: Sharing Adversarial Robustness in Federated Learning. (arXiv:2106.10196v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_F/0/1/0/all/0/1\">Florian Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1\">Johannes F&#xfc;rnkranz</a>",
          "description": "We investigate whether it is possible to learn rule sets efficiently in a\nnetwork structure with a single hidden layer using iterative refinements over\nmini-batches of examples. A first rudimentary version shows an acceptable\nperformance on all but one dataset, even though it does not yet reach the\nperformance levels of Ripper.",
          "link": "http://arxiv.org/abs/2106.10202",
          "publishedOn": "2021-06-21T02:07:39.723Z",
          "wordCount": 491,
          "title": "An Investigation into Mini-Batch Rule Learning. (arXiv:2106.10202v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heidrich_B/0/1/0/all/0/1\">Benedikt Heidrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartschat_A/0/1/0/all/0/1\">Andreas Bartschat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turowski_M/0/1/0/all/0/1\">Marian Turowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_O/0/1/0/all/0/1\">Oliver Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phipps_K/0/1/0/all/0/1\">Kaleb Phipps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meisenbacher_S/0/1/0/all/0/1\">Stefan Meisenbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmieder_K/0/1/0/all/0/1\">Kai Schmieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludwig_N/0/1/0/all/0/1\">Nicole Ludwig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikut_R/0/1/0/all/0/1\">Ralf Mikut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagenmeyer_V/0/1/0/all/0/1\">Veit Hagenmeyer</a>",
          "description": "Time series data are fundamental for a variety of applications, ranging from\nfinancial markets to energy systems. Due to their importance, the number and\ncomplexity of tools and methods used for time series analysis is constantly\nincreasing. However, due to unclear APIs and a lack of documentation,\nresearchers struggle to integrate them into their research projects and\nreplicate results. Additionally, in time series analysis there exist many\nrepetitive tasks, which are often re-implemented for each project,\nunnecessarily costing time. To solve these problems we present\n\\texttt{pyWATTS}, an open-source Python-based package that is a non-sequential\nworkflow automation tool for the analysis of time series data. pyWATTS includes\nmodules with clearly defined interfaces to enable seamless integration of new\nor existing methods, subpipelining to easily reproduce repetitive tasks, load\nand save functionality to simply replicate results, and native support for key\nPython machine learning libraries such as scikit-learn, PyTorch, and Keras.",
          "link": "http://arxiv.org/abs/2106.10157",
          "publishedOn": "2021-06-21T02:07:39.716Z",
          "wordCount": 590,
          "title": "pyWATTS: Python Workflow Automation Tool for Time Series. (arXiv:2106.10157v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Suyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wonho Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1\">Suman Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Meeyoung Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sooel Son</a>",
          "description": "Trigger set-based watermarking schemes have gained emerging attention as they\nprovide a means to prove ownership for deep neural network model owners. In\nthis paper, we argue that state-of-the-art trigger set-based watermarking\nalgorithms do not achieve their designed goal of proving ownership. We posit\nthat this impaired capability stems from two common experimental flaws that the\nexisting research practice has committed when evaluating the robustness of\nwatermarking algorithms: (1) incomplete adversarial evaluation and (2)\noverlooked adaptive attacks.\n\nWe conduct a comprehensive adversarial evaluation of 10 representative\nwatermarking schemes against six of the existing attacks and demonstrate that\neach of these watermarking schemes lacks robustness against at least two\nattacks. We also propose novel adaptive attacks that harness the adversary's\nknowledge of the underlying watermarking algorithm of a target model. We\ndemonstrate that the proposed attacks effectively break all of the 10\nwatermarking schemes, consequently allowing adversaries to obscure the\nownership of any watermarked model. We encourage follow-up studies to consider\nour guidelines when evaluating the robustness of their watermarking schemes via\nconducting comprehensive adversarial evaluation that include our adaptive\nattacks to demonstrate a meaningful upper bound of watermark robustness.",
          "link": "http://arxiv.org/abs/2106.10147",
          "publishedOn": "2021-06-21T02:07:39.710Z",
          "wordCount": 635,
          "title": "Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in Deep Neural Networks. (arXiv:2106.10147v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Konyushkova_K/0/1/0/all/0/1\">Ksenia Konyushkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yutian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paine_T/0/1/0/all/0/1\">Thomas Paine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1\">Caglar Gulcehre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paduraru_C/0/1/0/all/0/1\">Cosmin Paduraru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mankowitz_D/0/1/0/all/0/1\">Daniel J Mankowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denil_M/0/1/0/all/0/1\">Misha Denil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1\">Nando de Freitas</a>",
          "description": "This paper addresses the problem of policy selection in domains with abundant\nlogged data, but with a very restricted interaction budget. Solving this\nproblem would enable safe evaluation and deployment of offline reinforcement\nlearning policies in industry, robotics, and healthcare domain among others.\nSeveral off-policy evaluation (OPE) techniques have been proposed to assess the\nvalue of policies using only logged data. However, there is still a big gap\nbetween the evaluation by OPE and the full online evaluation in the real\nenvironment. To reduce this gap, we introduce a novel \\emph{active offline\npolicy selection} problem formulation, which combined logged data and limited\nonline interactions to identify the best policy. We rely on the advances in OPE\nto warm start the evaluation. We build upon Bayesian optimization to\niteratively decide which policies to evaluate in order to utilize the limited\nenvironment interactions wisely. Many candidate policies could be proposed,\nthus, we focus on making our approach scalable and introduce a kernel function\nto model similarity between policies. We use several benchmark environments to\nshow that the proposed approach improves upon state-of-the-art OPE estimates\nand fully online policy evaluation with limited budget. Additionally, we show\nthat each component of the proposed method is important, it works well with\nvarious number and quality of OPE estimates and even with a large number of\ncandidate policies.",
          "link": "http://arxiv.org/abs/2106.10251",
          "publishedOn": "2021-06-21T02:07:39.660Z",
          "wordCount": 662,
          "title": "Active Offline Policy Selection. (arXiv:2106.10251v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09832",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1\">Nicol&#xe1;s Gaggion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansilla_L/0/1/0/all/0/1\">Lucas Mansilla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1\">Diego Milone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>",
          "description": "In this work we address the problem of landmark-based segmentation for\nanatomical structures. We propose HybridGNet, an encoder-decoder neural\narchitecture which combines standard convolutions for image feature encoding,\nwith graph convolutional neural networks to decode plausible representations of\nanatomical structures. We benchmark the proposed architecture considering other\nstandard landmark and pixel-based models for anatomical segmentation in chest\nx-ray images, and found that HybridGNet is more robust to image occlusions. We\nalso show that it can be used to construct landmark-based segmentations from\npixel level annotations. Our experimental results suggest that HybridGNet\nproduces accurate and anatomically plausible landmark-based segmentations, by\nnaturally incorporating shape constraints within the decoding process via\nspectral convolutions.",
          "link": "http://arxiv.org/abs/2106.09832",
          "publishedOn": "2021-06-21T02:07:39.646Z",
          "wordCount": 567,
          "title": "Hybrid graph convolutional neural networks for landmark-based anatomical segmentation. (arXiv:2106.09832v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kristiadi_A/0/1/0/all/0/1\">Agustinus Kristiadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1\">Matthias Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1\">Philipp Hennig</a>",
          "description": "Despite their compelling theoretical properties, Bayesian neural networks\n(BNNs) tend to perform worse than frequentist methods in classification-based\nuncertainty quantification (UQ) tasks such as out-of-distribution (OOD)\ndetection and dataset-shift robustness. In this work, based on empirical\nfindings in prior works, we hypothesize that this issue is due to the avoidance\nof Bayesian methods in the so-called \"OOD training\" -- a family of techniques\nfor incorporating OOD data during training process, which has since been an\nintegral part of state-of-the-art frequentist UQ methods. To validate this, we\ntreat OOD data as a first-class citizen in BNN training by exploring four\ndifferent ways of incorporating OOD data in Bayesian inference. We show in\nextensive experiments that OOD-trained BNNs are competitive to, if not better\nthan recent frequentist baselines. This work thus provides strong baselines for\nfuture work in both Bayesian and frequentist UQ.",
          "link": "http://arxiv.org/abs/2106.10065",
          "publishedOn": "2021-06-21T02:07:39.628Z",
          "wordCount": 574,
          "title": "Being a Bit Frequentist Improves Bayesian Neural Networks. (arXiv:2106.10065v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10166",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Cheshire_J/0/1/0/all/0/1\">James Cheshire</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Menard_P/0/1/0/all/0/1\">Pierre M&#xe9;nard</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carpentier_A/0/1/0/all/0/1\">Alexandra Carpentier</a>",
          "description": "We investigate the problem dependent regime in the stochastic Thresholding\nBandit problem (TBP) under several shape constraints. In the TBP, the objective\nof the learner is to output, at the end of a sequential game, the set of arms\nwhose means are above a given threshold. The vanilla, unstructured, case is\nalready well studied in the literature. Taking $K$ as the number of arms, we\nconsider the case where (i) the sequence of arm's means $(\\mu_k)_{k=1}^K$ is\nmonotonically increasing (MTBP) and (ii) the case where $(\\mu_k)_{k=1}^K$ is\nconcave (CTBP). We consider both cases in the problem dependent regime and\nstudy the probability of error - i.e. the probability to mis-classify at least\none arm. In the fixed budget setting, we provide upper and lower bounds for the\nprobability of error in both the concave and monotone settings, as well as\nassociated algorithms. In both settings the bounds match in the problem\ndependent regime up to universal constants in the exponential.",
          "link": "http://arxiv.org/abs/2106.10166",
          "publishedOn": "2021-06-21T02:07:39.620Z",
          "wordCount": 601,
          "title": "Problem Dependent View on Structured Thresholding Bandit Problems. (arXiv:2106.10166v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watson_D/0/1/0/all/0/1\">David S. Watson</a>",
          "description": "Explaining the predictions of opaque machine learning algorithms is an\nimportant and challenging task, especially as complex models are increasingly\nused to assist in high-stakes decisions such as those arising in healthcare and\nfinance. Most popular tools for post-hoc explainable artificial intelligence\n(XAI) are either insensitive to context (e.g., feature attributions) or\ndifficult to summarize (e.g., counterfactuals). In this paper, I introduce\n\\emph{rational Shapley values}, a novel XAI method that synthesizes and extends\nthese seemingly incompatible approaches in a rigorous, flexible manner. I\nleverage tools from decision theory and causal modeling to formalize and\nimplement a pragmatic approach that resolves a number of known challenges in\nXAI. By pairing the distribution of random variables with the appropriate\nreference class for a given explanation task, I illustrate through theory and\nexperiments how user goals and knowledge can inform and constrain the solution\nset in an iterative fashion. The method compares favorably to state of the art\nXAI tools in a range of quantitative and qualitative comparisons.",
          "link": "http://arxiv.org/abs/2106.10191",
          "publishedOn": "2021-06-21T02:07:39.612Z",
          "wordCount": 590,
          "title": "Rational Shapley Values. (arXiv:2106.10191v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1\">Daniel A. Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaida_S/0/1/0/all/0/1\">Sho Yaida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanin_B/0/1/0/all/0/1\">Boris Hanin</a>",
          "description": "This book develops an effective theory approach to understanding deep neural\nnetworks of practical relevance. Beginning from a first-principles\ncomponent-level picture of networks, we explain how to determine an accurate\ndescription of the output of trained networks by solving layer-to-layer\niteration equations and nonlinear learning dynamics. A main result is that the\npredictions of networks are described by nearly-Gaussian distributions, with\nthe depth-to-width aspect ratio of the network controlling the deviations from\nthe infinite-width Gaussian description. We explain how these effectively-deep\nnetworks learn nontrivial representations from training and more broadly\nanalyze the mechanism of representation learning for nonlinear models. From a\nnearly-kernel-methods perspective, we find that the dependence of such models'\npredictions on the underlying learning algorithm can be expressed in a simple\nand universal way. To obtain these results, we develop the notion of\nrepresentation group flow (RG flow) to characterize the propagation of signals\nthrough the network. By tuning networks to criticality, we give a practical\nsolution to the exploding and vanishing gradient problem. We further explain\nhow RG flow leads to near-universal behavior and lets us categorize networks\nbuilt from different activation functions into universality classes.\nAltogether, we show that the depth-to-width ratio governs the effective model\ncomplexity of the ensemble of trained networks. By using information-theoretic\ntechniques, we estimate the optimal aspect ratio at which we expect the network\nto be practically most useful and show how residual connections can be used to\npush this scale to arbitrary depths. With these tools, we can learn in detail\nabout the inductive bias of architectures, hyperparameters, and optimizers.",
          "link": "http://arxiv.org/abs/2106.10165",
          "publishedOn": "2021-06-21T02:07:39.603Z",
          "wordCount": 716,
          "title": "The Principles of Deep Learning Theory. (arXiv:2106.10165v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chenjun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Ilbin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesvari</a>",
          "description": "We study the fundamental question of the sample complexity of learning a good\npolicy in finite Markov decision processes (MDPs) when the data available for\nlearning is obtained by following a logging policy that must be chosen without\nknowledge of the underlying MDP. Our main results show that the sample\ncomplexity, the minimum number of transitions necessary and sufficient to\nobtain a good policy, is an exponential function of the relevant quantities\nwhen the planning horizon $H$ is finite. In particular, we prove that the\nsample complexity of obtaining $\\epsilon$-optimal policies is at least\n$\\Omega(\\mathrm{A}^{\\min(\\mathrm{S}-1, H+1)})$ for $\\gamma$-discounted\nproblems, where $\\mathrm{S}$ is the number of states, $\\mathrm{A}$ is the\nnumber of actions, and $H$ is the effective horizon defined as $H=\\lfloor\n\\tfrac{\\ln(1/\\epsilon)}{\\ln(1/\\gamma)} \\rfloor$; and it is at least\n$\\Omega(\\mathrm{A}^{\\min(\\mathrm{S}-1, H)}/\\varepsilon^2)$ for finite horizon\nproblems, where $H$ is the planning horizon of the problem. This lower bound is\nessentially matched by an upper bound. For the average-reward setting we show\nthat there is no algorithm finding $\\epsilon$-optimal policies with a finite\namount of data.",
          "link": "http://arxiv.org/abs/2106.09973",
          "publishedOn": "2021-06-21T02:07:39.596Z",
          "wordCount": 620,
          "title": "On the Sample Complexity of Batch Reinforcement Learning with Policy-Induced Data. (arXiv:2106.09973v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_I/0/1/0/all/0/1\">I&#xf1;igo Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viles_E/0/1/0/all/0/1\">Elisabeth Viles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrejas_I/0/1/0/all/0/1\">I&#xf1;aki Cabrejas</a>",
          "description": "A failure detection system is the first step towards predictive maintenance\nstrategies. A popular data-driven method to detect incipient failures and\nanomalies is the training of normal behaviour models by applying a machine\nlearning technique like feed-forward neural networks (FFNN) or extreme learning\nmachines (ELM). However, the performance of any of these modelling techniques\ncan be deteriorated by the unexpected rise of non-stationarities in the dynamic\nenvironment in which industrial assets operate. This unpredictable statistical\nchange in the measured variable is known as concept drift. In this article a\nwind turbine maintenance case is presented, where non-stationarities of various\nkinds can happen unexpectedly. Such concept drift events are desired to be\ndetected by means of statistical detectors and window-based approaches.\nHowever, in real complex systems, concept drifts are not as clear and evident\nas in artificially generated datasets. In order to evaluate the effectiveness\nof current drift detectors and also to design an appropriate novel technique\nfor this specific industrial application, it is essential to dispose beforehand\nof a characterization of the existent drifts. Under the lack of information in\nthis regard, a methodology for labelling concept drift events in the lifetime\nof wind turbines is proposed. This methodology will facilitate the creation of\na drift database that will serve both as a training ground for concept drift\ndetectors and as a valuable information to enhance the knowledge about\nmaintenance of complex systems.",
          "link": "http://arxiv.org/abs/2106.09951",
          "publishedOn": "2021-06-21T02:07:39.589Z",
          "wordCount": 698,
          "title": "Labelling Drifts in a Fault Detection System for Wind Turbine Maintenance. (arXiv:2106.09951v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frigo_O/0/1/0/all/0/1\">Oriel Frigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1\">R&#xe9;my Brossard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1\">David Dehaene</a>",
          "description": "We propose the Graph Context Encoder (GCE), a simple but efficient approach\nfor graph representation learning based on graph feature masking and\nreconstruction.\n\nGCE models are trained to efficiently reconstruct input graphs similarly to a\ngraph autoencoder where node and edge labels are masked. In particular, our\nmodel is also allowed to change graph structures by masking and reconstructing\ngraphs augmented by random pseudo-edges.\n\nWe show that GCE can be used for novel graph generation, with applications\nfor molecule generation. Used as a pretraining method, we also show that GCE\nimproves baseline performances in supervised classification tasks tested on\nmultiple standard benchmark graph datasets.",
          "link": "http://arxiv.org/abs/2106.10124",
          "publishedOn": "2021-06-21T02:07:39.575Z",
          "wordCount": 547,
          "title": "Graph Context Encoder: Graph Feature Inpainting for Graph Generation and Self-supervised Pretraining. (arXiv:2106.10124v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1\">Stefan Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janschek_M/0/1/0/all/0/1\">Michael Janschek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uelwer_T/0/1/0/all/0/1\">Tobias Uelwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harmeling_S/0/1/0/all/0/1\">Stefan Harmeling</a>",
          "description": "We propose a new approach to increase inference performance in environments\nthat require a specific sequence of actions in order to be solved. This is for\nexample the case for maze environments where ideally an optimal path is\ndetermined. Instead of learning a policy for a single step, we want to learn a\npolicy that can predict n actions in advance. Our proposed method called policy\nhorizon regression (PHR) uses knowledge of the environment sampled by A2C to\nlearn an n dimensional policy vector in a policy distillation setup which\nyields n sequential actions per observation. We test our method on the MiniGrid\nand Pong environments and show drastic speedup during inference time by\nsuccessfully predicting sequences of actions on a single observation.",
          "link": "http://arxiv.org/abs/2106.10075",
          "publishedOn": "2021-06-21T02:07:39.547Z",
          "wordCount": 576,
          "title": "Learning to Plan via a Multi-Step Policy Regression Method. (arXiv:2106.10075v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1\">Maura Pintor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demetrio_L/0/1/0/all/0/1\">Luca Demetrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotgiu_A/0/1/0/all/0/1\">Angelo Sotgiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manca_G/0/1/0/all/0/1\">Giovanni Manca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1\">Fabio Roli</a>",
          "description": "Evaluating robustness of machine-learning models to adversarial examples is a\nchallenging problem. Many defenses have been shown to provide a false sense of\nsecurity by causing gradient-based attacks to fail, and they have been broken\nunder more rigorous evaluations. Although guidelines and best practices have\nbeen suggested to improve current adversarial robustness evaluations, the lack\nof automatic testing and debugging tools makes it difficult to apply these\nrecommendations in a systematic manner. In this work, we overcome these\nlimitations by (i) defining a set of quantitative indicators which unveil\ncommon failures in the optimization of gradient-based attacks, and (ii)\nproposing specific mitigation strategies within a systematic evaluation\nprotocol. Our extensive experimental analysis shows that the proposed\nindicators of failure can be used to visualize, debug and improve current\nadversarial robustness evaluations, providing a first concrete step towards\nautomatizing and systematizing current adversarial robustness evaluations. Our\nopen-source code is available at:\nhttps://github.com/pralab/IndicatorsOfAttackFailure.",
          "link": "http://arxiv.org/abs/2106.09947",
          "publishedOn": "2021-06-21T02:07:39.539Z",
          "wordCount": 609,
          "title": "Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples. (arXiv:2106.09947v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Adi Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melamed_O/0/1/0/all/0/1\">Odelia Melamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+BenShmuel_O/0/1/0/all/0/1\">Oriel BenShmuel</a>",
          "description": "The extreme fragility of deep neural networks when presented with tiny\nperturbations in their inputs was independently discovered by several research\ngroups in 2013, but in spite of enormous effort these adversarial examples\nremained a baffling phenomenon with no clear explanation. In this paper we\nintroduce a new conceptual framework (which we call the Dimpled Manifold Model)\nwhich provides a simple explanation for why adversarial examples exist, why\ntheir perturbations have such tiny norms, why these perturbations look like\nrandom noise, and why a network which was adversarially trained with\nincorrectly labeled images can still correctly classify test images. In the\nlast part of the paper we describe the results of numerous experiments which\nstrongly support this new model, and in particular our assertion that\nadversarial perturbations are roughly perpendicular to the low dimensional\nmanifold which contains all the training examples.",
          "link": "http://arxiv.org/abs/2106.10151",
          "publishedOn": "2021-06-21T02:07:39.525Z",
          "wordCount": 583,
          "title": "The Dimpled Manifold Model of Adversarial Examples in Machine Learning. (arXiv:2106.10151v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wensheng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Ying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoyong Yuan</a>",
          "description": "Vertical federated learning is a collaborative machine learning framework to\ntrain deep leaning models on vertically partitioned data with\nprivacy-preservation. It attracts much attention both from academia and\nindustry. Unfortunately, applying most existing vertical federated learning\nmethods in real-world applications still faces two daunting challenges. First,\nmost existing vertical federated learning methods have a strong assumption that\nat least one party holds the complete set of labels of all data samples, while\nthis assumption is not satisfied in many practical scenarios, where labels are\nhorizontally partitioned and the parties only hold partial labels. Existing\nvertical federated learning methods can only utilize partial labels, which may\nlead to inadequate model update in end-to-end backpropagation. Second,\ncomputational and communication resources vary in parties. Some parties with\nlimited computational and communication resources will become the stragglers\nand slow down the convergence of training. Such straggler problem will be\nexaggerated in the scenarios of horizontally partitioned labels in vertical\nfederated learning. To address these challenges, we propose a novel vertical\nfederated learning framework named Cascade Vertical Federated Learning (CVFL)\nto fully utilize all horizontally partitioned labels to train neural networks\nwith privacy-preservation. To mitigate the straggler problem, we design a novel\noptimization objective which can increase straggler's contribution to the\ntrained models. We conduct a series of qualitative experiments to rigorously\nverify the effectiveness of CVFL. It is demonstrated that CVFL can achieve\ncomparable performance (e.g., accuracy for classification tasks) with\ncentralized training. The new optimization objective can further mitigate the\nstraggler problem comparing with only using the asynchronous aggregation\nmechanism during training.",
          "link": "http://arxiv.org/abs/2106.10056",
          "publishedOn": "2021-06-21T02:07:39.510Z",
          "wordCount": 702,
          "title": "A Vertical Federated Learning Framework for Horizontally Partitioned Labels. (arXiv:2106.10056v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lowy_A/0/1/0/all/0/1\">Andrew Lowy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1\">Meisam Razaviyayn</a>",
          "description": "Federated learning (FL) is a distributed learning paradigm in which many\nclients with heterogeneous, unbalanced, and often sensitive local data,\ncollaborate to learn a model. Local Differential Privacy (LDP) provides a\nstrong guarantee that each client's data cannot be leaked during and after\ntraining, without relying on a trusted third party. While LDP is often believed\nto be too stringent to allow for satisfactory utility, our paper challenges\nthis belief. We consider a general setup with unbalanced, heterogeneous data,\ndisparate privacy needs across clients, and unreliable communication, where a\nrandom number/subset of clients is available each round. We propose three LDP\nalgorithms for smooth (strongly) convex FL; each are noisy variations of\ndistributed minibatch SGD. One is accelerated and one involves novel\ntime-varying noise, which we use to obtain the first non-trivial LDP excess\nrisk bound for the fully general non-i.i.d. FL problem. Specializing to i.i.d.\nclients, our risk bounds interpolate between the best known and/or optimal\nbounds in the centralized setting and the cross-device setting, where each\nclient represents just one person's data. Furthermore, we show that in certain\nregimes, our convergence rate (nearly) matches the corresponding non-private\nlower bound or outperforms state of the art non-private algorithms (``privacy\nfor free''). Finally, we validate our theoretical results and illustrate the\npractical utility of our algorithm with numerical experiments.",
          "link": "http://arxiv.org/abs/2106.09779",
          "publishedOn": "2021-06-21T02:07:39.480Z",
          "wordCount": 666,
          "title": "Locally Differentially Private Federated Learning: Efficient Algorithms with Tight Risk Bounds. (arXiv:2106.09779v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jenner_E/0/1/0/all/0/1\">Erik Jenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1\">Maurice Weiler</a>",
          "description": "Recent work in equivariant deep learning bears strong similarities to\nphysics. Fields over a base space are fundamental entities in both subjects, as\nare equivariant maps between these fields. In deep learning, however, these\nmaps are usually defined by convolutions with a kernel, whereas they are\npartial differential operators (PDOs) in physics. Developing the theory of\nequivariant PDOs in the context of deep learning could bring these subjects\neven closer together and lead to a stronger flow of ideas. In this work, we\nderive a $G$-steerability constraint that completely characterizes when a PDO\nbetween feature vector fields is equivariant, for arbitrary symmetry groups\n$G$. We then fully solve this constraint for several important groups. We use\nour solutions as equivariant drop-in replacements for convolutional layers and\nbenchmark them in that role. Finally, we develop a framework for equivariant\nmaps based on Schwartz distributions that unifies classical convolutions and\ndifferential operators and gives insight about the relation between the two.",
          "link": "http://arxiv.org/abs/2106.10163",
          "publishedOn": "2021-06-21T02:07:39.473Z",
          "wordCount": 603,
          "title": "Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piriyajitakonkij_M/0/1/0/all/0/1\">Maytus Piriyajitakonkij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itthipuripat_S/0/1/0/all/0/1\">Sirawaj Itthipuripat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1\">Theerawit Wilaiprasitporn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilokthanakul_N/0/1/0/all/0/1\">Nat Dilokthanakul</a>",
          "description": "Supervised deep convolutional neural networks (DCNNs) are currently one of\nthe best computational models that can explain how the primate ventral visual\nstream solves object recognition. However, embodied cognition has not been\nconsidered in the existing visual processing models. From the ecological\nstandpoint, humans learn to recognize objects by interacting with them,\nallowing better classification, specialization, and generalization. Here, we\nask if computational models under the embodied learning framework can explain\nmechanisms underlying object recognition in the primate visual system better\nthan the existing supervised models? To address this question, we use\nreinforcement learning to train neural network models to play a 3D computer\ngame and we find that these reinforcement learning models achieve neural\nresponse prediction accuracy scores in the early visual areas (e.g., V1 and V2)\nin the levels that are comparable to those accomplished by the supervised\nneural network model. In contrast, the supervised neural network models yield\nbetter neural response predictions in the higher visual areas, compared to the\nreinforcement learning models. Our preliminary results suggest the future\ndirection of visual neuroscience in which deep reinforcement learning should be\nincluded to fill the missing embodiment concept.",
          "link": "http://arxiv.org/abs/2106.10112",
          "publishedOn": "2021-06-21T02:07:39.466Z",
          "wordCount": 635,
          "title": "Deep Reinforcement Learning Models Predict Visual Responses in the Brain: A Preliminary Result. (arXiv:2106.10112v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>",
          "description": "In recent years, multi-view subspace clustering has achieved impressive\nperformance due to the exploitation of complementary imformation across\nmultiple views. However, multi-view data can be very complicated and are not\neasy to cluster in real-world applications. Most existing methods operate on\nraw data and may not obtain the optimal solution. In this work, we propose a\nnovel multi-view clustering method named smoothed multi-view subspace\nclustering (SMVSC) by employing a novel technique, i.e., graph filtering, to\nobtain a smooth representation for each view, in which similar data points have\nsimilar feature values. Specifically, it retains the graph geometric features\nthrough applying a low-pass filter. Consequently, it produces a\n``clustering-friendly\" representation and greatly facilitates the downstream\nclustering task. Extensive experiments on benchmark datasets validate the\nsuperiority of our approach. Analysis shows that graph filtering increases the\nseparability of classes.",
          "link": "http://arxiv.org/abs/2106.09875",
          "publishedOn": "2021-06-21T02:07:39.460Z",
          "wordCount": 584,
          "title": "Smoothed Multi-View Subspace Clustering. (arXiv:2106.09875v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tijin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yufeng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuanqing Xia</a>",
          "description": "Multivariate time series prediction has attracted a lot of attention because\nof its wide applications such as intelligence transportation, AIOps. Generative\nmodels have achieved impressive results in time series modeling because they\ncan model data distribution and take noise into consideration. However, many\nexisting works can not be widely used because of the constraints of functional\nform of generative models or the sensitivity to hyperparameters. In this paper,\nwe propose ScoreGrad, a multivariate probabilistic time series forecasting\nframework based on continuous energy-based generative models. ScoreGrad is\ncomposed of time series feature extraction module and conditional stochastic\ndifferential equation based score matching module. The prediction can be\nachieved by iteratively solving reverse-time SDE. To the best of our knowledge,\nScoreGrad is the first continuous energy based generative model used for time\nseries forecasting. Furthermore, ScoreGrad achieves state-of-the-art results on\nsix real-world datasets. The impact of hyperparameters and sampler types on the\nperformance are also explored. Code is available at\nhttps://github.com/yantijin/ScoreGradPred.",
          "link": "http://arxiv.org/abs/2106.10121",
          "publishedOn": "2021-06-21T02:07:39.445Z",
          "wordCount": 609,
          "title": "ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models. (arXiv:2106.10121v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosato_A/0/1/0/all/0/1\">Antonello Rosato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panella_M/0/1/0/all/0/1\">Massimo Panella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osipov_E/0/1/0/all/0/1\">Evgeny Osipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1\">Denis Kleyko</a>",
          "description": "A change of the prevalent supervised learning techniques is foreseeable in\nthe near future: from the complex, computational expensive algorithms to more\nflexible and elementary training ones. The strong revitalization of randomized\nalgorithms can be framed in this prospect steering. We recently proposed a\nmodel for distributed classification based on randomized neural networks and\nhyperdimensional computing, which takes into account cost of information\nexchange between agents using compression. The use of compression is important\nas it addresses the issues related to the communication bottleneck, however,\nthe original approach is rigid in the way the compression is used. Therefore,\nin this work, we propose a more flexible approach to compression and compare it\nto conventional compression algorithms, dimensionality reduction, and\nquantization techniques.",
          "link": "http://arxiv.org/abs/2106.09831",
          "publishedOn": "2021-06-21T02:07:39.332Z",
          "wordCount": 569,
          "title": "On Effects of Compression with Hyperdimensional Computing in Distributed Randomized Neural Networks. (arXiv:2106.09831v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12866",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1\">Xiaoliang Wan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tang_K/0/1/0/all/0/1\">Kejun Tang</a>",
          "description": "In this work, we have proposed augmented KRnets including both discrete and\ncontinuous models. One difficulty in flow-based generative modeling is to\nmaintain the invertibility of the transport map, which is often a trade-off\nbetween effectiveness and robustness. The exact invertibility has been achieved\nin the real NVP using a specific pattern to exchange information between two\nseparated groups of dimensions. KRnet has been developed to enhance the\ninformation exchange among data dimensions by incorporating the\nKnothe-Rosenblatt rearrangement into the structure of the transport map. Due to\nthe maintenance of exact invertibility, a full nonlinear update of all data\ndimensions needs three iterations in KRnet. To alleviate this issue, we will\nadd augmented dimensions that act as a channel for communications among the\ndata dimensions. In the augmented KRnet, a fully nonlinear update is achieved\nin two iterations. We also show that the augmented KRnet can be reformulated as\nthe discretization of a neural ODE, where the exact invertibility is kept such\nthat the adjoint method can be formulated with respect to the discretized ODE\nto obtain the exact gradient. Numerical experiments have been implemented to\ndemonstrate the effectiveness of our models.",
          "link": "http://arxiv.org/abs/2105.12866",
          "publishedOn": "2021-06-21T02:07:39.325Z",
          "wordCount": 638,
          "title": "Augmented KRnet for density estimation and approximation. (arXiv:2105.12866v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06300",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Plassier_V/0/1/0/all/0/1\">Vincent Plassier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vono_M/0/1/0/all/0/1\">Maxime Vono</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1\">Eric Moulines</a>",
          "description": "Performing reliable Bayesian inference on a big data scale is becoming a\nkeystone in the modern era of machine learning. A workhorse class of methods to\nachieve this task are Markov chain Monte Carlo (MCMC) algorithms and their\ndesign to handle distributed datasets has been the subject of many works.\nHowever, existing methods are not completely either reliable or computationally\nefficient. In this paper, we propose to fill this gap in the case where the\ndataset is partitioned and stored on computing nodes within a cluster under a\nmaster/slaves architecture. We derive a user-friendly centralised distributed\nMCMC algorithm with provable scaling in high-dimensional settings. We\nillustrate the relevance of the proposed methodology on both synthetic and real\ndata experiments.",
          "link": "http://arxiv.org/abs/2106.06300",
          "publishedOn": "2021-06-21T02:07:39.319Z",
          "wordCount": 599,
          "title": "DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm via Langevin Monte Carlo within Gibbs. (arXiv:2106.06300v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pawelczyk_M/0/1/0/all/0/1\">Martin Pawelczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shalmali Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sohini Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "Counterfactual explanations and adversarial examples have emerged as critical\nresearch areas for addressing the explainability and robustness goals of\nmachine learning (ML). While counterfactual explanations were developed with\nthe goal of providing recourse to individuals adversely impacted by algorithmic\ndecisions, adversarial examples were designed to expose the vulnerabilities of\nML models. While prior research has hinted at the commonalities between these\nframeworks, there has been little to no work on systematically exploring the\nconnections between the literature on counterfactual explanations and\nadversarial examples. In this work, we make one of the first attempts at\nformalizing the connections between counterfactual explanations and adversarial\nexamples. More specifically, we theoretically analyze salient counterfactual\nexplanation and adversarial example generation methods, and highlight the\nconditions under which they behave similarly. Our analysis demonstrates that\nseveral popular counterfactual explanation and adversarial example generation\nmethods such as the ones proposed by Wachter et. al. and Carlini and Wagner\n(with mean squared error loss), and C-CHVAE and natural adversarial examples by\nZhao et. al. are equivalent. We also bound the distance between counterfactual\nexplanations and adversarial examples generated by Wachter et. al. and DeepFool\nmethods for linear models. Finally, we empirically validate our theoretical\nfindings using extensive experimentation with synthetic and real world\ndatasets.",
          "link": "http://arxiv.org/abs/2106.09992",
          "publishedOn": "2021-06-21T02:07:39.312Z",
          "wordCount": 643,
          "title": "On the Connections between Counterfactual Explanations and Adversarial Examples. (arXiv:2106.09992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatel_S/0/1/0/all/0/1\">Sylvain Chatel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyrgelis_A/0/1/0/all/0/1\">Apostolos Pyrgelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troncoso_Pastoriza_J/0/1/0/all/0/1\">Juan Ramon Troncoso-Pastoriza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubaux_J/0/1/0/all/0/1\">Jean-Pierre Hubaux</a>",
          "description": "Tree-based models are among the most efficient machine learning techniques\nfor data mining nowadays due to their accuracy, interpretability, and\nsimplicity. The recent orthogonal needs for more data and privacy protection\ncall for collaborative privacy-preserving solutions. In this work, we survey\nthe literature on distributed and privacy-preserving training of tree-based\nmodels and we systematize its knowledge based on four axes: the learning\nalgorithm, the collaborative model, the protection mechanism, and the threat\nmodel. We use this to identify the strengths and limitations of these works and\nprovide for the first time a framework analyzing the information leakage\noccurring in distributed tree-based model learning.",
          "link": "http://arxiv.org/abs/2103.08987",
          "publishedOn": "2021-06-21T02:07:39.305Z",
          "wordCount": 577,
          "title": "SoK: Privacy-Preserving Collaborative Tree-based Model Learning. (arXiv:2103.08987v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10064",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bellec_G/0/1/0/all/0/1\">Guillaume Bellec</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Shuqi Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Modirshanechi_A/0/1/0/all/0/1\">Alireza Modirshanechi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Brea_J/0/1/0/all/0/1\">Johanni Brea</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gerstner_W/0/1/0/all/0/1\">Wulfram Gerstner</a>",
          "description": "Fitting network models to neural activity is becoming an important tool in\nneuroscience. A popular approach is to model a brain area with a probabilistic\nrecurrent spiking network whose parameters maximize the likelihood of the\nrecorded activity. Although this is widely used, we show that the resulting\nmodel does not produce realistic neural activity and wrongly estimates the\nconnectivity matrix when neurons that are not recorded have a substantial\nimpact on the recorded network. To correct for this, we suggest to augment the\nlog-likelihood with terms that measure the dissimilarity between simulated and\nrecorded activity. This dissimilarity is defined via summary statistics\ncommonly used in neuroscience, and the optimization is efficient because it\nrelies on back-propagation through the stochastically simulated spike trains.\nWe analyze this method theoretically and show empirically that it generates\nmore realistic activity statistics and recovers the connectivity matrix better\nthan other methods.",
          "link": "http://arxiv.org/abs/2106.10064",
          "publishedOn": "2021-06-21T02:07:39.287Z",
          "wordCount": 595,
          "title": "Fitting summary statistics of neural data with a differentiable spiking network simulator. (arXiv:2106.10064v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Runyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaolin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Na Li</a>",
          "description": "We study the performance of the gradient play algorithm for multi-agent\ntabular Markov decision processes (MDPs), which are also known as stochastic\ngames (SGs), where each agent tries to maximize its own total discounted reward\nby making decisions independently based on current state information which is\nshared between agents. Policies are directly parameterized by the probability\nof choosing a certain action at a given state. We show that Nash equilibria\n(NEs) and first order stationary policies are equivalent in this setting, and\ngive a non-asymptotic global convergence rate analysis to an $\\epsilon$-NE for\na subclass of multi-agent MDPs called Markov potential games, which includes\nthe cooperative setting with identical rewards among agents as an important\nspecial case. Our result shows that the number of iterations to reach an\n$\\epsilon$-NE scales linearly, instead of exponentially, with the number of\nagents. Local geometry and local stability are also considered. For Markov\npotential games, we prove that strict NEs are local maxima of the total\npotential function and fully-mixed NEs are saddle points. We also give a local\nconvergence rate around strict NEs for more general settings.",
          "link": "http://arxiv.org/abs/2106.00198",
          "publishedOn": "2021-06-21T02:07:39.281Z",
          "wordCount": 653,
          "title": "Gradient Play in Multi-Agent Markov Stochastic Games: Stationary Points and Convergence. (arXiv:2106.00198v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.05976",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Matsuda_T/0/1/0/all/0/1\">Takeru Matsuda</a>, <a href=\"http://arxiv.org/find/math/1/au:+Uehara_M/0/1/0/all/0/1\">Masatoshi Uehara</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hyvarinen_A/0/1/0/all/0/1\">Aapo Hyvarinen</a>",
          "description": "Many statistical models are given in the form of non-normalized densities\nwith an intractable normalization constant. Since maximum likelihood estimation\nis computationally intensive for these models, several estimation methods have\nbeen developed which do not require explicit computation of the normalization\nconstant, such as noise contrastive estimation (NCE) and score matching.\nHowever, model selection methods for general non-normalized models have not\nbeen proposed so far. In this study, we develop information criteria for\nnon-normalized models estimated by NCE or score matching. They are\napproximately unbiased estimators of discrepancy measures for non-normalized\nmodels. Simulation results and applications to real data demonstrate that the\nproposed criteria enable selection of the appropriate non-normalized model in a\ndata-driven manner.",
          "link": "http://arxiv.org/abs/1905.05976",
          "publishedOn": "2021-06-21T02:07:39.272Z",
          "wordCount": 583,
          "title": "Information criteria for non-normalized models. (arXiv:1905.05976v4 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Nanqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggioni_M/0/1/0/all/0/1\">Matteo Maggioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1\">Eduardo P&#xe9;rez-Pellitero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>",
          "description": "The breakthrough of contrastive learning (CL) has fueled the recent success\nof self-supervised learning (SSL) in high-level vision tasks on RGB images.\nHowever, CL is still ill-defined for low-level vision tasks, such as joint\ndemosaicking and denoising (JDD), in the RAW domain. To bridge this\nmethodological gap, we present a novel CL approach on RAW images, residual\ncontrastive learning (RCL), which aims to learn meaningful representations for\nJDD. Our work is built on the assumption that noise contained in each RAW image\nis signal-dependent, thus two crops from the same RAW image should have more\nsimilar noise distribution than two crops from different RAW images. We use\nresiduals as a discriminative feature and the earth mover's distance to measure\nthe distribution divergence for the contrastive loss. To evaluate the proposed\nCL strategy, we simulate a series of unsupervised JDD experiments with\nlarge-scale data corrupted by synthetic signal-dependent noise, where we set a\nnew benchmark for unsupervised JDD tasks with unknown (random) noise variance.\nOur empirical study not only validates that CL can be applied on distributions\n(c.f. features), but also exposes the lack of robustness of previous non-ML and\nSSL JDD methods when the statistics of the noise are unknown, thus providing\nsome further insight into signal-dependent noise problems.",
          "link": "http://arxiv.org/abs/2106.10070",
          "publishedOn": "2021-06-21T02:07:39.266Z",
          "wordCount": 653,
          "title": "Residual Contrastive Learning for Joint Demosaicking and Denoising. (arXiv:2106.10070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingjing Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koloskova_A/0/1/0/all/0/1\">Anastasia Koloskova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>",
          "description": "Decentralized training of deep learning models enables on-device learning\nover networks, as well as efficient scaling to large compute clusters.\nExperiments in earlier works reveal that, even in a data-center setup,\ndecentralized training often suffers from the degradation in the quality of the\nmodel: the training and test performance of models trained in a decentralized\nfashion is in general worse than that of models trained in a centralized\nfashion, and this performance drop is impacted by parameters such as network\nsize, communication topology and data partitioning. We identify the changing\nconsensus distance between devices as a key parameter to explain the gap\nbetween centralized and decentralized training.\n\nWe show in theory that when the training consensus distance is lower than a\ncritical quantity, decentralized training converges as fast as the centralized\ncounterpart. We empirically validate that the relation between generalization\nperformance and consensus distance is consistent with this theoretical\nobservation. Our empirical insights allow the principled design of better\ndecentralized training schemes that mitigate the performance drop. To this end,\nwe provide practical training guidelines and exemplify its effectiveness on the\ndata-center setup as the important first step.",
          "link": "http://arxiv.org/abs/2102.04828",
          "publishedOn": "2021-06-21T02:07:39.259Z",
          "wordCount": 672,
          "title": "Consensus Control for Decentralized Deep Learning. (arXiv:2102.04828v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">John D. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modayil_J/0/1/0/all/0/1\">Joseph Modayil</a>",
          "description": "The performance of a reinforcement learning (RL) system depends on the\ncomputational architecture used to approximate a value function. Deep learning\nmethods provide both optimization techniques and architectures for\napproximating nonlinear functions from noisy, high-dimensional observations.\nHowever, prevailing optimization techniques are not designed for\nstrictly-incremental online updates. Nor are standard architectures designed\nfor observations with an a priori unknown structure: for example, light sensors\nrandomly dispersed in space. This paper proposes an online RL prediction\nalgorithm with an adaptive architecture that efficiently finds useful nonlinear\nfeatures. The algorithm is evaluated in a spatial domain with high-dimensional,\nstochastic observations. The algorithm outperforms non-adaptive baseline\narchitectures and approaches the performance of an architecture given\nside-channel information. These results are a step towards scalable RL\nalgorithms for more general problems, where the observation structure is not\navailable.",
          "link": "http://arxiv.org/abs/2106.09776",
          "publishedOn": "2021-06-21T02:07:39.240Z",
          "wordCount": 568,
          "title": "Adapting the Function Approximation Architecture in Online Reinforcement Learning. (arXiv:2106.09776v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lucas Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1\">Bistra Dilkina</a>",
          "description": "Recent work has shown that deep learning models can be used to classify\nland-use data from geospatial satellite imagery. We show that when these deep\nlearning models are trained on data from specific continents/seasons, there is\na high degree of variability in model performance on out-of-sample\ncontinents/seasons. This suggests that just because a model accurately predicts\nland-use classes in one continent or season does not mean that the model will\naccurately predict land-use classes in a different continent or season. We then\nuse clustering techniques on satellite imagery from different continents to\nvisualize the differences in landscapes that make geospatial generalization\nparticularly difficult, and summarize our takeaways for future satellite\nimagery-related applications.",
          "link": "http://arxiv.org/abs/2008.10351",
          "publishedOn": "2021-06-21T02:07:39.234Z",
          "wordCount": 606,
          "title": "Model Generalization in Deep Learning Applications for Land Cover Mapping. (arXiv:2008.10351v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07030",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Araman_V/0/1/0/all/0/1\">Victor F. Araman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Caldentey_R/0/1/0/all/0/1\">Rene Caldentey</a>",
          "description": "We consider a decision maker who must choose an action in order to maximize a\nreward function that depends also on an unknown parameter {\\Theta}. The\ndecision maker can delay taking the action in order to experiment and gather\nadditional information on {\\Theta}. We model the decision maker's problem using\na Bayesian sequential experimentation framework and use dynamic programming and\ndiffusion-asymptotic analysis to solve it. For that, we scale our problem in a\nway that both the average number of experiments that is conducted per unit of\ntime is large and the informativeness of each individual experiment is low.\nUnder such regime, we derive a diffusion approximation for the sequential\nexperimentation problem, which provides a number of important insights about\nthe nature of the problem and its solution. Our solution method also shows that\nthe complexity of the problem grows only quadratically with the cardinality of\nthe set of actions from which the decision maker can choose. We illustrate our\nmethodology and results using a concrete application in the context of\nassortment selection and new product introduction. Specifically, we study the\nproblem of a seller who wants to select an optimal assortment of products to\nlaunch into the marketplace and is uncertain about consumers' preferences.\nMotivated by emerging practices in e-commerce, we assume that the seller is\nable to use a crowdvoting system to learn these preferences before a final\nassortment decision is made. In this context, we undertake an extensive\nnumerical analysis to assess the value of learning and demonstrate the\neffectiveness and robustness of the heuristics derived from the diffusion\napproximation.",
          "link": "http://arxiv.org/abs/2102.07030",
          "publishedOn": "2021-06-21T02:07:39.226Z",
          "wordCount": 713,
          "title": "Diffusion Approximations for a Class of Sequential Testing Problems. (arXiv:2102.07030v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.01607",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1\">Tian Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>",
          "description": "Pseudo-healthy synthesis is the task of creating a subject-specific `healthy'\nimage from a pathological one. Such images can be helpful in tasks such as\nanomaly detection and understanding changes induced by pathology and disease.\nIn this paper, we present a model that is encouraged to disentangle the\ninformation of pathology from what seems to be healthy. We disentangle what\nappears to be healthy and where disease is as a segmentation map, which are\nthen recombined by a network to reconstruct the input disease image. We train\nour models adversarially using either paired or unpaired settings, where we\npair disease images and maps when available. We quantitatively and\nsubjectively, with a human study, evaluate the quality of pseudo-healthy images\nusing several criteria. We show in a series of experiments, performed on ISLES,\nBraTS and Cam-CAN datasets, that our method is better than several baselines\nand methods from the literature. We also show that due to better training\nprocesses we could recover deformations, on surrounding tissue, caused by\ndisease. Our implementation is publicly available at\nhttps://github.com/xiat0616/pseudo-healthy-synthesis. This paper has been\naccepted by Medical Image Analysis:\nhttps://doi.org/10.1016/j.media.2020.101719.",
          "link": "http://arxiv.org/abs/2005.01607",
          "publishedOn": "2021-06-21T02:07:39.216Z",
          "wordCount": 674,
          "title": "Pseudo-healthy synthesis with pathology disentanglement and adversarial learning. (arXiv:2005.01607v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_S/0/1/0/all/0/1\">Sauptik Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heydari_J/0/1/0/all/0/1\">Javad Heydari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Samarth Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurup_U/0/1/0/all/0/1\">Unmesh Kurup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mohak Shah</a>",
          "description": "Limited availability of labeled-data makes any supervised learning problem\nchallenging. Alternative learning settings like semi-supervised and universum\nlearning alleviate the dependency on labeled data, but still require a large\namount of unlabeled data, which may be unavailable or expensive to acquire.\nGAN-based synthetic data generation methods have recently shown promise by\ngenerating synthetic samples to improve task at hand. However, these samples\ncannot be used for other purposes. In this paper, we propose a GAN game which\nprovides improved discriminator accuracy under limited data settings, while\ngenerating realistic synthetic data. This provides the added advantage that now\nthe generated data can be used for other similar tasks. We provide the\ntheoretical guarantees and empirical results in support of our approach.",
          "link": "http://arxiv.org/abs/2106.09946",
          "publishedOn": "2021-06-21T02:07:39.208Z",
          "wordCount": 574,
          "title": "Evolving GANs: When Contradictions Turn into Compliance. (arXiv:2106.09946v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenrui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Junni Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>",
          "description": "Graph convolution networks, like message passing graph convolution networks\n(MPGCNs), have been a powerful tool in representation learning of networked\ndata. However, when data is heterogeneous, most architectures are limited as\nthey employ a single strategy to handle multi-channel graph signals and they\ntypically focus on low-frequency information. In this paper, we present a novel\ngraph convolution operator, termed BankGCN, which keeps benefits of message\npassing models, but extends their capabilities beyond `low-pass' features. It\ndecomposes multi-channel signals on graphs into subspaces and handles\nparticular information in each subspace with an adapted filter. The filters of\nall subspaces have different frequency responses and together form a filter\nbank. Furthermore, each filter in the spectral domain corresponds to a message\npassing scheme, and diverse schemes are implemented via the filter bank.\nImportantly, the filter bank and the signal decomposition are jointly learned\nto adapt to the spectral characteristics of data and to target applications.\nFurthermore, this is implemented almost without extra parameters in comparison\nwith most existing MPGCNs. Experimental results show that the proposed\nconvolution operator permits to achieve excellent performance in graph\nclassification on a collection of benchmark graph datasets.",
          "link": "http://arxiv.org/abs/2106.09910",
          "publishedOn": "2021-06-21T02:07:39.189Z",
          "wordCount": 642,
          "title": "Message Passing in Graph Convolution Networks via Adaptive Filter Banks. (arXiv:2106.09910v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lordeiro_I/0/1/0/all/0/1\">Igor Q. Lordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddad_D/0/1/0/all/0/1\">Diego B. Haddad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_D/0/1/0/all/0/1\">Douglas O. Cardoso</a>",
          "description": "A popular computer puzzle, the game of Minesweeper requires its human players\nto have a mix of both luck and strategy to succeed. Analyzing these aspects\nmore formally, in our research we assessed the feasibility of a novel\nmethodology based on Reinforcement Learning as an adequate approach to tackle\nthe problem presented by this game. For this purpose we employed Multi-Armed\nBandit algorithms which were carefully adapted in order to enable their use to\ndefine autonomous computational players, targeting to make the best use of some\ngame peculiarities. After experimental evaluation, results showed that this\napproach was indeed successful, especially in smaller game boards, such as the\nstandard beginner level. Despite this fact the main contribution of this work\nis a detailed examination of Minesweeper from a learning perspective, which led\nto various original insights which are thoroughly discussed.",
          "link": "http://arxiv.org/abs/2007.12824",
          "publishedOn": "2021-06-21T02:07:39.182Z",
          "wordCount": 619,
          "title": "Multi-Armed Bandits for Minesweeper: Profiting from Exploration-Exploitation Synergy. (arXiv:2007.12824v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fornoni_M/0/1/0/all/0/1\">Marco Fornoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chaochao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Liangchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1\">Kimberly Wilber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stark_A/0/1/0/all/0/1\">Alex Stark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1\">Andrew Howard</a>",
          "description": "When interacting with objects through cameras, or pictures, users often have\na specific intent. For example, they may want to perform a visual search.\nHowever, most object detection models ignore the user intent, relying on image\npixels as their only input. This often leads to incorrect results, such as lack\nof a high-confidence detection on the object of interest, or detection with a\nwrong class label. In this paper we investigate techniques to modulate standard\nobject detectors to explicitly account for the user intent, expressed as an\nembedding of a simple query. Compared to standard object detectors,\nquery-modulated detectors show superior performance at detecting objects for a\ngiven label of interest. Thanks to large-scale training data synthesized from\nstandard object detection annotations, query-modulated detectors can also\noutperform specialized referring expression recognition systems. Furthermore,\nthey can be simultaneously trained to solve for both query-modulated detection\nand standard object detection.",
          "link": "http://arxiv.org/abs/2106.10258",
          "publishedOn": "2021-06-21T02:07:39.175Z",
          "wordCount": 606,
          "title": "Bridging the Gap Between Object Detection and User Intent via Query-Modulation. (arXiv:2106.10258v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Luofeng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jia Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolar_M/0/1/0/all/0/1\">Mladen Kolar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Large scale convex-concave minimax problems arise in numerous applications,\nincluding game theory, robust training, and training of generative adversarial\nnetworks. Despite their wide applicability, solving such problems efficiently\nand effectively is challenging in the presence of large amounts of data using\nexisting stochastic minimax methods. We study a class of stochastic minimax\nmethods and develop a communication-efficient distributed stochastic\nextragradient algorithm, LocalAdaSEG, with an adaptive learning rate suitable\nfor solving convex-concave minimax problem in the Parameter-Server model.\nLocalAdaSEG has three main features: (i) periodic communication strategy\nreduces the communication cost between workers and the server; (ii) an adaptive\nlearning rate that is computed locally and allows for tuning-free\nimplementation; and (iii) theoretically, a nearly linear speed-up with respect\nto the dominant variance term, arising from estimation of the stochastic\ngradient, is proven in both the smooth and nonsmooth convex-concave settings.\nLocalAdaSEG is used to solve a stochastic bilinear game, and train generative\nadversarial network. We compare LocalAdaSEG against several existing optimizers\nfor minimax problems and demonstrate its efficacy through several experiments\nin both the homogeneous and heterogeneous settings.",
          "link": "http://arxiv.org/abs/2106.10022",
          "publishedOn": "2021-06-21T02:07:39.168Z",
          "wordCount": 625,
          "title": "Local AdaGrad-Type Algorithm for Stochastic Convex-Concave Minimax Problems. (arXiv:2106.10022v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">An-phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_M/0/1/0/all/0/1\">Maria Rodriguez Martinez</a>",
          "description": "Interpretability has become a necessary feature for machine learning models\ndeployed in critical scenarios, e.g. legal systems, healthcare. In these\nsituations, algorithmic decisions may have (potentially negative) long-lasting\neffects on the end-user affected by the decision. In many cases, the\nrepresentational power of deep learning models is not needed, therefore simple\nand interpretable models (e.g. linear models) should be preferred. However, in\nhigh-dimensional and/or complex domains (e.g. computer vision), the universal\napproximation capabilities of neural networks is required. Inspired by linear\nmodels and the Kolmogorov-Arnol representation theorem, we propose a novel\nclass of structurally-constrained neural networks, which we call FLANs\n(Feature-wise Latent Additive Networks). Crucially, FLANs process each input\nfeature separately, computing for each of them a representation in a common\nlatent space. These feature-wise latent representations are then simply summed,\nand the aggregated representation is used for prediction. These constraints\n(which are at the core of the interpretability of linear models) allow an user\nto estimate the effect of each individual feature independently from the\nothers, enhancing interpretability. In a set of experiments across different\ndomains, we show how without compromising excessively the test performance, the\nstructural constraints proposed in FLANs indeed increase the interpretability\nof deep learning models.",
          "link": "http://arxiv.org/abs/2106.10086",
          "publishedOn": "2021-06-21T02:07:39.162Z",
          "wordCount": 634,
          "title": "It's FLAN time! Summing feature-wise latent representations for interpretability. (arXiv:2106.10086v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1\">Andreas Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wightman_R/0/1/0/all/0/1\">Ross Wightman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1\">Jakob Uszkoreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>",
          "description": "Vision Transformers (ViT) have been shown to attain highly competitive\nperformance for a wide range of vision applications, such as image\nclassification, object detection and semantic image segmentation. In comparison\nto convolutional neural networks, the Vision Transformer's weaker inductive\nbias is generally found to cause an increased reliance on model regularization\nor data augmentation (``AugReg'' for short) when training on smaller training\ndatasets. We conduct a systematic empirical study in order to better understand\nthe interplay between the amount of training data, AugReg, model size and\ncompute budget. As one result of this study we find that the combination of\nincreased compute and AugReg can yield models with the same performance as\nmodels trained on an order of magnitude more training data: we train ViT models\nof various sizes on the public ImageNet-21k dataset which either match or\noutperform their counterparts trained on the larger, but not publicly available\nJFT-300M dataset.",
          "link": "http://arxiv.org/abs/2106.10270",
          "publishedOn": "2021-06-21T02:07:39.136Z",
          "wordCount": 649,
          "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. (arXiv:2106.10270v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rego_R/0/1/0/all/0/1\">Rosana C. B. Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Ver&#xf4;nica M. L. Silva</a>",
          "description": "Predicting gender by the name is not a simple task. In many applications,\nespecially in the natural language processing (NLP) field, this task may be\nnecessary, mainly when considering foreign names. Some machine learning\nalgorithms can satisfactorily perform the prediction. In this paper, we\nexamined and implemented feedforward and recurrent deep neural network models,\nsuch as MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first\nname. A dataset of Brazilian names is used to train and evaluate the models. We\nanalyzed the accuracy, recall, precision, and confusion matrix to measure the\nmodels' performances. The results indicate that the gender prediction can be\nperformed from the feature extraction strategy looking at the names as a set of\nstrings. Some models accurately predict the gender in more than 90% of the\ncases. The recurrent models overcome the feedforward models in this binary\nclassification problem.",
          "link": "http://arxiv.org/abs/2106.10156",
          "publishedOn": "2021-06-21T02:07:39.129Z",
          "wordCount": 589,
          "title": "Predicting gender of Brazilian names using deep learning. (arXiv:2106.10156v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1\">Robert Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1\">Peizhen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo E Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chasmai_M/0/1/0/all/0/1\">Mustafa Chasmai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schobs_L/0/1/0/all/0/1\">Lawrence Schobs</a>",
          "description": "Machine learning is a general-purpose technology holding promises for many\ninterdisciplinary research problems. However, significant barriers exist in\ncrossing disciplinary boundaries when most machine learning tools are developed\nin different areas separately. We present Pykale - a Python library for\nknowledge-aware machine learning on graphs, images, texts, and videos to enable\nand accelerate interdisciplinary research. We formulate new green machine\nlearning guidelines based on standard software engineering practices and\npropose a novel pipeline-based application programming interface (API). PyKale\nfocuses on leveraging knowledge from multiple sources for accurate and\ninterpretable prediction, thus supporting multimodal learning and transfer\nlearning (particularly domain adaptation) with latest deep learning and\ndimensionality reduction models. We build PyKale on PyTorch and leverage the\nrich PyTorch ecosystem. Our pipeline-based API design enforces standardization\nand minimalism, embracing green machine learning concepts via reducing\nrepetitions and redundancy, reusing existing resources, and recycling learning\nmodels across areas. We demonstrate its interdisciplinary nature via examples\nin bioinformatics, knowledge graph, image/video recognition, and medical\nimaging.",
          "link": "http://arxiv.org/abs/2106.09756",
          "publishedOn": "2021-06-21T02:07:39.121Z",
          "wordCount": 628,
          "title": "PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python. (arXiv:2106.09756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boucher_N/0/1/0/all/0/1\">Nicholas Boucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1\">Ilia Shumailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1\">Ross Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1\">Nicolas Papernot</a>",
          "description": "Several years of research have shown that machine-learning systems are\nvulnerable to adversarial examples, both in theory and in practice. Until now,\nsuch attacks have primarily targeted visual models, exploiting the gap between\nhuman and machine perception. Although text-based models have also been\nattacked with adversarial examples, such attacks struggled to preserve semantic\nmeaning and indistinguishability. In this paper, we explore a large class of\nadversarial examples that can be used to attack text-based models in a\nblack-box setting without making any human-perceptible visual modification to\ninputs. We use encoding-specific perturbations that are imperceptible to the\nhuman eye to manipulate the outputs of a wide range of Natural Language\nProcessing (NLP) systems from neural machine-translation pipelines to web\nsearch engines. We find that with a single imperceptible encoding injection --\nrepresenting one invisible character, homoglyph, reordering, or deletion -- an\nattacker can significantly reduce the performance of vulnerable models, and\nwith three injections most models can be functionally broken. Our attacks work\nagainst currently-deployed commercial systems, including those produced by\nMicrosoft and Google, in addition to open source models published by Facebook\nand IBM. This novel series of attacks presents a significant threat to many\nlanguage processing systems: an attacker can affect systems in a targeted\nmanner without any assumptions about the underlying model. We conclude that\ntext-based NLP systems require careful input sanitization, just like\nconventional applications, and that given such systems are now being deployed\nrapidly at scale, the urgent attention of architects and operators is required.",
          "link": "http://arxiv.org/abs/2106.09898",
          "publishedOn": "2021-06-21T02:07:39.113Z",
          "wordCount": 683,
          "title": "Bad Characters: Imperceptible NLP Attacks. (arXiv:2106.09898v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gudur_G/0/1/0/all/0/1\">Gautham Krishna Gudur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perepu_S/0/1/0/all/0/1\">Satheesh K. Perepu</a>",
          "description": "Federated learning is an effective way of extracting insights from different\nuser devices while preserving the privacy of users. However, new classes with\ncompletely unseen data distributions can stream across any device in a\nfederated learning setting, whose data cannot be accessed by the global server\nor other users. To this end, we propose a unified zero-shot framework to handle\nthese aforementioned challenges during federated learning. We simulate two\nscenarios here -- 1) when the new class labels are not reported by the user,\nthe traditional FL setting is used; 2) when new class labels are reported by\nthe user, we synthesize Anonymized Data Impressions by calculating class\nsimilarity matrices corresponding to each device's new classes followed by\nunsupervised clustering to distinguish between new classes across different\nusers. Moreover, our proposed framework can also handle statistical\nheterogeneities in both labels and models across the participating users. We\nempirically evaluate our framework on-device across different communication\nrounds (FL iterations) with new classes in both local and global updates, along\nwith heterogeneous labels and models, on two widely used audio classification\napplications -- keyword spotting and urban sound classification, and observe an\naverage deterministic accuracy increase of ~4.041% and ~4.258% respectively.",
          "link": "http://arxiv.org/abs/2106.10019",
          "publishedOn": "2021-06-21T02:07:39.095Z",
          "wordCount": 664,
          "title": "Zero-Shot Federated Learning with New Classes for Audio Classification. (arXiv:2106.10019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongqi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doya_K/0/1/0/all/0/1\">Kenji Doya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tani_J/0/1/0/all/0/1\">Jun Tani</a>",
          "description": "What is the difference between goal-directed and habitual behavior? We\npropose a novel computational framework of decision making with Bayesian\ninference, in which everything is integrated as an entire neural network model.\nThe model learns to predict environmental state transitions by self-exploration\nand generating motor actions by sampling stochastic internal states $z$.\nHabitual behavior, which is obtained from the prior distribution of $z$, is\nacquired by reinforcement learning. Goal-directed behavior is determined from\nthe posterior distribution of $z$ by planning, using active inference, to\nminimize the free energy for goal observation. We demonstrate the effectiveness\nof the proposed framework by experiments in a sensorimotor navigation task with\ncamera observations and continuous motor actions.",
          "link": "http://arxiv.org/abs/2106.09938",
          "publishedOn": "2021-06-21T02:07:39.088Z",
          "wordCount": 552,
          "title": "Goal-Directed Planning by Reinforcement Learning and Active Inference. (arXiv:2106.09938v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09762",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Detommaso_G/0/1/0/all/0/1\">Gianluca Detommaso</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bruckner_M/0/1/0/all/0/1\">Michael Br&#xfc;ckner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schulz_P/0/1/0/all/0/1\">Philip Schulz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1\">Victor Chernozhukov</a>",
          "description": "In this work we develop a novel characterization of marginal causal effect\nand causal bias in the continuous treatment setting. We show they can be\nexpressed as an expectation with respect to a conditional probability\ndistribution, which can be estimated via standard statistical and probabilistic\nmethods. All terms in the expectations can be computed via automatic\ndifferentiation, also for highly non-linear models. We further develop a new\ncomplete criterion for identifiability of causal effects via covariate\nadjustment, showing the bias equals zero if the criterion is met. We study the\neffectiveness of our framework in three different scenarios: linear models\nunder confounding, overcontrol and endogenous selection bias; a non-linear\nmodel where full identifiability cannot be achieved because of missing data; a\nsimulated medical study of statins and atherosclerotic cardiovascular disease.",
          "link": "http://arxiv.org/abs/2106.09762",
          "publishedOn": "2021-06-21T02:07:39.077Z",
          "wordCount": 565,
          "title": "Causal Bias Quantification for Continuous Treatment. (arXiv:2106.09762v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09815",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Davis_D/0/1/0/all/0/1\">Damek Davis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Diaz_M/0/1/0/all/0/1\">Mateo D&#xed;az</a>, <a href=\"http://arxiv.org/find/math/1/au:+Drusvyatskiy_D/0/1/0/all/0/1\">Dmitriy Drusvyatskiy</a>",
          "description": "Recent work has shown that stochastically perturbed gradient methods can\nefficiently escape strict saddle points of smooth functions. We extend this\nbody of work to nonsmooth optimization, by analyzing an inexact analogue of a\nstochastically perturbed gradient method applied to the Moreau envelope. The\nmain conclusion is that a variety of algorithms for nonsmooth optimization can\nescape strict saddle points of the Moreau envelope at a controlled rate. The\nmain technical insight is that typical algorithms applied to the proximal\nsubproblem yield directions that approximate the gradient of the Moreau\nenvelope in relative terms.",
          "link": "http://arxiv.org/abs/2106.09815",
          "publishedOn": "2021-06-21T02:07:39.068Z",
          "wordCount": 551,
          "title": "Escaping strict saddle points of the Moreau envelope in nonsmooth optimization. (arXiv:2106.09815v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avellaneda_F/0/1/0/all/0/1\">Florent Avellaneda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villemaire_R/0/1/0/all/0/1\">Roger Villemaire</a>",
          "description": "The Boolean matrix factorization problem consists in approximating a matrix\nby the Boolean product of two smaller Boolean matrices. To obtain optimal\nsolutions when the matrices to be factorized are small, we propose SAT and\nMaxSAT encoding; however, when the matrices to be factorized are large, we\npropose a heuristic based on the search for maximal biclique edge cover. We\nexperimentally demonstrate that our approaches allow a better factorization\nthan existing approaches while keeping reasonable computation times. Our\nmethods also allow the handling of incomplete matrices with missing entries.",
          "link": "http://arxiv.org/abs/2106.10105",
          "publishedOn": "2021-06-21T02:07:39.056Z",
          "wordCount": 512,
          "title": "Boolean Matrix Factorization with SAT and MaxSAT. (arXiv:2106.10105v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Collecting training data from untrusted sources exposes machine learning\nservices to poisoning adversaries, who maliciously manipulate training data to\ndegrade the model accuracy. When trained on offline datasets, poisoning\nadversaries have to inject the poisoned data in advance before training, and\nthe order of feeding these poisoned batches into the model is stochastic. In\ncontrast, practical systems are more usually trained/fine-tuned on sequentially\ncaptured real-time data, in which case poisoning adversaries could dynamically\npoison each data batch according to the current model state. In this paper, we\nfocus on the real-time settings and propose a new attacking strategy, which\naffiliates an accumulative phase with poisoning attacks to secretly (i.e.,\nwithout affecting accuracy) magnify the destructive effect of a (poisoned)\ntrigger batch. By mimicking online learning and federated learning on CIFAR-10,\nwe show that the model accuracy will significantly drop by a single update step\non the trigger batch after the accumulative phase. Our work validates that a\nwell-designed but straightforward attacking strategy can dramatically amplify\nthe poisoning effects, with no need to explore complex techniques.",
          "link": "http://arxiv.org/abs/2106.09993",
          "publishedOn": "2021-06-21T02:07:39.049Z",
          "wordCount": 615,
          "title": "Accumulative Poisoning Attacks on Real-time Data. (arXiv:2106.09993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Guang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Fei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_V/0/1/0/all/0/1\">Vincent CS Lee</a>",
          "description": "Detecting anomalies for dynamic graphs has drawn increasing attention due to\ntheir wide applications in social networks, e-commerce, and cybersecurity. The\nrecent deep learning-based approaches have shown promising results over shallow\nmethods. However, they fail to address two core challenges of anomaly detection\nin dynamic graphs: the lack of informative encoding for unattributed nodes and\nthe difficulty of learning discriminate knowledge from coupled spatial-temporal\ndynamic graphs. To overcome these challenges, in this paper, we present a novel\nTransformer-based Anomaly Detection framework for DYnamic graph (TADDY). Our\nframework constructs a comprehensive node encoding strategy to better represent\neach node's structural and temporal roles in an evolving graphs stream.\nMeanwhile, TADDY captures informative representation from dynamic graphs with\ncoupled spatial-temporal patterns via a dynamic graph transformer model. The\nextensive experimental results demonstrate that our proposed TADDY framework\noutperforms the state-of-the-art methods by a large margin on four real-world\ndatasets.",
          "link": "http://arxiv.org/abs/2106.09876",
          "publishedOn": "2021-06-21T02:07:39.031Z",
          "wordCount": 585,
          "title": "Anomaly Detection in Dynamic Graphs via Transformer. (arXiv:2106.09876v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1\">Richard Nock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sypherd_T/0/1/0/all/0/1\">Tyler Sypherd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_L/0/1/0/all/0/1\">Lalitha Sankar</a>",
          "description": "In today's ML, data can be twisted (changed) in various ways, either for bad\nor good intent. Such twisted data challenges the founding theory of properness\nfor supervised losses which form the basis for many popular losses for class\nprobability estimation. Unfortunately, at its core, properness ensures that the\noptimal models also learn the twist. In this paper, we analyse such class\nprobability-based losses when they are stripped off the mandatory properness;\nwe define twist-proper losses as losses formally able to retrieve the optimum\n(untwisted) estimate off the twists, and show that a natural extension of a\nhalf-century old loss introduced by S. Arimoto is twist proper. We then turn to\na theory that has provided some of the best off-the-shelf algorithms for proper\nlosses, boosting. Boosting can require access to the derivative of the convex\nconjugate of a loss to compute examples weights. Such a function can be hard to\nget, for computational or mathematical reasons; this turns out to be the case\nfor Arimoto's loss. We bypass this difficulty by inverting the problem as\nfollows: suppose a blueprint boosting algorithm is implemented with a general\nweight update function. What are the losses for which boosting-compliant\nminimisation happens? Our answer comes as a general boosting algorithm which\nmeets the optimal boosting dependence on the number of calls to the weak\nlearner; when applied to Arimoto's loss, it leads to a simple optimisation\nalgorithm whose performances are showcased on several domains and twists.",
          "link": "http://arxiv.org/abs/2106.09920",
          "publishedOn": "2021-06-21T02:07:39.025Z",
          "wordCount": 674,
          "title": "Being Properly Improper. (arXiv:2106.09920v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>",
          "description": "To evaluate clustering results is a significant part of cluster analysis.\nSince there are no true class labels for clustering in typical unsupervised\nlearning, many internal cluster validity indices (CVIs), which use predicted\nlabels and data, have been created. Without true labels, to design an effective\nCVI is as difficult as to create a clustering method. And it is crucial to have\nmore CVIs because there are no universal CVIs that can be used to measure all\ndatasets and no specific methods of selecting a proper CVI for clusters without\ntrue labels. Therefore, to apply a variety of CVIs to evaluate clustering\nresults is necessary. In this paper, we propose a novel internal CVI -- the\nDistance-based Separability Index (DSI), based on a data separability measure.\nWe compared the DSI with eight internal CVIs including studies from early Dunn\n(1974) to most recent CVDD (2019) and an external CVI as ground truth, by using\nclustering results of five clustering algorithms on 12 real and 97 synthetic\ndatasets. Results show DSI is an effective, unique, and competitive CVI to\nother compared CVIs. We also summarized the general process to evaluate CVIs\nand created the rank-difference metric for comparison of CVIs' results.",
          "link": "http://arxiv.org/abs/2106.09794",
          "publishedOn": "2021-06-21T02:07:39.009Z",
          "wordCount": 647,
          "title": "A Distance-based Separability Measure for Internal Cluster Validation. (arXiv:2106.09794v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "This paper investigates two techniques for developing efficient\nself-supervised vision transformers (EsViT) for visual representation learning.\nFirst, we show through a comprehensive empirical study that multi-stage\narchitectures with sparse self-attentions can significantly reduce modeling\ncomplexity but with a cost of losing the ability to capture fine-grained\ncorrespondences between image regions. Second, we propose a new pre-training\ntask of region matching which allows the model to capture fine-grained region\ndependencies and as a result significantly improves the quality of the learned\nvision representations. Our results show that combining the two techniques,\nEsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation,\noutperforming prior arts with around an order magnitude of higher throughput.\nWhen transferring to downstream linear classification tasks, EsViT outperforms\nits supervised counterpart on 17 out of 18 datasets. The code and models will\nbe publicly available.",
          "link": "http://arxiv.org/abs/2106.09785",
          "publishedOn": "2021-06-21T02:07:39.002Z",
          "wordCount": 595,
          "title": "Efficient Self-supervised Vision Transformers for Representation Learning. (arXiv:2106.09785v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheu_A/0/1/0/all/0/1\">Albert Cheu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_M/0/1/0/all/0/1\">Matthew Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jieming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Binghui Peng</a>",
          "description": "In shuffle privacy, each user sends a collection of randomized messages to a\ntrusted shuffler, the shuffler randomly permutes these messages, and the\nresulting shuffled collection of messages must satisfy differential privacy.\nPrior work in this model has largely focused on protocols that use a single\nround of communication to compute algorithmic primitives like means,\nhistograms, and counts. In this work, we present interactive shuffle protocols\nfor stochastic convex optimization. Our optimization protocols rely on a new\nnoninteractive protocol for summing vectors of bounded $\\ell_2$ norm. By\ncombining this sum subroutine with techniques including mini-batch stochastic\ngradient descent, accelerated gradient descent, and Nesterov's smoothing\nmethod, we obtain loss guarantees for a variety of convex loss functions that\nsignificantly improve on those of the local model and sometimes match those of\nthe central model.",
          "link": "http://arxiv.org/abs/2106.09805",
          "publishedOn": "2021-06-21T02:07:38.982Z",
          "wordCount": 574,
          "title": "Shuffle Private Stochastic Convex Optimization. (arXiv:2106.09805v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eugene Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1\">David D. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1\">Ophir Frieder</a>",
          "description": "Technology-assisted review (TAR) refers to human-in-the-loop active learning\nworkflows for finding relevant documents in large collections. These workflows\noften must meet a target for the proportion of relevant documents found (i.e.\nrecall) while also holding down costs. A variety of heuristic stopping rules\nhave been suggested for striking this tradeoff in particular settings, but none\nhave been tested against a range of recall targets and tasks. We propose two\nnew heuristic stopping rules, Quant and QuantCI based on model-based estimation\ntechniques from survey research. We compare them against a range of proposed\nheuristics and find they are accurate at hitting a range of recall targets\nwhile substantially reducing review costs.",
          "link": "http://arxiv.org/abs/2106.09871",
          "publishedOn": "2021-06-21T02:07:38.976Z",
          "wordCount": 549,
          "title": "Heuristic Stopping Rules For Technology-Assisted Review. (arXiv:2106.09871v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09834",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebrahimian_S/0/1/0/all/0/1\">Shadi Ebrahimian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hengyong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalra_M/0/1/0/all/0/1\">Mannu Kalra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>",
          "description": "By the ALARA (As Low As Reasonably Achievable) principle, ultra-low-dose CT\nreconstruction is a holy grail to minimize cancer risks and genetic damages,\nespecially for children. With the development of medical CT technologies, the\niterative algorithms are widely used to reconstruct decent CT images from a\nlow-dose scan. Recently, artificial intelligence (AI) techniques have shown a\ngreat promise in further reducing CT radiation dose to the next level. In this\npaper, we demonstrate that AI-powered CT reconstruction offers diagnostic image\nquality at an ultra-low-dose level comparable to that of radiography.\nSpecifically, here we develop a Split Unrolled Grid-like Alternative\nReconstruction (SUGAR) network, in which deep learning, physical modeling and\nimage prior are integrated. The reconstruction results from clinical datasets\nshow that excellent images can be reconstructed using SUGAR from 36\nprojections. This approach has a potential to change future healthcare.",
          "link": "http://arxiv.org/abs/2106.09834",
          "publishedOn": "2021-06-21T02:07:38.970Z",
          "wordCount": 597,
          "title": "AI-Enabled Ultra-Low-Dose CT Reconstruction. (arXiv:2106.09834v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khezeli_K/0/1/0/all/0/1\">Kia Khezeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaas_A/0/1/0/all/0/1\">Arno Blaas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soboczenski_F/0/1/0/all/0/1\">Frank Soboczenski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chia_N/0/1/0/all/0/1\">Nicholas Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalantari_J/0/1/0/all/0/1\">John Kalantari</a>",
          "description": "The Invariant Risk Minimization (IRM) principle was first proposed by\nArjovsky et al. [2019] to address the domain generalization problem by\nleveraging data heterogeneity from differing experimental conditions.\nSpecifically, IRM seeks to find a data representation under which an optimal\nclassifier remains invariant across all domains. Despite the conceptual appeal\nof IRM, the effectiveness of the originally proposed invariance penalty has\nrecently been brought into question. In particular, there exists\ncounterexamples for which that invariance penalty can be arbitrarily small for\nnon-invariant data representations. We propose an alternative invariance\npenalty by revisiting the Gramian matrix of the data representation. We discuss\nthe role of its eigenvalues in the relationship between the risk and the\ninvariance penalty, and demonstrate that it is ill-conditioned for said\ncounterexamples. The proposed approach is guaranteed to recover an invariant\nrepresentation for linear settings under mild non-degeneracy conditions. Its\neffectiveness is substantiated by experiments on DomainBed and\nInvarianceUnitTest, two extensive test beds for domain generalization.",
          "link": "http://arxiv.org/abs/2106.09777",
          "publishedOn": "2021-06-21T02:07:38.963Z",
          "wordCount": 591,
          "title": "On Invariance Penalties for Risk Minimization. (arXiv:2106.09777v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schaefermeier_B/0/1/0/all/0/1\">Bastian Schaefermeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1\">Gerd Stumme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1\">Tom Hanika</a>",
          "description": "The ubiquitous presence of WiFi access points and mobile devices capable of\nmeasuring WiFi signal strengths allow for real-world applications in indoor\nlocalization and mapping. In particular, no additional infrastructure is\nrequired. Previous approaches in this field were, however, often hindered by\nproblems such as effortful map-building processes, changing environments and\nhardware differences. We tackle these problems focussing on topological maps.\nThese represent discrete locations, such as rooms, and their relations, e.g.,\ndistances and transition frequencies. In our unsupervised method, we employ\nWiFi signal strength distributions, dimension reduction and clustering. It can\nbe used in settings where users carry mobile devices and follow their normal\nroutine. We aim for applications in short-lived indoor events such as\nconferences.",
          "link": "http://arxiv.org/abs/2106.09789",
          "publishedOn": "2021-06-21T02:07:38.953Z",
          "wordCount": 557,
          "title": "Topological Indoor Mapping through WiFi Signals. (arXiv:2106.09789v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muzellec_B/0/1/0/all/0/1\">Boris Muzellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "Kernel mean embeddings are a popular tool that consists in representing\nprobability measures by their infinite-dimensional mean embeddings in a\nreproducing kernel Hilbert space. When the kernel is characteristic, mean\nembeddings can be used to define a distance between probability measures, known\nas the maximum mean discrepancy (MMD). A well-known advantage of mean\nembeddings and MMD is their low computational cost and low sample complexity.\nHowever, kernel mean embeddings have had limited applications to problems that\nconsist in optimizing distributions, due to the difficulty of characterizing\nwhich Hilbert space vectors correspond to a probability distribution. In this\nnote, we propose to leverage the kernel sums-of-squares parameterization of\npositive functions of Marteau-Ferey et al. [2020] to fit distributions in the\nMMD geometry. First, we show that when the kernel is characteristic,\ndistributions with a kernel sum-of-squares density are dense. Then, we provide\nalgorithms to optimize such distributions in the finite-sample setting, which\nwe illustrate in a density fitting numerical experiment.",
          "link": "http://arxiv.org/abs/2106.09994",
          "publishedOn": "2021-06-21T02:07:38.946Z",
          "wordCount": 593,
          "title": "A Note on Optimizing Distributions using Kernel Mean Embeddings. (arXiv:2106.09994v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Tomoki Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>",
          "description": "We propose a novel GAN training scheme that can handle any level of labeling\nin a unified manner. Our scheme introduces a form of artificial labeling that\ncan incorporate manually defined labels, when available, and induce an\nalignment between them. To define the artificial labels, we exploit the\nassumption that neural network generators can be trained more easily to map\nnearby latent vectors to data with semantic similarities, than across separate\ncategories. We use generated data samples and their corresponding artificial\nconditioning labels to train a classifier. The classifier is then used to\nself-label real data. To boost the accuracy of the self-labeling, we also use\nthe exponential moving average of the classifier. However, because the\nclassifier might still make mistakes, especially at the beginning of the\ntraining, we also refine the labels through self-attention, by using the\nlabeling of real data samples only when the classifier outputs a high\nclassification probability score. We evaluate our approach on CIFAR-10, STL-10\nand SVHN, and show that both self-labeling and self-attention consistently\nimprove the quality of generated data. More surprisingly, we find that the\nproposed scheme can even outperform class-conditional GANs.",
          "link": "http://arxiv.org/abs/2106.09914",
          "publishedOn": "2021-06-21T02:07:38.927Z",
          "wordCount": 626,
          "title": "A Unified Generative Adversarial Network Training via Self-Labeling and Self-Attention. (arXiv:2106.09914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yabin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "Domain adaptation becomes more challenging with increasing gaps between\nsource and target domains. Motivated from an empirical analysis on the\nreliability of labeled source data for the use of distancing target domains, we\npropose self-training of auxiliary models (AuxSelfTrain) that learns models for\nintermediate domains and gradually combats the distancing shifts across\ndomains. We introduce evolving intermediate domains as combinations of\ndecreasing proportion of source data and increasing proportion of target data,\nwhich are sampled to minimize the domain distance between consecutive domains.\nThen the source model could be gradually adapted for the use in the target\ndomain by self-training of auxiliary models on evolving intermediate domains.\nWe also introduce an enhanced indicator for sample selection via implicit\nensemble and extend the proposed method to semi-supervised domain adaptation.\nExperiments on benchmark datasets of unsupervised and semi-supervised domain\nadaptation verify its efficacy.",
          "link": "http://arxiv.org/abs/2106.09890",
          "publishedOn": "2021-06-21T02:07:38.921Z",
          "wordCount": 579,
          "title": "Gradual Domain Adaptation via Self-Training of Auxiliary Models. (arXiv:2106.09890v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1\">Prashant Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Bikramjit Banerjee</a>",
          "description": "Recent renewed interest in multi-agent reinforcement learning (MARL) has\ngenerated an impressive array of techniques that leverage deep reinforcement\nlearning, primarily actor-critic architectures, and can be applied to a limited\nrange of settings in terms of observability and communication. However, a\ncontinuing limitation of much of this work is the curse of dimensionality when\nit comes to representations based on joint actions, which grow exponentially\nwith the number of agents. In this paper, we squarely focus on this challenge\nof scalability. We apply the key insight of action anonymity, which leads to\npermutation invariance of joint actions, to two recently presented deep MARL\nalgorithms, MADDPG and IA2C, and compare these instantiations to another recent\ntechnique that leverages action anonymity, viz., mean-field MARL. We show that\nour instantiations can learn the optimal behavior in a broader class of agent\nnetworks than the mean-field method, using a recently introduced pragmatic\ndomain.",
          "link": "http://arxiv.org/abs/2106.09825",
          "publishedOn": "2021-06-21T02:07:38.914Z",
          "wordCount": 583,
          "title": "Many Agent Reinforcement Learning Under Partial Observability. (arXiv:2106.09825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09759",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1\">A. Ben Hamza</a>",
          "description": "We introduce a new dataset called Synthetic COVID-19 Chest X-ray Dataset for\ntraining machine learning models. The dataset consists of 21,295 synthetic\nCOVID-19 chest X-ray images to be used for computer-aided diagnosis. These\nimages, generated via an unsupervised domain adaptation approach, are of high\nquality. We find that the synthetic images not only improve performance of\nvarious deep learning architectures when used as additional training data under\nheavy imbalance conditions, but also detect the target class with high\nconfidence. We also find that comparable performance can also be achieved when\ntrained only on synthetic images. Further, salient features of the synthetic\nCOVID-19 images indicate that the distribution is significantly different from\nNon-COVID-19 classes, enabling a proper decision boundary. We hope the\navailability of such high fidelity chest X-ray images of COVID-19 will\nencourage advances in the development of diagnostic and/or management tools.",
          "link": "http://arxiv.org/abs/2106.09759",
          "publishedOn": "2021-06-21T02:07:38.900Z",
          "wordCount": 633,
          "title": "Synthetic COVID-19 Chest X-ray Dataset for Computer-Aided Diagnosis. (arXiv:2106.09759v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shibo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1\">Robert M. Kirby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_S/0/1/0/all/0/1\">Shandian Zhe</a>",
          "description": "Bayesian optimization (BO) is a powerful approach for optimizing black-box,\nexpensive-to-evaluate functions. To enable a flexible trade-off between the\ncost and accuracy, many applications allow the function to be evaluated at\ndifferent fidelities. In order to reduce the optimization cost while maximizing\nthe benefit-cost ratio, in this paper, we propose Batch Multi-fidelity Bayesian\nOptimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of\nBayesian neural networks to construct a fully auto-regressive model, which is\nexpressive enough to capture strong yet complex relationships across all the\nfidelities, so as to improve the surrogate learning and optimization\nperformance. Furthermore, to enhance the quality and diversity of queries, we\ndevelop a simple yet efficient batch querying method, without any combinatorial\nsearch over the fidelities. We propose a batch acquisition function based on\nMax-value Entropy Search (MES) principle, which penalizes highly correlated\nqueries and encourages diversity. We use posterior samples and moment matching\nto fulfill efficient computation of the acquisition function and conduct\nalternating optimization over every fidelity-input pair, which guarantees an\nimprovement at each step. We demonstrate the advantage of our approach on four\nreal-world hyperparameter optimization applications.",
          "link": "http://arxiv.org/abs/2106.09884",
          "publishedOn": "2021-06-21T02:07:38.875Z",
          "wordCount": 620,
          "title": "Batch Multi-Fidelity Bayesian Optimization with Deep Auto-Regressive Networks. (arXiv:2106.09884v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongmin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiucai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imakura_A/0/1/0/all/0/1\">Akira Imakura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakurai_T/0/1/0/all/0/1\">Tetsuya Sakurai</a>",
          "description": "Ensemble clustering is a fundamental problem in the machine learning field,\ncombining multiple base clusterings into a better clustering result. However,\nmost of the existing methods are unsuitable for large-scale ensemble clustering\ntasks due to the efficiency bottleneck. In this paper, we propose a large-scale\nspectral ensemble clustering (LSEC) method to strike a good balance between\nefficiency and effectiveness. In LSEC, a large-scale spectral clustering based\nefficient ensemble generation framework is designed to generate various base\nclusterings within a low computational complexity. Then all based clustering\nare combined through a bipartite graph partition based consensus function into\na better consensus clustering result. The LSEC method achieves a lower\ncomputational complexity than most existing ensemble clustering methods.\nExperiments conducted on ten large-scale datasets show the efficiency and\neffectiveness of the LSEC method. The MATLAB code of the proposed method and\nexperimental datasets are available at https://github.com/Li-\nHongmin/MyPaperWithCode.",
          "link": "http://arxiv.org/abs/2106.09852",
          "publishedOn": "2021-06-21T02:07:38.868Z",
          "wordCount": 574,
          "title": "LSEC: Large-scale spectral ensemble clustering. (arXiv:2106.09852v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dominguez_Caballero_J/0/1/0/all/0/1\">Javier Dominguez-Caballero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_R/0/1/0/all/0/1\">Rob Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayvar_Soberanis_S/0/1/0/all/0/1\">Sabino Ayvar-Soberanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curtis_D/0/1/0/all/0/1\">David Curtis</a>",
          "description": "Accurate prediction of machining cycle times is important in the\nmanufacturing industry. Usually, Computer Aided Manufacturing (CAM) software\nestimates the machining times using the commanded feedrate from the toolpath\nfile using basic kinematic settings. Typically, the methods do not account for\ntoolpath geometry or toolpath tolerance and therefore under estimate the\nmachining cycle times considerably. Removing the need for machine specific\nknowledge, this paper presents a data-driven feedrate and machining cycle time\nprediction method by building a neural network model for each machine tool\naxis. In this study, datasets composed of the commanded feedrate, nominal\nacceleration, toolpath geometry and the measured feedrate were used to train a\nneural network model. Validation trials using a representative industrial thin\nwall structure component on a commercial machining centre showed that this\nmethod estimated the machining time with more than 90% accuracy. This method\nshowed that neural network models have the capability to learn the behavior of\na complex machine tool system and predict cycle times. Further integration of\nthe methods will be critical in the implantation of digital twins in Industry\n4.0.",
          "link": "http://arxiv.org/abs/2106.09719",
          "publishedOn": "2021-06-21T02:07:38.862Z",
          "wordCount": 629,
          "title": "Machining Cycle Time Prediction: Data-driven Modelling of Machine Tool Feedrate Behavior with Neural Networks. (arXiv:2106.09719v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sangdon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1\">Edgar Dobriban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>",
          "description": "An important challenge facing modern machine learning is how to rigorously\nquantify the uncertainty of model predictions. Conveying uncertainty is\nespecially important when there are changes to the underlying data distribution\nthat might invalidate the predictive model. Yet, most existing uncertainty\nquantification algorithms break down in the presence of such shifts. We propose\na novel approach that addresses this challenge by constructing \\emph{probably\napproximately correct (PAC)} prediction sets in the presence of covariate\nshift. Our approach focuses on the setting where there is a covariate shift\nfrom the source distribution (where we have labeled training examples) to the\ntarget distribution (for which we want to quantify uncertainty). Our algorithm\nassumes given importance weights that encode how the probabilities of the\ntraining examples change under the covariate shift. In practice, importance\nweights typically need to be estimated; thus, we extend our algorithm to the\nsetting where we are given confidence intervals for the importance weights\nrather than their true value. We demonstrate the effectiveness of our approach\non various covariate shifts designed based on the DomainNet and ImageNet\ndatasets.",
          "link": "http://arxiv.org/abs/2106.09848",
          "publishedOn": "2021-06-21T02:07:38.855Z",
          "wordCount": 609,
          "title": "PAC Prediction Sets Under Covariate Shift. (arXiv:2106.09848v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09780",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Huhn_F/0/1/0/all/0/1\">Francisco Huhn</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Magri_L/0/1/0/all/0/1\">Luca Magri</a>",
          "description": "We develop a versatile optimization method, which finds the design parameters\nthat minimize time-averaged acoustic cost functionals. The method is\ngradient-free, model-informed, and data-driven with reservoir computing based\non echo state networks. First, we analyse the predictive capabilities of echo\nstate networks both in the short- and long-time prediction of the dynamics. We\nfind that both fully data-driven and model-informed architectures learn the\nchaotic acoustic dynamics, both time-accurately and statistically. Informing\nthe training with a physical reduced-order model with one acoustic mode\nmarkedly improves the accuracy and robustness of the echo state networks,\nwhilst keeping the computational cost low. Echo state networks offer accurate\npredictions of the long-time dynamics, which would be otherwise expensive by\nintegrating the governing equations to evaluate the time-averaged quantity to\noptimize. Second, we couple echo state networks with a Bayesian technique to\nexplore the design thermoacoustic parameter space. The computational method is\nminimally intrusive. Third, we find the set of flame parameters that minimize\nthe time-averaged acoustic energy of chaotic oscillations, which are caused by\nthe positive feedback with a heat source, such as a flame in gas turbines or\nrocket motors. These oscillations are known as thermoacoustic oscillations. The\noptimal set of flame parameters is found with the same accuracy as brute-force\ngrid search, but with a convergence rate that is more than one order of\nmagnitude faster. This work opens up new possibilities for non-intrusive\n(``hands-off'') optimization of chaotic systems, in which the cost of\ngenerating data, for example from high-fidelity simulations and experiments, is\nhigh.",
          "link": "http://arxiv.org/abs/2106.09780",
          "publishedOn": "2021-06-21T02:07:38.849Z",
          "wordCount": 693,
          "title": "Gradient-free optimization of chaotic acoustics with reservoir computing. (arXiv:2106.09780v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cranmer_M/0/1/0/all/0/1\">Miles Cranmer</a> (Princeton), <a href=\"http://arxiv.org/find/cs/1/au:+Melchior_P/0/1/0/all/0/1\">Peter Melchior</a> (Princeton), <a href=\"http://arxiv.org/find/cs/1/au:+Nord_B/0/1/0/all/0/1\">Brian Nord</a> (Fermilab)",
          "description": "We present an approach for maximizing a global utility function by learning\nhow to allocate resources in an unsupervised way. We expect interactions\nbetween allocation targets to be important and therefore propose to learn the\nreward structure for near-optimal allocation policies with a GNN. By relaxing\nthe resource constraint, we can employ gradient-based optimization in contrast\nto more standard evolutionary algorithms. Our algorithm is motivated by a\nproblem in modern astronomy, where one needs to select-based on limited initial\ninformation-among $10^9$ galaxies those whose detailed measurement will lead to\noptimal inference of the composition of the universe. Our technique presents a\nway of flexibly learning an allocation strategy by only requiring forward\nsimulators for the physics of interest and the measurement process. We\nanticipate that our technique will also find applications in a range of\nresource allocation problems.",
          "link": "http://arxiv.org/abs/2106.09761",
          "publishedOn": "2021-06-21T02:07:38.831Z",
          "wordCount": 602,
          "title": "Unsupervised Resource Allocation with Graph Neural Networks. (arXiv:2106.09761v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenfeld_E/0/1/0/all/0/1\">Elan Rosenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1\">Mark Sellke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1\">Andrej Risteski</a>",
          "description": "Domain generalization aims at performing well on unseen test environments\nwith data from a limited number of training environments. Despite a\nproliferation of proposal algorithms for this task, assessing their\nperformance, both theoretically and empirically is still very challenging.\nMoreover, recent approaches such as Invariant Risk Minimization (IRM) require a\nprohibitively large number of training environments - linear in the dimension\nof the spurious feature space $d_s$ - even on simple data models like the one\nproposed by [Rosenfeld et al., 2021]. Under a variant of this model, we show\nthat both ERM and IRM cannot generalize with $o(d_s)$ environments. We then\npresent a new algorithm based on performing iterative feature matching that is\nguaranteed with high probability to yield a predictor that generalizes after\nseeing only $O(\\log{d_s})$ environments.",
          "link": "http://arxiv.org/abs/2106.09913",
          "publishedOn": "2021-06-21T02:07:38.824Z",
          "wordCount": 570,
          "title": "Iterative Feature Matching: Toward Provable Domain Generalization with Logarithmic Environments. (arXiv:2106.09913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kapishnikov_A/0/1/0/all/0/1\">Andrei Kapishnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1\">Besim Avci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedin_B/0/1/0/all/0/1\">Ben Wedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1\">Michael Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1\">Tolga Bolukbasi</a>",
          "description": "Integrated Gradients (IG) is a commonly used feature attribution method for\ndeep neural networks. While IG has many desirable properties, the method often\nproduces spurious/noisy pixel attributions in regions that are not related to\nthe predicted class when applied to visual models. While this has been\npreviously noted, most existing solutions are aimed at addressing the symptoms\nby explicitly reducing the noise in the resulting attributions. In this work,\nwe show that one of the causes of the problem is the accumulation of noise\nalong the IG path. To minimize the effect of this source of noise, we propose\nadapting the attribution path itself -- conditioning the path not just on the\nimage but also on the model being explained. We introduce Adaptive Path Methods\n(APMs) as a generalization of path methods, and Guided IG as a specific\ninstance of an APM. Empirically, Guided IG creates saliency maps better aligned\nwith the model's prediction and the input image that is being explained. We\nshow through qualitative and quantitative experiments that Guided IG\noutperforms other, related methods in nearly every experiment.",
          "link": "http://arxiv.org/abs/2106.09788",
          "publishedOn": "2021-06-21T02:07:38.816Z",
          "wordCount": 657,
          "title": "Guided Integrated Gradients: An Adaptive Path Method for Removing Noise. (arXiv:2106.09788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mauritz_R/0/1/0/all/0/1\">R.R. Mauritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nijweide_F/0/1/0/all/0/1\">F.P.J. Nijweide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goseling_J/0/1/0/all/0/1\">J. Goseling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keulen_M/0/1/0/all/0/1\">M. van Keulen</a>",
          "description": "In the field of data integration, data quality problems are often encountered\nwhen extracting, combining, and merging data. The probabilistic data\nintegration approach represents information about such problems as\nuncertainties in a probabilistic database. In this paper, we propose a\ndata-cleaning autoencoder capable of near-automatic data quality improvement.\nIt learns the structure and dependencies in the data to identify and correct\ndoubtful values. A theoretical framework is provided, and experiments show that\nit can remove significant amounts of noise from categorical and numeric\nprobabilistic data. Our method does not require clean data. We do, however,\nshow that manually cleaning a small fraction of the data significantly improves\nperformance.",
          "link": "http://arxiv.org/abs/2106.09764",
          "publishedOn": "2021-06-21T02:07:38.763Z",
          "wordCount": 561,
          "title": "Autoencoder-based cleaning in probabilistic databases. (arXiv:2106.09764v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2102.13186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1\">Marinka Zitnik</a>",
          "description": "As the representations output by Graph Neural Networks (GNNs) are\nincreasingly employed in real-world applications, it becomes important to\nensure that these representations are fair and stable. In this work, we\nestablish a key connection between counterfactual fairness and stability and\nleverage it to propose a novel framework, NIFTY (uNIfying Fairness and\nstabiliTY), which can be used with any GNN to learn fair and stable\nrepresentations. We introduce a novel objective function that simultaneously\naccounts for fairness and stability and develop a layer-wise weight\nnormalization using the Lipschitz constant to enhance neural message passing in\nGNNs. In doing so, we enforce fairness and stability both in the objective\nfunction as well as in the GNN architecture. Further, we show theoretically\nthat our layer-wise weight normalization promotes counterfactual fairness and\nstability in the resulting representations. We introduce three new graph\ndatasets comprising of high-stakes decisions in criminal justice and financial\nlending domains. Extensive experimentation with the above datasets demonstrates\nthe efficacy of our framework.",
          "link": "http://arxiv.org/abs/2102.13186",
          "publishedOn": "2021-06-18T02:06:46.697Z",
          "wordCount": 626,
          "title": "Towards a Unified Framework for Fair and Stable Graph Representation Learning. (arXiv:2102.13186v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cabana_A/0/1/0/all/0/1\">Alejandro Cabana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lago_Fernandez_L/0/1/0/all/0/1\">Luis F. Lago-Fern&#xe1;ndez</a>",
          "description": "We introduce a new technique for gradient normalization during neural network\ntraining. The gradients are rescaled during the backward pass using\nnormalization layers introduced at certain points within the network\narchitecture. These normalization nodes do not affect forward activity\npropagation, but modify backpropagation equations to permit a well-scaled\ngradient flow that reaches the deepest network layers without experimenting\nvanishing or explosion. Results on tests with very deep neural networks show\nthat the new technique can do an effective control of the gradient norm,\nallowing the update of weights in the deepest layers and improving network\naccuracy on several experimental conditions.",
          "link": "http://arxiv.org/abs/2106.09475",
          "publishedOn": "2021-06-18T02:06:46.689Z",
          "wordCount": 526,
          "title": "Backward Gradient Normalization in Deep Neural Networks. (arXiv:2106.09475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liventsev_V/0/1/0/all/0/1\">Vadim Liventsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harma_A/0/1/0/all/0/1\">Aki H&#xe4;rm&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovic_M/0/1/0/all/0/1\">Milan Petkovi&#x107;</a>",
          "description": "Most state of the art decision systems based on Reinforcement Learning (RL)\nare data-driven black-box neural models, where it is often difficult to\nincorporate expert knowledge into the models or let experts review and validate\nthe learned decision mechanisms. Knowledge-insertion and model review are\nimportant requirements in many applications involving human health and safety.\nOne way to bridge the gap between data and knowledge driven systems is program\nsynthesis: replacing a neural network that outputs decisions with a symbolic\nprogram generated by a neural network or by means of genetic programming. We\npropose a new programming language, BF++, designed specifically for automatic\nprogramming of agents in a Partially Observable Markov Decision Process (POMDP)\nsetting and apply neural program synthesis to solve standard OpenAI Gym\nbenchmarks.",
          "link": "http://arxiv.org/abs/2101.09571",
          "publishedOn": "2021-06-18T02:06:46.682Z",
          "wordCount": 602,
          "title": "BF++: a language for general-purpose program synthesis. (arXiv:2101.09571v4 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09028",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Yamasaki_H/0/1/0/all/0/1\">Hayata Yamasaki</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sonoda_S/0/1/0/all/0/1\">Sho Sonoda</a>",
          "description": "Random features are a central technique for scalable learning algorithms\nbased on kernel methods. A recent work has shown that an algorithm for machine\nlearning by quantum computer, quantum machine learning (QML), can exponentially\nspeed up sampling of optimized random features, even without imposing\nrestrictive assumptions on sparsity and low-rankness of matrices that had\nlimited applicability of conventional QML algorithms; this QML algorithm makes\nit possible to significantly reduce and provably minimize the required number\nof features for regression tasks. However, a major interest in the field of QML\nis how widely the advantages of quantum computation can be exploited, not only\nin the regression tasks. We here construct a QML algorithm for a classification\ntask accelerated by the optimized random features. We prove that the QML\nalgorithm for sampling optimized random features, combined with stochastic\ngradient descent (SGD), can achieve state-of-the-art exponential convergence\nspeed of reducing classification error in a classification task under a\nlow-noise condition; at the same time, our algorithm with optimized random\nfeatures can take advantage of the significant reduction of the required number\nof features so as to accelerate each iteration in the SGD and evaluation of the\nclassifier obtained from our algorithm. These results discover a promising\napplication of QML to significant acceleration of the leading classification\nalgorithm based on kernel methods, without ruining its applicability to a\npractical class of data sets and the exponential error-convergence speed.",
          "link": "http://arxiv.org/abs/2106.09028",
          "publishedOn": "2021-06-18T02:06:46.675Z",
          "wordCount": 688,
          "title": "Exponential Error Convergence in Data Classification with Optimized Random Features: Acceleration by Quantum Machine Learning. (arXiv:2106.09028v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_N/0/1/0/all/0/1\">Neel Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shakti Mishra</a>",
          "description": "Reinforcement Learning has applications in field of mechatronics, robotics,\nand other resource-constrained control system. Problem of resource allocation\nis primarily solved using traditional predefined techniques and modern deep\nlearning methods. The drawback of predefined and most deep learning methods for\nresource allocation is failing to meet the requirements in cases of uncertain\nsystem environment. We can approach problem of resource allocation in uncertain\nsystem environment alongside following certain criteria using deep\nreinforcement learning. Also, reinforcement learning has ability for adapting\nto new uncertain environment for prolonged period of time. The paper provides a\ndetailed comparative analysis on various deep reinforcement learning methods by\napplying different components to modify architecture of reinforcement learning\nwith use of noisy layers, prioritized replay, bagging, duelling networks, and\nother related combination to obtain improvement in terms of performance and\nreduction of computational cost. The paper identifies problem of resource\nallocation in uncertain environment could be effectively solved using Noisy\nBagging duelling double deep Q network achieving efficiency of 97.7% by\nmaximizing reward with significant exploration in given simulated environment\nfor resource allocation.",
          "link": "http://arxiv.org/abs/2106.09461",
          "publishedOn": "2021-06-18T02:06:38.123Z",
          "wordCount": 630,
          "title": "Modelling resource allocation in uncertain system environment through deep reinforcement learning. (arXiv:2106.09461v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mugunthan_V/0/1/0/all/0/1\">Vaikkunth Mugunthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kagal_L/0/1/0/all/0/1\">Lalana Kagal</a>",
          "description": "Vertical Federated Learning (VFL) refers to the collaborative training of a\nmodel on a dataset where the features of the dataset are split among multiple\ndata owners, while label information is owned by a single data owner. In this\npaper, we propose a novel method, Multi Vertical Federated Learning\n(Multi-VFL), to train VFL models when there are multiple data and label owners.\nOur approach is the first to consider the setting where $D$-data owners (across\nwhich features are distributed) and $K$-label owners (across which labels are\ndistributed) exist. This proposed configuration allows different entities to\ntrain and learn optimal models without having to share their data. Our\nframework makes use of split learning and adaptive federated optimizers to\nsolve this problem. For empirical evaluation, we run experiments on the MNIST\nand FashionMNIST datasets. Our results show that using adaptive optimizers for\nmodel aggregation fastens convergence and improves accuracy.",
          "link": "http://arxiv.org/abs/2106.05468",
          "publishedOn": "2021-06-18T02:06:38.112Z",
          "wordCount": 600,
          "title": "Multi-VFL: A Vertical Federated Learning System for Multiple Data and Label Owners. (arXiv:2106.05468v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruichu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1\">Jie Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhifeng Hao</a>",
          "description": "Causal discovery from observational data is an important but challenging task\nin many scientific fields. Recently, NOTEARS [Zheng et al., 2018] formulates\nthe causal structure learning problem as a continuous optimization problem\nusing least-square loss with an acyclicity constraint. Though the least-square\nloss function is well justified under the standard Gaussian noise assumption,\nit is limited if the assumption does not hold. In this work, we theoretically\nshow that the violation of the Gaussian noise assumption will hinder the causal\ndirection identification, making the causal orientation fully determined by the\ncausal strength as well as the variances of noises in the linear case and the\nnoises of strong non-Gaussianity in the nonlinear case. Consequently, we\npropose a more general entropy-based loss that is theoretically consistent with\nthe likelihood score under any noise distribution. We run extensive empirical\nevaluations on both synthetic data and real-world data to validate the\neffectiveness of the proposed method and show that our method achieves the best\nin Structure Hamming Distance, False Discovery Rate, and True Positive Rate\nmatrices.",
          "link": "http://arxiv.org/abs/2106.02835",
          "publishedOn": "2021-06-18T02:06:38.019Z",
          "wordCount": 633,
          "title": "On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization. (arXiv:2106.02835v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunlu_O/0/1/0/all/0/1\">Onur G&#xfc;nl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloch_M/0/1/0/all/0/1\">Matthieu Bloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaefer_R/0/1/0/all/0/1\">Rafael F. Schaefer</a>",
          "description": "We consider a distributed function computation problem in which parties\nobserving noisy versions of a remote source facilitate the computation of a\nfunction of their observations at a fusion center through public communication.\nThe distributed function computation is subject to constraints, including not\nonly reliability and storage but also privacy and secrecy. Specifically, 1) the\nremote source should remain private from an eavesdropper and the fusion center,\nmeasured in terms of the information leaked about the remote source; 2) the\nfunction computed should remain secret from the eavesdropper, measured in terms\nof the information leaked about the arguments of the function, to ensure\nsecrecy regardless of the exact function used. We derive the exact rate regions\nfor lossless and lossy single-function computation and illustrate the lossy\nsingle-function computation rate region for an information bottleneck example,\nin which the optimal auxiliary random variables are characterized for\nbinary-input symmetric-output channels. We extend the approach to lossless and\nlossy asynchronous multiple-function computations with joint secrecy and\nprivacy constraints, in which case inner and outer bounds for the rate regions\ndiffering only in the Markov chain conditions imposed are characterized.",
          "link": "http://arxiv.org/abs/2106.09485",
          "publishedOn": "2021-06-18T02:06:38.013Z",
          "wordCount": 650,
          "title": "Secure Multi-Function Computation with Private Remote Sources. (arXiv:2106.09485v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zaiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1\">Siva Theja Maguluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>",
          "description": "Stochastic Approximation (SA) is a popular approach for solving fixed-point\nequations where the information is corrupted by noise. In this paper, we\nconsider an SA involving a contraction mapping with respect to an arbitrary\nnorm, and show its finite-sample error bounds while using different stepsizes.\nThe idea is to construct a smooth Lyapunov function using the generalized\nMoreau envelope, and show that the iterates of SA have negative drift with\nrespect to that Lyapunov function. Our result is applicable in Reinforcement\nLearning (RL). In particular, we use it to establish the first-known\nconvergence rate of the V-trace algorithm for off-policy TD-learning. Moreover,\nwe also use it to study TD-learning in the on-policy setting, and recover the\nexisting state-of-the-art results for $Q$-learning. Importantly, our\nconstruction results in only a logarithmic dependence of the convergence bound\non the size of the state-space.",
          "link": "http://arxiv.org/abs/2002.00874",
          "publishedOn": "2021-06-18T02:06:38.004Z",
          "wordCount": 640,
          "title": "Finite-Sample Analysis of Stochastic Approximation Using Smooth Convex Envelopes. (arXiv:2002.00874v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaewoong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1\">Geonho Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungjoo Kang</a>",
          "description": "In the real-world data, there are common variations shared by all classes\n(e.g. category label) and exclusive variations of each class. We propose a\nvariant of VAE capable of disentangling both of these variations. To represent\nthese generative factors of data, we introduce two sets of continuous latent\nvariables, private variable and public variable. Our proposed framework models\nthe private variable as a Mixture of Gaussian and the public variable as a\nGaussian, respectively. Each mode of the private variable is responsible for a\nclass of the discrete variable.\n\nMost of the previous attempts to integrate the discrete generative factors to\ndisentanglement assume statistical independence between the continuous and\ndiscrete variables. Our proposed model, which we call Discond-VAE, DISentangles\nthe class-dependent CONtinuous factors from the Discrete factors by introducing\nthe private variables. The experiments show that Discond-VAE can discover the\nprivate and public factors from data. Moreover, even under the dataset with\nonly public factors, Discond-VAE does not fail and adapts the private variables\nto represent the public factors.",
          "link": "http://arxiv.org/abs/2009.08039",
          "publishedOn": "2021-06-18T02:06:37.997Z",
          "wordCount": 627,
          "title": "Discond-VAE: Disentangling Continuous Factors from the Discrete. (arXiv:2009.08039v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03017",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Alquier_P/0/1/0/all/0/1\">Pierre Alquier</a>",
          "description": "We tackle the problem of online optimization with a general, possibly\nunbounded, loss function. It is well known that when the loss is bounded, the\nexponentially weighted aggregation strategy (EWA) leads to a regret in\n$\\sqrt{T}$ after $T$ steps. In this paper, we study a generalized aggregation\nstrategy, where the weights no longer depend exponentially on the losses. Our\nstrategy is based on Follow The Regularized Leader (FTRL): we minimize the\nexpected losses plus a regularizer, that is here a $\\phi$-divergence. When the\nregularizer is the Kullback-Leibler divergence, we obtain EWA as a special\ncase. Using alternative divergences enables unbounded losses, at the cost of a\nworst regret bound in some cases.",
          "link": "http://arxiv.org/abs/2009.03017",
          "publishedOn": "2021-06-18T02:06:37.989Z",
          "wordCount": 587,
          "title": "Non-exponentially weighted aggregation: regret bounds for unbounded loss functions. (arXiv:2009.03017v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07476",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Matthias M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>",
          "description": "Deep graph neural networks (GNNs) have achieved excellent results on various\ntasks on increasingly large graph datasets with millions of nodes and edges.\nHowever, memory complexity has become a major obstacle when training deep GNNs\nfor practical applications due to the immense number of nodes, edges, and\nintermediate activations. To improve the scalability of GNNs, prior works\npropose smart graph sampling or partitioning strategies to train GNNs with a\nsmaller set of nodes or sub-graphs. In this work, we study reversible\nconnections, group convolutions, weight tying, and equilibrium models to\nadvance the memory and parameter efficiency of GNNs. We find that reversible\nconnections in combination with deep network architectures enable the training\nof overparameterized GNNs that significantly outperform existing methods on\nmultiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels each)\nand RevGNN-Wide (448 layers with 224 channels each) were both trained on a\nsingle commodity GPU and achieve an ROC-AUC of $87.74 \\pm 0.13$ and $88.24 \\pm\n0.15$ on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep\nis the deepest GNN in the literature by one order of magnitude. Please visit\nour project website https://www.deepgcns.org/arch/gnn1000 for more information.",
          "link": "http://arxiv.org/abs/2106.07476",
          "publishedOn": "2021-06-18T02:06:37.968Z",
          "wordCount": 670,
          "title": "Training Graph Neural Networks with 1000 Layers. (arXiv:2106.07476v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07263",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_Y/0/1/0/all/0/1\">Yongyi Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Coey_D/0/1/0/all/0/1\">Dominic Coey</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Konutgan_M/0/1/0/all/0/1\">Mikael Konutgan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_W/0/1/0/all/0/1\">Wenting Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schoener_C/0/1/0/all/0/1\">Chris Schoener</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goldman_M/0/1/0/all/0/1\">Matt Goldman</a>",
          "description": "We consider the problem of variance reduction in randomized controlled\ntrials, through the use of covariates correlated with the outcome but\nindependent of the treatment. We propose a machine learning regression-adjusted\ntreatment effect estimator, which we call MLRATE. MLRATE uses machine learning\npredictors of the outcome to reduce estimator variance. It employs\ncross-fitting to avoid overfitting biases, and we prove consistency and\nasymptotic normality under general conditions. MLRATE is robust to poor\npredictions from the machine learning step: if the predictions are uncorrelated\nwith the outcomes, the estimator performs asymptotically no worse than the\nstandard difference-in-means estimator, while if predictions are highly\ncorrelated with outcomes, the efficiency gains are large. In A/A tests, for a\nset of 48 outcome metrics commonly monitored in Facebook experiments the\nestimator has over 70% lower variance than the simple difference-in-means\nestimator, and about 19% lower variance than the common univariate procedure\nwhich adjusts only for pre-experiment values of the outcome.",
          "link": "http://arxiv.org/abs/2106.07263",
          "publishedOn": "2021-06-18T02:06:37.961Z",
          "wordCount": 611,
          "title": "Machine Learning for Variance Reduction in Online Experiments. (arXiv:2106.07263v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Songtao Lu</a>",
          "description": "Multiple views of data, both naturally acquired (e.g., image and audio) and\nartificially produced (e.g., via adding different noise to data samples), have\nproven useful in enhancing representation learning. Natural views are often\nhandled by multiview analysis tools, e.g., (deep) canonical correlation\nanalysis [(D)CCA], while the artificial ones are frequently used in\nself-supervised learning (SSL) paradigms, e.g., SimCLR and Barlow Twins. Both\ntypes of approaches often involve learning neural feature extractors such that\nthe embeddings of data exhibit high cross-view correlations. Although\nintuitive, the effectiveness of correlation-based neural embedding is only\nempirically validated. This work puts forth a theory-backed framework for\nunsupervised multiview learning. Our development starts with proposing a\nmultiview model, where each view is a nonlinear mixture of shared and private\ncomponents. Consequently, the learning problem boils down to shared/private\ncomponent identification and disentanglement. Under this model, latent\ncorrelation maximization is shown to guarantee the extraction of the shared\ncomponents across views (up to certain ambiguities). In addition, the private\ninformation in each view can be provably disentangled from the shared using\nproper regularization design. The method is tested on a series of tasks, e.g.,\ndownstream clustering, which all show promising performance. Our development\nalso provides a unifying perspective for understanding various DCCA and SSL\nschemes.",
          "link": "http://arxiv.org/abs/2106.07115",
          "publishedOn": "2021-06-18T02:06:37.952Z",
          "wordCount": 679,
          "title": "Latent Correlation-Based Multiview Learning and Self-Supervision: A Unifying Perspective. (arXiv:2106.07115v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhaozhi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zame_W/0/1/0/all/0/1\">William R. Zame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuren_L/0/1/0/all/0/1\">Lucas M. Fleuren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbers_P/0/1/0/all/0/1\">Paul Elbers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "Modeling a system's temporal behaviour in reaction to external stimuli is a\nfundamental problem in many areas. Pure Machine Learning (ML) approaches often\nfail in the small sample regime and cannot provide actionable insights beyond\npredictions. A promising modification has been to incorporate expert domain\nknowledge into ML models. The application we consider is predicting the\nprogression of disease under medications, where a plethora of domain knowledge\nis available from pharmacology. Pharmacological models describe the dynamics of\ncarefully-chosen medically meaningful variables in terms of systems of Ordinary\nDifferential Equations (ODEs). However, these models only describe a limited\ncollection of variables, and these variables are often not observable in\nclinical environments. To close this gap, we propose the latent hybridisation\nmodel (LHM) that integrates a system of expert-designed ODEs with\nmachine-learned Neural ODEs to fully describe the dynamics of the system and to\nlink the expert and latent variables to observable quantities. We evaluated LHM\non synthetic data as well as real-world intensive care data of COVID-19\npatients. LHM consistently outperforms previous works, especially when few\ntraining samples are available such as at the beginning of the pandemic.",
          "link": "http://arxiv.org/abs/2106.02875",
          "publishedOn": "2021-06-18T02:06:37.946Z",
          "wordCount": 702,
          "title": "Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease Progression. (arXiv:2106.02875v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simon_Gabriel_C/0/1/0/all/0/1\">Carl-Johann Simon-Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barp_A/0/1/0/all/0/1\">Alessandro Barp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "This paper characterizes the maximum mean discrepancies (MMD) that metrize\nthe weak convergence of probability measures for a wide class of kernels. More\nprecisely, we prove that, on a locally compact, non-compact, Hausdorff space,\nthe MMD of a bounded continuous Borel measurable kernel k, whose reproducing\nkernel Hilbert space (RKHS) functions vanish at infinity, metrizes the weak\nconvergence of probability measures if and only if k is continuous and\nintegrally strictly positive definite (i.s.p.d.) over all signed, finite,\nregular Borel measures. We also correct a prior result of Simon-Gabriel &\nSch\\\"olkopf (JMLR, 2018, Thm.12) by showing that there exist both bounded\ncontinuous i.s.p.d. kernels that do not metrize weak convergence and bounded\ncontinuous non-i.s.p.d. kernels that do metrize it.",
          "link": "http://arxiv.org/abs/2006.09268",
          "publishedOn": "2021-06-18T02:06:37.939Z",
          "wordCount": 621,
          "title": "Metrizing Weak Convergence with Maximum Mean Discrepancies. (arXiv:2006.09268v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stoidis_D/0/1/0/all/0/1\">Dimitrios Stoidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>",
          "description": "Besides its linguistic content, our speech is rich in biometric information\nthat can be inferred by classifiers. Learning privacy-preserving\nrepresentations for speech signals enables downstream tasks without sharing\nunnecessary, private information about an individual. In this paper, we show\nthat protecting gender information in speech is more effective than modelling\nspeaker-identity information only when generating a non-sensitive\nrepresentation of speech. Our method relies on reconstructing speech by\ndecoding linguistic content along with gender information using a variational\nautoencoder. Specifically, we exploit disentangled representation learning to\nencode information about different attributes into separate subspaces that can\nbe factorised independently. We present a novel way to encode gender\ninformation and disentangle two sensitive biometric identifiers, namely gender\nand identity, in a privacy-protecting setting. Experiments on the LibriSpeech\ndataset show that gender recognition and speaker verification can be reduced to\na random guess, protecting against classification-based attacks.",
          "link": "http://arxiv.org/abs/2104.11051",
          "publishedOn": "2021-06-18T02:06:37.932Z",
          "wordCount": 609,
          "title": "Protecting gender and identity with disentangled speech representations. (arXiv:2104.11051v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Siyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_A/0/1/0/all/0/1\">Andreas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Brian Williams</a>",
          "description": "In order to provide adaptive and user-friendly solutions to robotic\nmanipulation, it is important that the agent can learn to accomplish tasks even\nif they are only provided with very sparse instruction signals. To address the\nissues reinforcement learning algorithms face when task rewards are sparse,\nthis paper proposes an intrinsic motivation approach that can be easily\nintegrated into any standard reinforcement learning algorithm and can allow\nrobotic manipulators to learn useful manipulation skills with only sparse\nextrinsic rewards. Through integrating and balancing empowerment and curiosity,\nthis approach shows superior performance compared to other state-of-the-art\nintrinsic exploration approaches during extensive empirical testing.\nQualitative analysis also shows that when combined with diversity-driven\nintrinsic motivations, this approach can help manipulators learn a set of\ndiverse skills which could potentially be applied to other more complicated\nmanipulation tasks and accelerate their learning process.",
          "link": "http://arxiv.org/abs/2010.07986",
          "publishedOn": "2021-06-18T02:06:37.916Z",
          "wordCount": 620,
          "title": "An Empowerment-based Solution to Robotic Manipulation Tasks with Sparse Rewards. (arXiv:2010.07986v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1\">Ilias Diakonikolas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1\">Daniel M. Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pensia_A/0/1/0/all/0/1\">Ankit Pensia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pittas_T/0/1/0/all/0/1\">Thanasis Pittas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_A/0/1/0/all/0/1\">Alistair Stewart</a>",
          "description": "We study the problem of list-decodable linear regression, where an adversary\ncan corrupt a majority of the examples. Specifically, we are given a set $T$ of\nlabeled examples $(x, y) \\in \\mathbb{R}^d \\times \\mathbb{R}$ and a parameter\n$0< \\alpha <1/2$ such that an $\\alpha$-fraction of the points in $T$ are i.i.d.\nsamples from a linear regression model with Gaussian covariates, and the\nremaining $(1-\\alpha)$-fraction of the points are drawn from an arbitrary noise\ndistribution. The goal is to output a small list of hypothesis vectors such\nthat at least one of them is close to the target regression vector. Our main\nresult is a Statistical Query (SQ) lower bound of $d^{\\mathrm{poly}(1/\\alpha)}$\nfor this problem. Our SQ lower bound qualitatively matches the performance of\npreviously developed algorithms, providing evidence that current upper bounds\nfor this task are nearly best possible.",
          "link": "http://arxiv.org/abs/2106.09689",
          "publishedOn": "2021-06-18T02:06:37.910Z",
          "wordCount": 592,
          "title": "Statistical Query Lower Bounds for List-Decodable Linear Regression. (arXiv:2106.09689v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2103.07176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leporowski_B/0/1/0/all/0/1\">B&#x142;a&#x17c;ej Leporowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>",
          "description": "Despite the popularisation of machine learning models, more often than not,\nthey still operate as black boxes with no insight into what is happening inside\nthe model. There exist a few methods that allow to visualise and explain why a\nmodel has made a certain prediction. Those methods, however, allow\nvisualisation of the link between the input and output of the model without\npresenting how the model learns to represent the data used to train the model\nas whole. In this paper, a method that addresses that issue is proposed, with a\nfocus on visualising multi-dimensional time-series data. Experiments on a\nhigh-frequency stock market dataset show that the method provides fast and\ndiscernible visualisations. Large datasets can be visualised quickly and on one\nplot, which makes it easy for a user to compare the learned representations of\nthe data. The developed method successfully combines known techniques to\nprovide an insight into the inner workings of time-series classification\nmodels.",
          "link": "http://arxiv.org/abs/2103.07176",
          "publishedOn": "2021-06-18T02:06:37.905Z",
          "wordCount": 612,
          "title": "Visualising Deep Network's Time-Series Representations. (arXiv:2103.07176v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.01072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>",
          "description": "This paper introduces the task of factual error correction: performing edits\nto a claim so that the generated rewrite is better supported by evidence. This\nextends the well-studied task of fact verification by providing a mechanism to\ncorrect written texts that are refuted or only partially supported by evidence.\nWe demonstrate that it is feasible to train factual error correction systems\nfrom existing fact checking datasets which only contain labeled claims\naccompanied by evidence, but not the correction. We achieve this by employing a\ntwo-stage distant supervision approach that incorporates evidence into masked\nclaims when generating corrections. Our approach, based on the T5 transformer\nand using retrieved evidence, achieved better results than existing work which\nused a pointer copy network and gold evidence, producing accurate factual error\ncorrections for 5x more instances in human evaluation and a .125 increase in\nSARI score. The evaluation is conducted on a dataset of 65,000 instances based\non a recent fact verification shared task and we release it to enable further\nwork on the task.",
          "link": "http://arxiv.org/abs/2106.01072",
          "publishedOn": "2021-06-18T02:06:37.898Z",
          "wordCount": 640,
          "title": "Evidence-based Factual Error Correction. (arXiv:2106.01072v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1\">Dilip Arumugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>",
          "description": "State abstraction has been an essential tool for dramatically improving the\nsample efficiency of reinforcement-learning algorithms. Indeed, by exposing and\naccentuating various types of latent structure within the environment,\ndifferent classes of state abstraction have enabled improved theoretical\nguarantees and empirical performance. When dealing with state abstractions that\ncapture structure in the value function, however, a standard assumption is that\nthe true abstraction has been supplied or unrealistically computed a priori,\nleaving open the question of how to efficiently uncover such latent structure\nwhile jointly seeking out optimal behavior. Taking inspiration from the bandit\nliterature, we propose that an agent seeking out latent task structure must\nexplicitly represent and maintain its uncertainty over that structure as part\nof its overall uncertainty about the environment. We introduce a practical\nalgorithm for doing this using two posterior distributions over state\nabstractions and abstract-state values. In empirically validating our approach,\nwe find that substantial performance gains lie in the multi-task setting where\ntasks share a common, low-dimensional representation.",
          "link": "http://arxiv.org/abs/2010.02383",
          "publishedOn": "2021-06-18T02:06:37.881Z",
          "wordCount": 639,
          "title": "Randomized Value Functions via Posterior State-Abstraction Sampling. (arXiv:2010.02383v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03586",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zebin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aijun Zhang</a>",
          "description": "Hyperparameter optimization (HPO) plays a central role in the automated\nmachine learning (AutoML). It is a challenging task as the response surfaces of\nhyperparameters are generally unknown, hence essentially a global optimization\nproblem. This paper reformulates HPO as a computer experiment and proposes a\nnovel sequential uniform design (SeqUD) strategy with three-fold advantages: a)\nthe hyperparameter space is adaptively explored with evenly spread design\npoints, without the need of expensive meta-modeling and acquisition\noptimization; b) the batch-by-batch design points are sequentially generated\nwith parallel processing support; c) a new augmented uniform design algorithm\nis developed for the efficient real-time generation of follow-up design points.\nExtensive experiments are conducted on both global optimization tasks and HPO\napplications. The numerical results show that the proposed SeqUD strategy\noutperforms benchmark HPO methods, and it can be therefore a promising and\ncompetitive alternative to existing AutoML tools.",
          "link": "http://arxiv.org/abs/2009.03586",
          "publishedOn": "2021-06-18T02:06:37.874Z",
          "wordCount": 603,
          "title": "Hyperparameter Optimization via Sequential Uniform Designs. (arXiv:2009.03586v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02321",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Junhyeok Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1\">Seungu Han</a>",
          "description": "In this work, we introduce NU-Wave, the first neural audio upsampling model\nto produce waveforms of sampling rate 48kHz from coarse 16kHz or 24kHz inputs,\nwhile prior works could generate only up to 16kHz. NU-Wave is the first\ndiffusion probabilistic model for audio super-resolution which is engineered\nbased on neural vocoders. NU-Wave generates high-quality audio that achieves\nhigh performance in terms of signal-to-noise ratio (SNR), log-spectral distance\n(LSD), and accuracy of the ABX test. In all cases, NU-Wave outperforms the\nbaseline models despite the substantially smaller model capacity (3.0M\nparameters) than baselines (5.4-21%). The audio samples of our model are\navailable at https://mindslab-ai.github.io/nuwave, and the code will be made\navailable soon.",
          "link": "http://arxiv.org/abs/2104.02321",
          "publishedOn": "2021-06-18T02:06:37.868Z",
          "wordCount": 575,
          "title": "NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling. (arXiv:2104.02321v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.07382",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guan_Z/0/1/0/all/0/1\">Zoe Guan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Parmigiani_G/0/1/0/all/0/1\">Giovanni Parmigiani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Patil_P/0/1/0/all/0/1\">Prasad Patil</a>",
          "description": "A critical decision point when training predictors using multiple studies is\nwhether these studies should be combined or treated separately. We compare two\nmulti-study learning approaches in the presence of potential heterogeneity in\npredictor-outcome relationships across datasets. We consider 1) merging all of\nthe datasets and training a single learner, and 2) multi-study ensembling,\nwhich involves training a separate learner on each dataset and combining the\npredictions resulting from each learner. In a linear regression setting, we\nshow analytically and confirm via simulation that merging yields lower\nprediction error than ensembling when the predictor-outcome relationships are\nrelatively homogeneous across studies. However, as cross-study heterogeneity\nincreases, there exists a transition point beyond which ensembling outperforms\nmerging. We provide analytic expressions for the transition point in various\nscenarios, study asymptotic properties, and illustrate how transition point\ntheory can be used for deciding when studies should be combined with an\napplication from metabolomics.",
          "link": "http://arxiv.org/abs/1905.07382",
          "publishedOn": "2021-06-18T02:06:37.860Z",
          "wordCount": 614,
          "title": "Merging versus Ensembling in Multi-Study Prediction: Theoretical Insight from Random Effects. (arXiv:1905.07382v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1811.09003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Fenglei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dayang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hengtao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qikui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hengyong Yu</a>",
          "description": "Over recent years, deep learning has become the mainstream data-driven\napproach to solve many important real-world problems. In the successful network\narchitectures, shortcut connections are well established to take the outputs of\nearlier layers as additional inputs to later layers, which have produced\nexcellent results. Despite the extraordinary effectiveness of shortcuts, there\nremain important questions on the underlying mechanism and associated\nfunctionalities. For example, why are shortcuts powerful? Why shortcuts\ngeneralize well? To address these questions, we investigate the representation\nand generalization ability of a sparse shortcut topology. Specifically, we\nfirst demonstrate that this topology can empower a one-neuron-wide deep network\nto approximate any univariate continuous function. Then, we present a novel\nwidth-bounded universal approximator in contrast to depth-bounded universal\napproximators, and also extend the approximation result to a family of networks\nsuch that in the view of approximation ability, these networks are equally\ncompetent. Furthermore, we use the generalization bound theory to show that the\ninvestigated shortcut topology enjoys an excellent generalizability. Finally,\nwe corroborate our theoretical analyses with experiments on some well-known\nbenchmarks.",
          "link": "http://arxiv.org/abs/1811.09003",
          "publishedOn": "2021-06-18T02:06:37.854Z",
          "wordCount": 653,
          "title": "On a Sparse Shortcut Topology of Artificial Neural Networks. (arXiv:1811.09003v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.07557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Edouard Pauwels</a> (IRIT-ADRIA)",
          "description": "Minibatch decomposition methods for empirical risk minimization are commonly\nanalysed in a stochastic approximation setting, also known as sampling with\nreplacement. On the other hands modern implementations of such techniques are\nincremental: they rely on sampling without replacement, for which available\nanalysis are much scarcer. We provide convergence guaranties for the latter\nvariant by analysing a versatile incremental gradient scheme. For this scheme,\nwe consider constant, decreasing or adaptive step sizes. In the smooth setting\nwe obtain explicit complexity estimates in terms of epoch counter. In the\nnonsmooth setting we prove that the sequence is attracted by solutions of\noptimality conditions of the problem.",
          "link": "http://arxiv.org/abs/2007.07557",
          "publishedOn": "2021-06-18T02:06:37.849Z",
          "wordCount": 572,
          "title": "Incremental Without Replacement Sampling in Nonconvex Optimization. (arXiv:2007.07557v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1\">Nathalie Baracaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1\">Ali Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_J/0/1/0/all/0/1\">James Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludwig_H/0/1/0/all/0/1\">Heiko Ludwig</a>",
          "description": "Federated learning (FL) has been proposed to allow collaborative training of\nmachine learning (ML) models among multiple parties where each party can keep\nits data private. In this paradigm, only model updates, such as model weights\nor gradients, are shared. Many existing approaches have focused on horizontal\nFL, where each party has the entire feature set and labels in the training data\nset. However, many real scenarios follow a vertically-partitioned FL setup,\nwhere a complete feature set is formed only when all the datasets from the\nparties are combined, and the labels are only available to a single party.\nPrivacy-preserving vertical FL is challenging because complete sets of labels\nand features are not owned by one entity. Existing approaches for vertical FL\nrequire multiple peer-to-peer communications among parties, leading to lengthy\ntraining times, and are restricted to (approximated) linear models and just two\nparties. To close this gap, we propose FedV, a framework for secure gradient\ncomputation in vertical settings for several widely used ML models such as\nlinear models, logistic regression, and support vector machines. FedV removes\nthe need for peer-to-peer communication among parties by using functional\nencryption schemes; this allows FedV to achieve faster training times. It also\nworks for larger and changing sets of parties. We empirically demonstrate the\napplicability for multiple types of ML models and show a reduction of 10%-70%\nof training time and 80% to 90% in data transfer with respect to the\nstate-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2103.03918",
          "publishedOn": "2021-06-18T02:06:37.840Z",
          "wordCount": 721,
          "title": "FedV: Privacy-Preserving Federated Learning over Vertically Partitioned Data. (arXiv:2103.03918v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuxuan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1\">Zheng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhenglun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>",
          "description": "With the increasing demand to efficiently deploy DNNs on mobile edge devices,\nit becomes much more important to reduce unnecessary computation and increase\nthe execution speed. Prior methods towards this goal, including model\ncompression and network architecture search (NAS), are largely performed\nindependently and do not fully consider compiler-level optimizations which is a\nmust-do for mobile acceleration. In this work, we first propose (i) a general\ncategory of fine-grained structured pruning applicable to various DNN layers,\nand (ii) a comprehensive, compiler automatic code generation framework\nsupporting different DNNs and different pruning schemes, which bridge the gap\nof model compression and NAS. We further propose NPAS, a compiler-aware unified\nnetwork pruning, and architecture search. To deal with large search space, we\npropose a meta-modeling procedure based on reinforcement learning with fast\nevaluation and Bayesian optimization, ensuring the total number of training\nepochs comparable with representative NAS frameworks. Our framework achieves\n6.7ms, 5.9ms, 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3\nlevel), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an\noff-the-shelf mobile phone, consistently outperforming prior work.",
          "link": "http://arxiv.org/abs/2012.00596",
          "publishedOn": "2021-06-18T02:06:37.823Z",
          "wordCount": 720,
          "title": "NPAS: A Compiler-aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration. (arXiv:2012.00596v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raghuram_J/0/1/0/all/0/1\">Jayaram Raghuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Varun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Suman Banerjee</a>",
          "description": "Detecting anomalous inputs, such as adversarial and out-of-distribution (OOD)\ninputs, is critical for classifiers (including deep neural networks or DNNs)\ndeployed in real-world applications. While prior works have proposed various\nmethods to detect such anomalous samples using information from the internal\nlayer representations of a DNN, there is a lack of consensus on a principled\napproach for the different components of such a detection method. As a result,\noften heuristic and one-off methods are applied for different aspects of this\nproblem. We propose an unsupervised anomaly detection framework based on the\ninternal DNN layer representations in the form of a meta-algorithm with\nconfigurable components. We proceed to propose specific instantiations for each\ncomponent of the meta-algorithm based on ideas grounded in statistical testing\nand anomaly detection. We evaluate the proposed methods on well-known image\nclassification datasets with strong adversarial attacks and OOD inputs,\nincluding an adaptive attack that uses the internal layer representations of\nthe DNN (often not considered in prior work). Comparisons with five\nrecently-proposed competing detection methods demonstrates the effectiveness of\nour method in detecting adversarial and OOD inputs.",
          "link": "http://arxiv.org/abs/2007.15147",
          "publishedOn": "2021-06-18T02:06:37.815Z",
          "wordCount": 651,
          "title": "A General Framework For Detecting Anomalous Inputs to DNN Classifiers. (arXiv:2007.15147v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madumal_P/0/1/0/all/0/1\">Prashan Madumal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1\">Tim Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1\">Krista A. Ehinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Benjamin I. P. Rubinstein</a>",
          "description": "Convolutional neural network (CNN) models for computer vision are powerful\nbut lack explainability in their most basic form. This deficiency remains a key\nchallenge when applying CNNs in important domains. Recent work on explanations\nthrough feature importance of approximate linear models has moved from\ninput-level features (pixels or segments) to features from mid-layer feature\nmaps in the form of concept activation vectors (CAVs). CAVs contain\nconcept-level information and could be learned via clustering. In this work, we\nrethink the ACE algorithm of Ghorbani et~al., proposing an alternative\ninvertible concept-based explanation (ICE) framework to overcome its\nshortcomings. Based on the requirements of fidelity (approximate models to\ntarget models) and interpretability (being meaningful to people), we design\nmeasurements and evaluate a range of matrix factorization methods with our\nframework. We find that non-negative concept activation vectors (NCAVs) from\nnon-negative matrix factorization provide superior performance in\ninterpretability and fidelity based on computational and human subject\nexperiments. Our framework provides both local and global concept-level\nexplanations for pre-trained CNN models.",
          "link": "http://arxiv.org/abs/2006.15417",
          "publishedOn": "2021-06-18T02:06:37.807Z",
          "wordCount": 656,
          "title": "Invertible Concept-based Explanations for CNN Models with Non-negative Concept Activation Vectors. (arXiv:2006.15417v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1\">Lewis Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amersfoort_J/0/1/0/all/0/1\">Joost van Amersfoort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haiwen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>",
          "description": "ResNets constrained to be bi-Lipschitz, that is, approximately distance\npreserving, have been a crucial component of recently proposed techniques for\ndeterministic uncertainty quantification in neural models. We show that\ntheoretical justifications for recent regularisation schemes trying to enforce\nsuch a constraint suffer from a crucial flaw -- the theoretical link between\nthe regularisation scheme used and bi-Lipschitzness is only valid under\nconditions which do not hold in practice, rendering existing theory of limited\nuse, despite the strong empirical performance of these models. We provide a\ntheoretical explanation for the effectiveness of these regularisation schemes\nusing a frequency analysis perspective, showing that under mild conditions\nthese schemes will enforce a lower Lipschitz bound on the low-frequency\nprojection of images. We then provide empirical evidence supporting our\ntheoretical claims, and perform further experiments which demonstrate that our\nbroader conclusions appear to hold when some of the mathematical assumptions of\nour proof are relaxed, corresponding to the setup used in prior work. In\naddition, we present a simple constructive algorithm to search for counter\nexamples to the distance preservation condition, and discuss possible\nimplications of our theory for future model design.",
          "link": "http://arxiv.org/abs/2106.02469",
          "publishedOn": "2021-06-18T02:06:37.791Z",
          "wordCount": 665,
          "title": "Can convolutional ResNets approximately preserve input distances? A frequency analysis perspective. (arXiv:2106.02469v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tavares_A/0/1/0/all/0/1\">Anderson R. Tavares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avelar_P/0/1/0/all/0/1\">Pedro Avelar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flach_J/0/1/0/all/0/1\">Jo&#xe3;o M. Flach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolau_M/0/1/0/all/0/1\">Marcio Nicolau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1\">Luis C. Lamb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardi_M/0/1/0/all/0/1\">Moshe Vardi</a>",
          "description": "Computational learning theory states that many classes of boolean formulas\nare learnable in polynomial time. This paper addresses the understudied subject\nof how, in practice, such formulas can be learned by deep neural networks.\nSpecifically, we analyse boolean formulas associated with the decision version\nof combinatorial optimisation problems, model sampling benchmarks, and random\n3-CNFs with varying degrees of constrainedness. Our extensive experiments\nindicate that: (i) regardless of the combinatorial optimisation problem,\nrelatively small and shallow neural networks are very good approximators of the\nassociated formulas; (ii) smaller formulas seem harder to learn, possibly due\nto the fewer positive (satisfying) examples available; and (iii) interestingly,\nunderconstrained 3-CNF formulas are more challenging to learn than\noverconstrained ones. Source code and relevant datasets are publicly available\n(https://github.com/machine-reasoning-ufrgs/mlbf).",
          "link": "http://arxiv.org/abs/2009.05908",
          "publishedOn": "2021-06-18T02:06:37.784Z",
          "wordCount": 585,
          "title": "Understanding Boolean Function Learnability on Deep Neural Networks. (arXiv:2009.05908v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1\">Chengxuan Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1\">Guolin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "The Transformer architecture has become a dominant choice in many domains,\nsuch as natural language processing and computer vision. Yet, it has not\nachieved competitive performance on popular leaderboards of graph-level\nprediction compared to mainstream GNN variants. Therefore, it remains a mystery\nhow Transformers could perform well for graph representation learning. In this\npaper, we solve this mystery by presenting Graphormer, which is built upon the\nstandard Transformer architecture, and could attain excellent results on a\nbroad range of graph representation learning tasks, especially on the recent\nOGB Large-Scale Challenge. Our key insight to utilizing Transformer in the\ngraph is the necessity of effectively encoding the structural information of a\ngraph into the model. To this end, we propose several simple yet effective\nstructural encoding methods to help Graphormer better model graph-structured\ndata. Besides, we mathematically characterize the expressive power of\nGraphormer and exhibit that with our ways of encoding the structural\ninformation of graphs, many popular GNN variants could be covered as the\nspecial cases of Graphormer.",
          "link": "http://arxiv.org/abs/2106.05234",
          "publishedOn": "2021-06-18T02:06:37.777Z",
          "wordCount": 635,
          "title": "Do Transformers Really Perform Bad for Graph Representation?. (arXiv:2106.05234v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Girgis_R/0/1/0/all/0/1\">Roger Girgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1\">Florian Golemo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codevilla_F/0/1/0/all/0/1\">Felipe Codevilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jim Aldon D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_M/0/1/0/all/0/1\">Martin Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1\">Samira Ebrahimi Kahou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "Robust multi-agent trajectory prediction is essential for the safe control of\nrobots and vehicles that interact with humans. Many existing methods treat\nsocial and temporal information separately and therefore fall short of\nmodelling the joint future trajectories of all agents in a socially consistent\nway. To address this, we propose a new class of Latent Variable Sequential Set\nTransformers which autoregressively model multi-agent trajectories. We refer to\nthese architectures as \"AutoBots\". AutoBots model the contents of sets (e.g.\nrepresenting the properties of agents in a scene) over time and employ\nmulti-head self-attention blocks over these sequences of sets to encode the\nsociotemporal relationships between the different actors of a scene. This\nproduces either the trajectory of one ego-agent or a distribution over the\nfuture trajectories for all agents under consideration. Our approach works for\ngeneral sequences of sets and we provide illustrative experiments modelling the\nsequential structure of the multiple strokes that make up symbols in the\nOmniglot data. For the single-agent prediction case, we validate our model on\nthe NuScenes motion prediction task and achieve competitive results on the\nglobal leaderboard. In the multi-agent forecasting setting, we validate our\nmodel on TrajNet. We find that our method outperforms physical extrapolation\nand recurrent network baselines and generates scene-consistent trajectories.",
          "link": "http://arxiv.org/abs/2104.00563",
          "publishedOn": "2021-06-18T02:06:37.771Z",
          "wordCount": 685,
          "title": "Autobots: Latent Variable Sequential Set Transformers. (arXiv:2104.00563v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10360",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1\">Doudou Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1\">Tianxi Cai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lu_J/0/1/0/all/0/1\">Junwei Lu</a>",
          "description": "Matrix completion has attracted attention in many fields, including\nstatistics, applied mathematics, and electrical engineering. Most of the works\nfocus on the independent sampling models under which the observed entries are\nsampled independently. Motivated by applications in the integration of multiple\nElectronic Health Record (EHR) datasets, we propose the method {\\bf B}lock-wise\nmissing {\\bf E}mbedding {\\bf L}earning {\\bf T}ransformer (BELT) to treat\nrow-wise/column-wise missingness. Specifically, BELT can recover block-wise\nmissing matrices efficiently when every pair of matrices has an overlap. Our\nidea is to exploit the orthogonal Procrustes problem to align the eigenspace of\nthe two sub-matrices using their overlap, then complete the missing blocks by\nthe inner product of the two low-rank components. Besides, we prove the\nstatistical rate for the eigenspace of the underlying matrix, which is\ncomparable to the rate under the independently missing assumption. Simulation\nstudies show that the method performs well under a variety of configurations.\nIn the real data analysis, the method is applied to two tasks: (i) the\nintegrating of several point-wise mutual information matrices built by English\nEHR and Chinese medical text data, and (ii) the machine translation between\nEnglish and Chinese medical concepts. Our method shows an advantage over\nexisting methods.",
          "link": "http://arxiv.org/abs/2105.10360",
          "publishedOn": "2021-06-18T02:06:37.764Z",
          "wordCount": 657,
          "title": "BELT: Block-wise Missing Embedding Learning Transformer. (arXiv:2105.10360v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02081",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1\">Francisco Vargas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1\">Pierre Thodoroff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1\">Neil D. Lawrence</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1\">Austen Lamacraft</a>",
          "description": "The Schr\\\"odinger bridge problem (SBP) finds the most likely stochastic\nevolution between two probability distributions given a prior stochastic\nevolution. As well as applications in the natural sciences, problems of this\nkind have important applications in machine learning such as dataset alignment\nand hypothesis testing. Whilst the theory behind this problem is relatively\nmature, scalable numerical recipes to estimate the Schr\\\"odinger bridge remain\nan active area of research. We prove an equivalence between the SBP and maximum\nlikelihood estimation enabling direct application of successful machine\nlearning techniques. We propose a numerical procedure to estimate SBPs using\nGaussian process and demonstrate the practical usage of our approach in\nnumerical simulations and experiments.",
          "link": "http://arxiv.org/abs/2106.02081",
          "publishedOn": "2021-06-18T02:06:37.757Z",
          "wordCount": 567,
          "title": "Solving Schr\\\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruichu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Keli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuozhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenjie Zhang</a>",
          "description": "Domain adaptation on time series data is an important but challenging task.\nMost of the existing works in this area are based on the learning of the\ndomain-invariant representation of the data with the help of restrictions like\nMMD. However, such extraction of the domain-invariant representation is a\nnon-trivial task for time series data, due to the complex dependence among the\ntimestamps. In detail, in the fully dependent time series, a small change of\nthe time lags or the offsets may lead to difficulty in the domain invariant\nextraction. Fortunately, the stability of the causality inspired us to explore\nthe domain invariant structure of the data. To reduce the difficulty in the\ndiscovery of causal structure, we relax it to the sparse associative structure\nand propose a novel sparse associative structure alignment model for domain\nadaptation. First, we generate the segment set to exclude the obstacle of\noffsets. Second, the intra-variables and inter-variables sparse attention\nmechanisms are devised to extract associative structure time-series data with\nconsidering time lags. Finally, the associative structure alignment is used to\nguide the transfer of knowledge from the source domain to the target one.\nExperimental studies not only verify the good performance of our methods on\nthree real-world datasets but also provide some insightful discoveries on the\ntransferred knowledge.",
          "link": "http://arxiv.org/abs/2012.11797",
          "publishedOn": "2021-06-18T02:06:37.747Z",
          "wordCount": 687,
          "title": "Time Series Domain Adaptation via Sparse Associative Structure Alignment. (arXiv:2012.11797v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zaiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1\">Siva Theja Maguluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>",
          "description": "This paper develops an unified framework to study finite-sample convergence\nguarantees of a large class of value-based asynchronous reinforcement learning\n(RL) algorithms. We do this by first reformulating the RL algorithms as\n\\textit{Markovian Stochastic Approximation} (SA) algorithms to solve\nfixed-point equations. We then develop a Lyapunov analysis and derive\nmean-square error bounds on the convergence of the Markovian SA. Based on this\nresult, we establish finite-sample mean-square convergence bounds for\nasynchronous RL algorithms such as $Q$-learning, $n$-step TD, TD$(\\lambda)$,\nand off-policy TD algorithms including V-trace. As a by-product, by analyzing\nthe convergence bounds of $n$-step TD and TD$(\\lambda)$, we provide theoretical\ninsights into the bias-variance trade-off, i.e., efficiency of bootstrapping in\nRL. This was first posed as an open problem in (Sutton, 1999).",
          "link": "http://arxiv.org/abs/2102.01567",
          "publishedOn": "2021-06-18T02:06:37.739Z",
          "wordCount": 612,
          "title": "A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants. (arXiv:2102.01567v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03416",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ng_E/0/1/0/all/0/1\">Edwin G. Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>",
          "description": "We combine recent advancements in end-to-end speech recognition to\nnon-autoregressive automatic speech recognition. We push the limits of\nnon-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,\nFisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC\non giant Conformer neural network architectures with SpecAugment and wav2vec2\npre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,\n5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without\na language model.",
          "link": "http://arxiv.org/abs/2104.03416",
          "publishedOn": "2021-06-18T02:06:37.719Z",
          "wordCount": 553,
          "title": "Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09620",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Halva_H/0/1/0/all/0/1\">Hermanni H&#xe4;lv&#xe4;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Corff_S/0/1/0/all/0/1\">Sylvain Le Corff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lehericy_L/0/1/0/all/0/1\">Luc Leh&#xe9;ricy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+So_J/0/1/0/all/0/1\">Jonathan So</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongjie Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gassiat_E/0/1/0/all/0/1\">Elisabeth Gassiat</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hyvarinen_A/0/1/0/all/0/1\">Aapo Hyvarinen</a>",
          "description": "We introduce a new general identifiable framework for principled\ndisentanglement referred to as Structured Nonlinear Independent Component\nAnalysis (SNICA). Our contribution is to extend the identifiability theory of\ndeep generative models for a very broad class of structured models. While\nprevious works have shown identifiability for specific classes of time-series\nmodels, our theorems extend this to more general temporal structures as well as\nto models with more complex structures such as spatial dependencies. In\nparticular, we establish the major result that identifiability for this\nframework holds even in the presence of noise of unknown distribution. The\nSNICA setting therefore subsumes all the existing nonlinear ICA models for\ntime-series and also allows for new much richer identifiable models. Finally,\nas an example of our framework's flexibility, we introduce the first nonlinear\nICA model for time-series that combines the following very useful properties:\nit accounts for both nonstationarity and autocorrelation in a fully\nunsupervised setting; performs dimensionality reduction; models hidden states;\nand enables principled estimation and inference by variational\nmaximum-likelihood.",
          "link": "http://arxiv.org/abs/2106.09620",
          "publishedOn": "2021-06-18T02:06:37.703Z",
          "wordCount": 614,
          "title": "Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA. (arXiv:2106.09620v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.12557",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1\">Avi Schwarzschild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arjun Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John P Dickerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Data poisoning and backdoor attacks manipulate training data in order to\ncause models to fail during inference. A recent survey of industry\npractitioners found that data poisoning is the number one concern among threats\nranging from model stealing to adversarial attacks. However, it remains unclear\nexactly how dangerous poisoning methods are and which ones are more effective\nconsidering that these methods, even ones with identical objectives, have not\nbeen tested in consistent or realistic settings. We observe that data poisoning\nand backdoor attacks are highly sensitive to variations in the testing setup.\nMoreover, we find that existing methods may not generalize to realistic\nsettings. While these existing works serve as valuable prototypes for data\npoisoning, we apply rigorous tests to determine the extent to which we should\nfear them. In order to promote fair comparison in future work, we develop\nstandardized benchmarks for data poisoning and backdoor attacks.",
          "link": "http://arxiv.org/abs/2006.12557",
          "publishedOn": "2021-06-18T02:06:37.685Z",
          "wordCount": 660,
          "title": "Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks. (arXiv:2006.12557v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Michael Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Seyed Ali Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansch_R/0/1/0/all/0/1\">Ronny H&#xe4;nsch</a>",
          "description": "Annotated datasets have become one of the most crucial preconditions for the\ndevelopment and evaluation of machine learning-based methods designed for the\nautomated interpretation of remote sensing data. In this paper, we review the\nhistoric development of such datasets, discuss their features based on a few\nselected examples, and address open issues for future developments.",
          "link": "http://arxiv.org/abs/2105.11726",
          "publishedOn": "2021-06-18T02:06:37.679Z",
          "wordCount": 550,
          "title": "There is no data like more data -- current status of machine learning datasets in remote sensing. (arXiv:2105.11726v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.04990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tengyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan Jiang</a>",
          "description": "We make progress in a long-standing problem of batch reinforcement learning\n(RL): learning $Q^\\star$ from an exploratory and polynomial-sized dataset,\nusing a realizable and otherwise arbitrary function class. In fact, all\nexisting algorithms demand function-approximation assumptions stronger than\nrealizability, and the mounting negative evidence has led to a conjecture that\nsample-efficient learning is impossible in this setting (Chen and Jiang, 2019).\nOur algorithm, BVFT, breaks the hardness conjecture (albeit under a stronger\nnotion of exploratory data) via a tournament procedure that reduces the\nlearning problem to pairwise comparison, and solves the latter with the help of\na state-action partition constructed from the compared functions. We also\ndiscuss how BVFT can be applied to model selection among other extensions and\nopen problems.",
          "link": "http://arxiv.org/abs/2008.04990",
          "publishedOn": "2021-06-18T02:06:37.659Z",
          "wordCount": 590,
          "title": "Batch Value-function Approximation with Only Realizability. (arXiv:2008.04990v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00668",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Stites_S/0/1/0/all/0/1\">Sam Stites</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zimmermann_H/0/1/0/all/0/1\">Heiko Zimmermann</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sennesh_E/0/1/0/all/0/1\">Eli Sennesh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>",
          "description": "We develop operators for construction of proposals in probabilistic programs,\nwhich we refer to as inference combinators. Inference combinators define a\ngrammar over importance samplers that compose primitive operations such as\napplication of a transition kernel and importance resampling. Proposals in\nthese samplers can be parameterized using neural networks, which in turn can be\ntrained by optimizing variational objectives. The result is a framework for\nuser-programmable variational methods that are correct by construction and can\nbe tailored to specific models. We demonstrate the flexibility of this\nframework by implementing advanced variational methods based on amortized Gibbs\nsampling and annealing.",
          "link": "http://arxiv.org/abs/2103.00668",
          "publishedOn": "2021-06-18T02:06:37.652Z",
          "wordCount": 572,
          "title": "Learning Proposals for Probabilistic Programs with Inference Combinators. (arXiv:2103.00668v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fasfous_N/0/1/0/all/0/1\">Nael Fasfous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemparala_M/0/1/0/all/0/1\">Manoj-Rohit Vemparala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frickenstein_A/0/1/0/all/0/1\">Alexander Frickenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frickenstein_L/0/1/0/all/0/1\">Lukas Frickenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stechele_W/0/1/0/all/0/1\">Walter Stechele</a>",
          "description": "Face masks have long been used in many areas of everyday life to protect\nagainst the inhalation of hazardous fumes and particles. They also offer an\neffective solution in healthcare for bi-directional protection against\nair-borne diseases. Wearing and positioning the mask correctly is essential for\nits function. Convolutional neural networks (CNNs) offer an excellent solution\nfor face recognition and classification of correct mask wearing and\npositioning. In the context of the ongoing COVID-19 pandemic, such algorithms\ncan be used at entrances to corporate buildings, airports, shopping areas, and\nother indoor locations, to mitigate the spread of the virus. These application\nscenarios impose major challenges to the underlying compute platform. The\ninference hardware must be cheap, small and energy efficient, while providing\nsufficient memory and compute power to execute accurate CNNs at a reasonably\nlow latency. To maintain data privacy of the public, all processing must remain\non the edge-device, without any communication with cloud servers. To address\nthese challenges, we present a low-power binary neural network classifier for\ncorrect facial-mask wear and positioning. The classification task is\nimplemented on an embedded FPGA, performing high-throughput binary operations.\nClassification can take place at up to ~6400 frames-per-second, easily enabling\nmulti-camera, speed-gate settings or statistics collection in crowd settings.\nWhen deployed on a single entrance or gate, the idle power consumption is\nreduced to 1.6W, improving the battery-life of the device. We achieve an\naccuracy of up to 98% for four wearing positions of the MaskedFace-Net dataset.\nTo maintain equivalent classification accuracy for all face structures,\nskin-tones, hair types, and mask types, the algorithms are tested for their\nability to generalize the relevant features over all subjects using the\nGrad-CAM approach.",
          "link": "http://arxiv.org/abs/2102.03456",
          "publishedOn": "2021-06-18T02:06:37.646Z",
          "wordCount": 801,
          "title": "BinaryCoP: Binary Neural Network-based COVID-19 Face-Mask Wear and Positioning Predictor on Edge Devices. (arXiv:2102.03456v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.08085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gribonval_R/0/1/0/all/0/1\">R&#xe9;mi Gribonval</a> (PANAMA, DANTE), <a href=\"http://arxiv.org/find/cs/1/au:+Blanchard_G/0/1/0/all/0/1\">Gilles Blanchard</a> (LMO), <a href=\"http://arxiv.org/find/cs/1/au:+Keriven_N/0/1/0/all/0/1\">Nicolas Keriven</a> (GIPSA-GAIA), <a href=\"http://arxiv.org/find/cs/1/au:+Traonmilin_Y/0/1/0/all/0/1\">Yann Traonmilin</a> (IMB)",
          "description": "We provide statistical learning guarantees for two unsupervised learning\ntasks in the context of compressive statistical learning, a general framework\nfor resource-efficient large-scale learning that we introduced in a companion\npaper. The principle of compressive statistical learning is to compress a\ntraining collection, in one pass, into a low-dimensional sketch (a vector of\nrandom empirical generalized moments) that captures the information relevant to\nthe considered learning task. We explicit random feature functions which\nempirical averages preserve the needed information for compressive clustering\nand compressive Gaussian mixture modeling with fixed known variance, and\nestablish sufficient sketch sizes given the problem dimensions.",
          "link": "http://arxiv.org/abs/2004.08085",
          "publishedOn": "2021-06-18T02:06:37.639Z",
          "wordCount": 620,
          "title": "Statistical Learning Guarantees for Compressive Clustering and Compressive Mixture Modeling. (arXiv:2004.08085v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1\">Elijah Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorieul_T/0/1/0/all/0/1\">Titouan Lorieul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1\">Dan Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>",
          "description": "Predicting all applicable labels for a given image is known as multi-label\nclassification. Compared to the standard multi-class case (where each image has\nonly one label), it is considerably more challenging to annotate training data\nfor multi-label classification. When the number of potential labels is large,\nhuman annotators find it difficult to mention all applicable labels for each\ntraining image. Furthermore, in some settings detection is intrinsically\ndifficult e.g. finding small object instances in high resolution images. As a\nresult, multi-label training data is often plagued by false negatives. We\nconsider the hardest version of this problem, where annotators provide only one\nrelevant label for each image. As a result, training sets will have only one\npositive label per image and no confirmed negatives. We explore this special\ncase of learning from missing labels across four different multi-label image\nclassification datasets for both linear classifiers and end-to-end fine-tuned\ndeep networks. We extend existing multi-label losses to this setting and\npropose novel variants that constrain the number of expected positive labels\nduring training. Surprisingly, we show that in some cases it is possible to\napproach the performance of fully labeled classifiers despite training with\nsignificantly fewer confirmed labels.",
          "link": "http://arxiv.org/abs/2106.09708",
          "publishedOn": "2021-06-18T02:06:37.631Z",
          "wordCount": 641,
          "title": "Multi-Label Learning from Single Positive Labels. (arXiv:2106.09708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.03482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Beibei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+wang_R/0/1/0/all/0/1\">Rui wang</a>",
          "description": "Recent studies have revealed the vulnerability of graph convolutional\nnetworks (GCNs) to edge-perturbing attacks, such as maliciously inserting or\ndeleting graph edges. However, a theoretical proof of such vulnerability\nremains a big challenge, and effective defense schemes are still open issues.\nIn this paper, we first generalize the formulation of edge-perturbing attacks\nand strictly prove the vulnerability of GCNs to such attacks in node\nclassification tasks. Following this, an anonymous graph convolutional network,\nnamed AN-GCN, is proposed to counter against edge-perturbing attacks.\nSpecifically, we present a node localization theorem to demonstrate how the GCN\nlocates nodes during its training phase. In addition, we design a staggered\nGaussian noise based node position generator, and devise a spectral graph\nconvolution based discriminator in detecting the generated node positions.\nFurther, we give the optimization of the above generator and discriminator.\nAN-GCN can classify nodes without taking their position as input. It is\ndemonstrated that the AN-GCN is secure against edge-perturbing attacks in node\nclassification tasks, as AN-GCN classifies nodes without the edge information\nand thus makes it impossible for attackers to perturb edges anymore. Extensive\nevaluations demonstrated the effectiveness of the general edge-perturbing\nattack model in manipulating the classification results of the target nodes.\nMore importantly, the proposed AN-GCN can achieve 82.7% in node classification\naccuracy without the edge-reading permission, which outperforms the\nstate-of-the-art GCN.",
          "link": "http://arxiv.org/abs/2005.03482",
          "publishedOn": "2021-06-18T02:06:37.613Z",
          "wordCount": 730,
          "title": "AN-GCN: An Anonymous Graph Convolutional Network Defense Against Edge-Perturbing Attack. (arXiv:2005.03482v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minchao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norrish_M/0/1/0/all/0/1\">Michael Norrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1\">Christian Walder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dezfouli_A/0/1/0/all/0/1\">Amir Dezfouli</a>",
          "description": "We propose a novel approach to interactive theorem-proving (ITP) using deep\nreinforcement learning. The proposed framework is able to learn proof search\nstrategies as well as tactic and arguments prediction in an end-to-end manner.\nWe formulate the process of ITP as a Markov decision process (MDP) in which\neach state represents a set of potential derivation paths. This structure\nallows us to introduce a novel backtracking mechanism which enables the agent\nto efficiently discard (predicted) dead-end derivations and restart from\npromising alternatives. We implement the framework in the HOL4 theorem prover.\nExperimental results show that the framework outperforms existing automated\ntheorem provers (i.e., hammers) available in HOL4 when evaluated on unseen\nproblems. We further elaborate the role of key components of the framework\nusing ablation studies.",
          "link": "http://arxiv.org/abs/2102.09756",
          "publishedOn": "2021-06-18T02:06:37.606Z",
          "wordCount": 606,
          "title": "TacticZero: Learning to Prove Theorems from Scratch with Deep Reinforcement Learning. (arXiv:2102.09756v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09663",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1\">Zhize Li</a>",
          "description": "In this note, we first recall the nonconvex problem setting and introduce the\noptimal PAGE algorithm (Li et al., ICML'21). Then we provide a simple and clean\nconvergence analysis of PAGE for achieving optimal convergence rates. Moreover,\nPAGE and its analysis can be easily adopted and generalized to other works. We\nhope that this note provides the insights and is helpful for future works.",
          "link": "http://arxiv.org/abs/2106.09663",
          "publishedOn": "2021-06-18T02:06:37.600Z",
          "wordCount": 519,
          "title": "A Short Note of PAGE: Optimal Convergence Rates for Nonconvex Optimization. (arXiv:2106.09663v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1\">Nian Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1\">Jose Blanchet</a>",
          "description": "Policy learning using historical observational data is an important problem\nthat has found widespread applications. Examples include selecting offers,\nprices, advertisements to send to customers, as well as selecting which\nmedication to prescribe to a patient. However, existing literature rests on the\ncrucial assumption that the future environment where the learned policy will be\ndeployed is the same as the past environment that has generated the data--an\nassumption that is often false or too coarse an approximation. In this paper,\nwe lift this assumption and aim to learn a distributional robust policy with\nincomplete (bandit) observational data. We propose a novel learning algorithm\nthat is able to learn a robust policy to adversarial perturbations and unknown\ncovariate shifts. We first present a policy evaluation procedure in the\nambiguous environment and then give a performance guarantee based on the theory\nof uniform convergence. Additionally, we also give a heuristic algorithm to\nsolve the distributional robust policy learning problems efficiently. Finally,\nwe demonstrate the robustness of our methods in the synthetic and real-world\ndatasets.",
          "link": "http://arxiv.org/abs/2006.05630",
          "publishedOn": "2021-06-18T02:06:37.594Z",
          "wordCount": 651,
          "title": "Distributional Robust Batch Contextual Bandits. (arXiv:2006.05630v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12873",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengdong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Voiculescu_I/0/1/0/all/0/1\">Irina Voiculescu</a>",
          "description": "Segmentation algorithms for medical images are widely studied for various\nclinical and research purposes. In this paper, we propose a new and efficient\nmethod for medical image segmentation under noisy labels. The method operates\nunder a deep learning paradigm, incorporating four novel contributions.\nFirstly, a residual interconnection is explored in different scale encoders to\ntransfer gradient information efficiently. Secondly, four copy-and-crop\nconnections are replaced by residual-block-based concatenation to alleviate the\ndisparity between encoders and decoders. Thirdly, convolutional attention\nmodules for feature refinement are studied on all scale decoders. Finally, an\nadaptive denoising learning strategy (ADL) is introduced into the training\nprocess to avoid too much influence from the noisy labels. Experimental results\nare illustrated on a publicly available benchmark database of spine CTs. Our\nproposed method achieves competitive performance against other state-of-the-art\nmethods over a variety of different evaluation measures.",
          "link": "http://arxiv.org/abs/2009.12873",
          "publishedOn": "2021-06-18T02:06:37.586Z",
          "wordCount": 633,
          "title": "RAR-U-Net: a Residual Encoder to Attention Decoder by Residual Connections Framework for Spine Segmentation under Noisy Labels. (arXiv:2009.12873v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.05937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>",
          "description": "Deep neural models in recent years have been successful in almost every\nfield, including extremely complex problem statements. However, these models\nare huge in size, with millions (and even billions) of parameters, thus\ndemanding more heavy computation power and failing to be deployed on edge\ndevices. Besides, the performance boost is highly dependent on redundant\nlabeled data. To achieve faster speeds and to handle the problems caused by the\nlack of data, knowledge distillation (KD) has been proposed to transfer\ninformation learned from one model to another. KD is often characterized by the\nso-called `Student-Teacher' (S-T) learning framework and has been broadly\napplied in model compression and knowledge transfer. This paper is about KD and\nS-T learning, which are being actively studied in recent years. First, we aim\nto provide explanations of what KD is and how/why it works. Then, we provide a\ncomprehensive survey on the recent progress of KD methods together with S-T\nframeworks typically for vision tasks. In general, we consider some fundamental\nquestions that have been driving this research area and thoroughly generalize\nthe research progress and technical details. Additionally, we systematically\nanalyze the research status of KD in vision applications. Finally, we discuss\nthe potentials and open challenges of existing methods and prospect the future\ndirections of KD and S-T learning.",
          "link": "http://arxiv.org/abs/2004.05937",
          "publishedOn": "2021-06-18T02:06:37.569Z",
          "wordCount": 771,
          "title": "Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks. (arXiv:2004.05937v7 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.07838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lina Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Rui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yawei Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xuemei Zeng</a>",
          "description": "The vulnerability of deep neural networks (DNNs) to adversarial attack, which\nis an attack that can mislead state-of-the-art classifiers into making an\nincorrect classification with high confidence by deliberately perturbing the\noriginal inputs, raises concerns about the robustness of DNNs to such attacks.\nAdversarial training, which is the main heuristic method for improving\nadversarial robustness and the first line of defense against adversarial\nattacks, requires many sample-by-sample calculations to increase training size\nand is usually insufficiently strong for an entire network. This paper provides\na new perspective on the issue of adversarial robustness, one that shifts the\nfocus from the network as a whole to the critical part of the region close to\nthe decision boundary corresponding to a given class. From this perspective, we\npropose a method to generate a single but image-agnostic adversarial\nperturbation that carries the semantic information implying the directions to\nthe fragile parts on the decision boundary and causes inputs to be\nmisclassified as a specified target. We call the adversarial training based on\nsuch perturbations \"region adversarial training\" (RAT), which resembles\nclassical adversarial training but is distinguished in that it reinforces the\nsemantic information missing in the relevant regions. Experimental results on\nthe MNIST and CIFAR-10 datasets show that this approach greatly improves\nadversarial robustness even using a very small dataset from the training data;\nmoreover, it can defend against FGSM adversarial attacks that have a completely\ndifferent pattern from the model seen during retraining.",
          "link": "http://arxiv.org/abs/2008.07838",
          "publishedOn": "2021-06-18T02:06:37.562Z",
          "wordCount": 735,
          "title": "Improving adversarial robustness of deep neural networks by using semantic information. (arXiv:2008.07838v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05404",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Guo_S/0/1/0/all/0/1\">Shangjie Guo</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Fritsch_A/0/1/0/all/0/1\">Amilson R. Fritsch</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Greenberg_C/0/1/0/all/0/1\">Craig Greenberg</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Spielman_I/0/1/0/all/0/1\">I. B. Spielman</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zwolak_J/0/1/0/all/0/1\">Justyna P. Zwolak</a>",
          "description": "Most data in cold-atom experiments comes from images, the analysis of which\nis limited by our preconceptions of the patterns that could be present in the\ndata. We focus on the well-defined case of detecting dark solitons -- appearing\nas local density depletions in a Bose-Einstein condensate (BEC) -- using a\nmethodology that is extensible to the general task of pattern recognition in\nimages of cold atoms. Studying soliton dynamics over a wide range of parameters\nrequires the analysis of large datasets, making the existing\nhuman-inspection-based methodology a significant bottleneck. Here we describe\nan automated classification and positioning system for identifying localized\nexcitations in atomic BECs utilizing deep convolutional neural networks to\neliminate the need for human image examination. Furthermore, we openly publish\nour labeled dataset of dark solitons, the first of its kind, for further\nmachine learning research.",
          "link": "http://arxiv.org/abs/2101.05404",
          "publishedOn": "2021-06-18T02:06:37.555Z",
          "wordCount": 626,
          "title": "Machine-learning enhanced dark soliton detection in Bose-Einstein condensates. (arXiv:2101.05404v2 [cond-mat.quant-gas] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bernaerts_Y/0/1/0/all/0/1\">Yves Bernaerts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berens_P/0/1/0/all/0/1\">Philipp Berens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobak_D/0/1/0/all/0/1\">Dmitry Kobak</a>",
          "description": "Patch-seq, a recently developed experimental technique, allows\nneuroscientists to obtain transcriptomic and electrophysiological information\nfrom the same neurons. Efficiently analyzing and visualizing such paired\nmultivariate data in order to extract biologically meaningful interpretations\nhas, however, remained a challenge. Here, we use sparse deep neural networks\nwith a two-dimensional bottleneck and group lasso penalty to predict\nelectrophysiological features from the transcriptomic ones, yielding concise\nand biologically interpretable two-dimensional visualizations. In two large\nexample data sets, this visualization reveals known neural classes and their\nmarker genes without biological prior knowledge.",
          "link": "http://arxiv.org/abs/2006.10411",
          "publishedOn": "2021-06-18T02:06:37.548Z",
          "wordCount": 559,
          "title": "Sparse bottleneck neural networks for exploratory non-linear visualization of Patch-seq data. (arXiv:2006.10411v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_G/0/1/0/all/0/1\">Gaurav Sinha</a>",
          "description": "We develop efficient randomized algorithms to solve the black-box\nreconstruction problem for polynomials over finite fields, computable by depth\nthree arithmetic circuits with alternating addition/multiplication gates, such\nthat output gate is an addition gate with in-degree two. These circuits compute\npolynomials of form $G\\times(T_1 + T_2)$, where $G,T_1,T_2$ are product of\naffine forms, and polynomials $T_1,T_2$ have no common factors. Rank of such a\ncircuit is defined as dimension of vector space spanned by all affine factors\nof $T_1$ and $T_2$. For any polynomial $f$ computable by such a circuit,\n$rank(f)$ is defined to be the minimum rank of any such circuit computing it.\nOur work develops randomized reconstruction algorithms which take as input\nblack-box access to a polynomial $f$ (over finite field $\\mathbb{F}$),\ncomputable by such a circuit. Here are the results.\n\n1 [Low rank]: When $5\\leq rank(f) = O(\\log^3 d)$, it runs in time\n$(nd^{\\log^3d}\\log |\\mathbb{F}|)^{O(1)}$, and, with high probability, outputs a\ndepth three circuit computing $f$, with top addition gate having in-degree\n$\\leq d^{rank(f)}$.\n\n2 [High rank]: When $rank(f) = \\Omega(\\log^3 d)$, it runs in time $(nd\\log\n|\\mathbb{F}|)^{O(1)}$, and, with high probability, outputs a depth three\ncircuit computing $f$, with top addition gate having in-degree two.\n\nOurs is the first blackbox reconstruction algorithm for this circuit class,\nthat runs in time polynomial in $\\log |\\mathbb{F}|$. This problem has been\nmentioned as an open problem in [GKL12] (STOC 2012)",
          "link": "http://arxiv.org/abs/2103.07445",
          "publishedOn": "2021-06-18T02:06:37.531Z",
          "wordCount": 696,
          "title": "Efficient reconstruction of depth three circuits with top fan-in two. (arXiv:2103.07445v2 [cs.CC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bibas_K/0/1/0/all/0/1\">Koby Bibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_M/0/1/0/all/0/1\">Meir Feder</a>",
          "description": "A fundamental principle of learning theory is that there is a trade-off\nbetween the complexity of a prediction rule and its ability to generalize.\nModern machine learning models do not obey this paradigm: They produce an\naccurate prediction even with a perfect fit to the training set. We investigate\nover-parameterized linear regression models focusing on the minimum norm\nsolution: This is the solution with the minimal norm that attains a perfect fit\nto the training set. We utilize the recently proposed predictive normalized\nmaximum likelihood (pNML) learner which is the min-max regret solution for the\ndistribution-free setting. We derive an upper bound of this min-max regret\nwhich is associated with the prediction uncertainty. We show that if the test\nsample lies mostly in a subspace spanned by the eigenvectors associated with\nthe large eigenvalues of the empirical correlation matrix of the training data,\nthe model generalizes despite its over-parameterized nature. We demonstrate the\nuse of the pNML regret as a point-wise learnability measure on synthetic data\nand successfully observe the double-decent phenomenon of the over-parameterized\nmodels on UCI datasets.",
          "link": "http://arxiv.org/abs/2102.07181",
          "publishedOn": "2021-06-18T02:06:37.215Z",
          "wordCount": 644,
          "title": "Distribution Free Uncertainty for the Minimum Norm Solution of Over-parameterized Linear Regression. (arXiv:2102.07181v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1\">Jeff Z. HaoChen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "Recent works in self-supervised learning have advanced the state-of-the-art\nby relying on the contrastive learning paradigm, which learns representations\nby pushing positive pairs, or similar examples from the same class, closer\ntogether while keeping negative pairs far apart. Despite the empirical\nsuccesses, theoretical foundations are limited -- prior analyses assume\nconditional independence of the positive pairs given the same class label, but\nrecent empirical applications use heavily correlated positive pairs (i.e., data\naugmentations of the same image). Our work analyzes contrastive learning\nwithout assuming conditional independence of positive pairs using a novel\nconcept of the augmentation graph on data. Edges in this graph connect\naugmentations of the same data, and ground-truth classes naturally form\nconnected sub-graphs. We propose a loss that performs spectral decomposition on\nthe population augmentation graph and can be succinctly written as a\ncontrastive learning objective on neural net representations. Minimizing this\nobjective leads to features with provable accuracy guarantees under linear\nprobe evaluation. By standard generalization bounds, these accuracy guarantees\nalso hold when minimizing the training contrastive loss. Empirically, the\nfeatures learned by our objective can match or outperform several strong\nbaselines on benchmark vision datasets. In all, this work provides the first\nprovable analysis for contrastive learning where guarantees for linear probe\nevaluation can apply to realistic empirical settings.",
          "link": "http://arxiv.org/abs/2106.04156",
          "publishedOn": "2021-06-18T02:06:37.208Z",
          "wordCount": 672,
          "title": "Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>",
          "description": "Few-shot learning (FSL) aims to classify images under low-data regimes, where\nthe conventional pooled global representation is likely to lose useful local\ncharacteristics. Recent work has achieved promising performances by using deep\ndescriptors. They generally take all deep descriptors from neural networks into\nconsideration while ignoring that some of them are useless in classification\ndue to their limited receptive field, e.g., task-irrelevant descriptors could\nbe misleading and multiple aggregative descriptors from background clutter\ncould even overwhelm the object's presence. In this paper, we argue that a\nMutual Nearest Neighbor (MNN) relation should be established to explicitly\nselect the query descriptors that are most relevant to each task and discard\nless relevant ones from aggregative clutters in FSL. Specifically, we propose\nDiscriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive\nexperiments demonstrate that our method not only qualitatively selects\ntask-relevant descriptors but also quantitatively outperforms the existing\nstate-of-the-arts by a large margin of 1.8~4.9% on fine-grained CUB, a\nconsiderable margin of 1.4~2.2% on both supervised and semi-supervised\nminiImagenet, and ~1.4% on challenging tieredimagenet.",
          "link": "http://arxiv.org/abs/2103.08160",
          "publishedOn": "2021-06-18T02:06:37.194Z",
          "wordCount": 653,
          "title": "DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network. (arXiv:2103.08160v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dawkins_Q/0/1/0/all/0/1\">Quinlan Dawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haifeng Xu</a>",
          "description": "Diffusion source identification on networks is a problem of fundamental\nimportance in a broad class of applications, including rumor controlling and\nvirus identification. Though this problem has received significant recent\nattention, most studies have focused only on very restrictive settings and lack\ntheoretical guarantees for more realistic networks. We introduce a statistical\nframework for the study of diffusion source identification and develop a\nconfidence set inference approach inspired by hypothesis testing. Our method\nefficiently produces a small subset of nodes, which provably covers the source\nnode with any pre-specified confidence level without restrictive assumptions on\nnetwork structures. Moreover, we propose multiple Monte Carlo strategies for\nthe inference procedure based on network topology and the probabilistic\nproperties that significantly improve the scalability. To our knowledge, this\nis the first diffusion source identification method with a practically useful\ntheoretical guarantee on general networks. We demonstrate our approach via\nextensive synthetic experiments on well-known random network models and a\nmobility network between cities concerning the COVID-19 spreading.",
          "link": "http://arxiv.org/abs/2106.04800",
          "publishedOn": "2021-06-18T02:06:37.188Z",
          "wordCount": 667,
          "title": "Diffusion Source Identification on Networks with Statistical Confidence. (arXiv:2106.04800v2 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.10088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1\">Paul Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbonell_J/0/1/0/all/0/1\">Jaime Carbonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "To acquire a new skill, humans learn better and faster if a tutor, based on\ntheir current knowledge level, informs them of how much attention they should\npay to particular content or practice problems. Similarly, a machine learning\nmodel could potentially be trained better with a scorer that \"adapts\" to its\ncurrent learning state and estimates the importance of each training data\ninstance. Training such an adaptive scorer efficiently is a challenging\nproblem; in order to precisely quantify the effect of a data instance at a\ngiven time during the training, it is typically necessary to first complete the\nentire training process. To efficiently optimize data usage, we propose a\nreinforcement learning approach called Differentiable Data Selection (DDS). In\nDDS, we formulate a scorer network as a learnable function of the training\ndata, which can be efficiently updated along with the main model being trained.\nSpecifically, DDS updates the scorer with an intuitive reward signal: it should\nup-weigh the data that has a similar gradient with a dev set upon which we\nwould finally like to perform well. Without significant computing overhead, DDS\ndelivers strong and consistent improvements over several strong baselines on\ntwo very different tasks of machine translation and image classification.",
          "link": "http://arxiv.org/abs/1911.10088",
          "publishedOn": "2021-06-18T02:06:37.159Z",
          "wordCount": 688,
          "title": "Optimizing Data Usage via Differentiable Rewards. (arXiv:1911.10088v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wenjin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaomeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_T/0/1/0/all/0/1\">Tao Jia</a>",
          "description": "The prediction for information diffusion on social networks has great\npractical significance in marketing and public opinion control. Cascade\nprediction aims to predict the individuals who will potentially repost the\nmessage on the social network. One kind of methods either exploit\ndemographical, structural, and temporal features for prediction, or explicitly\nrely on particular information diffusion models. The other kind of models are\nfully data-driven and do not require a global network structure. Thus massive\ndiffusion prediction models based on network embedding are proposed. These\nmodels embed the users into the latent space using their cascade information,\nbut are lack of consideration for the intervene among users when embedding. In\nthis paper, we propose an independent asymmetric embedding method to learn\nsocial embedding for cascade prediction. Different from existing methods, our\nmethod embeds each individual into one latent influence space and multiple\nlatent susceptibility spaces. Furthermore, our method captures the\nco-occurrence regulation of user combination in cascades to improve the\ncalculating effectiveness. The results of extensive experiments conducted on\nreal-world datasets verify both the predictive accuracy and cost-effectiveness\nof our approach.",
          "link": "http://arxiv.org/abs/2105.08291",
          "publishedOn": "2021-06-18T02:06:37.142Z",
          "wordCount": 633,
          "title": "Independent Asymmetric Embedding for Cascade Prediction on Social Networks. (arXiv:2105.08291v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Amish Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1\">Sourav Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1\">Arnhav Datar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1\">Juned Kadiwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1\">Hrithwik Shalu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Jimson Mathew</a>",
          "description": "Reliable detection of the prodromal stages of Alzheimer's disease (AD)\nremains difficult even today because, unlike other neurocognitive impairments,\nthere is no definitive diagnosis of AD in vivo. In this context, existing\nresearch has shown that patients often develop language impairment even in mild\nAD conditions. We propose a multimodal deep learning method that utilizes\nspeech and the corresponding transcript simultaneously to detect AD. For audio\nsignals, the proposed audio-based network, a convolutional neural network (CNN)\nbased model, predicts the diagnosis for multiple speech segments, which are\ncombined for the final prediction. Similarly, we use contextual embedding\nextracted from BERT concatenated with a CNN-generated embedding for classifying\nthe transcript. The individual predictions of the two models are then combined\nto make the final classification. We also perform experiments to analyze the\nmodel performance when Automated Speech Recognition (ASR) system generated\ntranscripts are used instead of manual transcription in the text-based model.\nThe proposed method achieves 85.3% 10-fold cross-validation accuracy when\ntrained and evaluated on the Dementiabank Pitt corpus.",
          "link": "http://arxiv.org/abs/2012.00096",
          "publishedOn": "2021-06-18T02:06:37.135Z",
          "wordCount": 642,
          "title": "Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bejani_M/0/1/0/all/0/1\">Mohammad Mahdi Bejani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghatee_M/0/1/0/all/0/1\">Mehdi Ghatee</a>",
          "description": "Overfitting is one of the critical problems in deep neural networks. Many\nregularization schemes try to prevent overfitting blindly. However, they\ndecrease the convergence speed of training algorithms. Adaptive regularization\nschemes can solve overfitting more intelligently. They usually do not affect\nthe entire network weights. This paper detects a subset of the weighting layers\nthat cause overfitting. The overfitting recognizes by matrix and tensor\ncondition numbers. An adaptive regularization scheme entitled Adaptive Low-Rank\n(ALR) is proposed that converges a subset of the weighting layers to their\nLow-Rank Factorization (LRF). It happens by minimizing a new Tikhonov-based\nloss function. ALR also encourages lazy weights to contribute to the\nregularization when epochs grow up. It uses a damping sequence to increment\nlayer selection likelihood in the last generations. Thus before falling the\ntraining accuracy, ALR reduces the lazy weights and regularizes the network\nsubstantially. The experimental results show that ALR regularizes the deep\nnetworks well with high training speed and low resource usage.",
          "link": "http://arxiv.org/abs/2106.09677",
          "publishedOn": "2021-06-18T02:06:37.127Z",
          "wordCount": 634,
          "title": "Adaptive Low-Rank Regularization with Damping Sequences to Restrict Lazy Weights in Deep Networks. (arXiv:2106.09677v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.05865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>",
          "description": "The goal of supervised representation learning is to construct effective data\nrepresentations for prediction. Among all the characteristics of an ideal\nnonparametric representation of high-dimensional complex data, sufficiency, low\ndimensionality and disentanglement are some of the most essential ones. We\npropose a deep dimension reduction approach to learning representations with\nthese characteristics. The proposed approach is a nonparametric generalization\nof the sufficient dimension reduction method. We formulate the ideal\nrepresentation learning task as that of finding a nonparametric representation\nthat minimizes an objective function characterizing conditional independence\nand promoting disentanglement at the population level. We then estimate the\ntarget representation at the sample level nonparametrically using deep neural\nnetworks. We show that the estimated deep nonparametric representation is\nconsistent in the sense that its excess risk converges to zero. Our extensive\nnumerical experiments using simulated and real benchmark data demonstrate that\nthe proposed methods have better performance than several existing dimension\nreduction methods and the standard deep learning models in the context of\nclassification and regression.",
          "link": "http://arxiv.org/abs/2006.05865",
          "publishedOn": "2021-06-18T02:06:37.121Z",
          "wordCount": 630,
          "title": "Deep Dimension Reduction for Supervised Representation Learning. (arXiv:2006.05865v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.01212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1\">Tamali Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1\">Rudra Murthy V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>",
          "description": "In this paper, we identify an interesting kind of error in the output of\nUnsupervised Neural Machine Translation (UNMT) systems like\n\\textit{Undreamt}(footnote). We refer to this error type as \\textit{Scrambled\nTranslation problem}. We observe that UNMT models which use \\textit{word\nshuffle} noise (as in case of Undreamt) can generate correct words, but fail to\nstitch them together to form phrases. As a result, words of the translated\nsentence look \\textit{scrambled}, resulting in decreased BLEU. We hypothesise\nthat the reason behind \\textit{scrambled translation problem} is 'shuffling\nnoise' which is introduced in every input sentence as a denoising strategy. To\ntest our hypothesis, we experiment by retraining UNMT models with a simple\n\\textit{retraining} strategy. We stop the training of the Denoising UNMT model\nafter a pre-decided number of iterations and resume the training for the\nremaining iterations -- which number is also pre-decided -- using original\nsentence as input without adding any noise. Our proposed solution achieves\nsignificant performance improvement UNMT models that train conventionally. We\ndemonstrate these performance gains on four language pairs, \\textit{viz.},\nEnglish-French, English-German, English-Spanish, Hindi-Punjabi. Our qualitative\nand quantitative analysis shows that the retraining strategy helps achieve\nbetter alignment as observed by attention heatmap and better phrasal\ntranslation, leading to statistically significant improvement in BLEU scores.",
          "link": "http://arxiv.org/abs/1911.01212",
          "publishedOn": "2021-06-18T02:06:37.114Z",
          "wordCount": 682,
          "title": "Scrambled Translation Problem: A Problem of Denoising UNMT. (arXiv:1911.01212v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Punnakkal_A/0/1/0/all/0/1\">Abhinanda R. Punnakkal</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1\">Arjun Chandrasekaran</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1\">Nikos Athanasiou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Quiros_Ramirez_A/0/1/0/all/0/1\">Alejandra Quiros-Ramirez</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a> (1) ((1) Max Planck Institute for Intelligent Systems, (2) Universitat Konstanz)",
          "description": "Understanding the semantics of human movement -- the what, how and why of the\nmovement -- is an important problem that requires datasets of human actions\nwith semantic labels. Existing datasets take one of two approaches. Large-scale\nvideo datasets contain many action labels but do not contain ground-truth 3D\nhuman motion. Alternatively, motion-capture (mocap) datasets have precise body\nmotions but are limited to a small number of actions. To address this, we\npresent BABEL, a large dataset with language labels describing the actions\nbeing performed in mocap sequences. BABEL consists of action labels for about\n43 hours of mocap sequences from AMASS. Action labels are at two levels of\nabstraction -- sequence labels describe the overall action in the sequence, and\nframe labels describe all actions in every frame of the sequence. Each frame\nlabel is precisely aligned with the duration of the corresponding action in the\nmocap sequence, and multiple actions can overlap. There are over 28k sequence\nlabels, and 63k frame labels in BABEL, which belong to over 250 unique action\ncategories. Labels from BABEL can be leveraged for tasks like action\nrecognition, temporal action localization, motion synthesis, etc. To\ndemonstrate the value of BABEL as a benchmark, we evaluate the performance of\nmodels on 3D action recognition. We demonstrate that BABEL poses interesting\nlearning challenges that are applicable to real-world scenarios, and can serve\nas a useful benchmark of progress in 3D action recognition. The dataset,\nbaseline method, and evaluation code is made available, and supported for\nacademic research purposes at https://babel.is.tue.mpg.de/.",
          "link": "http://arxiv.org/abs/2106.09696",
          "publishedOn": "2021-06-18T02:06:37.094Z",
          "wordCount": 728,
          "title": "BABEL: Bodies, Action and Behavior with English Labels. (arXiv:2106.09696v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09702",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lubold_S/0/1/0/all/0/1\">Shane Lubold</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_B/0/1/0/all/0/1\">Bolun Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+McCormick_T/0/1/0/all/0/1\">Tyler H. McCormick</a>",
          "description": "Networks describe the, often complex, relationships between individual\nactors. In this work, we address the question of how to determine whether a\nparametric model, such as a stochastic block model or latent space model, fits\na dataset well and will extrapolate to similar data. We use recent results in\nrandom matrix theory to derive a general goodness-of-fit test for dyadic data.\nWe show that our method, when applied to a specific model of interest, provides\nan straightforward, computationally fast way of selecting parameters in a\nnumber of commonly used network models. For example, we show how to select the\ndimension of the latent space in latent space models. Unlike other network\ngoodness-of-fit methods, our general approach does not require simulating from\na candidate parametric model, which can be cumbersome with large graphs, and\neliminates the need to choose a particular set of statistics on the graph for\ncomparison. It also allows us to perform goodness-of-fit tests on partial\nnetwork data, such as Aggregated Relational Data. We show with simulations that\nour method performs well in many situations of interest. We analyze several\nempirically relevant networks and show that our method leads to improved\ncommunity detection algorithms. R code to implement our method is available on\nGithub.",
          "link": "http://arxiv.org/abs/2106.09702",
          "publishedOn": "2021-06-18T02:06:37.086Z",
          "wordCount": 651,
          "title": "Spectral goodness-of-fit tests for complete and partial network data. (arXiv:2106.09702v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2012.13623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1\">Alex Fedorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sylvain_T/0/1/0/all/0/1\">Tristan Sylvain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geenjaar_E/0/1/0/all/0/1\">Eloy Geenjaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luck_M/0/1/0/all/0/1\">Margaux Luck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeRamus_T/0/1/0/all/0/1\">Thomas P. DeRamus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirilin_A/0/1/0/all/0/1\">Alex Kirilin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleklov_D/0/1/0/all/0/1\">Dmitry Bleklov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1\">Vince D. Calhoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1\">Sergey M. Plis</a>",
          "description": "Sensory input from multiple sources is crucial for robust and coherent human\nperception. Different sources contribute complementary explanatory factors.\nSimilarly, research studies often collect multimodal imaging data, each of\nwhich can provide shared and unique information. This observation motivated the\ndesign of powerful multimodal self-supervised representation-learning\nalgorithms. In this paper, we unify recent work on multimodal self-supervised\nlearning under a single framework. Observing that most self-supervised methods\noptimize similarity metrics between a set of model components, we propose a\ntaxonomy of all reasonable ways to organize this process. We first evaluate\nmodels on toy multimodal MNIST datasets and then apply them to a multimodal\nneuroimaging dataset with Alzheimer's disease patients. We find that (1)\nmultimodal contrastive learning has significant benefits over its unimodal\ncounterpart, (2) the specific composition of multiple contrastive objectives is\ncritical to performance on a downstream task, (3) maximization of the\nsimilarity between representations has a regularizing effect on a neural\nnetwork, which can sometimes lead to reduced downstream performance but still\nreveal multimodal relations. Results show that the proposed approach\noutperforms previous self-supervised encoder-decoder methods based on canonical\ncorrelation analysis (CCA) or the mixture-of-experts multimodal variational\nautoEncoder (MMVAE) on various datasets with a linear evaluation protocol.\nImportantly, we find a promising solution to uncover connections between\nmodalities through a jointly shared subspace that can help advance work in our\nsearch for neuroimaging biomarkers.",
          "link": "http://arxiv.org/abs/2012.13623",
          "publishedOn": "2021-06-18T02:06:37.079Z",
          "wordCount": 733,
          "title": "Self-Supervised Multimodal Domino: in Search of Biomarkers for Alzheimer's Disease. (arXiv:2012.13623v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Songhua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiaobo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>",
          "description": "Learning with noisy labels has attracted a lot of attention in recent years,\nwhere the mainstream approaches are in pointwise manners. Meanwhile, pairwise\nmanners have shown great potential in supervised metric learning and\nunsupervised contrastive learning. Thus, a natural question is raised: does\nlearning in a pairwise manner mitigate label noise? To give an affirmative\nanswer, in this paper, we propose a framework called Class2Simi: it transforms\ndata points with noisy class labels to data pairs with noisy similarity labels,\nwhere a similarity label denotes whether a pair shares the class label or not.\nThrough this transformation, the reduction of the noise rate is theoretically\nguaranteed, and hence it is in principle easier to handle noisy similarity\nlabels. Amazingly, DNNs that predict the clean class labels can be trained from\nnoisy data pairs if they are first pretrained from noisy data points.\nClass2Simi is computationally efficient because not only this transformation is\non-the-fly in mini-batches, but also it just changes loss computation on top of\nmodel prediction into a pairwise manner. Its effectiveness is verified by\nextensive experiments.",
          "link": "http://arxiv.org/abs/2006.07831",
          "publishedOn": "2021-06-18T02:06:37.072Z",
          "wordCount": 656,
          "title": "Class2Simi: A Noise Reduction Perspective on Learning with Noisy Labels. (arXiv:2006.07831v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01385",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Han_J/0/1/0/all/0/1\">Jiequn Han</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hu_R/0/1/0/all/0/1\">Ruimeng Hu</a>",
          "description": "Stochastic control problems with delay are challenging due to the\npath-dependent feature of the system and thus its intrinsic high dimensions. In\nthis paper, we propose and systematically study deep neural networks-based\nalgorithms to solve stochastic control problems with delay features.\nSpecifically, we employ neural networks for sequence modeling (\\emph{e.g.},\nrecurrent neural networks such as long short-term memory) to parameterize the\npolicy and optimize the objective function. The proposed algorithms are tested\non three benchmark examples: a linear-quadratic problem, optimal consumption\nwith fixed finite delay, and portfolio optimization with complete memory.\nParticularly, we notice that the architecture of recurrent neural networks\nnaturally captures the path-dependent feature with much flexibility and yields\nbetter performance with more efficient and stable training of the network\ncompared to feedforward networks. The superiority is even evident in the case\nof portfolio optimization with complete memory, which features infinite delay.",
          "link": "http://arxiv.org/abs/2101.01385",
          "publishedOn": "2021-06-18T02:06:37.066Z",
          "wordCount": 599,
          "title": "Recurrent Neural Networks for Stochastic Control Problems with Delay. (arXiv:2101.01385v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Sanjib Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghimire_G/0/1/0/all/0/1\">Ganesh Raj Ghimire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddique_R/0/1/0/all/0/1\">Ridwan Siddique</a>",
          "description": "Skillful streamflow forecasting informs decisions in various areas of water\npolicy and management. We integrate dynamical modeling with machine learning to\ndemonstrate the enhanced quality of streamflow forecasts at short-to\nmedium-range timescales (1 - 7 days). Dynamical modeling generates ensemble\nstreamflow forecasts by forcing a hydrological model with numerical weather\nprediction model outputs. We employ a Long Short-Term Memory (LSTM) neural\nnetwork to correct forecast biases in raw ensemble streamflow forecasts\nobtained from dynamical modeling. For forecast verification, we use different\nmetrics such as skill score and reliability diagram conditioned upon the lead\ntime, flow threshold, and season. The verification results show that the LSTM\ncan improve streamflow forecasts relative to climatological, temporal\npersistence, deterministic, and raw ensemble forecasts. The LSTM demonstrates\nimprovement across all lead times, flow thresholds, and seasons. As compared to\nthe raw ensembles, relative gain in forecast skill from LSTM is generally\nhigher at medium-range timescales compared to initial lead time; high flows\ncompared to low-moderate flows; and warm-season compared to the cool ones.\nOverall, our results highlight the benefits of LSTM for improving both the\nskill and reliability of streamflow forecasts.",
          "link": "http://arxiv.org/abs/2106.09547",
          "publishedOn": "2021-06-18T02:06:37.047Z",
          "wordCount": 611,
          "title": "Machine Learning for Postprocessing Ensemble Streamflow Forecasts. (arXiv:2106.09547v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09539",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Vaaras_E/0/1/0/all/0/1\">Einari Vaaras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahlqvist_Bjorkroth_S/0/1/0/all/0/1\">Sari Ahlqvist-Bj&#xf6;rkroth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drossos_K/0/1/0/all/0/1\">Konstantinos Drossos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>",
          "description": "Researchers have recently started to study how the emotional speech heard by\nyoung infants can affect their developmental outcomes. As a part of this\nresearch, hundreds of hours of daylong recordings from preterm infants' audio\nenvironments were collected from two hospitals in Finland and Estonia in the\ncontext of so-called APPLE study. In order to analyze the emotional content of\nspeech in such a massive dataset, an automatic speech emotion recognition (SER)\nsystem is required. However, there are no emotion labels or existing indomain\nSER systems to be used for this purpose. In this paper, we introduce this\ninitially unannotated large-scale real-world audio dataset and describe the\ndevelopment of a functional SER system for the Finnish subset of the data. We\nexplore the effectiveness of alternative state-of-the-art techniques to deploy\na SER system to a new domain, comparing cross-corpus generalization, WGAN-based\ndomain adaptation, and active learning in the task. As a result, we show that\nthe best-performing models are able to achieve a classification performance of\n73.4% unweighted average recall (UAR) and 73.2% UAR for a binary classification\nfor valence and arousal, respectively. The results also show that active\nlearning achieves the most consistent performance compared to the two\nalternatives.",
          "link": "http://arxiv.org/abs/2106.09539",
          "publishedOn": "2021-06-18T02:06:37.040Z",
          "wordCount": 665,
          "title": "Automatic Analysis of the Emotional Content of Speech in Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit. (arXiv:2106.09539v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tierney_G/0/1/0/all/0/1\">Graham Tierney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bail_C/0/1/0/all/0/1\">Christopher Bail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volfovsky_A/0/1/0/all/0/1\">Alexander Volfovsky</a>",
          "description": "Analysis of short text, such as social media posts, is extremely difficult\nbecause it relies on observing many document-level word co-occurrence pairs.\nBeyond topic distributions, a common downstream task of the modeling is\ngrouping the authors of these documents for subsequent analyses. Traditional\nmodels estimate the document groupings and identify user clusters with an\nindependent procedure. We propose a novel model that expands on the Latent\nDirichlet Allocation by modeling strong dependence among the words in the same\ndocument, with user-level topic distributions. We also simultaneously cluster\nusers, removing the need for post-hoc cluster estimation and improving topic\nestimation by shrinking noisy user-level topic distributions towards typical\nvalues. Our method performs as well as -- or better -- than traditional\napproaches to problems arising in short text, and we demonstrate its usefulness\non a dataset of tweets from United States Senators, recovering both meaningful\ntopics and clusters that reflect partisan ideology.",
          "link": "http://arxiv.org/abs/2106.09533",
          "publishedOn": "2021-06-18T02:06:37.033Z",
          "wordCount": 590,
          "title": "Author Clustering and Topic Estimation for Short Texts. (arXiv:2106.09533v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marris_L/0/1/0/all/0/1\">Luke Marris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1\">Paul Muller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1\">Marc Lanctot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuyls_K/0/1/0/all/0/1\">Karl Tuyls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grapael_T/0/1/0/all/0/1\">Thore Grapael</a>",
          "description": "Two-player, constant-sum games are well studied in the literature, but there\nhas been limited progress outside of this setting. We propose Joint\nPolicy-Space Response Oracles (JPSRO), an algorithm for training agents in\nn-player, general-sum extensive form games, which provably converges to an\nequilibrium. We further suggest correlated equilibria (CE) as promising\nmeta-solvers, and propose a novel solution concept Maximum Gini Correlated\nEquilibrium (MGCE), a principled and computationally efficient family of\nsolutions for solving the correlated equilibrium selection problem. We conduct\nseveral experiments using CE meta-solvers for JPSRO and demonstrate convergence\non n-player, general-sum games.",
          "link": "http://arxiv.org/abs/2106.09435",
          "publishedOn": "2021-06-18T02:06:37.027Z",
          "wordCount": 558,
          "title": "Multi-Agent Training beyond Zero-Sum with Correlated Equilibrium Meta-Solvers. (arXiv:2106.09435v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>",
          "description": "Multi-head attention plays a crucial role in the recent success of\nTransformer models, which leads to consistent performance improvements over\nconventional attention in various applications. The popular belief is that this\neffectiveness stems from the ability of jointly attending multiple positions.\nIn this paper, we first demonstrate that jointly attending multiple positions\nis not a unique feature of multi-head attention, as multi-layer single-head\nattention also attends multiple positions and is more effective. Then, we\nsuggest the main advantage of the multi-head attention is the training\nstability, since it has less number of layers than the single-head attention,\nwhen attending the same number of positions. For example, 24-layer 16-head\nTransformer (BERT-large) and 384-layer single-head Transformer has the same\ntotal attention head number and roughly the same model size, while the\nmulti-head one is significantly shallower. Meanwhile, we show that, with recent\nadvances in deep learning, we can successfully stabilize the training of the\n384-layer Transformer. As the training difficulty is no longer a bottleneck,\nsubstantially deeper single-head Transformer achieves consistent performance\nimprovements without tuning hyper-parameters.",
          "link": "http://arxiv.org/abs/2106.09650",
          "publishedOn": "2021-06-18T02:06:37.020Z",
          "wordCount": 614,
          "title": "Multi-head or Single-head? An Empirical Comparison for Transformer Training. (arXiv:2106.09650v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1912.08061",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Modanwal_G/0/1/0/all/0/1\">Gourav Modanwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vellal_A/0/1/0/all/0/1\">Adithya Vellal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej A. Mazurowski</a>",
          "description": "Dynamic Contrast Enhanced-Magnetic Resonance Imaging (DCE-MRI) is widely used\nto complement ultrasound examinations and x-ray mammography during the early\ndetection and diagnosis of breast cancer. However, images generated by various\nMRI scanners (e.g. GE Healthcare vs Siemens) differ both in intensity and noise\ndistribution, preventing algorithms trained on MRIs from one scanner to\ngeneralize to data from other scanners successfully. We propose a method for\nimage normalization to solve this problem. MRI normalization is challenging\nbecause it requires both normalizing intensity values and mapping between the\nnoise distributions of different scanners. We utilize a cycle-consistent\ngenerative adversarial network to learn a bidirectional mapping between MRIs\nproduced by GE Healthcare and Siemens scanners. This allows us learning the\nmapping between two different scanner types without matched data, which is not\ncommonly available. To ensure the preservation of breast shape and structures\nwithin the breast, we propose two technical innovations. First, we incorporate\na mutual information loss with the CycleGAN architecture to ensure that the\nstructure of the breast is maintained. Second, we propose a modified\ndiscriminator architecture which utilizes a smaller field-of-view to ensure the\npreservation of finer details in the breast tissue. Quantitative and\nqualitative evaluations show that the second proposed method was able to\nconsistently preserve a high level of detail in the breast structure while also\nperforming the proper intensity normalization and noise mapping. Our results\ndemonstrate that the proposed model can successfully learn a bidirectional\nmapping between MRIs produced by different vendors, potentially enabling\nimproved accuracy of downstream computational algorithms for diagnosis and\ndetection of breast cancer. All the data used in this study are publicly\navailable.",
          "link": "http://arxiv.org/abs/1912.08061",
          "publishedOn": "2021-06-18T02:06:37.000Z",
          "wordCount": 758,
          "title": "Normalization of breast MRIs using Cycle-Consistent Generative Adversarial Networks. (arXiv:1912.08061v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nadkarni_R/0/1/0/all/0/1\">Rahul Nadkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>",
          "description": "Biomedical knowledge graphs (KGs) hold rich information on entities such as\ndiseases, drugs, and genes. Predicting missing links in these graphs can boost\nmany important applications, such as drug design and repurposing. Recent work\nhas shown that general-domain language models (LMs) can serve as \"soft\" KGs,\nand that they can be fine-tuned for the task of KG completion. In this work, we\nstudy scientific LMs for KG completion, exploring whether we can tap into their\nlatent knowledge to enhance biomedical link prediction. We evaluate several\ndomain-specific LMs, fine-tuning them on datasets centered on drugs and\ndiseases that we represent as KGs and enrich with textual entity descriptions.\nWe integrate the LM-based models with KG embedding models, using a router\nmethod that learns to assign each input example to either type of model and\nprovides a substantial boost in performance. Finally, we demonstrate the\nadvantage of LM models in the inductive setting with novel scientific entities.\nOur datasets and code are made publicly available.",
          "link": "http://arxiv.org/abs/2106.09700",
          "publishedOn": "2021-06-18T02:06:36.993Z",
          "wordCount": 610,
          "title": "Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. (arXiv:2106.09700v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+El_Nouby_A/0/1/0/all/0/1\">Alaaeldin El-Nouby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1\">Hugo Touvron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1\">Natalia Neverova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1\">Jakob Verbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1\">Herv&#xe9; Jegou</a>",
          "description": "Following their success in natural language processing, transformers have\nrecently shown much promise for computer vision. The self-attention operation\nunderlying transformers yields global interactions between all tokens ,i.e.\nwords or image patches, and enables flexible modelling of image data beyond the\nlocal interactions of convolutions. This flexibility, however, comes with a\nquadratic complexity in time and memory, hindering application to long\nsequences and high-resolution images. We propose a \"transposed\" version of\nself-attention that operates across feature channels rather than tokens, where\nthe interactions are based on the cross-covariance matrix between keys and\nqueries. The resulting cross-covariance attention (XCA) has linear complexity\nin the number of tokens, and allows efficient processing of high-resolution\nimages. Our cross-covariance image transformer (XCiT) is built upon XCA. It\ncombines the accuracy of conventional transformers with the scalability of\nconvolutional architectures. We validate the effectiveness and generality of\nXCiT by reporting excellent results on multiple vision benchmarks, including\nimage classification and self-supervised feature learning on ImageNet-1k,\nobject detection and instance segmentation on COCO, and semantic segmentation\non ADE20k.",
          "link": "http://arxiv.org/abs/2106.09681",
          "publishedOn": "2021-06-18T02:06:36.987Z",
          "wordCount": 617,
          "title": "XCiT: Cross-Covariance Image Transformers. (arXiv:2106.09681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baldock_R/0/1/0/all/0/1\">Robert J. N. Baldock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maennel_H/0/1/0/all/0/1\">Hartmut Maennel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>",
          "description": "Existing work on understanding deep learning often employs measures that\ncompress all data-dependent information into a few numbers. In this work, we\nadopt a perspective based on the role of individual examples. We introduce a\nmeasure of the computational difficulty of making a prediction for a given\ninput: the (effective) prediction depth. Our extensive investigation reveals\nsurprising yet simple relationships between the prediction depth of a given\ninput and the model's uncertainty, confidence, accuracy and speed of learning\nfor that data point. We further categorize difficult examples into three\ninterpretable groups, demonstrate how these groups are processed differently\ninside deep models and showcase how this understanding allows us to improve\nprediction accuracy. Insights from our study lead to a coherent view of a\nnumber of separately reported phenomena in the literature: early layers\ngeneralize while later layers memorize; early layers converge faster and\nnetworks learn easy data and simple functions first.",
          "link": "http://arxiv.org/abs/2106.09647",
          "publishedOn": "2021-06-18T02:06:36.977Z",
          "wordCount": 598,
          "title": "Deep Learning Through the Lens of Example Difficulty. (arXiv:2106.09647v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nori_H/0/1/0/all/0/1\">Harsha Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caruana_R/0/1/0/all/0/1\">Rich Caruana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Judy Hanwen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1\">Janardhan Kulkarni</a>",
          "description": "We show that adding differential privacy to Explainable Boosting Machines\n(EBMs), a recent method for training interpretable ML models, yields\nstate-of-the-art accuracy while protecting privacy. Our experiments on multiple\nclassification and regression datasets show that DP-EBM models suffer\nsurprisingly little accuracy loss even with strong differential privacy\nguarantees. In addition to high accuracy, two other benefits of applying DP to\nEBMs are: a) trained models provide exact global and local interpretability,\nwhich is often important in settings where differential privacy is needed; and\nb) the models can be edited after training without loss of privacy to correct\nerrors which DP noise may have introduced.",
          "link": "http://arxiv.org/abs/2106.09680",
          "publishedOn": "2021-06-18T02:06:36.970Z",
          "wordCount": 552,
          "title": "Accuracy, Interpretability, and Differential Privacy via Explainable Boosting. (arXiv:2106.09680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chengrun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chee_J/0/1/0/all/0/1\">Jerry Chee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>",
          "description": "Low-precision arithmetic trains deep learning models using less energy, less\nmemory and less time. However, we pay a price for the savings: lower precision\nmay yield larger round-off error and hence larger prediction error. As\napplications proliferate, users must choose which precision to use to train a\nnew model, and chip manufacturers must decide which precisions to manufacture.\nWe view these precision choices as a hyperparameter tuning problem, and borrow\nideas from meta-learning to learn the tradeoff between memory and error. In\nthis paper, we introduce Pareto Estimation to Pick the Perfect Precision\n(PEPPP). We use matrix factorization to find non-dominated configurations (the\nPareto frontier) with a limited number of network evaluations. For any given\nmemory budget, the precision that minimizes error is a point on this frontier.\nPractitioners can use the frontier to trade memory for error and choose the\nbest precision for their goals.",
          "link": "http://arxiv.org/abs/2106.09686",
          "publishedOn": "2021-06-18T02:06:36.945Z",
          "wordCount": 590,
          "title": "How Low Can We Go: Trading Memory for Error in Low-Precision Training. (arXiv:2106.09686v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09556",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bi_Y/0/1/0/all/0/1\">Yifei Bi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1\">Xinyi Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xiao_C/0/1/0/all/0/1\">Caihui Xiao</a>",
          "description": "Adapting the idea of training CartPole with Deep Q-learning agent, we are\nable to find a promising result that prevent the pole from falling down. The\ncapacity of reinforcement learning (RL) to learn from the interaction between\nthe environment and agent provides an optimal control strategy. In this paper,\nwe aim to solve the classic pendulum swing-up problem that making the learned\npendulum to be in upright position and balanced. Deep Deterministic Policy\nGradient algorithm is introduced to operate over continuous action domain in\nthis problem. Salient results of optimal pendulum are proved with increasing\naverage return, decreasing loss, and live video in the code part.",
          "link": "http://arxiv.org/abs/2106.09556",
          "publishedOn": "2021-06-18T02:06:36.939Z",
          "wordCount": 545,
          "title": "A Deep Reinforcement Learning Approach towards Pendulum Swing-up Problem based on TF-Agents. (arXiv:2106.09556v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisdom_S/0/1/0/all/0/1\">Scott Wisdom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershey_J/0/1/0/all/0/1\">John R. Hershey</a>",
          "description": "We introduce a state-of-the-art audio-visual on-screen sound separation\nsystem which is capable of learning to separate sounds and associate them with\non-screen objects by looking at in-the-wild videos. We identify limitations of\nprevious work on audiovisual on-screen sound separation, including the\nsimplicity and coarse resolution of spatio-temporal attention, and poor\nconvergence of the audio separation model. Our proposed model addresses these\nissues using cross-modal and self-attention modules that capture audio-visual\ndependencies at a finer resolution over time, and by unsupervised pre-training\nof audio separation model. These improvements allow the model to generalize to\na much wider set of unseen videos. For evaluation and semi-supervised training,\nwe collected human annotations of on-screen audio from a large database of\nin-the-wild videos (YFCC100M). Our results show marked improvements in\non-screen separation performance, in more general conditions than previous\nmethods.",
          "link": "http://arxiv.org/abs/2106.09669",
          "publishedOn": "2021-06-18T02:06:36.931Z",
          "wordCount": 581,
          "title": "Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention. (arXiv:2106.09669v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Edward J. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1\">Phillip Wallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shean Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>",
          "description": "The dominant paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, conventional fine-tuning, which\nretrains all model parameters, becomes less feasible. Using GPT-3 175B as an\nexample, deploying many independent instances of fine-tuned models, each with\n175B parameters, is extremely expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. For GPT-3,\nLoRA can reduce the number of trainable parameters by 10,000 times and the\ncomputation hardware requirement by 3 times compared to full fine-tuning. LoRA\nperforms on-par or better than fine-tuning in model quality on both GPT-3 and\nGPT-2, despite having fewer trainable parameters, a higher training throughput,\nand no additional inference latency. We also provide an empirical investigation\ninto rank-deficiency in language model adaptations, which sheds light on the\nefficacy of LoRA. We release our implementation in GPT-2 at\nhttps://github.com/microsoft/LoRA .",
          "link": "http://arxiv.org/abs/2106.09685",
          "publishedOn": "2021-06-18T02:06:36.924Z",
          "wordCount": 623,
          "title": "LoRA: Low-Rank Adaptation of Large Language Models. (arXiv:2106.09685v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">James Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balloch_J/0/1/0/all/0/1\">Jonathan Balloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "Modern computer vision applications suffer from catastrophic forgetting when\nincrementally learning new concepts over time. The most successful approaches\nto alleviate this forgetting require extensive replay of previously seen data,\nwhich is problematic when memory constraints or data legality concerns exist.\nIn this work, we consider the high-impact problem of Data-Free\nClass-Incremental Learning (DFCIL), where an incremental learning agent must\nlearn new concepts over time without storing generators or training data from\npast tasks. One approach for DFCIL is to replay synthetic images produced by\ninverting a frozen copy of the learner's classification model, but we show this\napproach fails for common class-incremental benchmarks when using standard\ndistillation strategies. We diagnose the cause of this failure and propose a\nnovel incremental distillation strategy for DFCIL, contributing a modified\ncross-entropy training and importance-weighted feature distillation, and show\nthat our method results in up to a 25.1% increase in final task accuracy\n(absolute difference) compared to SOTA DFCIL methods for common\nclass-incremental benchmarks. Our method even outperforms several standard\nreplay based methods which store a coreset of images.",
          "link": "http://arxiv.org/abs/2106.09701",
          "publishedOn": "2021-06-18T02:06:36.917Z",
          "wordCount": 624,
          "title": "Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning. (arXiv:2106.09701v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kaihua Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1\">Mingyuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>",
          "description": "Adversarial training is the de facto most promising defense against\nadversarial examples. Yet, its passive nature inevitably prevents it from being\nimmune to unknown attackers. To achieve a proactive defense, we need a more\nfundamental understanding of adversarial examples, beyond the popular bounded\nthreat model. In this paper, we provide a causal viewpoint of adversarial\nvulnerability: the cause is the confounder ubiquitously existing in learning,\nwhere attackers are precisely exploiting the confounding effect. Therefore, a\nfundamental solution for adversarial robustness is causal intervention. As the\nconfounder is unobserved in general, we propose to use the instrumental\nvariable that achieves intervention without the need for confounder\nobservation. We term our robust training method as Causal intervention by\ninstrumental Variable (CiiV). It has a differentiable retinotopic sampling\nlayer and a consistency loss, which is stable and guaranteed not to suffer from\ngradient obfuscation. Extensive experiments on a wide spectrum of attackers and\nsettings applied in MNIST, CIFAR-10, and mini-ImageNet datasets empirically\ndemonstrate that CiiV is robust to adaptive attacks.",
          "link": "http://arxiv.org/abs/2106.09534",
          "publishedOn": "2021-06-18T02:06:36.901Z",
          "wordCount": 609,
          "title": "Adversarial Visual Robustness by Causal Intervention. (arXiv:2106.09534v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1\">Ondrej Bohdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy Hospedales</a>",
          "description": "Calibration of neural networks is a topical problem that is becoming\nincreasingly important for real-world use of neural networks. The problem is\nespecially noticeable when using modern neural networks, for which there is\nsignificant difference between the model confidence and the confidence it\nshould have. Various strategies have been successfully proposed, yet there is\nmore space for improvements. We propose a novel approach that introduces a\ndifferentiable metric for expected calibration error and successfully uses it\nas an objective for meta-learning, achieving competitive results with\nstate-of-the-art approaches. Our approach presents a new direction of using\nmeta-learning to directly optimize model calibration, which we believe will\ninspire further work in this promising and new direction.",
          "link": "http://arxiv.org/abs/2106.09613",
          "publishedOn": "2021-06-18T02:06:36.895Z",
          "wordCount": 551,
          "title": "Meta-Calibration: Meta-Learning of Model Calibration Using Differentiable Expected Calibration Error. (arXiv:2106.09613v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09664",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Sen_J/0/1/0/all/0/1\">Jaydip Sen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mehtab_S/0/1/0/all/0/1\">Sidra Mehtab</a>",
          "description": "Building predictive models for robust and accurate prediction of stock prices\nand stock price movement is a challenging research problem to solve. The\nwell-known efficient market hypothesis believes in the impossibility of\naccurate prediction of future stock prices in an efficient stock market as the\nstock prices are assumed to be purely stochastic. However, numerous works\nproposed by researchers have demonstrated that it is possible to predict future\nstock prices with a high level of precision using sophisticated algorithms,\nmodel architectures, and the selection of appropriate variables in the models.\nThis chapter proposes a collection of predictive regression models built on\ndeep learning architecture for robust and precise prediction of the future\nprices of a stock listed in the diversified sectors in the National Stock\nExchange (NSE) of India. The Metastock tool is used to download the historical\nstock prices over a period of two years (2013- 2014) at 5 minutes intervals.\nWhile the records for the first year are used to train the models, the testing\nis carried out using the remaining records. The design approaches of all the\nmodels and their performance results are presented in detail. The models are\nalso compared based on their execution time and accuracy of prediction.",
          "link": "http://arxiv.org/abs/2106.09664",
          "publishedOn": "2021-06-18T02:06:36.889Z",
          "wordCount": 709,
          "title": "Design and Analysis of Robust Deep Learning Models for Stock Price Prediction. (arXiv:2106.09664v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Cheng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_V/0/1/0/all/0/1\">Vimukthini Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gamage_C/0/1/0/all/0/1\">Chathura Gamage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renz_J/0/1/0/all/0/1\">Jochen Renz</a>",
          "description": "Reasoning about the behaviour of physical objects is a key capability of\nagents operating in physical worlds. Humans are very experienced in physical\nreasoning while it remains a major challenge for AI. To facilitate research\naddressing this problem, several benchmarks have been proposed recently.\nHowever, these benchmarks do not enable us to measure an agent's granular\nphysical reasoning capabilities when solving a complex reasoning task. In this\npaper, we propose a new benchmark for physical reasoning that allows us to test\nindividual physical reasoning capabilities. Inspired by how humans acquire\nthese capabilities, we propose a general hierarchy of physical reasoning\ncapabilities with increasing complexity. Our benchmark tests capabilities\naccording to this hierarchy through generated physical reasoning tasks in the\nvideo game Angry Birds. This benchmark enables us to conduct a comprehensive\nagent evaluation by measuring the agent's granular physical reasoning\ncapabilities. We conduct an evaluation with human players, learning agents, and\nheuristic agents and determine their capabilities. Our evaluation shows that\nlearning agents, with good local generalization ability, still struggle to\nlearn the underlying physical reasoning capabilities and perform worse than\ncurrent state-of-the-art heuristic agents and humans. We believe that this\nbenchmark will encourage researchers to develop intelligent agents with\nadvanced, human-like physical reasoning capabilities. URL:\nhttps://github.com/Cheng-Xue/Hi-Phy",
          "link": "http://arxiv.org/abs/2106.09692",
          "publishedOn": "2021-06-18T02:06:36.882Z",
          "wordCount": 643,
          "title": "Hi-Phy: A Benchmark for Hierarchical Physical Reasoning. (arXiv:2106.09692v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark O. Riedl</a>",
          "description": "World models improve a learning agent's ability to efficiently operate in\ninteractive and situated environments. This work focuses on the task of\nbuilding world models of text-based game environments. Text-based games, or\ninteractive narratives, are reinforcement learning environments in which agents\nperceive and interact with the world using textual natural language. These\nenvironments contain long, multi-step puzzles or quests woven through a world\nthat is filled with hundreds of characters, locations, and objects. Our world\nmodel learns to simultaneously: (1) predict changes in the world caused by an\nagent's actions when representing the world as a knowledge graph; and (2)\ngenerate the set of contextually relevant natural language actions required to\noperate in the world. We frame this task as a Set of Sequences generation\nproblem by exploiting the inherent structure of knowledge graphs and actions\nand introduce both a transformer-based multi-task architecture and a loss\nfunction to train it. A zero-shot ablation study on never-before-seen textual\nworlds shows that our methodology significantly outperforms existing textual\nworld modeling techniques as well as the importance of each of our\ncontributions.",
          "link": "http://arxiv.org/abs/2106.09608",
          "publishedOn": "2021-06-18T02:06:36.874Z",
          "wordCount": 619,
          "title": "Learning Knowledge Graph-based World Models of Textual Environments. (arXiv:2106.09608v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1\">Lucas Caccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1\">Marc&#x27;Aurelio Ranzato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denoyer_L/0/1/0/all/0/1\">Ludovic Denoyer</a>",
          "description": "Classical machine learning frameworks assume access to a possibly large\ndataset in order to train a predictive model. In many practical applications\nhowever, data does not arrive all at once, but in batches over time. This\ncreates a natural trade-off between accuracy of a model and time to obtain such\na model. A greedy predictor could produce non-trivial predictions by\nimmediately training on batches as soon as these become available but, it may\nalso make sub-optimal use of future data. On the other hand, a tardy predictor\ncould wait for a long time to aggregate several batches into a larger dataset,\nbut ultimately deliver a much better performance. In this work, we consider\nsuch a streaming learning setting, which we dub {\\em anytime learning at\nmacroscale} (ALMA). It is an instance of anytime learning applied not at the\nlevel of a single chunk of data, but at the level of the entire sequence of\nlarge batches. We first formalize this learning setting, we then introduce\nmetrics to assess how well learners perform on the given task for a given\nmemory and compute budget, and finally we test several baseline approaches on\nstandard benchmarks repurposed for anytime learning at macroscale. The general\nfinding is that bigger models always generalize better. In particular, it is\nimportant to grow model capacity over time if the initial model is relatively\nsmall. Moreover, updating the model at an intermediate rate strikes the best\ntrade off between accuracy and time to obtain a useful predictor.",
          "link": "http://arxiv.org/abs/2106.09563",
          "publishedOn": "2021-06-18T02:06:36.856Z",
          "wordCount": 681,
          "title": "On Anytime Learning at Macroscale. (arXiv:2106.09563v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jingkai Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1\">John Wright</a>",
          "description": "We propose a new framework -- Square Root Principal Component Pursuit -- for\nlow-rank matrix recovery from observations corrupted with noise and outliers.\nInspired by the square root Lasso, this new formulation does not require prior\nknowledge of the noise level. We show that a single, universal choice of the\nregularization parameter suffices to achieve reconstruction error proportional\nto the (a priori unknown) noise level. In comparison, previous formulations\nsuch as stable PCP rely on noise-dependent parameters to achieve similar\nperformance, and are therefore challenging to deploy in applications where the\nnoise level is unknown. We validate the effectiveness of our new method through\nexperiments on simulated and real datasets. Our simulations corroborate the\nclaim that a universal choice of the regularization parameter yields near\noptimal performance across a range of noise levels, indicating that the\nproposed method outperforms the (somewhat loose) bound proved here.",
          "link": "http://arxiv.org/abs/2106.09211",
          "publishedOn": "2021-06-18T02:06:36.849Z",
          "wordCount": 588,
          "title": "Square Root Principal Component Pursuit: Tuning-Free Noisy Robust Matrix Recovery. (arXiv:2106.09211v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Donahue_K/0/1/0/all/0/1\">Kate Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1\">Jon Kleinberg</a>",
          "description": "Federated learning is a distributed learning paradigm where multiple agents,\neach only with access to local data, jointly learn a global model. There has\nrecently been an explosion of research aiming not only to improve the accuracy\nrates of federated learning, but also provide certain guarantees around social\ngood properties such as total error. One branch of this research has taken a\ngame-theoretic approach, and in particular, prior work has viewed federated\nlearning as a hedonic game, where error-minimizing players arrange themselves\ninto federating coalitions. This past work proves the existence of stable\ncoalition partitions, but leaves open a wide range of questions, including how\nfar from optimal these stable solutions are. In this work, we motivate and\ndefine a notion of optimality given by the average error rates among federating\nagents (players). First, we provide and prove the correctness of an efficient\nalgorithm to calculate an optimal (error minimizing) arrangement of players.\nNext, we analyze the relationship between the stability and optimality of an\narrangement. First, we show that for some regions of parameter space, all\nstable arrangements are optimal (Price of Anarchy equal to 1). However, we show\nthis is not true for all settings: there exist examples of stable arrangements\nwith higher cost than optimal (Price of Anarchy greater than 1). Finally, we\ngive the first constant-factor bound on the performance gap between stability\nand optimality, proving that the total error of the worst stable solution can\nbe no higher than 9 times the total error of an optimal solution (Price of\nAnarchy bound of 9).",
          "link": "http://arxiv.org/abs/2106.09580",
          "publishedOn": "2021-06-18T02:06:36.842Z",
          "wordCount": 710,
          "title": "Optimality and Stability in Federated Learning: A Game-theoretic Approach. (arXiv:2106.09580v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1\">Peiyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mengshu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuxuan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weiwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xulong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "Efficient deployment of Deep Neural Networks (DNNs) on edge devices (i.e.,\nFPGAs and mobile platforms) is very challenging, especially under a recent\nwitness of the increasing DNN model size and complexity. Although various\noptimization approaches have been proven to be effective in many DNNs on edge\ndevices, most state-of-the-art work focuses on ad-hoc optimizations, and there\nlacks a thorough study to comprehensively reveal the potentials and constraints\nof different edge devices when considering different optimizations. In this\npaper, we qualitatively and quantitatively compare the energy-efficiency of\nFPGA-based and mobile-based DNN executions, and provide detailed analysis.",
          "link": "http://arxiv.org/abs/2106.09166",
          "publishedOn": "2021-06-18T02:06:36.836Z",
          "wordCount": 580,
          "title": "Work in Progress: Mobile or FPGA? A Comprehensive Evaluation on Energy Efficiency and a Unified Optimization Framework. (arXiv:2106.09166v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_E/0/1/0/all/0/1\">Ernst Moritz Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1\">Mateo Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schewe_S/0/1/0/all/0/1\">Sven Schewe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somenzi_F/0/1/0/all/0/1\">Fabio Somenzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Ashutosh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojtczak_D/0/1/0/all/0/1\">Dominik Wojtczak</a>",
          "description": "Reinforcement learning synthesizes controllers without prior knowledge of the\nsystem. At each timestep, a reward is given. The controllers optimize the\ndiscounted sum of these rewards. Applying this class of algorithms requires\ndesigning a reward scheme, which is typically done manually. The designer must\nensure that their intent is accurately captured. This may not be trivial, and\nis prone to error. An alternative to this manual programming, akin to\nprogramming directly in assembly, is to specify the objective in a formal\nlanguage and have it \"compiled\" to a reward scheme. Mungojerrie\n($\\href{https://plv.colorado.edu/mungojerrie/}{plv.colorado.edu/mungojerrie}$)\nis a tool for testing reward schemes for $\\omega$-regular objectives on finite\nmodels. The tool contains reinforcement learning algorithms and a probabilistic\nmodel checker. Mungojerrie supports models specified in PRISM and\n$\\omega$-automata specified in HOA.",
          "link": "http://arxiv.org/abs/2106.09161",
          "publishedOn": "2021-06-18T02:06:36.830Z",
          "wordCount": 583,
          "title": "Mungojerrie: Reinforcement Learning of Linear-Time Objectives. (arXiv:2106.09161v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kelner_J/0/1/0/all/0/1\">Jonathan Kelner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1\">Frederic Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meka_R/0/1/0/all/0/1\">Raghu Meka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohatgi_D/0/1/0/all/0/1\">Dhruv Rohatgi</a>",
          "description": "Sparse linear regression is a fundamental problem in high-dimensional\nstatistics, but strikingly little is known about how to efficiently solve it\nwithout restrictive conditions on the design matrix. We consider the\n(correlated) random design setting, where the covariates are independently\ndrawn from a multivariate Gaussian $N(0,\\Sigma)$ with $\\Sigma : n \\times n$,\nand seek estimators $\\hat{w}$ minimizing $(\\hat{w}-w^*)^T\\Sigma(\\hat{w}-w^*)$,\nwhere $w^*$ is the $k$-sparse ground truth. Information theoretically, one can\nachieve strong error bounds with $O(k \\log n)$ samples for arbitrary $\\Sigma$\nand $w^*$; however, no efficient algorithms are known to match these guarantees\neven with $o(n)$ samples, without further assumptions on $\\Sigma$ or $w^*$. As\nfar as hardness, computational lower bounds are only known with worst-case\ndesign matrices. Random-design instances are known which are hard for the\nLasso, but these instances can generally be solved by Lasso after a simple\nchange-of-basis (i.e. preconditioning).\n\nIn this work, we give upper and lower bounds clarifying the power of\npreconditioning in sparse linear regression. First, we show that the\npreconditioned Lasso can solve a large class of sparse linear regression\nproblems nearly optimally: it succeeds whenever the dependency structure of the\ncovariates, in the sense of the Markov property, has low treewidth -- even if\n$\\Sigma$ is highly ill-conditioned. Second, we construct (for the first time)\nrandom-design instances which are provably hard for an optimally preconditioned\nLasso. In fact, we complete our treewidth classification by proving that for\nany treewidth-$t$ graph, there exists a Gaussian Markov Random Field on this\ngraph such that the preconditioned Lasso, with any choice of preconditioner,\nrequires $\\Omega(t^{1/20})$ samples to recover $O(\\log n)$-sparse signals when\ncovariates are drawn from this model.",
          "link": "http://arxiv.org/abs/2106.09207",
          "publishedOn": "2021-06-18T02:06:36.813Z",
          "wordCount": 727,
          "title": "On the Power of Preconditioning in Sparse Linear Regression. (arXiv:2106.09207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haoyun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1\">Evgeny Kharlamov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>",
          "description": "Entity alignment, aiming to identify equivalent entities across different\nknowledge graphs (KGs), is a fundamental problem for constructing large-scale\nKGs. Over the course of its development, supervision has been considered\nnecessary for accurate alignments. Inspired by the recent progress of\nself-supervised learning, we explore the extent to which we can get rid of\nsupervision for entity alignment. Existing supervised methods for this task\nfocus on pulling each pair of positive (labeled) entities close to each other.\nHowever, our analysis suggests that the learning of entity alignment can\nactually benefit more from pushing sampled (unlabeled) negatives far away than\npulling positive aligned pairs close. We present SelfKG by leveraging this\ndiscovery to design a contrastive learning strategy across two KGs. Extensive\nexperiments on benchmark datasets demonstrate that SelfKG without supervision\ncan match or achieve comparable results with state-of-the-art supervised\nbaselines. The performance of SelfKG demonstrates self-supervised learning\noffers great potential for entity alignment in KGs.",
          "link": "http://arxiv.org/abs/2106.09395",
          "publishedOn": "2021-06-18T02:06:36.806Z",
          "wordCount": 595,
          "title": "A Self-supervised Method for Entity Alignment. (arXiv:2106.09395v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1\">Jordan T. Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Surbhi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Akshay Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham Kakade</a>",
          "description": "There is an increasing need for effective active learning algorithms that are\ncompatible with deep neural networks. While there are many classic,\nwell-studied sample selection methods, the non-convexity and varying internal\nrepresentation of neural models make it unclear how to extend these approaches.\nThis article introduces BAIT, a practical, tractable, and high-performing\nactive learning algorithm for neural networks that addresses these concerns.\nBAIT draws inspiration from the theoretical analysis of maximum likelihood\nestimators (MLE) for parametric models. It selects batches of samples by\noptimizing a bound on the MLE error in terms of the Fisher information, which\nwe show can be implemented efficiently at scale by exploiting linear-algebraic\nstructure especially amenable to execution on modern hardware. Our experiments\nshow that BAIT outperforms the previous state of the art on both classification\nand regression problems, and is flexible enough to be used with a variety of\nmodel architectures.",
          "link": "http://arxiv.org/abs/2106.09675",
          "publishedOn": "2021-06-18T02:06:36.799Z",
          "wordCount": 578,
          "title": "Gone Fishing: Neural Active Learning with Fisher Embeddings. (arXiv:2106.09675v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhe Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liqian Peng</a>",
          "description": "Although projection-based reduced-order models (ROMs) for parameterized\nnonlinear dynamical systems have demonstrated exciting results across a range\nof applications, their broad adoption has been limited by their intrusivity:\nimplementing such a reduced-order model typically requires significant\nmodifications to the underlying simulation code. To address this, we propose a\nmethod that enables traditionally intrusive reduced-order models to be\naccurately approximated in a non-intrusive manner. Specifically, the approach\napproximates the low-dimensional operators associated with projection-based\nreduced-order models (ROMs) using modern machine-learning regression\ntechniques. The only requirement of the simulation code is the ability to\nexport the velocity given the state and parameters as this functionality is\nused to train the approximated low-dimensional operators. In addition to\nenabling nonintrusivity, we demonstrate that the approach also leads to very\nlow computational complexity, achieving up to $1000\\times$ reduction in run\ntime. We demonstrate the effectiveness of the proposed technique on two types\nof PDEs.",
          "link": "http://arxiv.org/abs/2106.09658",
          "publishedOn": "2021-06-18T02:06:36.791Z",
          "wordCount": 595,
          "title": "Non-intrusive Nonlinear Model Reduction via Machine Learning Approximations to Low-dimensional Operators. (arXiv:2106.09658v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shuaibi_M/0/1/0/all/0/1\">Muhammed Shuaibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolluru_A/0/1/0/all/0/1\">Adeesh Kolluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Abhishek Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Anuroop Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulissi_Z/0/1/0/all/0/1\">Zachary Ulissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnick_C/0/1/0/all/0/1\">C. Lawrence Zitnick</a>",
          "description": "Progress towards the energy breakthroughs needed to combat climate change can\nbe significantly accelerated through the efficient simulation of atomic\nsystems. Simulation techniques based on first principles, such as Density\nFunctional Theory (DFT), are limited in their practical use due to their high\ncomputational expense. Machine learning approaches have the potential to\napproximate DFT in a computationally efficient manner, which could dramatically\nincrease the impact of computational simulations on real-world problems.\nApproximating DFT poses several challenges. These include accurately modeling\nthe subtle changes in the relative positions and angles between atoms, and\nenforcing constraints such as rotation invariance or energy conservation. We\nintroduce a novel approach to modeling angular information between sets of\nneighboring atoms in a graph neural network. Rotation invariance is achieved\nfor the network's edge messages through the use of a per-edge local coordinate\nframe and a novel spin convolution over the remaining degree of freedom. Two\nmodel variants are proposed for the applications of structure relaxation and\nmolecular dynamics. State-of-the-art results are demonstrated on the\nlarge-scale Open Catalyst 2020 dataset. Comparisons are also performed on the\nMD17 and QM9 datasets.",
          "link": "http://arxiv.org/abs/2106.09575",
          "publishedOn": "2021-06-18T02:06:36.785Z",
          "wordCount": 636,
          "title": "Rotation Invariant Graph Neural Networks using Spin Convolutions. (arXiv:2106.09575v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seyedi_S/0/1/0/all/0/1\">Salman Seyedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levey_A/0/1/0/all/0/1\">Allan Levey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1\">Gari D. Clifford</a>",
          "description": "The expanding usage of complex machine learning methods like deep learning\nhas led to an explosion in human activity recognition, particularly applied to\nhealth. In particular, as part of a larger body sensor network system, face and\nfull-body analysis is becoming increasingly common for evaluating health\nstatus. However, complex models which handle private and sometimes protected\ndata, raise concerns about the potential leak of identifiable data. In this\nwork, we focus on the case of a deep network model trained on images of\nindividual faces. Full-face video recordings taken from 493 individuals\nundergoing an eye-tracking based evaluation of neurological function were used.\nOutputs, gradients, intermediate layer outputs, loss, and labels were used as\ninputs for a deep network with an added support vector machine emission layer\nto recognize membership in the training data. The inference attack method and\nassociated mathematical analysis indicate that there is a low likelihood of\nunintended memorization of facial features in the deep learning model. In this\nstudy, it is showed that the named model preserves the integrity of training\ndata with reasonable confidence. The same process can be implemented in similar\nconditions for different models.",
          "link": "http://arxiv.org/abs/2106.09621",
          "publishedOn": "2021-06-18T02:06:36.768Z",
          "wordCount": 624,
          "title": "Privacy-Preserving Eye-tracking Using Deep Learning. (arXiv:2106.09621v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zi-Yuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuojia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>",
          "description": "Graph-level representations are critical in various real-world applications,\nsuch as predicting the properties of molecules. But in practice, precise graph\nannotations are generally very expensive and time-consuming. To address this\nissue, graph contrastive learning constructs instance discrimination task which\npulls together positive pairs (augmentation pairs of the same graph) and pushes\naway negative pairs (augmentation pairs of different graphs) for unsupervised\nrepresentation learning. However, since for a query, its negatives are\nuniformly sampled from all graphs, existing methods suffer from the critical\nsampling bias issue, i.e., the negatives likely having the same semantic\nstructure with the query, leading to performance degradation. To mitigate this\nsampling bias issue, in this paper, we propose a Prototypical Graph Contrastive\nLearning (PGCL) approach. Specifically, PGCL models the underlying semantic\nstructure of the graph data via clustering semantically similar graphs into the\nsame group, and simultaneously encourages the clustering consistency for\ndifferent augmentations of the same graph. Then given a query, it performs\nnegative sampling via drawing the graphs from those clusters that differ from\nthe cluster of query, which ensures the semantic difference between query and\nits negative samples. Moreover, for a query, PGCL further reweights its\nnegative samples based on the distance between their prototypes (cluster\ncentroids) and the query prototype such that those negatives having moderate\nprototype distance enjoy relatively large weights. This reweighting strategy is\nproved to be more effective than uniform sampling. Experimental results on\nvarious graph benchmarks testify the advantages of our PGCL over\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.09645",
          "publishedOn": "2021-06-18T02:06:36.763Z",
          "wordCount": 682,
          "title": "Prototypical Graph Contrastive Learning. (arXiv:2106.09645v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Som_A/0/1/0/all/0/1\">Anirudh Som</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sujeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Prado_B/0/1/0/all/0/1\">Bladimir Lopez-Prado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamija_S/0/1/0/all/0/1\">Svati Dhamija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alozie_N/0/1/0/all/0/1\">Nonye Alozie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamrakar_A/0/1/0/all/0/1\">Amir Tamrakar</a>",
          "description": "Collaboration is identified as a required and necessary skill for students to\nbe successful in the fields of Science, Technology, Engineering and Mathematics\n(STEM). However, due to growing student population and limited teaching staff\nit is difficult for teachers to provide constructive feedback and instill\ncollaborative skills using instructional methods. Development of simple and\neasily explainable machine-learning-based automated systems can help address\nthis problem. Improving upon our previous work, in this paper we propose using\nsimple temporal-CNN deep-learning models to assess student group collaboration\nthat take in temporal representations of individual student roles as input. We\ncheck the applicability of dynamically changing feature representations for\nstudent group collaboration assessment and how they impact the overall\nperformance. We also use Grad-CAM visualizations to better understand and\ninterpret the important temporal indices that led to the deep-learning model's\ndecision.",
          "link": "http://arxiv.org/abs/2106.09623",
          "publishedOn": "2021-06-18T02:06:36.756Z",
          "wordCount": 597,
          "title": "Towards Explainable Student Group Collaboration Assessment Models Using Temporal Representations of Individual Student Roles. (arXiv:2106.09623v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09667",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terzis_A/0/1/0/all/0/1\">Andreas Terzis</a>",
          "description": "Contrastive learning methods like CLIP train on noisy and uncurated training\ndatasets. This is cheaper than labeling datasets manually, and even improves\nout-of-distribution robustness. We show that this practice makes backdoor and\npoisoning attacks a significant threat. By poisoning just 0.005% of a dataset\n(e.g., just 150 images of the 3 million-example Conceptual Captions dataset),\nwe can cause the model to misclassify test images by overlaying a small patch.\nTargeted poisoning attacks, whereby the model misclassifies a particular test\ninput with an adversarially-desired label, are even easier requiring control of\nless than 0.0001% of the dataset (e.g., just two out of the 3 million images).\nOur attacks call into question whether training on noisy and uncurated Internet\nscrapes is desirable.",
          "link": "http://arxiv.org/abs/2106.09667",
          "publishedOn": "2021-06-18T02:06:36.749Z",
          "wordCount": 540,
          "title": "Poisoning and Backdooring Contrastive Learning. (arXiv:2106.09667v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1\">Jerret Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belgodere_B/0/1/0/all/0/1\">Brian Belgodere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1\">Vijil Chenthamarakshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1\">Inkit Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1\">Youssef Mroueh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>",
          "description": "Predicting chemical properties from the structure of a molecule is of great\nimportance in many applications including drug discovery and material design.\nMachine learning based molecular property prediction holds the promise of\nenabling accurate predictions at much less complexity, when compared to, for\nexample Density Functional Theory (DFT) calculations. Features extracted from\nmolecular graphs, using graph neural nets in a supervised manner, have emerged\nas strong baselines for such tasks. However, the vast chemical space together\nwith the limited availability of labels makes supervised learning challenging,\ncalling for learning a general-purpose molecular representation. Recently,\npre-trained transformer-based language models (PTLMs) on large unlabeled corpus\nhave produced state-of-the-art results in many downstream natural language\nprocessing tasks. Inspired by this development, here we present molecular\nembeddings obtained by training an efficient transformer encoder model,\nreferred to as MoLFormer. This model was employed with a linear attention\nmechanism and highly paralleized training on 1D SMILES sequences of 1.1 billion\nunlabeled molecules from the PubChem and ZINC datasets. Experiments show that\nthe learned molecular representation performs competitively, when compared to\nexisting graph-based and fingerprint-based supervised learning baselines, on\nthe challenging tasks of predicting properties of QM8 and QM9 molecules.\nFurther task-specific fine-tuning of the MoLFormerr representation improves\nperformance on several of those property prediction benchmarks. These results\nprovide encouraging evidence that large-scale molecular language models can\ncapture sufficient structural information to be able to accurately predict\nquantum chemical properties and beyond.",
          "link": "http://arxiv.org/abs/2106.09553",
          "publishedOn": "2021-06-18T02:06:36.743Z",
          "wordCount": 688,
          "title": "Do Large Scale Molecular Language Representations Capture Important Structural Information?. (arXiv:2106.09553v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomes_H/0/1/0/all/0/1\">Heitor Murilo Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzenda_M/0/1/0/all/0/1\">Maciej Grzenda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_R/0/1/0/all/0/1\">Rodrigo Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1\">Jesse Read</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Huong Le Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bifet_A/0/1/0/all/0/1\">Albert Bifet</a>",
          "description": "Unlabelled data appear in many domains and are particularly relevant to\nstreaming applications, where even though data is abundant, labelled data is\nrare. To address the learning problems associated with such data, one can\nignore the unlabelled data and focus only on the labelled data (supervised\nlearning); use the labelled data and attempt to leverage the unlabelled data\n(semi-supervised learning); or assume some labels will be available on request\n(active learning). The first approach is the simplest, yet the amount of\nlabelled data available will limit the predictive performance. The second\nrelies on finding and exploiting the underlying characteristics of the data\ndistribution. The third depends on an external agent to provide the required\nlabels in a timely fashion. This survey pays special attention to methods that\nleverage unlabelled data in a semi-supervised setting. We also discuss the\ndelayed labelling issue, which impacts both fully supervised and\nsemi-supervised methods. We propose a unified problem setting, discuss the\nlearning guarantees and existing methods, explain the differences between\nrelated problem settings. Finally, we review the current benchmarking practices\nand propose adaptations to enhance them.",
          "link": "http://arxiv.org/abs/2106.09170",
          "publishedOn": "2021-06-18T02:06:36.731Z",
          "wordCount": 626,
          "title": "A Survey on Semi-Supervised Learning for Delayed Partially Labelled Data Streams. (arXiv:2106.09170v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09222",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dia_O/0/1/0/all/0/1\">Ousmane Amadou Dia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karaletsos_T/0/1/0/all/0/1\">Theofanis Karaletsos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hazirbas_C/0/1/0/all/0/1\">Caner Hazirbas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kabul_I/0/1/0/all/0/1\">Ilknur Kaynar Kabul</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meijer_E/0/1/0/all/0/1\">Erik Meijer</a>",
          "description": "The susceptibility of deep learning models to adversarial perturbations has\nstirred renewed attention in adversarial examples resulting in a number of\nattacks. However, most of these attacks fail to encompass a large spectrum of\nadversarial perturbations that are imperceptible to humans. In this paper, we\npresent localized uncertainty attacks, a novel class of threat models against\ndeterministic and stochastic classifiers. Under this threat model, we create\nadversarial examples by perturbing only regions in the inputs where a\nclassifier is uncertain. To find such regions, we utilize the predictive\nuncertainty of the classifier when the classifier is stochastic or, we learn a\nsurrogate model to amortize the uncertainty when it is deterministic. Unlike\n$\\ell_p$ ball or functional attacks which perturb inputs indiscriminately, our\ntargeted changes can be less perceptible. When considered under our threat\nmodel, these attacks still produce strong adversarial examples; with the\nexamples retaining a greater degree of similarity with the inputs.",
          "link": "http://arxiv.org/abs/2106.09222",
          "publishedOn": "2021-06-18T02:06:36.713Z",
          "wordCount": 607,
          "title": "Localized Uncertainty Attacks. (arXiv:2106.09222v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09660",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_N/0/1/0/all/0/1\">Nanxin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zen_H/0/1/0/all/0/1\">Heiga Zen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weiss_R/0/1/0/all/0/1\">Ron J. Weiss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dehak_N/0/1/0/all/0/1\">Najim Dehak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>",
          "description": "This paper introduces WaveGrad 2, a non-autoregressive generative model for\ntext-to-speech synthesis. WaveGrad 2 is trained to estimate the gradient of the\nlog conditional density of the waveform given a phoneme sequence. The model\ntakes an input phoneme sequence, and through an iterative refinement process,\ngenerates an audio waveform. This contrasts to the original WaveGrad vocoder\nwhich conditions on mel-spectrogram features, generated by a separate model.\nThe iterative refinement process starts from Gaussian noise, and through a\nseries of refinement steps (e.g., 50 steps), progressively recovers the audio\nsequence. WaveGrad 2 offers a natural way to trade-off between inference speed\nand sample quality, through adjusting the number of refinement steps.\nExperiments show that the model can generate high fidelity audio, approaching\nthe performance of a state-of-the-art neural TTS system. We also report various\nablation studies over different model configurations. Audio samples are\navailable at https://wavegrad.github.io/v2.",
          "link": "http://arxiv.org/abs/2106.09660",
          "publishedOn": "2021-06-18T02:06:36.706Z",
          "wordCount": 658,
          "title": "WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis. (arXiv:2106.09660v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09473",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Sutera_A/0/1/0/all/0/1\">Antonio Sutera</a>",
          "description": "Nowadays new technologies, and especially artificial intelligence, are more\nand more established in our society. Big data analysis and machine learning,\ntwo sub-fields of artificial intelligence, are at the core of many recent\nbreakthroughs in many application fields (e.g., medicine, communication,\nfinance, ...), including some that are strongly related to our day-to-day life\n(e.g., social networks, computers, smartphones, ...). In machine learning,\nsignificant improvements are usually achieved at the price of an increasing\ncomputational complexity and thanks to bigger datasets. Currently, cutting-edge\nmodels built by the most advanced machine learning algorithms typically became\nsimultaneously very efficient and profitable but also extremely complex. Their\ncomplexity is to such an extent that these models are commonly seen as\nblack-boxes providing a prediction or a decision which can not be interpreted\nor justified. Nevertheless, whether these models are used autonomously or as a\nsimple decision-making support tool, they are already being used in machine\nlearning applications where health and human life are at stake. Therefore, it\nappears to be an obvious necessity not to blindly believe everything coming out\nof those models without a detailed understanding of their predictions or\ndecisions. Accordingly, this thesis aims at improving the interpretability of\nmodels built by a specific family of machine learning algorithms, the so-called\ntree-based methods. Several mechanisms have been proposed to interpret these\nmodels and we aim along this thesis to improve their understanding, study their\nproperties, and define their limitations.",
          "link": "http://arxiv.org/abs/2106.09473",
          "publishedOn": "2021-06-18T02:06:36.699Z",
          "wordCount": 679,
          "title": "Importance measures derived from random forests: characterisation and extension. (arXiv:2106.09473v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rangwani_H/0/1/0/all/0/1\">Harsh Rangwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1\">Konda Reddy Mopuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>",
          "description": "Generative Adversarial Networks (GANs) have swiftly evolved to imitate\nincreasingly complex image distributions. However, majority of the developments\nfocus on performance of GANs on balanced datasets. We find that the existing\nGANs and their training regimes which work well on balanced datasets fail to be\neffective in case of imbalanced (i.e. long-tailed) datasets. In this work we\nintroduce a novel theoretically motivated Class Balancing regularizer for\ntraining GANs. Our regularizer makes use of the knowledge from a pre-trained\nclassifier to ensure balanced learning of all the classes in the dataset. This\nis achieved via modelling the effective class frequency based on the\nexponential forgetting observed in neural networks and encouraging the GAN to\nfocus on underrepresented classes. We demonstrate the utility of our\nregularizer in learning representations for long-tailed distributions via\nachieving better performance than existing approaches over multiple datasets.\nSpecifically, when applied to an unconditional GAN, it improves the FID from\n$13.03$ to $9.01$ on the long-tailed iNaturalist-$2019$ dataset.",
          "link": "http://arxiv.org/abs/2106.09402",
          "publishedOn": "2021-06-18T02:06:36.681Z",
          "wordCount": 606,
          "title": "Class Balancing GAN with a Classifier in the Loop. (arXiv:2106.09402v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09512",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Schulz_B/0/1/0/all/0/1\">Benedikt Schulz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lerch_S/0/1/0/all/0/1\">Sebastian Lerch</a>",
          "description": "Postprocessing ensemble weather predictions to correct systematic errors has\nbecome a standard practice in research and operations. However, only few recent\nstudies have focused on ensemble postprocessing of wind gust forecasts, despite\nits importance for severe weather warnings. Here, we provide a comprehensive\nreview and systematic comparison of eight statistical and machine learning\nmethods for probabilistic wind gust forecasting via ensemble postprocessing,\nthat can be divided in three groups: State of the art postprocessing techniques\nfrom statistics (ensemble model output statistics (EMOS), member-by-member\npostprocessing, isotonic distributional regression), established machine\nlearning methods (gradient-boosting extended EMOS, quantile regression forests)\nand neural network-based approaches (distributional regression network,\nBernstein quantile network, histogram estimation network). The methods are\nsystematically compared using six years of data from a high-resolution,\nconvection-permitting ensemble prediction system that was run operationally at\nthe German weather service, and hourly observations at 175 surface weather\nstations in Germany. While all postprocessing methods yield calibrated\nforecasts and are able to correct the systematic errors of the raw ensemble\npredictions, incorporating information from additional meteorological predictor\nvariables beyond wind gusts leads to significant improvements in forecast\nskill. In particular, we propose a flexible framework of locally adaptive\nneural networks with different probabilistic forecast types as output, which\nnot only significantly outperform all benchmark postprocessing methods but also\nlearn physically consistent relations associated with the diurnal cycle,\nespecially the evening transition of the planetary boundary layer.",
          "link": "http://arxiv.org/abs/2106.09512",
          "publishedOn": "2021-06-18T02:06:36.675Z",
          "wordCount": 681,
          "title": "Machine learning methods for postprocessing ensemble forecasts of wind gusts: A systematic comparison. (arXiv:2106.09512v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slowik_A/0/1/0/all/0/1\">Agnieszka S&#x142;owik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bottou_L/0/1/0/all/0/1\">L&#xe9;on Bottou</a>",
          "description": "Machine learning systems based on minimizing average error have been shown to\nperform inconsistently across notable subsets of the data, which is not exposed\nby a low average error for the entire dataset. In consequential social and\neconomic applications, where data represent people, this can lead to\ndiscrimination of underrepresented gender and ethnic groups. Given the\nimportance of bias mitigation in machine learning, the topic leads to\ncontentious debates on how to ensure fairness in practice (data bias versus\nalgorithmic bias). Distributionally Robust Optimization (DRO) seemingly\naddresses this problem by minimizing the worst expected risk across\nsubpopulations. We establish theoretical results that clarify the relation\nbetween DRO and the optimization of the same loss averaged on an adequately\nweighted training dataset. The results cover finite and infinite number of\ntraining distributions, as well as convex and non-convex loss functions. We\nshow that neither DRO nor curating the training set should be construed as a\ncomplete solution for bias mitigation: in the same way that there is no\nuniversally robust training set, there is no universal way to setup a DRO\nproblem and ensure a socially acceptable set of results. We then leverage these\ninsights to provide a mininal set of practical recommendations for addressing\nbias with DRO. Finally, we discuss ramifications of our results in other\nrelated applications of DRO, using an example of adversarial robustness. Our\nresults show that there is merit to both the algorithm-focused and the\ndata-focused side of the bias debate, as long as arguments in favor of these\npositions are precisely qualified and backed by relevant mathematics known\ntoday.",
          "link": "http://arxiv.org/abs/2106.09467",
          "publishedOn": "2021-06-18T02:06:36.667Z",
          "wordCount": 710,
          "title": "Algorithmic Bias and Data Bias: Understanding the Relation between Distributionally Robust Optimization and Data Curation. (arXiv:2106.09467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1\">Ravi Shankar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1\">Kartik Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasiwasia_N/0/1/0/all/0/1\">Nikhil Rasiwasia</a>",
          "description": "In this paper, we present SANTA, a scalable framework to automatically\nnormalize E-commerce attribute values (e.g. \"Win 10 Pro\") to a fixed set of\npre-defined canonical values (e.g. \"Windows 10\"). Earlier works on attribute\nnormalization focused on fuzzy string matching (also referred as syntactic\nmatching in this paper). In this work, we first perform an extensive study of\nnine syntactic matching algorithms and establish that 'cosine' similarity leads\nto best results, showing 2.7% improvement over commonly used Jaccard index.\nNext, we argue that string similarity alone is not sufficient for attribute\nnormalization as many surface forms require going beyond syntactic matching\n(e.g. \"720p\" and \"HD\" are synonyms). While semantic techniques like\nunsupervised embeddings (e.g. word2vec/fastText) have shown good results in\nword similarity tasks, we observed that they perform poorly to distinguish\nbetween close canonical forms, as these close forms often occur in similar\ncontexts. We propose to learn token embeddings using a twin network with\ntriplet loss. We propose an embedding learning task leveraging raw attribute\nvalues and product titles to learn these embeddings in a self-supervised\nfashion. We show that providing supervision using our proposed task improves\nover both syntactic and unsupervised embeddings based techniques for attribute\nnormalization. Experiments on a real-world attribute normalization dataset of\n50 attributes show that the embeddings trained using our proposed approach\nobtain 2.3% improvement over best string matching and 19.3% improvement over\nbest unsupervised embeddings.",
          "link": "http://arxiv.org/abs/2106.09493",
          "publishedOn": "2021-06-18T02:06:36.659Z",
          "wordCount": 678,
          "title": "Scalable Approach for Normalizing E-commerce Text Attributes (SANTA). (arXiv:2106.09493v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09532",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shenoy_A/0/1/0/all/0/1\">Ashish Shenoy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>",
          "description": "Automatic Speech Recognition (ASR) robustness toward slot entities are\ncritical in e-commerce voice assistants that involve monetary transactions and\npurchases. Along with effective domain adaptation, it is intuitive that cross\nutterance contextual cues play an important role in disambiguating domain\nspecific content words from speech. In this paper, we investigate various\ntechniques to improve contextualization, content word robustness and domain\nadaptation of a Transformer-XL neural language model (NLM) to rescore ASR\nN-best hypotheses. To improve contextualization, we utilize turn level dialogue\nacts along with cross utterance context carry over. Additionally, to adapt our\ndomain-general NLM towards e-commerce on-the-fly, we use embeddings derived\nfrom a finetuned masked LM on in-domain data. Finally, to improve robustness\ntowards in-domain content words, we propose a multi-task model that can jointly\nperform content word detection and language modeling tasks. Compared to a\nnon-contextual LSTM LM baseline, our best performing NLM rescorer results in a\ncontent WER reduction of 19.2% on e-commerce audio test set and a slot labeling\nF1 improvement of 6.4%.",
          "link": "http://arxiv.org/abs/2106.09532",
          "publishedOn": "2021-06-18T02:06:36.653Z",
          "wordCount": 633,
          "title": "ASR Adaptation for E-commerce Chatbots using Cross-Utterance Context and Multi-Task Language Modeling. (arXiv:2106.09532v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09488",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elibol_O/0/1/0/all/0/1\">Oguz Elibol</a>",
          "description": "There is a recent trend in machine learning to increase model quality by\ngrowing models to sizes previously thought to be unreasonable. Recent work has\nshown that autoregressive generative models with cross-entropy objective\nfunctions exhibit smooth power-law relationships, or scaling laws, that predict\nmodel quality from model size, training set size, and the available compute\nbudget. These scaling laws allow one to choose nearly optimal hyper-parameters\ngiven constraints on available training data, model parameter count, or\ntraining computation budget. In this paper, we demonstrate that acoustic models\ntrained with an auto-predictive coding loss behave as if they are subject to\nsimilar scaling laws. We extend previous work to jointly predict loss due to\nmodel size, to training set size, and to the inherent \"irreducible loss\" of the\ntask. We find that the scaling laws accurately match model performance over two\norders of magnitude in both model size and training set size, and make\npredictions about the limits of model performance.",
          "link": "http://arxiv.org/abs/2106.09488",
          "publishedOn": "2021-06-18T02:06:36.643Z",
          "wordCount": 606,
          "title": "Scaling Laws for Acoustic Models. (arXiv:2106.09488v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saputra_Y/0/1/0/all/0/1\">Yuris Mulya Saputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Diep N. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1\">Dinh Thai Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutkiewicz_E/0/1/0/all/0/1\">Eryk Dutkiewicz</a>",
          "description": "By encoding computing tasks, coded computing can not only mitigate straggling\nproblems in federated learning (FL), but also preserve privacy of sensitive\ndata uploaded/contributed by participating mobile users (MUs) to the\ncentralized server, owned by a mobile application provider (MAP). However,\nthese advantages come with extra coding cost/complexity and communication\noverhead (referred to as \\emph{privacy cost}) that must be considered given the\nlimited computing/communications resources at MUs/MAP, the rationality and\nincentive competition among MUs in contributing data to the MAP. This article\nproposes a novel coded FL-based framework for a privacy-aware mobile\napplication service to address these challenges. In particular, the MAP first\ndetermines a set of the best MUs for the FL process based on MUs' provided\ninformation/features. Then, each selected MU can propose a contract to the MAP\naccording to its expected trainable local data and privacy-protected coded\ndata. To find the optimal contracts that can maximize utilities of the MAP and\nall the participating MUs while maintaining high learning quality of the whole\nsystem, we first develop a multi-principal one-agent contract-based problem\nleveraging coded FL-based multiple utility functions under the MUs' privacy\ncost, the MAP's limited computing resource, and asymmetric information between\nthe MAP and MUs. Then, we transform the problem into an equivalent\nlow-complexity problem and develop an iterative algorithm to solve it.\nExperiments with a real-world dataset show that our framework can speed up\ntraining time up to 49% and improve prediction accuracy up to 4.6 times while\nenhancing network's social welfare, i.e., total utility of all participating\nentities, up to 114% under the privacy cost consideration compared with those\nof baseline methods.",
          "link": "http://arxiv.org/abs/2106.09261",
          "publishedOn": "2021-06-18T02:06:36.638Z",
          "wordCount": 724,
          "title": "Coded Federated Learning Framework for AI-Based Mobile Application Services with Privacy-Awareness. (arXiv:2106.09261v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_I/0/1/0/all/0/1\">I-Chung Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng-Te Li</a>",
          "description": "Attributed network embedding (ANE) is to learn low-dimensional vectors so\nthat not only the network structure but also node attributes can be preserved\nin the embedding space. Existing ANE models do not consider the specific\ncombination between graph structure and attributes. While each node has its\nstructural characteristics, such as highly-interconnected neighbors along with\ntheir certain patterns of attribute distribution, each node's neighborhood\nshould be not only depicted by multi-hop nodes, but consider certain clusters\nor social circles. To model such information, in this paper, we propose a novel\nANE model, Context Co-occurrence-aware Attributed Network Embedding (CoANE).\nThe basic idea of CoANE is to model the context attributes that each node's\ninvolved diverse patterns, and apply the convolutional mechanism to encode\npositional information by treating each attribute as a channel. The learning of\ncontext co-occurrence can capture the latent social circles of each node. To\nbetter encode structural and semantic knowledge of nodes, we devise a three-way\nobjective function, consisting of positive graph likelihood, contextual\nnegative sampling, and attribute reconstruction. We conduct experiments on five\nreal datasets in the tasks of link prediction, node label classification, and\nnode clustering. The results exhibit that CoANE can significantly outperform\nstate-of-the-art ANE models.",
          "link": "http://arxiv.org/abs/2106.09241",
          "publishedOn": "2021-06-18T02:06:36.631Z",
          "wordCount": 652,
          "title": "CoANE: Modeling Context Co-occurrence for Attributed Network Embedding. (arXiv:2106.09241v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_A/0/1/0/all/0/1\">Andac Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koike_Akino_T/0/1/0/all/0/1\">Toshiaki Koike-Akino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haruna_M/0/1/0/all/0/1\">Masaki Haruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1\">Deniz Erdogmus</a>",
          "description": "Convolutional neural networks (CNN) have been frequently used to extract\nsubject-invariant features from electroencephalogram (EEG) for classification\ntasks. This approach holds the underlying assumption that electrodes are\nequidistant analogous to pixels of an image and hence fails to explore/exploit\nthe complex functional neural connectivity between different electrode sites.\nWe overcome this limitation by tailoring the concepts of convolution and\npooling applied to 2D grid-like inputs for the functional network of electrode\nsites. Furthermore, we develop various graph neural network (GNN) models that\nproject electrodes onto the nodes of a graph, where the node features are\nrepresented as EEG channel samples collected over a trial, and nodes can be\nconnected by weighted/unweighted edges according to a flexible policy\nformulated by a neuroscientist. The empirical evaluations show that our\nproposed GNN-based framework outperforms standard CNN classifiers across ErrP,\nand RSVP datasets, as well as allowing neuroscientific interpretability and\nexplainability to deep learning methods tailored to EEG related classification\nproblems. Another practical advantage of our GNN-based framework is that it can\nbe used in EEG channel selection, which is critical for reducing computational\ncost, and designing portable EEG headsets.",
          "link": "http://arxiv.org/abs/2106.09135",
          "publishedOn": "2021-06-18T02:06:36.605Z",
          "wordCount": 637,
          "title": "EEG-GNN: Graph Neural Networks for Classification of Electroencephalogram (EEG) Signals. (arXiv:2106.09135v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1\">Marinka Zitnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "As Graph Neural Networks (GNNs) are increasingly employed in real-world\napplications, it becomes critical to ensure that the stakeholders understand\nthe rationale behind their predictions. While several GNN explanation methods\nhave been proposed recently, there has been little to no work on theoretically\nanalyzing the behavior of these methods or systematically evaluating their\neffectiveness. Here, we introduce the first axiomatic framework for\ntheoretically analyzing, evaluating, and comparing state-of-the-art GNN\nexplanation methods. We outline and formalize the key desirable properties that\nall GNN explanation methods should satisfy in order to generate reliable\nexplanations, namely, faithfulness, stability, and fairness. We leverage these\nproperties to present the first ever theoretical analysis of the effectiveness\nof state-of-the-art GNN explanation methods. Our analysis establishes upper\nbounds on all the aforementioned properties for popular GNN explanation\nmethods. We also leverage our framework to empirically evaluate these methods\non multiple real-world datasets from diverse domains. Our empirical results\ndemonstrate that some popular GNN explanation methods (e.g., gradient-based\nmethods) perform no better than a random baseline and that methods which\nleverage the graph structure are more effective than those that solely rely on\nthe node features.",
          "link": "http://arxiv.org/abs/2106.09078",
          "publishedOn": "2021-06-18T02:06:36.592Z",
          "wordCount": 619,
          "title": "Towards a Rigorous Theoretical Analysis and Evaluation of GNN Explanations. (arXiv:2106.09078v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnam_P/0/1/0/all/0/1\">Payman Behnam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiee_A/0/1/0/all/0/1\">Ali Shafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuehai Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojnordi_M/0/1/0/all/0/1\">Mahdi Nazm Bojnordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>",
          "description": "Recent works demonstrated the promise of using resistive random access memory\n(ReRAM) as an emerging technology to perform inherently parallel analog domain\nin-situ matrix-vector multiplication -- the intensive and key computation in\nDNNs. With weights stored in the ReRAM crossbar cells as conductance, when the\ninput vector is applied to word lines, the matrix-vector multiplication results\ncan be generated as the current in bit lines. A key problem is that the weight\ncan be either positive or negative, but the in-situ computation assumes all\ncells on each crossbar column with the same sign. The current architectures\neither use two ReRAM crossbars for positive and negative weights, or add an\noffset to weights so that all values become positive. Neither solution is\nideal: they either double the cost of crossbars, or incur extra offset\ncircuity. To better solve this problem, this paper proposes FORMS, a\nfine-grained ReRAM-based DNN accelerator with polarized weights. Instead of\ntrying to represent the positive/negative weights, our key design principle is\nto enforce exactly what is assumed in the in-situ computation -- ensuring that\nall weights in the same column of a crossbar have the same sign. It naturally\navoids the cost of an additional crossbar. Such weights can be nicely generated\nusing alternating direction method of multipliers (ADMM) regularized\noptimization, which can exactly enforce certain patterns in DNN weights. To\nachieve high accuracy, we propose to use fine-grained sub-array columns, which\nprovide a unique opportunity for input zero-skipping, significantly avoiding\nunnecessary computations. It also makes the hardware much easier to implement.\nPutting all together, with the same optimized models, FORMS achieves\nsignificant throughput improvement and speed up in frame per second over ISAAC\nwith similar area cost.",
          "link": "http://arxiv.org/abs/2106.09144",
          "publishedOn": "2021-06-18T02:06:36.583Z",
          "wordCount": 751,
          "title": "FORMS: Fine-grained Polarized ReRAM-based In-situ Computation for Mixed-signal DNN Accelerator. (arXiv:2106.09144v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "Pretrained language models have achieved state-of-the-art performance when\nadapted to a downstream NLP task. However, theoretical analysis of these models\nis scarce and challenging since the pretraining and downstream tasks can be\nvery different. We propose an analysis framework that links the pretraining and\ndownstream tasks with an underlying latent variable generative model of text --\nthe downstream classifier must recover a function of the posterior distribution\nover the latent variables. We analyze head tuning (learning a classifier on top\nof the frozen pretrained model) and prompt tuning in this setting. The\ngenerative model in our analysis is either a Hidden Markov Model (HMM) or an\nHMM augmented with a latent memory component, motivated by long-term\ndependencies in natural language. We show that 1) under certain non-degeneracy\nconditions on the HMM, simple classification heads can solve the downstream\ntask, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy\nconditions, and 3) our recovery guarantees for the memory-augmented HMM are\nstronger than for the vanilla HMM because task-relevant information is easier\nto recover from the long-term memory. Experiments on synthetically generated\ndata from HMMs back our theoretical findings.",
          "link": "http://arxiv.org/abs/2106.09226",
          "publishedOn": "2021-06-18T02:06:36.564Z",
          "wordCount": 637,
          "title": "Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning. (arXiv:2106.09226v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingchuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mira_R/0/1/0/all/0/1\">Rodrigo Mira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Stavros Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>",
          "description": "The large amount of audiovisual content being shared online today has drawn\nsubstantial attention to the prospect of audiovisual self-supervised learning.\nRecent works have focused on each of these modalities separately, while others\nhave attempted to model both simultaneously in a cross-modal fashion. However,\ncomparatively little attention has been given to leveraging one modality as a\ntraining objective to learn from the other. In this work, we propose Learning\nvisual speech Representations from Audio via self-supervision (LiRA).\nSpecifically, we train a ResNet+Conformer model to predict acoustic features\nfrom unlabelled visual speech. We find that this pre-trained model can be\nleveraged towards word-level and sentence-level lip-reading through feature\nextraction and fine-tuning experiments. We show that our approach significantly\noutperforms other self-supervised methods on the Lip Reading in the Wild (LRW)\ndataset and achieves state-of-the-art performance on Lip Reading Sentences 2\n(LRS2) using only a fraction of the total labelled data.",
          "link": "http://arxiv.org/abs/2106.09171",
          "publishedOn": "2021-06-18T02:06:36.550Z",
          "wordCount": 608,
          "title": "LiRA: Learning Visual Speech Representations from Audio through Self-supervision. (arXiv:2106.09171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09481",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Asi_H/0/1/0/all/0/1\">Hilal Asi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jambulapati_A/0/1/0/all/0/1\">Arun Jambulapati</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jin_Y/0/1/0/all/0/1\">Yujia Jin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sidford_A/0/1/0/all/0/1\">Aaron Sidford</a>",
          "description": "We develop a new primitive for stochastic optimization: a low-bias, low-cost\nestimator of the minimizer $x_\\star$ of any Lipschitz strongly-convex function.\nIn particular, we use a multilevel Monte-Carlo approach due to Blanchet and\nGlynn to turn any optimal stochastic gradient method into an estimator of\n$x_\\star$ with bias $\\delta$, variance $O(\\log(1/\\delta))$, and an expected\nsampling cost of $O(\\log(1/\\delta))$ stochastic gradient evaluations. As an\nimmediate consequence, we obtain cheap and nearly unbiased gradient estimators\nfor the Moreau-Yoshida envelope of any Lipschitz convex function, allowing us\nto perform dimension-free randomized smoothing.\n\nWe demonstrate the potential of our estimator through four applications.\nFirst, we develop a method for minimizing the maximum of $N$ functions,\nimproving on recent results and matching a lower bound up logarithmic factors.\nSecond and third, we recover state-of-the-art rates for projection-efficient\nand gradient-efficient optimization using simple algorithms with a transparent\nanalysis. Finally, we show that an improved version of our estimator would\nyield a nearly linear-time, optimal-utility, differentially-private non-smooth\nstochastic optimization method.",
          "link": "http://arxiv.org/abs/2106.09481",
          "publishedOn": "2021-06-18T02:06:36.529Z",
          "wordCount": 599,
          "title": "Stochastic Bias-Reduced Gradient Methods. (arXiv:2106.09481v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Yun Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>",
          "description": "Learning to classify time series with limited data is a practical yet\nchallenging problem. Current methods are primarily based on hand-designed\nfeature extraction rules or domain-specific data augmentation. Motivated by the\nadvances in deep speech processing models and the fact that voice data are\nunivariate temporal signals, in this paper, we propose Voice2Series (V2S), a\nnovel end-to-end approach that reprograms acoustic models for time series\nclassification, through input transformation learning and output label mapping.\nLeveraging the representation learning power of a large-scale pre-trained\nspeech processing model, on 30 different time series tasks we show that V2S\neither outperforms or is tied with state-of-the-art methods on 20 tasks, and\nimproves their average accuracy by 1.84%. We further provide a theoretical\njustification of V2S by proving its population risk is upper bounded by the\nsource risk and a Wasserstein distance accounting for feature alignment via\nreprogramming. Our results offer new and effective means to time series\nclassification.",
          "link": "http://arxiv.org/abs/2106.09296",
          "publishedOn": "2021-06-18T02:06:36.507Z",
          "wordCount": 611,
          "title": "Voice2Series: Reprogramming Acoustic Models for Time Series Classification. (arXiv:2106.09296v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chijiwa_D/0/1/0/all/0/1\">Daiki Chijiwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1\">Shin&#x27;ya Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ida_Y/0/1/0/all/0/1\">Yasutoshi Ida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umakoshi_K/0/1/0/all/0/1\">Kenji Umakoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_T/0/1/0/all/0/1\">Tomohiro Inoue</a>",
          "description": "Pruning the weights of randomly initialized neural networks plays an\nimportant role in the context of lottery ticket hypothesis. Ramanujan et al.\n(2020) empirically showed that only pruning the weights can achieve remarkable\nperformance instead of optimizing the weight values. However, to achieve the\nsame level of performance as the weight optimization, the pruning approach\nrequires more parameters in the networks before pruning and thus more memory\nspace. To overcome this parameter inefficiency, we introduce a novel framework\nto prune randomly initialized neural networks with iteratively randomizing\nweight values (IteRand). Theoretically, we prove an approximation theorem in\nour framework, which indicates that the randomizing operations are provably\neffective to reduce the required number of the parameters. We also empirically\ndemonstrate the parameter efficiency in multiple experiments on CIFAR-10 and\nImageNet.",
          "link": "http://arxiv.org/abs/2106.09269",
          "publishedOn": "2021-06-18T02:06:36.500Z",
          "wordCount": 586,
          "title": "Pruning Randomly Initialized Neural Networks with Iterative Randomization. (arXiv:2106.09269v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yutian Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Sheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jueming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongming Liu</a>",
          "description": "To evaluate the robustness gain of Bayesian neural networks on image\nclassification tasks, we perform input perturbations, and adversarial attacks\nto the state-of-the-art Bayesian neural networks, with a benchmark CNN model as\nreference. The attacks are selected to simulate signal interference and\ncyberattacks towards CNN-based machine learning systems. The result shows that\na Bayesian neural network achieves significantly higher robustness against\nadversarial attacks generated against a deterministic neural network model,\nwithout adversarial training. The Bayesian posterior can act as the safety\nprecursor of ongoing malicious activities. Furthermore, we show that the\nstochastic classifier after the deterministic CNN extractor has sufficient\nrobustness enhancement rather than a stochastic feature extractor before the\nstochastic classifier. This advises on utilizing stochastic layers in building\ndecision-making pipelines within a safety-critical domain.",
          "link": "http://arxiv.org/abs/2106.09223",
          "publishedOn": "2021-06-18T02:06:36.490Z",
          "wordCount": 570,
          "title": "Evaluating the Robustness of Bayesian Neural Networks Against Different Types of Attacks. (arXiv:2106.09223v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.15706",
          "author": "<a href=\"http://arxiv.org/find/hep-th/1/au:+Erbin_H/0/1/0/all/0/1\">Harold Erbin</a>, <a href=\"http://arxiv.org/find/hep-th/1/au:+Finotello_R/0/1/0/all/0/1\">Riccardo Finotello</a>",
          "description": "We revisit the question of predicting both Hodge numbers $h^{1,1}$ and\n$h^{2,1}$ of complete intersection Calabi-Yau (CICY) 3-folds using machine\nlearning (ML), considering both the old and new datasets built respectively by\nCandelas-Dale-Lutken-Schimmrigk / Green-H\\\"ubsch-Lutken and by\nAnderson-Gao-Gray-Lee. In real world applications, implementing a ML system\nrarely reduces to feed the brute data to the algorithm. Instead, the typical\nworkflow starts with an exploratory data analysis (EDA) which aims at\nunderstanding better the input data and finding an optimal representation. It\nis followed by the design of a validation procedure and a baseline model.\nFinally, several ML models are compared and combined, often involving neural\nnetworks with a topology more complicated than the sequential models typically\nused in physics. By following this procedure, we improve the accuracy of ML\ncomputations for Hodge numbers with respect to the existing literature. First,\nwe obtain 97% (resp. 99%) accuracy for $h^{1,1}$ using a neural network\ninspired by the Inception model for the old dataset, using only 30% (resp. 70%)\nof the data for training. For the new one, a simple linear regression leads to\nalmost 100% accuracy with 30% of the data for training. The computation of\n$h^{2,1}$ is less successful as we manage to reach only 50% accuracy for both\ndatasets, but this is still better than the 16% obtained with a simple neural\nnetwork (SVM with Gaussian kernel and feature engineering and sequential\nconvolutional network reach at best 36%). This serves as a proof of concept\nthat neural networks can be valuable to study the properties of geometries\nappearing in string theory.",
          "link": "http://arxiv.org/abs/2007.15706",
          "publishedOn": "2021-06-18T02:06:36.481Z",
          "wordCount": 739,
          "title": "Machine learning for complete intersection Calabi-Yau manifolds: a methodological study. (arXiv:2007.15706v2 [hep-th] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_M/0/1/0/all/0/1\">Morteza Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hough_J/0/1/0/all/0/1\">Julian Hough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>",
          "description": "This paper is a submission to the Alzheimer's Dementia Recognition through\nSpontaneous Speech (ADReSS) challenge, which aims to develop methods that can\nassist in the automated prediction of severity of Alzheimer's Disease from\nspeech data. We focus on acoustic and natural language features for cognitive\nimpairment detection in spontaneous speech in the context of Alzheimer's\nDisease Diagnosis and the mini-mental state examination (MMSE) score\nprediction. We proposed a model that obtains unimodal decisions from different\nLSTMs, one for each modality of text and audio, and then combines them using a\ngating mechanism for the final prediction. We focused on sequential modelling\nof text and audio and investigated whether the disfluencies present in\nindividuals' speech relate to the extent of their cognitive impairment. Our\nresults show that the proposed classification and regression schemes obtain\nvery promising results on both development and test sets. This suggests\nAlzheimer's Disease can be detected successfully with sequence modeling of the\nspeech data of medical sessions.",
          "link": "http://arxiv.org/abs/2106.09668",
          "publishedOn": "2021-06-18T02:06:36.473Z",
          "wordCount": 637,
          "title": "Multi-modal fusion with gating using audio, lexical and disfluency features for Alzheimer's Dementia recognition from spontaneous speech. (arXiv:2106.09668v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_G/0/1/0/all/0/1\">Gaurav R. Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_Asl_R/0/1/0/all/0/1\">Reza Abbasi-Asl</a>",
          "description": "Multivariable time series classification problems are increasing in\nprevalence and complexity in a variety of domains, such as biology and finance.\nWhile deep learning methods are an effective tool for these problems, they\noften lack interpretability. In this work, we propose a novel modular prototype\nlearning framework for multivariable time series classification. In the first\nstage of our framework, encoders extract features from each variable\nindependently. Prototype layers identify single-variable prototypes in the\nresulting feature spaces. The next stage of our framework represents the\nmultivariable time series sample points in terms of their similarity to these\nsingle-variable prototypes. This results in an inherently interpretable\nrepresentation of multivariable patterns, on which prototype learning is\napplied to extract representative examples i.e. multivariable prototypes. Our\nframework is thus able to explicitly identify both informative patterns in the\nindividual variables, as well as the relationships between the variables. We\nvalidate our framework on a simulated dataset with embedded patterns, as well\nas a real human activity recognition problem. Our framework attains comparable\nor superior classification performance to existing time series classification\nmethods on these tasks. On the simulated dataset, we find that our model\nreturns interpretations consistent with the embedded patterns. Moreover, the\ninterpretations learned on the activity recognition dataset align with domain\nknowledge.",
          "link": "http://arxiv.org/abs/2106.09636",
          "publishedOn": "2021-06-18T02:06:36.453Z",
          "wordCount": 645,
          "title": "Multi-Modal Prototype Learning for Interpretable Multivariable Time Series Classification. (arXiv:2106.09636v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00727",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yoshida_T/0/1/0/all/0/1\">Tomoki Yoshida</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1\">Ichiro Takeuchi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karasuyama_M/0/1/0/all/0/1\">Masayuki Karasuyama</a>",
          "description": "Graphs are versatile tools for representing structured data. As a result, a\nvariety of machine learning methods have been studied for graph data analysis.\nAlthough many such learning methods depend on the measurement of differences\nbetween input graphs, defining an appropriate distance metric for graphs\nremains a controversial issue. Hence, we propose a supervised distance metric\nlearning method for the graph classification problem. Our method, named\ninterpretable graph metric learning (IGML), learns discriminative metrics in a\nsubgraph-based feature space, which has a strong graph representation\ncapability. By introducing a sparsity-inducing penalty on the weight of each\nsubgraph, IGML can identify a small number of important subgraphs that can\nprovide insight into the given classification task. Because our formulation has\na large number of optimization variables, an efficient algorithm that uses\npruning techniques based on safe screening and working set selection methods is\nalso proposed. An important property of IGML is that solution optimality is\nguaranteed because the problem is formulated as a convex problem and our\npruning strategies only discard unnecessary subgraphs. Furthermore, we show\nthat IGML is also applicable to other structured data such as itemset and\nsequence data, and that it can incorporate vertex-label similarity by using a\ntransportation-based subgraph feature. We empirically evaluate the\ncomputational efficiency and classification performance of IGML on several\nbenchmark datasets and provide some illustrative examples of how IGML\nidentifies important subgraphs from a given graph dataset.",
          "link": "http://arxiv.org/abs/2002.00727",
          "publishedOn": "2021-06-18T02:06:36.447Z",
          "wordCount": 711,
          "title": "Distance Metric Learning for Graph Structured Data. (arXiv:2002.00727v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poesia_G/0/1/0/all/0/1\">Gabriel Poesia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">WenXin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>",
          "description": "Abstract symbolic reasoning, as required in domains such as mathematics and\nlogic, is a key component of human intelligence. Solvers for these domains have\nimportant applications, especially to computer-assisted education. But learning\nto solve symbolic problems is challenging for machine learning algorithms.\nExisting models either learn from human solutions or use hand-engineered\nfeatures, making them expensive to apply in new domains. In this paper, we\ninstead consider symbolic domains as simple environments where states and\nactions are given as unstructured text, and binary rewards indicate whether a\nproblem is solved. This flexible setup makes it easy to specify new domains,\nbut search and planning become challenging. We introduce four environments\ninspired by the Mathematics Common Core Curriculum, and observe that existing\nReinforcement Learning baselines perform poorly. We then present a novel\nlearning algorithm, Contrastive Policy Learning (ConPoLe) that explicitly\noptimizes the InfoNCE loss, which lower bounds the mutual information between\nthe current state and next states that continue on a path to the solution.\nConPoLe successfully solves all four domains. Moreover, problem representations\nlearned by ConPoLe enable accurate prediction of the categories of problems in\na real mathematics curriculum. Our results suggest new directions for\nreinforcement learning in symbolic domains, as well as applications to\nmathematics education.",
          "link": "http://arxiv.org/abs/2106.09146",
          "publishedOn": "2021-06-18T02:06:36.440Z",
          "wordCount": 636,
          "title": "Contrastive Reinforcement Learning of Symbolic Reasoning Domains. (arXiv:2106.09146v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qinming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>",
          "description": "Smart contracts hold digital coins worth billions of dollars, their security\nissues have drawn extensive attention in the past years. Towards smart contract\nvulnerability detection, conventional methods heavily rely on fixed expert\nrules, leading to low accuracy and poor scalability. Recent deep learning\napproaches alleviate this issue but fail to encode useful expert knowledge. In\nthis paper, we explore combining deep learning with expert patterns in an\nexplainable fashion. Specifically, we develop automatic tools to extract expert\npatterns from the source code. We then cast the code into a semantic graph to\nextract deep graph features. Thereafter, the global graph feature and local\nexpert patterns are fused to cooperate and approach the final prediction, while\nyielding their interpretable weights. Experiments are conducted on all\navailable smart contracts with source code in two platforms, Ethereum and VNT\nChain. Empirically, our system significantly outperforms state-of-the-art\nmethods. Our code is released.",
          "link": "http://arxiv.org/abs/2106.09282",
          "publishedOn": "2021-06-18T02:06:36.434Z",
          "wordCount": 611,
          "title": "Smart Contract Vulnerability Detection: From Pure Neural Network to Interpretable Graph Feature and Expert Pattern Fusion. (arXiv:2106.09282v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bay_A/0/1/0/all/0/1\">Alessandro Bay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_B/0/1/0/all/0/1\">Biswa Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirabile_A/0/1/0/all/0/1\">Andrea Mirabile</a>",
          "description": "Modern neural networks are highly uncalibrated. It poses a significant\nchallenge for safety-critical systems to utilise deep neural networks (DNNs),\nreliably. Many recently proposed approaches have demonstrated substantial\nprogress in improving DNN calibration. However, they hardly touch upon\nrefinement, which historically has been an essential aspect of calibration.\nRefinement indicates separability of a network's correct and incorrect\npredictions. This paper presents a theoretically and empirically supported\nexposition for reviewing a model's calibration and refinement. Firstly, we show\nthe breakdown of expected calibration error (ECE), into predicted confidence\nand refinement. Connecting with this result, we highlight that regularisation\nbased calibration only focuses on naively reducing a model's confidence. This\nlogically has a severe downside to a model's refinement. We support our claims\nthrough rigorous empirical evaluations of many state of the art calibration\napproaches on standard datasets. We find that many calibration approaches with\nthe likes of label smoothing, mixup etc. lower the utility of a DNN by\ndegrading its refinement. Even under natural data shift, this\ncalibration-refinement trade-off holds for the majority of calibration methods.\nThese findings call for an urgent retrospective into some popular pathways\ntaken for modern DNN calibration.",
          "link": "http://arxiv.org/abs/2106.09385",
          "publishedOn": "2021-06-18T02:06:36.426Z",
          "wordCount": 638,
          "title": "On the Dark Side of Calibration for Modern Neural Networks. (arXiv:2106.09385v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zichen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zihan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jinfeng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>",
          "description": "Federated learning involves training machine learning models over devices or\ndata silos, such as edge processors or data warehouses, while keeping the data\nlocal. Training in heterogeneous and potentially massive networks introduces\nbias into the system, which is originated from the non-IID data and the low\nparticipation rate in reality. In this paper, we propose Elastic Federated\nLearning (EFL), an unbiased algorithm to tackle the heterogeneity in the\nsystem, which makes the most informative parameters less volatile during\ntraining, and utilizes the incomplete local updates. It is an efficient and\neffective algorithm that compresses both upstream and downstream\ncommunications. Theoretically, the algorithm has convergence guarantee when\ntraining on the non-IID data at the low participation rate. Empirical\nexperiments corroborate the competitive performance of EFL framework on the\nrobustness and the efficiency.",
          "link": "http://arxiv.org/abs/2106.09433",
          "publishedOn": "2021-06-18T02:06:36.406Z",
          "wordCount": 570,
          "title": "Towards Heterogeneous Clients with Elastic Federated Learning. (arXiv:2106.09433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Olano_D/0/1/0/all/0/1\">Diego Garcia-Olano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1\">Yasumasa Onoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1\">Ioana Baldini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1\">Joydeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1\">Kush R. Varshney</a>",
          "description": "Pre-trained language models induce dense entity representations that offer\nstrong performance on entity-centric NLP tasks, but such representations are\nnot immediately interpretable. This can be a barrier to model uptake in\nimportant domains such as biomedicine. There has been recent work on general\ninterpretable representation learning (Onoe and Durrett, 2020), but these\ndomain-agnostic representations do not readily transfer to the important domain\nof biomedicine. In this paper, we create a new entity type system and training\nset from a large corpus of biomedical texts by mapping entities to concepts in\na medical ontology, and from these to Wikipedia pages whose categories are our\ntypes. From this mapping we derive Biomedical Interpretable Entity\nRepresentations(BIERs), in which dimensions correspond to fine-grained entity\ntypes, and values are predicted probabilities that a given entity is of the\ncorresponding type. We propose a novel method that exploits BIER's final sparse\nand intermediate dense representations to facilitate model and entity type\ndebugging. We show that BIERs achieve strong performance in biomedical tasks\nincluding named entity disambiguation and entity label classification, and we\nprovide error analysis to highlight the utility of their interpretability,\nparticularly in low-supervision settings. Finally, we provide our induced 68K\nbiomedical type system, the corresponding 37 million triples of derived data\nused to train BIER models and our best performing model.",
          "link": "http://arxiv.org/abs/2106.09502",
          "publishedOn": "2021-06-18T02:06:36.399Z",
          "wordCount": 658,
          "title": "Biomedical Interpretable Entity Representations. (arXiv:2106.09502v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sean Bin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chenjuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jilin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>",
          "description": "Path representations are critical in a variety of transportation\napplications, such as estimating path ranking in path recommendation systems\nand estimating path travel time in navigation systems. Existing studies often\nlearn task-specific path representations in a supervised manner, which require\na large amount of labeled training data and generalize poorly to other tasks.\nWe propose an unsupervised learning framework Path InfoMax (PIM) to learn\ngeneric path representations that work for different downstream tasks. We first\npropose a curriculum negative sampling method, for each input path, to generate\na small amount of negative paths, by following the principles of curriculum\nlearning. Next, \\emph{PIM} employs mutual information maximization to learn\npath representations from both a global and a local view. In the global view,\nPIM distinguishes the representations of the input paths from those of the\nnegative paths. In the local view, \\emph{PIM} distinguishes the input path\nrepresentations from the representations of the nodes that appear only in the\nnegative paths. This enables the learned path representations to encode both\nglobal and local information at different scales. Extensive experiments on two\ndownstream tasks, ranking score estimation and travel time estimation, using\ntwo road network datasets suggest that PIM significantly outperforms other\nunsupervised methods and is also able to be used as a pre-training method to\nenhance supervised path representation learning.",
          "link": "http://arxiv.org/abs/2106.09373",
          "publishedOn": "2021-06-18T02:06:36.391Z",
          "wordCount": 663,
          "title": "Unsupervised Path Representation Learning with Curriculum Negative Sampling. (arXiv:2106.09373v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grunwald_P/0/1/0/all/0/1\">Peter Gr&#xfc;nwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1\">Thomas Steinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakynthinou_L/0/1/0/all/0/1\">Lydia Zakynthinou</a>",
          "description": "We give a novel, unified derivation of conditional PAC-Bayesian and mutual\ninformation (MI) generalization bounds. We derive conditional MI bounds as an\ninstance, with special choice of prior, of conditional MAC-Bayesian (Mean\nApproximately Correct) bounds, itself derived from conditional PAC-Bayesian\nbounds, where `conditional' means that one can use priors conditioned on a\njoint training and ghost sample. This allows us to get nontrivial PAC-Bayes and\nMI-style bounds for general VC classes, something recently shown to be\nimpossible with standard PAC-Bayesian/MI bounds. Second, it allows us to get\nfaster rates of order $O \\left(({\\text{KL}}/n)^{\\gamma}\\right)$ for $\\gamma >\n1/2$ if a Bernstein condition holds and for exp-concave losses (with\n$\\gamma=1$), which is impossible with both standard PAC-Bayes generalization\nand MI bounds. Our work extends the recent work by Steinke and Zakynthinou\n[2020] who handle MI with VC but neither PAC-Bayes nor fast rates, the recent\nwork of Hellstr\\\"om and Durisi [2020] who extend the latter to the PAC-Bayes\nsetting via a unifying exponential inequality, and Mhammedi et al. [2019] who\ninitiated fast rate PAC-Bayes generalization error bounds but handle neither MI\nnor general VC classes.",
          "link": "http://arxiv.org/abs/2106.09683",
          "publishedOn": "2021-06-18T02:06:36.382Z",
          "wordCount": 643,
          "title": "PAC-Bayes, MAC-Bayes and Conditional Mutual Information: Fast rate bounds that handle general VC classes. (arXiv:2106.09683v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soshnikov_D/0/1/0/all/0/1\">Dmitry Soshnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valieva_Y/0/1/0/all/0/1\">Yana Valieva</a>",
          "description": "In this paper, we present a new Python library called mPyPl, which is\nintended to simplify complex data processing tasks using functional approach.\nThis library defines operations on lazy data streams of named dictionaries\nrepresented as generators (so-called multi-field datastreams), and allows\nenriching those data streams with more 'fields' in the process of data\npreparation and feature extraction. Thus, most data preparation tasks can be\nexpressed in the form of neat linear 'pipeline', similar in syntax to UNIX\npipes, or |> functional composition operator in F#.\n\nWe define basic operations on multi-field data streams, which resemble\nclassical monadic operations, and show similarity of the proposed approach to\nmonads in functional programming. We also show how the library was used in\ncomplex deep learning tasks of event detection in video, and discuss different\nevaluation strategies that allow for different compromises in terms of memory\nand performance.",
          "link": "http://arxiv.org/abs/2106.09164",
          "publishedOn": "2021-06-18T02:06:36.371Z",
          "wordCount": 597,
          "title": "mPyPl: Python Monadic Pipeline Library for Complex Functional Data Processing. (arXiv:2106.09164v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jake Zhao</a> (Junbo), <a href=\"http://arxiv.org/find/cs/1/au:+Ou_M/0/1/0/all/0/1\">Mingfeng Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Linji Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yunkai Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>",
          "description": "Most, if not all, modern deep learning systems restrict themselves to a\nsingle dataset for neural network training and inference. In this article, we\nare interested in systematic ways to join datasets that are made of similar\npurposes. Unlike previous published works that ubiquitously conduct the dataset\njoining in the uninterpretable latent vectorial space, the core to our method\nis an augmentation procedure in the label space. The primary challenge to\naddress the label space for dataset joining is the discrepancy between labels:\nnon-overlapping label annotation sets, different labeling granularity or\nhierarchy and etc. Notably we propose a new technique leveraging artificially\ncreated knowledge graph, recurrent neural networks and policy gradient that\nsuccessfully achieve the dataset joining in the label space. Empirical results\non both image and text classification justify the validity of our approach.",
          "link": "http://arxiv.org/abs/2106.09260",
          "publishedOn": "2021-06-18T02:06:36.351Z",
          "wordCount": 590,
          "title": "Joining datasets via data augmentation in the label space for neural networks. (arXiv:2106.09260v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09276",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koehler_F/0/1/0/all/0/1\">Frederic Koehler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_L/0/1/0/all/0/1\">Lijia Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1\">Danica J. Sutherland</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Srebro_N/0/1/0/all/0/1\">Nathan Srebro</a>",
          "description": "We consider interpolation learning in high-dimensional linear regression with\nGaussian data, and prove a generic uniform convergence guarantee on the\ngeneralization error of interpolators in an arbitrary hypothesis class in terms\nof the class's Gaussian width. Applying the generic bound to Euclidean norm\nballs recovers the consistency result of Bartlett et al. (2020) for\nminimum-norm interpolators, and confirms a prediction of Zhou et al. (2020) for\nnear-minimal-norm interpolators in the special case of Gaussian data. We\ndemonstrate the generality of the bound by applying it to the simplex,\nobtaining a novel consistency result for minimum l1-norm interpolators (basis\npursuit). Our results show how norm-based generalization bounds can explain and\nbe used to analyze benign overfitting, at least in some settings.",
          "link": "http://arxiv.org/abs/2106.09276",
          "publishedOn": "2021-06-18T02:06:36.344Z",
          "wordCount": 568,
          "title": "Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and Benign Overfitting. (arXiv:2106.09276v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>",
          "description": "Most prior work on task-oriented dialogue systems are restricted to limited\ncoverage of domain APIs. However, users oftentimes have requests that are out\nof the scope of these APIs. This work focuses on responding to these\nbeyond-API-coverage user turns by incorporating external, unstructured\nknowledge sources. Our approach works in a pipelined manner with\nknowledge-seeking turn detection, knowledge selection, and response generation\nin sequence. We introduce novel data augmentation methods for the first two\nsteps and demonstrate that the use of information extracted from dialogue\ncontext improves the knowledge selection and end-to-end performances. Through\nexperiments, we achieve state-of-the-art performance for both automatic and\nhuman evaluation metrics on the DSTC9 Track 1 benchmark dataset, validating the\neffectiveness of our contributions.",
          "link": "http://arxiv.org/abs/2106.09174",
          "publishedOn": "2021-06-18T02:06:36.337Z",
          "wordCount": 579,
          "title": "Can I Be of Further Assistance? Using Unstructured Knowledge Access to Improve Task-oriented Conversational Modeling. (arXiv:2106.09174v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nissani_D/0/1/0/all/0/1\">Daniel N. Nissani</a> (Nissensohn)",
          "description": "Generative neural networks are able to mimic intricate probability\ndistributions such as those of handwritten text, natural images, etc. Since\ntheir inception several models were proposed. The most successful of these were\nbased on adversarial (GAN), auto-encoding (VAE) and maximum mean discrepancy\n(MMD) relatively complex architectures and schemes. Surprisingly, a very simple\narchitecture (a single feed-forward neural network) in conjunction with an\nobvious optimization goal (Kullback_Leibler divergence) was apparently\noverlooked. This paper demonstrates that such a model (denoted SGN for its\nsimplicity) is able to generate samples visually and quantitatively competitive\nas compared with the fore-mentioned state of the art methods.",
          "link": "http://arxiv.org/abs/2106.09330",
          "publishedOn": "2021-06-18T02:06:36.331Z",
          "wordCount": 523,
          "title": "A Simple Generative Network. (arXiv:2106.09330v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09188",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Howard_A/0/1/0/all/0/1\">Amanda A. Howard</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tartakovsky_A/0/1/0/all/0/1\">Alexandre M. Tartakovsky</a>",
          "description": "Redox flow batteries (RFBs) offer the capability to store large amounts of\nenergy cheaply and efficiently, however, there is a need for fast and accurate\nmodels of the charge-discharge curve of a RFB to potentially improve the\nbattery capacity and performance. We develop a multifidelity model for\npredicting the charge-discharge curve of a RFB. In the multifidelity model, we\nuse the Physics-informed CoKriging (CoPhIK) machine learning method that is\ntrained on experimental data and constrained by the so-called\n\"zero-dimensional\" physics-based model. Here we demonstrate that the model\nshows good agreement with experimental results and significant improvements\nover existing zero-dimensional models. We show that the proposed model is\nrobust as it is not sensitive to the input parameters in the zero-dimensional\nmodel. We also show that only a small amount of high-fidelity experimental\ndatasets are needed for accurate predictions for the range of considered input\nparameters, which include current density, flow rate, and initial\nconcentrations.",
          "link": "http://arxiv.org/abs/2106.09188",
          "publishedOn": "2021-06-18T02:06:36.324Z",
          "wordCount": 585,
          "title": "Physics-informed CoKriging model of a redox flow battery. (arXiv:2106.09188v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zijian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1\">Yevgeniy Vorobeychik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "We present the first framework of Certifying Robust Policies for\nreinforcement learning (CROP) against adversarial state perturbations. We\npropose two particular types of robustness certification criteria: robustness\nof per-state actions and lower bound of cumulative rewards. Specifically, we\ndevelop a local smoothing algorithm which uses a policy derived from\nQ-functions smoothed with Gaussian noise over each encountered state to\nguarantee the robustness of actions taken along this trajectory. Next, we\ndevelop a global smoothing algorithm for certifying the robustness of a\nfinite-horizon cumulative reward under adversarial state perturbations.\nFinally, we propose a local smoothing approach which makes use of adaptive\nsearch in order to obtain tight certification bounds for reward. We use the\nproposed RL robustness certification framework to evaluate six methods that\nhave previously been shown to yield empirically robust RL, including\nadversarial training and several forms of regularization, on two representative\nAtari games. We show that RegPGD, RegCVX, and RadialRL achieve high certified\nrobustness among these. Furthermore, we demonstrate that our certifications are\noften tight by evaluating these algorithms against adversarial attacks.",
          "link": "http://arxiv.org/abs/2106.09292",
          "publishedOn": "2021-06-18T02:06:36.304Z",
          "wordCount": 617,
          "title": "CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing. (arXiv:2106.09292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1\">Eric Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazlyab_M/0/1/0/all/0/1\">Mahyar Fazlyab</a>",
          "description": "Analyzing the worst-case performance of deep neural networks against input\nperturbations amounts to solving a large-scale non-convex optimization problem,\nfor which several past works have proposed convex relaxations as a promising\nalternative. However, even for reasonably-sized neural networks, these\nrelaxations are not tractable, and so must be replaced by even weaker\nrelaxations in practice. In this work, we propose a novel operator splitting\nmethod that can directly solve a convex relaxation of the problem to high\naccuracy, by splitting it into smaller sub-problems that often have analytical\nsolutions. The method is modular and scales to problem instances that were\npreviously impossible to solve exactly due to their size. Furthermore, the\nsolver operations are amenable to fast parallelization with GPU acceleration.\nWe demonstrate our method in obtaining tighter bounds on the worst-case\nperformance of large convolutional networks in image classification and\nreinforcement learning settings.",
          "link": "http://arxiv.org/abs/2106.09117",
          "publishedOn": "2021-06-18T02:06:36.297Z",
          "wordCount": 577,
          "title": "DeepSplit: Scalable Verification of Deep Neural Networks via Operator Splitting. (arXiv:2106.09117v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kay_J/0/1/0/all/0/1\">Justin Kay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrifield_M/0/1/0/all/0/1\">Matt Merrifield</a>",
          "description": "Camera-based electronic monitoring (EM) systems are increasingly being\ndeployed onboard commercial fishing vessels to collect essential data for\nfisheries management and regulation. These systems generate large quantities of\nvideo data which must be reviewed on land by human experts. Computer vision can\nassist this process by automatically detecting and classifying fish species,\nhowever the lack of existing public data in this domain has hindered progress.\nTo address this, we present the Fishnet Open Images Database, a large dataset\nof EM imagery for fish detection and fine-grained categorization onboard\ncommercial fishing vessels. The dataset consists of 86,029 images containing 34\nobject classes, making it the largest and most diverse public dataset of\nfisheries EM imagery to-date. It includes many of the characteristic challenges\nof EM data: visual similarity between species, skewed class distributions,\nharsh weather conditions, and chaotic crew activity. We evaluate the\nperformance of existing detection and classification algorithms and demonstrate\nthat the dataset can serve as a challenging benchmark for development of\ncomputer vision algorithms in fisheries. The dataset is available at\nhttps://www.fishnet.ai/.",
          "link": "http://arxiv.org/abs/2106.09178",
          "publishedOn": "2021-06-18T02:06:36.290Z",
          "wordCount": 637,
          "title": "The Fishnet Open Images Database: A Dataset for Fish Detection and Fine-Grained Categorization in Fisheries. (arXiv:2106.09178v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yuxin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>",
          "description": "With the surge in the number of hyperparameters and training times of modern\nmachine learning models, hyperparameter tuning is becoming increasingly\nexpensive. Although methods have been proposed to speed up tuning via knowledge\ntransfer, they typically require the final performance of hyperparameters and\ndo not focus on low-fidelity information. Nevertheless, this common practice is\nsuboptimal and can incur an unnecessary use of resources. It is more\ncost-efficient to instead leverage the low-fidelity tuning observations to\nmeasure inter-task similarity and transfer knowledge from existing to new tasks\naccordingly. However, performing multi-fidelity tuning comes with its own\nchallenges in the transfer setting: the noise in the additional observations\nand the need for performance forecasting. Therefore, we conduct a thorough\nanalysis of the multi-task multi-fidelity Bayesian optimization framework,\nwhich leads to the best instantiation--amortized auto-tuning (AT2). We further\npresent an offline-computed 27-task hyperparameter recommendation (HyperRec)\ndatabase to serve the community. Extensive experiments on HyperRec and other\nreal-world databases illustrate the effectiveness of our AT2 method.",
          "link": "http://arxiv.org/abs/2106.09179",
          "publishedOn": "2021-06-18T02:06:36.281Z",
          "wordCount": 600,
          "title": "Amortized Auto-Tuning: Cost-Efficient Transfer Optimization for Hyperparameter Recommendation. (arXiv:2106.09179v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pelecanos_J/0/1/0/all/0/1\">Jason Pelecanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>",
          "description": "Many neural network speaker recognition systems model each speaker using a\nfixed-dimensional embedding vector. These embeddings are generally compared\nusing either linear or 2nd-order scoring and, until recently, do not handle\nutterance-specific uncertainty. In this work we propose scoring these\nrepresentations in a way that can capture uncertainty, enroll/test asymmetry\nand additional non-linear information. This is achieved by incorporating a\n2nd-stage neural network (known as a decision network) as part of an end-to-end\ntraining regimen. In particular, we propose the concept of decision residual\nnetworks which involves the use of a compact decision network to leverage\ncosine scores and to model the residual signal that's needed. Additionally, we\npresent a modification to the generalized end-to-end softmax loss function to\ntarget the separation of same/different speaker scores. We observed significant\nperformance gains for the two techniques.",
          "link": "http://arxiv.org/abs/2104.01989",
          "publishedOn": "2021-06-18T02:06:36.274Z",
          "wordCount": 610,
          "title": "Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker Recognition. (arXiv:2104.01989v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Svetozarevic_B/0/1/0/all/0/1\">B. Svetozarevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumann_C/0/1/0/all/0/1\">C.Baumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muntwiler_S/0/1/0/all/0/1\">S. Muntwiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1\">L. Di Natale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeilinger_M/0/1/0/all/0/1\">M. Zeilinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heer_P/0/1/0/all/0/1\">P. Heer</a>",
          "description": "This work presents a fully data-driven, black-box pipeline to obtain an\noptimal control policy for a multi-loop building control problem based on\nhistorical building and weather data, thus without the need for complex\nphysics-based modelling. We demonstrate the method for joint control of room\ntemperature and bidirectional EV charging to maximize the occupant thermal\ncomfort and energy savings while leaving enough energy in the EV battery for\nthe next trip. We modelled the room temperature with a recurrent neural network\nand EV charging with a piece-wise linear function. Using these models as a\nsimulation environment, we applied a deep reinforcement learning (DRL)\nalgorithm to obtain an optimal control policy. The learnt policy achieves on\naverage 17% energy savings over the heating season and 19% better comfort\nsatisfaction than a standard RB room temperature controller. When a\nbidirectional EV is additionally connected and a two-tariff electricity pricing\nis applied, the MIMO DRL policy successfully leverages the battery and\ndecreases the overall cost of electricity compared to two standard RB\ncontrollers, one controlling the room temperature and another controlling the\nbidirectional EV (dis-)charging. Finally, we demonstrate a successful transfer\nof the learnt DRL policy from simulation onto a real building, the DFAB HOUSE\nat Empa Duebendorf in Switzerland, achieving up to 30% energy savings while\nmaintaining similar comfort levels compared to a conventional RB room\ntemperature controller over three weeks during the heating season.",
          "link": "http://arxiv.org/abs/2103.01886",
          "publishedOn": "2021-06-18T02:06:36.266Z",
          "wordCount": 722,
          "title": "Data-driven control of room temperature and bidirectional EV charging using deep reinforcement learning: simulations and experiments. (arXiv:2103.01886v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qingjiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>",
          "description": "Federated learning (FL) has been recognized as a viable distributed learning\nparadigm which trains a machine learning model collaboratively with massive\nmobile devices in the wireless edge while protecting user privacy. Although\nvarious communication schemes have been proposed to expedite the FL process,\nmost of them have assumed ideal wireless channels which provide reliable and\nlossless communication links between the server and mobile clients.\nUnfortunately, in practical systems with limited radio resources such as\nconstraint on the training latency and constraints on the transmission power\nand bandwidth, transmission of a large number of model parameters inevitably\nsuffers from quantization errors (QE) and transmission outage (TO). In this\npaper, we consider such non-ideal wireless channels, and carry out the first\nanalysis showing that the FL convergence can be severely jeopardized by TO and\nQE, but intriguingly can be alleviated if the clients have uniform outage\nprobabilities. These insightful results motivate us to propose a robust FL\nscheme, named FedTOE, which performs joint allocation of wireless resources and\nquantization bits across the clients to minimize the QE while making the\nclients have the same TO probability. Extensive experimental results are\npresented to show the superior performance of FedTOE for a deep learning-based\nclassification task with transmission latency constraints.",
          "link": "http://arxiv.org/abs/2106.09397",
          "publishedOn": "2021-06-18T02:06:36.245Z",
          "wordCount": 648,
          "title": "Quantized Federated Learning under Transmission Delay and Outage Constraints. (arXiv:2106.09397v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1\">Sadhika Malladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Sanjeev Arora</a>",
          "description": "It is generally recognized that finite learning rate (LR), in contrast to\ninfinitesimal LR, is important for good generalization in real-life deep nets.\nMost attempted explanations propose approximating finite-LR SGD with Ito\nStochastic Differential Equations (SDEs), but formal justification for this\napproximation (e.g., (Li et al., 2019)) only applies to SGD with tiny LR.\nExperimental verification of the approximation appears computationally\ninfeasible. The current paper clarifies the picture with the following\ncontributions: (a) An efficient simulation algorithm SVAG that provably\nconverges to the conventionally used Ito SDE approximation. (b) A theoretically\nmotivated testable necessary condition for the SDE approximation and its most\nfamous implication, the linear scaling rule (Goyal et al., 2017), to hold. (c)\nExperiments using this simulation to demonstrate that the previously proposed\nSDE approximation can meaningfully capture the training and generalization\nproperties of common deep nets.",
          "link": "http://arxiv.org/abs/2102.12470",
          "publishedOn": "2021-06-18T02:06:36.238Z",
          "wordCount": 609,
          "title": "On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs). (arXiv:2102.12470v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Springstein_M/0/1/0/all/0/1\">Matthias Springstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1\">Eric M&#xfc;ller-Budack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>",
          "description": "The recognition of handwritten mathematical expressions in images and video\nframes is a difficult and unsolved problem yet. Deep convectional neural\nnetworks are basically a promising approach, but typically require a large\namount of labeled training data. However, such a large training dataset does\nnot exist for the task of handwritten formula recognition. In this paper, we\nintroduce a system that creates a large set of synthesized training examples of\nmathematical expressions which are derived from LaTeX documents. For this\npurpose, we propose a novel attention-based generative adversarial network to\ntranslate rendered equations to handwritten formulas. The datasets generated by\nthis approach contain hundreds of thousands of formulas, making it ideal for\npretraining or the design of more complex models. We evaluate our synthesized\ndataset and the recognition approach on the CROHME 2014 benchmark dataset.\nExperimental results demonstrate the feasibility of the approach.",
          "link": "http://arxiv.org/abs/2106.09432",
          "publishedOn": "2021-06-18T02:06:36.231Z",
          "wordCount": 605,
          "title": "Unsupervised Training Data Generation of Handwritten Formulas using Generative Adversarial Networks with Self-Attention. (arXiv:2106.09432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Charlton_C/0/1/0/all/0/1\">Colleen E. Charlton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_M/0/1/0/all/0/1\">Michael Tin Chung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brennan_P/0/1/0/all/0/1\">Paul M. Brennan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuriot_J/0/1/0/all/0/1\">Jacques D. Fleuriot</a>",
          "description": "Prediction of survival in patients diagnosed with a brain tumour is\nchallenging because of heterogeneous tumour behaviours and responses to\ntreatment. Better estimations of prognosis would support treatment planning and\npatient support. Advances in machine learning have informed development of\nclinical predictive models, but their integration into clinical practice is\nalmost non-existent. One reasons for this is the lack of interpretability of\nmodels. In this paper, we use a novel brain tumour dataset to compare two\ninterpretable rule list models against popular machine learning approaches for\nbrain tumour survival prediction. All models are quantitatively evaluated using\nstandard performance metrics. The rule lists are also qualitatively assessed\nfor their interpretability and clinical utility. The interpretability of the\nblack box machine learning models is evaluated using two post-hoc explanation\ntechniques, LIME and SHAP. Our results show that the rule lists were only\nslightly outperformed by the black box models. We demonstrate that rule list\nalgorithms produced simple decision lists that align with clinical expertise.\nBy comparison, post-hoc interpretability methods applied to black box models\nmay produce unreliable explanations of local model predictions. Model\ninterpretability is essential for understanding differences in predictive\nperformance and for integration into clinical practice.",
          "link": "http://arxiv.org/abs/2106.09424",
          "publishedOn": "2021-06-18T02:06:36.222Z",
          "wordCount": 637,
          "title": "Interpretable Machine Learning Classifiers for Brain Tumour Survival Prediction. (arXiv:2106.09424v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xin-Qiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yao-Xiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zi-Xuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhi-Hua Zhou</a>",
          "description": "In many real-world imitation learning tasks, the demonstrator and the learner\nhave to act in different but full observation spaces. This situation generates\nsignificant obstacles for existing imitation learning approaches to work, even\nwhen they are combined with traditional space adaptation techniques. The main\nchallenge lies in bridging expert's occupancy measures to learner's dynamically\nchanging occupancy measures under the different observation spaces. In this\nwork, we model the above learning problem as Heterogeneous Observations\nImitation Learning (HOIL). We propose the Importance Weighting with REjection\n(IWRE) algorithm based on the techniques of importance-weighting, learning with\nrejection, and active querying to solve the key challenge of occupancy measure\nmatching. Experimental results show that IWRE can successfully solve HOIL\ntasks, including the challenging task of transforming the vision-based\ndemonstrations to random access memory (RAM)-based policies under the Atari\ndomain.",
          "link": "http://arxiv.org/abs/2106.09256",
          "publishedOn": "2021-06-18T02:06:36.215Z",
          "wordCount": 581,
          "title": "Seeing Differently, Acting Similarly: Imitation Learning with Heterogeneous Observations. (arXiv:2106.09256v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Utkin_L/0/1/0/all/0/1\">Lev V. Utkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstantinov_A/0/1/0/all/0/1\">Andrei V. Konstantinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishniakov_K/0/1/0/all/0/1\">Kirill A. Vishniakov</a>",
          "description": "One of the most popular methods of the machine learning prediction\nexplanation is the SHapley Additive exPlanations method (SHAP). An imprecise\nSHAP as a modification of the original SHAP is proposed for cases when the\nclass probability distributions are imprecise and represented by sets of\ndistributions. The first idea behind the imprecise SHAP is a new approach for\ncomputing the marginal contribution of a feature, which fulfils the important\nefficiency property of Shapley values. The second idea is an attempt to\nconsider a general approach to calculating and reducing interval-valued Shapley\nvalues, which is similar to the idea of reachable probability intervals in the\nimprecise probability theory. A simple special implementation of the general\napproach in the form of linear optimization problems is proposed, which is\nbased on using the Kolmogorov-Smirnov distance and imprecise contamination\nmodels. Numerical examples with synthetic and real data illustrate the\nimprecise SHAP.",
          "link": "http://arxiv.org/abs/2106.09111",
          "publishedOn": "2021-06-18T02:06:36.196Z",
          "wordCount": 599,
          "title": "An Imprecise SHAP as a Tool for Explaining the Class Probability Distributions under Limited Training Data. (arXiv:2106.09111v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorfman_A/0/1/0/all/0/1\">Adam Dorfman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McInnis_P/0/1/0/all/0/1\">Paul McInnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>",
          "description": "In this study, we take a departure and explore an explainability-driven\nstrategy to data auditing, where actionable insights into the data at hand are\ndiscovered through the eyes of quantitative explainability on the behaviour of\na dummy model prototype when exposed to data. We demonstrate this strategy by\nauditing two popular medical benchmark datasets, and discover hidden data\nquality issues that lead deep learning models to make predictions for the wrong\nreasons. The actionable insights gained from this explainability driven data\nauditing strategy is then leveraged to address the discovered issues to enable\nthe creation of high-performing deep learning models with appropriate\nprediction behaviour. The hope is that such an explainability-driven strategy\ncan be complimentary to data-driven strategies to facilitate for more\nresponsible development of machine learning algorithms for computer vision\napplications.",
          "link": "http://arxiv.org/abs/2106.09177",
          "publishedOn": "2021-06-18T02:06:36.189Z",
          "wordCount": 593,
          "title": "Insights into Data through Model Behaviour: An Explainability-driven Strategy for Data Auditing for Responsible Computer Vision Applications. (arXiv:2106.09177v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Juno Kim</a>",
          "description": "Social media creates crucial mass changes, as popular posts and opinions cast\na significant influence on users' decisions and thought processes. For example,\nthe recent Reddit uprising inspired by r/wallstreetbets which had remarkable\neconomic impact was started with a series of posts on the thread. The\nprediction of posts that may have a notable impact will allow for the\npreparation of possible following trends. This study aims to develop a machine\nlearning model capable of accurately predicting the popularity of a Reddit\npost. Specifically, the model is predicting the number of upvotes a post will\nreceive based on its textual content. I experimented with three different\nmodels: a baseline linear regression model, a random forest regression model,\nand a neural network. I collected Reddit post data from an online data set and\nanalyzed the model's performance when trained on a single subreddit and a\ncollection of subreddits. The results showed that the neural network model\nperformed the best when the loss of the models were compared. With the use of a\nmachine learning model to predict social trends through the reaction users have\nto post, a better picture of the near future can be envisioned.",
          "link": "http://arxiv.org/abs/2106.07380",
          "publishedOn": "2021-06-18T02:06:36.182Z",
          "wordCount": 645,
          "title": "Predicting the Popularity of Reddit Posts with AI. (arXiv:2106.07380v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mguni_D/0/1/0/all/0/1\">David Mguni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafferjee_T/0/1/0/all/0/1\">Taher Jafferjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Nieves_N/0/1/0/all/0/1\">Nicolas Perez-Nieves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenbin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaodong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_F/0/1/0/all/0/1\">Feifei Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiangcheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "Reward shaping (RS) is a powerful method in reinforcement learning (RL) for\novercoming the problem of sparse or uninformative rewards. However, RS\ntypically relies on manually engineered shaping-reward functions whose\nconstruction is time-consuming and error-prone. It also requires domain\nknowledge which runs contrary to the goal of autonomous learning. We introduce\nReinforcement Learning Optimal Shaping Algorithm (ROSA), an automated RS\nframework in which the shaping-reward function is constructed in a novel Markov\ngame between two agents. A reward-shaping agent (Shaper) uses switching\ncontrols to determine which states to add shaping rewards and their optimal\nvalues while the other agent (Controller) learns the optimal policy for the\ntask using these shaped rewards. We prove that ROSA, which easily adopts\nexisting RL algorithms, learns to construct a shaping-reward function that is\ntailored to the task thus ensuring efficient convergence to high performance\npolicies. We demonstrate ROSA's congenial properties in three carefully\ndesigned experiments and show its superior performance against state-of-the-art\nRS algorithms in challenging sparse reward environments.",
          "link": "http://arxiv.org/abs/2103.09159",
          "publishedOn": "2021-06-18T02:06:36.175Z",
          "wordCount": 654,
          "title": "Learning to Shape Rewards using a Game of Switching Controls. (arXiv:2103.09159v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Joonyoung Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>",
          "description": "Unsupervised image-to-image translation methods such as CycleGAN learn to\nconvert images from one domain to another using unpaired training data sets\nfrom different domains. Unfortunately, these approaches still require centrally\ncollected unpaired records, potentially violating privacy and security issues.\nAlthough the recent federated learning (FL) allows a neural network to be\ntrained without data exchange, the basic assumption of the FL is that all\nclients have their own training data from a similar domain, which is different\nfrom our image-to-image translation scenario in which each client has images\nfrom its unique domain and the goal is to learn image translation between\ndifferent domains without accessing the target domain data. To address this,\nhere we propose a novel federated CycleGAN architecture that can learn image\ntranslation in an unsupervised manner while maintaining the data privacy.\nSpecifically, our approach arises from a novel observation that CycleGAN loss\ncan be decomposed into the sum of client specific local objectives that can be\nevaluated using only their data. This local objective decomposition allows\nmultiple clients to participate in federated CycleGAN training without\nsacrificing performance. Furthermore, our method employs novel switchable\ngenerator and discriminator architecture using Adaptive Instance Normalization\n(AdaIN) that significantly reduces the band-width requirement of the federated\nlearning. Our experimental results on various unsupervised image translation\ntasks show that our federated CycleGAN provides comparable performance compared\nto the non-federated counterpart.",
          "link": "http://arxiv.org/abs/2106.09246",
          "publishedOn": "2021-06-18T02:06:36.169Z",
          "wordCount": 666,
          "title": "Federated CycleGAN for Privacy-Preserving Image-to-Image Translation. (arXiv:2106.09246v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">De-An Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>",
          "description": "Generalization has been a long-standing challenge for reinforcement learning\n(RL). Visual RL, in particular, can be easily distracted by irrelevant factors\nin high-dimensional observation space. In this work, we consider robust policy\nlearning which targets zero-shot generalization to unseen visual environments\nwith large distributional shift. We propose SECANT, a novel self-expert cloning\ntechnique that leverages image augmentation in two stages to decouple robust\nrepresentation learning from policy optimization. Specifically, an expert\npolicy is first trained by RL from scratch with weak augmentations. A student\nnetwork then learns to mimic the expert policy by supervised learning with\nstrong augmentations, making its representation more robust against visual\nvariations compared to the expert. Extensive experiments demonstrate that\nSECANT significantly advances the state of the art in zero-shot generalization\nacross 4 challenging domains. Our average reward improvements over prior SOTAs\nare: DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based\nautonomous driving (+47.7%), and indoor object navigation (+15.8%). Code\nrelease and video are available at https://linxifan.github.io/secant-site/.",
          "link": "http://arxiv.org/abs/2106.09678",
          "publishedOn": "2021-06-18T02:06:36.144Z",
          "wordCount": 621,
          "title": "SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual Policies. (arXiv:2106.09678v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dongjie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yundong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Haiwen Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhaoshuo Tian</a>",
          "description": "Attention mechanism enables the Graph Neural Networks(GNNs) to learn the\nattention weights between the target node and its one-hop neighbors, the\nperformance is further improved. However, the most existing GNNs are oriented\nto homogeneous graphs and each layer can only aggregate the information of\none-hop neighbors. Stacking multi-layer networks will introduce a lot of noise\nand easily lead to over smoothing. We propose a Multi-hop Heterogeneous\nNeighborhood information Fusion graph representation learning method (MHNF).\nSpecifically, we first propose a hybrid metapath autonomous extraction model to\nefficiently extract multi-hop hybrid neighbors. Then, we propose a hop-level\nheterogeneous Information aggregation model, which selectively aggregates\ndifferent-hop neighborhood information within the same hybrid metapath.\nFinally, a hierarchical semantic attention fusion model (HSAF) is proposed,\nwhich can efficiently integrate different-hop and different-path neighborhood\ninformation respectively. This paper can solve the problem of aggregating the\nmulti-hop neighborhood information and can learn hybrid metapaths for target\ntask, reducing the limitation of manually specifying metapaths. In addition,\nHSAF can extract the internal node information of the metapaths and better\nintegrate the semantic information of different levels. Experimental results on\nreal datasets show that MHNF is superior to state-of-the-art methods in node\nclassification and clustering tasks (10.94% - 69.09% and 11.58% - 394.93%\nrelative improvement on average, respectively).",
          "link": "http://arxiv.org/abs/2106.09289",
          "publishedOn": "2021-06-18T02:06:36.137Z",
          "wordCount": 645,
          "title": "MHNF: Multi-hop Heterogeneous Neighborhood information Fusion graph representation learning. (arXiv:2106.09289v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotzing_T/0/1/0/all/0/1\">Timo K&#xf6;tzing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_K/0/1/0/all/0/1\">Karen Seidel</a>",
          "description": "We investigate learning collections of languages from texts by an inductive\ninference machine with access to the current datum and a bounded memory in form\nof states. Such a bounded memory states (BMS) learner is considered successful\nin case it eventually settles on a correct hypothesis while exploiting only\nfinitely many different states.\n\nWe give the complete map of all pairwise relations for an established\ncollection of criteria of successfull learning. Most prominently, we show that\nnon-U-shapedness is not restrictive, while conservativeness and (strong)\nmonotonicity are. Some results carry over from iterative learning by a general\nlemma showing that, for a wealth of restrictions (the semantic restrictions),\niterative and bounded memory states learning are equivalent. We also give an\nexample of a non-semantic restriction (strongly non-U-shapedness) where the two\nsettings differ.",
          "link": "http://arxiv.org/abs/2010.04782",
          "publishedOn": "2021-06-18T02:06:36.130Z",
          "wordCount": 614,
          "title": "Learning Languages in the Limit from Positive Information with Finitely Many Memory Changes. (arXiv:2010.04782v3 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cang_C/0/1/0/all/0/1\">Catherine Cang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1\">Aravind Rajeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1\">Michael Laskin</a>",
          "description": "Offline Reinforcement Learning (RL) aims to extract near-optimal policies\nfrom imperfect offline data without additional environment interactions.\nExtracting policies from diverse offline datasets has the potential to expand\nthe range of applicability of RL by making the training process safer, faster,\nand more streamlined. We investigate how to improve the performance of offline\nRL algorithms, its robustness to the quality of offline data, as well as its\ngeneralization capabilities. To this end, we introduce Offline Model-based RL\nwith Adaptive Behavioral Priors (MABE). Our algorithm is based on the finding\nthat dynamics models, which support within-domain generalization, and\nbehavioral priors, which support cross-domain generalization, are\ncomplementary. When combined together, they substantially improve the\nperformance and generalization of offline RL policies. In the widely studied\nD4RL offline RL benchmark, we find that MABE achieves higher average\nperformance compared to prior model-free and model-based algorithms. In\nexperiments that require cross-domain generalization, we find that MABE\noutperforms prior methods. Our website is available at\nhttps://sites.google.com/berkeley.edu/mabe .",
          "link": "http://arxiv.org/abs/2106.09119",
          "publishedOn": "2021-06-18T02:06:36.122Z",
          "wordCount": 603,
          "title": "Behavioral Priors and Dynamics Models: Improving Performance and Domain Transfer in Offline RL. (arXiv:2106.09119v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krumnack_M/0/1/0/all/0/1\">Mats L. Richter Leila Malihi Anne-Kathrin Patricia Windler Ulf Krumnack</a>",
          "description": "In this work we explore the information processing inside neural networks\nusing logistic regression probes \\cite{probes} and the saturation metric\n\\cite{featurespace_saturation}. We show that problem difficulty and neural\nnetwork capacity affect the predictive performance in an antagonistic manner,\nopening the possibility of detecting over- and under-parameterization of neural\nnetworks for a given task. We further show that the observed effects are\nindependent from previously reported pathological patterns like the ``tail\npattern'' described in \\cite{featurespace_saturation}. Finally we are able to\nshow that saturation patterns converge early during training, allowing for a\nquicker cycle time during analysis",
          "link": "http://arxiv.org/abs/2106.09526",
          "publishedOn": "2021-06-18T02:06:36.115Z",
          "wordCount": 536,
          "title": "Exploring the Properties and Evolution of Neural Network Eigenspaces during Training. (arXiv:2106.09526v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diffenderfer_J/0/1/0/all/0/1\">James Diffenderfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartoldson_B/0/1/0/all/0/1\">Brian R. Bartoldson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaganti_S/0/1/0/all/0/1\">Shreya Chaganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jize Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>",
          "description": "Two crucial requirements for a successful adoption of deep learning (DL) in\nthe wild are: (1) robustness to distributional shifts, and (2) model\ncompactness for achieving efficiency. Unfortunately, efforts towards\nsimultaneously achieving Out-of-Distribution (OOD) robustness and extreme model\ncompactness without sacrificing accuracy have mostly been unsuccessful. This\nraises an important question: \"Is the inability to create compact, accurate,\nand robust deep neural networks (CARDs) fundamental?\" To answer this question,\nwe perform a large-scale analysis for a range of popular model compression\ntechniques which uncovers several intriguing patterns. Notably, in contrast to\ntraditional pruning approaches (e.g., fine tuning and gradual magnitude\npruning), we find that \"lottery ticket-style\" pruning approaches can\nsurprisingly be used to create high performing CARDs. Specifically, we are able\nto create extremely compact CARDs that are dramatically more robust than their\nsignificantly larger and full-precision counterparts while matching (or\nbeating) their test accuracy, simply by pruning and/or quantizing. To better\nunderstand these differences, we perform sensitivity analysis in the Fourier\ndomain for CARDs trained using different data augmentation methods. Motivated\nby our analysis, we develop a simple domain-adaptive test-time ensembling\napproach (CARD-Deck) that uses a gating module to dynamically select an\nappropriate CARD from the CARD-Deck based on their spectral-similarity with\ntest samples. By leveraging complementary frequency biases of different\ncompressed models, the proposed approach builds a \"winning hand\" of CARDs that\nestablishes a new state-of-the-art on CIFAR-10-C accuracies (i.e., 96.8% clean\nand 92.75% robust) with dramatically better memory usage than their\nnon-compressed counterparts. We also present some theoretical evidences\nsupporting our empirical findings.",
          "link": "http://arxiv.org/abs/2106.09129",
          "publishedOn": "2021-06-18T02:06:36.096Z",
          "wordCount": 692,
          "title": "A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness. (arXiv:2106.09129v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09415",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Thumwanit_N/0/1/0/all/0/1\">Napat Thumwanit</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lortararprasert_C/0/1/0/all/0/1\">Chayaphol Lortararprasert</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yano_H/0/1/0/all/0/1\">Hiroshi Yano</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Raymond_R/0/1/0/all/0/1\">Rudy Raymond</a>",
          "description": "Quantum classifiers provide sophisticated embeddings of input data in Hilbert\nspace promising quantum advantage. The advantage stems from quantum feature\nmaps encoding the inputs into quantum states with variational quantum circuits.\nA recent work shows how to map discrete features with fewer quantum bits using\nQuantum Random Access Coding (QRAC), an important primitive to encode binary\nstrings into quantum states. We propose a new method to embed discrete features\nwith trainable quantum circuits by combining QRAC and a recently proposed\nstrategy for training quantum feature map called quantum metric learning. We\nshow that the proposed trainable embedding requires not only as few qubits as\nQRAC but also overcomes the limitations of QRAC to classify inputs whose\nclasses are based on hard Boolean functions. We numerically demonstrate its use\nin variational quantum classifiers to achieve better performances in\nclassifying real-world datasets, and thus its possibility to leverage near-term\nquantum computers for quantum machine learning.",
          "link": "http://arxiv.org/abs/2106.09415",
          "publishedOn": "2021-06-18T02:06:36.090Z",
          "wordCount": 585,
          "title": "Trainable Discrete Feature Embeddings for Variational Quantum Classifier. (arXiv:2106.09415v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1\">Koushik Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Shilpak Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Ashish Kumar Pandey</a>",
          "description": "We have proposed orthogonal-Pad\\'e activation functions, which are trainable\nactivation functions and show that they have faster learning capability and\nimproves the accuracy in standard deep learning datasets and models. Based on\nour experiments, we have found two best candidates out of six orthogonal-Pad\\'e\nactivations, which we call safe Hermite-Pade (HP) activation functions, namely\nHP-1 and HP-2. When compared to ReLU, HP-1 and HP-2 has an increment in top-1\naccuracy by 5.06% and 4.63% respectively in PreActResNet-34, by 3.02% and 2.75%\nrespectively in MobileNet V2 model on CIFAR100 dataset while on CIFAR10 dataset\ntop-1 accuracy increases by 2.02% and 1.78% respectively in PreActResNet-34, by\n2.24% and 2.06% respectively in LeNet, by 2.15% and 2.03% respectively in\nEfficientnet B0.",
          "link": "http://arxiv.org/abs/2106.09693",
          "publishedOn": "2021-06-18T02:06:36.079Z",
          "wordCount": 581,
          "title": "Orthogonal-Pad\\'e Activation Functions: Trainable Activation functions for smooth and faster convergence in deep networks. (arXiv:2106.09693v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Q/0/1/0/all/0/1\">Qiuxia Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>",
          "description": "Time series is a special type of sequence data, a set of observations\ncollected at even intervals of time and ordered chronologically. Existing deep\nlearning techniques use generic sequence models (e.g., recurrent neural\nnetwork, Transformer model, or temporal convolutional network) for time series\nanalysis, which ignore some of its unique properties. For example, the\ndownsampling of time series data often preserves most of the information in the\ndata, while this is not true for general sequence data such as text sequence\nand DNA sequence. Motivated by the above, in this paper, we propose a novel\nneural network architecture and apply it for the time series forecasting\nproblem, wherein we conduct sample convolution and interaction at multiple\nresolutions for temporal modeling. The proposed architecture, namelySCINet,\nfacilitates extracting features with enhanced predictability. Experimental\nresults show that SCINet achieves significant prediction accuracy improvement\nover existing solutions across various real-world time series forecasting\ndatasets. In particular, it can achieve high fore-casting accuracy for those\ntemporal-spatial datasets without using sophisticated spatial modeling\ntechniques. Our codes and data are presented in the supplemental material.",
          "link": "http://arxiv.org/abs/2106.09305",
          "publishedOn": "2021-06-18T02:06:36.053Z",
          "wordCount": 619,
          "title": "Time Series is a Special Sequence: Forecasting with Sample Convolution and Interaction. (arXiv:2106.09305v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Da Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "We propose a reparametrization scheme to address the challenges of applying\ndifferentially private SGD on large neural networks, which are 1) the huge\nmemory cost of storing individual gradients, 2) the added noise suffering\nnotorious dimensional dependence. Specifically, we reparametrize each weight\nmatrix with two \\emph{gradient-carrier} matrices of small dimension and a\n\\emph{residual weight} matrix. We argue that such reparametrization keeps the\nforward/backward process unchanged while enabling us to compute the projected\ngradient without computing the gradient itself. To learn with differential\nprivacy, we design \\emph{reparametrized gradient perturbation (RGP)} that\nperturbs the gradients on gradient-carrier matrices and reconstructs an update\nfor the original weight from the noisy gradients. Importantly, we use\nhistorical updates to find the gradient-carrier matrices, whose optimality is\nrigorously justified under linear regression and empirically verified with deep\nlearning tasks. RGP significantly reduces the memory cost and improves the\nutility. For example, we are the first able to apply differential privacy on\nthe BERT model and achieve an average accuracy of $83.9\\%$ on four downstream\ntasks with $\\epsilon=8$, which is within $5\\%$ loss compared to the non-private\nbaseline but enjoys much lower privacy leakage risk.",
          "link": "http://arxiv.org/abs/2106.09352",
          "publishedOn": "2021-06-18T02:06:36.026Z",
          "wordCount": 644,
          "title": "Large Scale Private Learning via Low-rank Reparametrization. (arXiv:2106.09352v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09215",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1\">Chihua Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1\">Guang Cheng</a>",
          "description": "With increasingly more hyperparameters involved in their training, machine\nlearning systems demand a better understanding of hyperparameter tuning\nautomation. This has raised interest in studies of provably black-box\noptimization, which is made more practical by better exploration mechanism\nimplemented in algorithm design, managing the flux of both optimization and\nstatistical errors. Prior efforts focus on delineating optimization errors, but\nthis is deficient: black-box optimization algorithms can be inefficient without\nconsidering heterogeneity among reward samples. In this paper, we make the key\ndelineation on the role of statistical uncertainty in black-box optimization,\nguiding a more efficient algorithm design. We introduce\n\\textit{optimum-statistical collaboration}, a framework of managing the\ninteraction between optimization error flux and statistical error flux evolving\nin the optimization process. Inspired by this framework, we propose the\n\\texttt{VHCT} algorithms for objective functions with only local-smoothness\nassumptions. In theory, we prove our algorithm enjoys rate-optimal regret\nbounds; in experiments, we show the algorithm outperforms prior efforts in\nextensive settings.",
          "link": "http://arxiv.org/abs/2106.09215",
          "publishedOn": "2021-06-18T02:06:35.999Z",
          "wordCount": 586,
          "title": "Optimum-statistical collaboration towards efficient black-box optimization. (arXiv:2106.09215v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao%2A_Y/0/1/0/all/0/1\">Yulong Cao*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang%2A_N/0/1/0/all/0/1\">Ningfei Wang*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao%2A_C/0/1/0/all/0/1\">Chaowei Xiao*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang%2A_D/0/1/0/all/0/1\">Dawei Yang*</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Alfred Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a> (*co-first authors)",
          "description": "In Autonomous Driving (AD) systems, perception is both security and safety\ncritical. Despite various prior studies on its security issues, all of them\nonly consider attacks on camera- or LiDAR-based AD perception alone. However,\nproduction AD systems today predominantly adopt a Multi-Sensor Fusion (MSF)\nbased design, which in principle can be more robust against these attacks under\nthe assumption that not all fusion sources are (or can be) attacked at the same\ntime. In this paper, we present the first study of security issues of MSF-based\nperception in AD systems. We directly challenge the basic MSF design assumption\nabove by exploring the possibility of attacking all fusion sources\nsimultaneously. This allows us for the first time to understand how much\nsecurity guarantee MSF can fundamentally provide as a general defense strategy\nfor AD perception.\n\nWe formulate the attack as an optimization problem to generate a\nphysically-realizable, adversarial 3D-printed object that misleads an AD system\nto fail in detecting it and thus crash into it. We propose a novel attack\npipeline that addresses two main design challenges: (1) non-differentiable\ntarget camera and LiDAR sensing systems, and (2) non-differentiable cell-level\naggregated features popularly used in LiDAR-based AD perception. We evaluate\nour attack on MSF included in representative open-source industry-grade AD\nsystems in real-world driving scenarios. Our results show that the attack\nachieves over 90% success rate across different object types and MSF. Our\nattack is also found stealthy, robust to victim positions, transferable across\nMSF algorithms, and physical-world realizable after being 3D-printed and\ncaptured by LiDAR and camera devices. To concretely assess the end-to-end\nsafety impact, we further perform simulation evaluation and show that it can\ncause a 100% vehicle collision rate for an industry-grade AD system.",
          "link": "http://arxiv.org/abs/2106.09249",
          "publishedOn": "2021-06-18T02:06:35.985Z",
          "wordCount": 769,
          "title": "Invisible for both Camera and LiDAR: Security of Multi-Sensor Fusion based Perception in Autonomous Driving Under Physical-World Attacks. (arXiv:2106.09249v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pesme_S/0/1/0/all/0/1\">Scott Pesme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillaud_Vivien_L/0/1/0/all/0/1\">Loucas Pillaud-Vivien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1\">Nicolas Flammarion</a>",
          "description": "Understanding the implicit bias of training algorithms is of crucial\nimportance in order to explain the success of overparametrised neural networks.\nIn this paper, we study the dynamics of stochastic gradient descent over\ndiagonal linear networks through its continuous time version, namely stochastic\ngradient flow. We explicitly characterise the solution chosen by the stochastic\nflow and prove that it always enjoys better generalisation properties than that\nof gradient flow. Quite surprisingly, we show that the convergence speed of the\ntraining loss controls the magnitude of the biasing effect: the slower the\nconvergence, the better the bias. To fully complete our analysis, we provide\nconvergence guarantees for the dynamics. We also give experimental results\nwhich support our theoretical claims. Our findings highlight the fact that\nstructured noise can induce better generalisation and they help explain the\ngreater performances observed in practice of stochastic gradient descent over\ngradient descent.",
          "link": "http://arxiv.org/abs/2106.09524",
          "publishedOn": "2021-06-18T02:06:35.973Z",
          "wordCount": 584,
          "title": "Implicit Bias of SGD for Diagonal Linear Networks: a Provable Benefit of Stochasticity. (arXiv:2106.09524v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziko_I/0/1/0/all/0/1\">Imtiaz Masud Ziko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1\">Malik Boudiaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>",
          "description": "We investigate a general formulation for clustering and transductive few-shot\nlearning, which integrates prototype-based objectives, Laplacian regularization\nand supervision constraints from a few labeled data points. We propose a\nconcave-convex relaxation of the problem, and derive a computationally\nefficient block-coordinate bound optimizer, with convergence guarantee. At each\niteration,our optimizer computes independent (parallel) updates for each\npoint-to-cluster assignment. Therefore, it could be trivially distributed for\nlarge-scale clustering and few-shot tasks. Furthermore, we provides a thorough\nconvergence analysis based on point-to-set maps. Were port comprehensive\nclustering and few-shot learning experiments over various data sets, showing\nthat our method yields competitive performances, in term of accuracy and\noptimization quality, while scaling up to large problems. Using standard\ntraining on the base classes, without resorting to complex meta-learning and\nepisodic-training strategies, our approach outperforms state-of-the-art\nfew-shot methods by significant margins, across various models, settings and\ndata sets. Surprisingly, we found that even standard clustering procedures\n(e.g., K-means), which correspond to particular, non-regularized cases of our\ngeneral model, already achieve competitive performances in comparison to the\nstate-of-the-art in few-shot learning. These surprising results point to the\nlimitations of the current few-shot benchmarks, and question the viability of a\nlarge body of convoluted few-shot learning techniques in the recent literature.",
          "link": "http://arxiv.org/abs/2106.09516",
          "publishedOn": "2021-06-18T02:06:35.965Z",
          "wordCount": 644,
          "title": "Transductive Few-Shot Learning: Clustering is All You Need?. (arXiv:2106.09516v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Long-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Ying Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>",
          "description": "Transferability estimation has been an essential tool in selecting a\npre-trained model and the layers of it to transfer, so as to maximize the\nperformance on a target task and prevent negative transfer. Existing estimation\nalgorithms either require intensive training on target tasks or have\ndifficulties in evaluating the transferability between layers. We propose a\nsimple, efficient, and effective transferability measure named TransRate. With\nsingle pass through the target data, TransRate measures the transferability as\nthe mutual information between the features of target examples extracted by a\npre-trained model and labels of them. We overcome the challenge of efficient\nmutual information estimation by resorting to coding rate that serves as an\neffective alternative to entropy. TransRate is theoretically analyzed to be\nclosely related to the performance after transfer learning. Despite its\nextraordinary simplicity in 10 lines of codes, TransRate performs remarkably\nwell in extensive evaluations on 22 pre-trained models and 16 downstream tasks.",
          "link": "http://arxiv.org/abs/2106.09362",
          "publishedOn": "2021-06-18T02:06:35.956Z",
          "wordCount": 577,
          "title": "Frustratingly Easy Transferability Estimation. (arXiv:2106.09362v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wagener_N/0/1/0/all/0/1\">Nolan Wagener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1\">Byron Boots</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Ching-An Cheng</a>",
          "description": "Many sequential decision problems involve finding a policy that maximizes\ntotal reward while obeying safety constraints. Although much recent research\nhas focused on the development of safe reinforcement learning (RL) algorithms\nthat produce a safe policy after training, ensuring safety during training as\nwell remains an open problem. A fundamental challenge is performing exploration\nwhile still satisfying constraints in an unknown Markov decision process (MDP).\nIn this work, we address this problem for the chance-constrained setting. We\npropose a new algorithm, SAILR, that uses an intervention mechanism based on\nadvantage functions to keep the agent safe throughout training and optimizes\nthe agent's policy using off-the-shelf RL algorithms designed for unconstrained\nMDPs. Our method comes with strong guarantees on safety during both training\nand deployment (i.e., after training and without the intervention mechanism)\nand policy performance compared to the optimal safety-constrained policy. In\nour experiments, we show that SAILR violates constraints far less during\ntraining than standard safe RL and constrained MDP approaches and converges to\na well-performing policy that can be deployed safely without intervention. Our\ncode is available at https://github.com/nolanwagener/safe_rl.",
          "link": "http://arxiv.org/abs/2106.09110",
          "publishedOn": "2021-06-18T02:06:35.930Z",
          "wordCount": 625,
          "title": "Safe Reinforcement Learning Using Advantage-Based Intervention. (arXiv:2106.09110v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>",
          "description": "This work focuses on the problem of unraveling nonlinearly mixed latent\ncomponents in an unsupervised manner. The latent components are assumed to\nreside in the probability simplex, and are transformed by an unknown\npost-nonlinear mixing system. This problem finds various applications in signal\nand data analytics, e.g., nonlinear hyperspectral unmixing, image embedding,\nand nonlinear clustering. Linear mixture learning problems are already\nill-posed, as identifiability of the target latent components is hard to\nestablish in general. With unknown nonlinearity involved, the problem is even\nmore challenging. Prior work offered a function equation-based formulation for\nprovable latent component identification. However, the identifiability\nconditions are somewhat stringent and unrealistic. In addition, the\nidentifiability analysis is based on the infinite sample (i.e., population)\ncase, while the understanding for practical finite sample cases has been\nelusive. Moreover, the algorithm in the prior work trades model expressiveness\nwith computational convenience, which often hinders the learning performance.\nOur contribution is threefold. First, new identifiability conditions are\nderived under largely relaxed assumptions. Second, comprehensive sample\ncomplexity results are presented -- which are the first of the kind. Third, a\nconstrained autoencoder-based algorithmic framework is proposed for\nimplementation, which effectively circumvents the challenges in the existing\nalgorithm. Synthetic and real experiments corroborate our theoretical analyses.",
          "link": "http://arxiv.org/abs/2106.09070",
          "publishedOn": "2021-06-18T02:06:35.917Z",
          "wordCount": 639,
          "title": "Identifiability-Guaranteed Simplex-Structured Post-Nonlinear Mixture Learning via Autoencoder. (arXiv:2106.09070v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Shaashwat Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sagnik Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouedi_O/0/1/0/all/0/1\">Ons Aouedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yenduri_G/0/1/0/all/0/1\">Gokul Yenduri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piamrat_K/0/1/0/all/0/1\">Kandaraj Piamrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sweta Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddikunta_P/0/1/0/all/0/1\">Praveen Kumar Reddy Maddikunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadekallu_T/0/1/0/all/0/1\">Thippa Reddy Gadekallu</a>",
          "description": "The rapid development of the Internet and smart devices trigger surge in\nnetwork traffic making its infrastructure more complex and heterogeneous. The\npredominated usage of mobile phones, wearable devices and autonomous vehicles\nare examples of distributed networks which generate huge amount of data each\nand every day. The computational power of these devices have also seen steady\nprogression which has created the need to transmit information, store data\nlocally and drive network computations towards edge devices. Intrusion\ndetection systems play a significant role in ensuring security and privacy of\nsuch devices. Machine Learning and Deep Learning with Intrusion Detection\nSystems have gained great momentum due to their achievement of high\nclassification accuracy. However the privacy and security aspects potentially\ngets jeopardised due to the need of storing and communicating data to\ncentralized server. On the contrary, federated learning (FL) fits in\nappropriately as a privacy-preserving decentralized learning technique that\ndoes not transfer data but trains models locally and transfers the parameters\nto the centralized server. The present paper aims to present an extensive and\nexhaustive review on the use of FL in intrusion detection system. In order to\nestablish the need for FL, various types of IDS, relevant ML approaches and its\nassociated issues are discussed. The paper presents detailed overview of the\nimplementation of FL in various aspects of anomaly detection. The allied\nchallenges of FL implementations are also identified which provides idea on the\nscope of future direction of research. The paper finally presents the plausible\nsolutions associated with the identified challenges in FL based intrusion\ndetection system implementation acting as a baseline for prospective research.",
          "link": "http://arxiv.org/abs/2106.09527",
          "publishedOn": "2021-06-18T02:06:35.902Z",
          "wordCount": 726,
          "title": "Federated Learning for Intrusion Detection System: Concepts, Challenges and Future Directions. (arXiv:2106.09527v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gui_X/0/1/0/all/0/1\">Xian-Jin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhang-Hao Tian</a>",
          "description": "Deep neural networks need large amounts of labeled data to achieve good\nperformance. In real-world applications, labels are usually collected from\nnon-experts such as crowdsourcing to save cost and thus are noisy. In the past\nfew years, deep learning methods for dealing with noisy labels have been\ndeveloped, many of which are based on the small-loss criterion. However, there\nare few theoretical analyses to explain why these methods could learn well from\nnoisy labels. In this paper, we theoretically explain why the widely-used\nsmall-loss criterion works. Based on the explanation, we reformalize the\nvanilla small-loss criterion to better tackle noisy labels. The experimental\nresults verify our theoretical explanation and also demonstrate the\neffectiveness of the reformalization.",
          "link": "http://arxiv.org/abs/2106.09291",
          "publishedOn": "2021-06-18T02:06:35.882Z",
          "wordCount": 571,
          "title": "Towards Understanding Deep Learning from Noisy Labels with Small-Loss Criterion. (arXiv:2106.09291v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blau_T/0/1/0/all/0/1\">Tom Blau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_G/0/1/0/all/0/1\">Gilad Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morere_P/0/1/0/all/0/1\">Philippe Morere</a>",
          "description": "State-of-the-art reinforcement learning (RL) algorithms suffer from high\nsample complexity, particularly in the sparse reward case. A popular strategy\nfor mitigating this problem is to learn control policies by imitating a set of\nexpert demonstrations. The drawback of such approaches is that an expert needs\nto produce demonstrations, which may be costly in practice. To address this\nshortcoming, we propose Probabilistic Planning for Demonstration Discovery\n(P2D2), a technique for automatically discovering demonstrations without access\nto an expert. We formulate discovering demonstrations as a search problem and\nleverage widely-used planning algorithms such as Rapidly-exploring Random Tree\nto find demonstration trajectories. These demonstrations are used to initialize\na policy, then refined by a generic RL algorithm. We provide theoretical\nguarantees of P2D2 finding successful trajectories, as well as bounds for its\nsampling complexity. We experimentally demonstrate the method outperforms\nclassic and intrinsic exploration RL techniques in a range of classic control\nand robotics tasks, requiring only a fraction of exploration samples and\nachieving better asymptotic performance.",
          "link": "http://arxiv.org/abs/2106.09203",
          "publishedOn": "2021-06-18T02:06:35.868Z",
          "wordCount": 605,
          "title": "Learning from Demonstration without Demonstrations. (arXiv:2106.09203v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pappadopulo_D/0/1/0/all/0/1\">Duccio Pappadopulo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_L/0/1/0/all/0/1\">Lisa Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farina_M/0/1/0/all/0/1\">Marco Farina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irsoy_O/0/1/0/all/0/1\">Ozan &#x130;rsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>",
          "description": "Many modern messaging systems allow fast and synchronous textual\ncommunication among many users. The resulting sequence of messages hides a more\ncomplicated structure in which independent sub-conversations are interwoven\nwith one another. This poses a challenge for any task aiming to understand the\ncontent of the chat logs or gather information from them. The ability to\ndisentangle these conversations is then tantamount to the success of many\ndownstream tasks such as summarization and question answering. Structured\ninformation accompanying the text such as user turn, user mentions, timestamps,\nis used as a cue by the participants themselves who need to follow the\nconversation and has been shown to be important for disentanglement. DAG-LSTMs,\na generalization of Tree-LSTMs that can handle directed acyclic dependencies,\nare a natural way to incorporate such information and its non-sequential\nnature. In this paper, we apply DAG-LSTMs to the conversation disentanglement\ntask. We perform our experiments on the Ubuntu IRC dataset. We show that the\nnovel model we propose achieves state of the art status on the task of\nrecovering reply-to relations and it is competitive on other disentanglement\nmetrics.",
          "link": "http://arxiv.org/abs/2106.09024",
          "publishedOn": "2021-06-18T02:06:35.860Z",
          "wordCount": 625,
          "title": "Disentangling Online Chats with DAG-Structured LSTMs. (arXiv:2106.09024v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Siyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_A/0/1/0/all/0/1\">Andreas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Brian Williams</a>",
          "description": "We propose Automatic Curricula via Expert Demonstrations (ACED), a\nreinforcement learning (RL) approach that combines the ideas of imitation\nlearning and curriculum learning in order to solve challenging robotic\nmanipulation tasks with sparse reward functions. Curriculum learning solves\ncomplicated RL tasks by introducing a sequence of auxiliary tasks with\nincreasing difficulty, yet how to automatically design effective and\ngeneralizable curricula remains a challenging research problem. ACED extracts\ncurricula from a small amount of expert demonstration trajectories by dividing\ndemonstrations into sections and initializing training episodes to states\nsampled from different sections of demonstrations. Through moving the reset\nstates from the end to the beginning of demonstrations as the learning agent\nimproves its performance, ACED not only learns challenging manipulation tasks\nwith unseen initializations and goals, but also discovers novel solutions that\nare distinct from the demonstrations. In addition, ACED can be naturally\ncombined with other imitation learning methods to utilize expert demonstrations\nin a more efficient manner, and we show that a combination of ACED with\nbehavior cloning allows pick-and-place tasks to be learned with as few as 1\ndemonstration and block stacking tasks to be learned with 20 demonstrations.",
          "link": "http://arxiv.org/abs/2106.09159",
          "publishedOn": "2021-06-18T02:06:35.835Z",
          "wordCount": 620,
          "title": "Automatic Curricula via Expert Demonstrations. (arXiv:2106.09159v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanaspeze_A/0/1/0/all/0/1\">Antoine Wehenkel Damien Lanaspeze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1\">Bertrand Corn&#xe9;lusse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutera_A/0/1/0/all/0/1\">Antonio Sutera</a>",
          "description": "Greater direct electrification of end-use sectors with a higher share of\nrenewables is one of the pillars to power a carbon-neutral society by 2050.\nThis study uses a recent deep learning technique, the normalizing flows, to\nproduce accurate probabilistic forecasts that are crucial for decision-makers\nto face the new challenges in power systems applications. Through comprehensive\nempirical evaluations using the open data of the Global Energy Forecasting\nCompetition 2014, we demonstrate that our methodology is competitive with other\nstate-of-the-art deep learning generative models: generative adversarial\nnetworks and variational autoencoders. The models producing weather-based wind,\nsolar power, and load scenarios are properly compared both in terms of forecast\nvalue, by considering the case study of an energy retailer, and quality using\nseveral complementary metrics.",
          "link": "http://arxiv.org/abs/2106.09370",
          "publishedOn": "2021-06-18T02:06:35.828Z",
          "wordCount": 568,
          "title": "Deep generative modeling for probabilistic forecasting in power systems. (arXiv:2106.09370v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hanik_M/0/1/0/all/0/1\">Martin Hanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demirtas_M/0/1/0/all/0/1\">Mehmet Arif Demirta&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharsallaoui_M/0/1/0/all/0/1\">Mohammed Amine Gharsallaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>",
          "description": "Analyzing the relation between intelligence and neural activity is of the\nutmost importance in understanding the working principles of the human brain in\nhealth and disease. In existing literature, functional brain connectomes have\nbeen used successfully to predict cognitive measures such as intelligence\nquotient (IQ) scores in both healthy and disordered cohorts using machine\nlearning models. However, existing methods resort to flattening the brain\nconnectome (i.e., graph) through vectorization which overlooks its topological\nproperties. To address this limitation and inspired from the emerging graph\nneural networks (GNNs), we design a novel regression GNN model (namely RegGNN)\nfor predicting IQ scores from brain connectivity. On top of that, we introduce\na novel, fully modular sample selection method to select the best samples to\nlearn from for our target prediction task. However, since such deep learning\narchitectures are computationally expensive to train, we further propose a\n\\emph{learning-based sample selection} method that learns how to choose the\ntraining samples with the highest expected predictive power on unseen samples.\nFor this, we capitalize on the fact that connectomes (i.e., their adjacency\nmatrices) lie in the symmetric positive definite (SPD) matrix cone. Our results\non full-scale and verbal IQ prediction outperforms comparison methods in autism\nspectrum disorder cohorts and achieves a competitive performance for\nneurotypical subjects using 3-fold cross-validation. Furthermore, we show that\nour sample selection approach generalizes to other learning-based methods,\nwhich shows its usefulness beyond our GNN architecture.",
          "link": "http://arxiv.org/abs/2106.09408",
          "publishedOn": "2021-06-18T02:06:35.822Z",
          "wordCount": 678,
          "title": "Predicting cognitive scores with graph neural networks through sample selection learning. (arXiv:2106.09408v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jiahao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>",
          "description": "Enforcing orthogonality in neural networks is an antidote for gradient\nvanishing/exploding problems, sensitivity by adversarial perturbation, and\nbounding generalization errors. However, many previous approaches are\nheuristic, and the orthogonality of convolutional layers is not systematically\nstudied: some of these designs are not exactly orthogonal, while others only\nconsider standard convolutional layers and propose specific classes of their\nrealizations. To address this problem, we propose a theoretical framework for\northogonal convolutional layers, which establishes the equivalence between\nvarious orthogonal convolutional layers in the spatial domain and the\nparaunitary systems in the spectral domain. Since there exists a complete\nspectral factorization of paraunitary systems, any orthogonal convolution layer\ncan be parameterized as convolutions of spatial filters. Our framework endows\nhigh expressive power to various convolutional layers while maintaining their\nexact orthogonality. Furthermore, our layers are memory and computationally\nefficient for deep networks compared to previous designs. Our versatile\nframework, for the first time, enables the study of architecture designs for\ndeep orthogonal networks, such as choices of skip connection, initialization,\nstride, and dilation. Consequently, we scale up orthogonal networks to deep\narchitectures, including ResNet, WideResNet, and ShuffleNet, substantially\nincreasing the performance over the traditional shallow orthogonal networks.",
          "link": "http://arxiv.org/abs/2106.09121",
          "publishedOn": "2021-06-18T02:06:35.807Z",
          "wordCount": 638,
          "title": "Scaling-up Diverse Orthogonal Convolutional Networks with a Paraunitary Framework. (arXiv:2106.09121v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1\">Mert Seker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannisto_A/0/1/0/all/0/1\">Anssi M&#xe4;nnist&#xf6;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1\">Jenni Raitoharju</a>",
          "description": "Main characters in images are the most important humans that catch the\nviewer's attention upon first look, and they are emphasized by properties such\nas size, position, color saturation, and sharpness of focus. Identifying the\nmain character in images plays an important role in traditional photographic\nstudies and media analysis, but the task is performed manually and can be slow\nand laborious. Furthermore, selection of main characters can be sometimes\nsubjective. In this paper, we analyze the feasibility of solving the main\ncharacter recognition needed for photographic studies automatically and propose\na method for identifying the main characters. The proposed method uses machine\nlearning based human pose estimation along with traditional computer vision\napproaches for this task. We approach the task as a binary classification\nproblem where each detected human is classified either as a main character or\nnot. To evaluate both the subjectivity of the task and the performance of our\nmethod, we collected a dataset of 300 varying images from multiple sources and\nasked five people, a photographic researcher and four other persons, to\nannotate the main characters. Our analysis showed a relatively high agreement\nbetween different annotators. The proposed method achieved a promising F1 score\nof 0.83 on the full image set and 0.96 on a subset evaluated as most clear and\nimportant cases by the photographic researcher.",
          "link": "http://arxiv.org/abs/2106.09064",
          "publishedOn": "2021-06-18T02:06:35.776Z",
          "wordCount": 666,
          "title": "Automatic Main Character Recognition for Photographic Studies. (arXiv:2106.09064v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1\">Lucas Caccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>",
          "description": "This paper presents SPeCiaL: a method for unsupervised pretraining of\nrepresentations tailored for continual learning. Our approach devises a\nmeta-learning objective that differentiates through a sequential learning\nprocess. Specifically, we train a linear model over the representations to\nmatch different augmented views of the same image together, each view presented\nsequentially. The linear model is then evaluated on both its ability to\nclassify images it just saw, and also on images from previous iterations. This\ngives rise to representations that favor quick knowledge retention with minimal\nforgetting. We evaluate SPeCiaL in the Continual Few-Shot Learning setting, and\nshow that it can match or outperform other supervised pretraining approaches.",
          "link": "http://arxiv.org/abs/2106.09065",
          "publishedOn": "2021-06-18T02:06:35.762Z",
          "wordCount": 539,
          "title": "SPeCiaL: Self-Supervised Pretraining for Continual Learning. (arXiv:2106.09065v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chicchi_L/0/1/0/all/0/1\">Lorenzo Chicchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giambagli_L/0/1/0/all/0/1\">Lorenzo Giambagli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buffoni_L/0/1/0/all/0/1\">Lorenzo Buffoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carletti_T/0/1/0/all/0/1\">Timoteo Carletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciavarella_M/0/1/0/all/0/1\">Marco Ciavarella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanelli_D/0/1/0/all/0/1\">Duccio Fanelli</a>",
          "description": "Deep neural networks can be trained in reciprocal space, by acting on the\neigenvalues and eigenvectors of suitable transfer operators in direct space.\nAdjusting the eigenvalues, while freezing the eigenvectors, yields a\nsubstantial compression of the parameter space. This latter scales by\ndefinition with the number of computing neurons. The classification scores, as\nmeasured by the displayed accuracy, are however inferior to those attained when\nthe learning is carried in direct space, for an identical architecture and by\nemploying the full set of trainable parameters (with a quadratic dependence on\nthe size of neighbor layers). In this Letter, we propose a variant of the\nspectral learning method as appeared in Giambagli et al {Nat. Comm.} 2021,\nwhich leverages on two sets of eigenvalues, for each mapping between adjacent\nlayers. The eigenvalues act as veritable knobs which can be freely tuned so as\nto (i) enhance, or alternatively silence, the contribution of the input nodes,\n(ii) modulate the excitability of the receiving nodes with a mechanism which we\ninterpret as the artificial analogue of the homeostatic plasticity. The number\nof trainable parameters is still a linear function of the network size, but the\nperformances of the trained device gets much closer to those obtained via\nconventional algorithms, these latter requiring however a considerably heavier\ncomputational cost. The residual gap between conventional and spectral\ntrainings can be eventually filled by employing a suitable decomposition for\nthe non trivial block of the eigenvectors matrix. Each spectral parameter\nreflects back on the whole set of inter-nodes weights, an attribute which we\nshall effectively exploit to yield sparse networks with stunning classification\nabilities, as compared to their homologues trained with conventional means.",
          "link": "http://arxiv.org/abs/2106.09021",
          "publishedOn": "2021-06-18T02:06:35.746Z",
          "wordCount": 735,
          "title": "On the training of sparse and dense deep neural networks: less parameters, same performance. (arXiv:2106.09021v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mille_S/0/1/0/all/0/1\">Simon Mille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh D. Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahamood_S/0/1/0/all/0/1\">Saad Mahamood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Beltrachini_L/0/1/0/all/0/1\">Laura Perez-Beltrachini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miltenburg_E/0/1/0/all/0/1\">Emiel van Miltenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>",
          "description": "Machine learning approaches applied to NLP are often evaluated by summarizing\ntheir performance in a single number, for example accuracy. Since most test\nsets are constructed as an i.i.d. sample from the overall data, this approach\noverly simplifies the complexity of language and encourages overfitting to the\nhead of the data distribution. As such, rare language phenomena or text about\nunderrepresented groups are not equally included in the evaluation. To\nencourage more in-depth model analyses, researchers have proposed the use of\nmultiple test sets, also called challenge sets, that assess specific\ncapabilities of a model. In this paper, we develop a framework based on this\nidea which is able to generate controlled perturbations and identify subsets in\ntext-to-scalar, text-to-text, or data-to-text settings. By applying this\nframework to the GEM generation benchmark, we propose an evaluation suite made\nof 80 challenge sets, demonstrate the kinds of analyses that it enables and\nshed light onto the limits of current generation models.",
          "link": "http://arxiv.org/abs/2106.09069",
          "publishedOn": "2021-06-18T02:06:35.739Z",
          "wordCount": 609,
          "title": "Automatic Construction of Evaluation Suites for Natural Language Generation Datasets. (arXiv:2106.09069v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09082",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Maheshwari_C/0/1/0/all/0/1\">Chinmay Maheshwari</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chiu_C/0/1/0/all/0/1\">Chih-Yuan Chiu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mazumdar_E/0/1/0/all/0/1\">Eric Mazumdar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sastry_S/0/1/0/all/0/1\">S. Shankar Sastry</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ratliff_L/0/1/0/all/0/1\">Lillian J. Ratliff</a>",
          "description": "Min-max optimization is emerging as a key framework for analyzing problems of\nrobustness to strategically and adversarially generated data. We propose a\nrandom reshuffling-based gradient free Optimistic Gradient Descent-Ascent\nalgorithm for solving convex-concave min-max problems with finite sum\nstructure. We prove that the algorithm enjoys the same convergence rate as that\nof zeroth-order algorithms for convex minimization problems. We further\nspecialize the algorithm to solve distributionally robust, decision-dependent\nlearning problems, where gradient information is not readily available. Through\nillustrative simulations, we observe that our proposed approach learns models\nthat are simultaneously robust against adversarial distribution shifts and\nstrategic decisions from the data sources, and outperforms existing methods\nfrom the strategic classification literature.",
          "link": "http://arxiv.org/abs/2106.09082",
          "publishedOn": "2021-06-18T02:06:35.733Z",
          "wordCount": 563,
          "title": "Zeroth-Order Methods for Convex-Concave Minmax Problems: Applications to Decision-Dependent Risk Minimization. (arXiv:2106.09082v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhauri_Y/0/1/0/all/0/1\">Yash Akhauri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niranjan_A/0/1/0/all/0/1\">Adithya Niranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munoz_J/0/1/0/all/0/1\">J. Pablo Mu&#xf1;oz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Suvadeep Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davare_A/0/1/0/all/0/1\">Abhijit Davare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cocchini_P/0/1/0/all/0/1\">Pasquale Cocchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokin_A/0/1/0/all/0/1\">Anton A. Sorokin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Ravi Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nilesh Jain</a>",
          "description": "The rapidly evolving field of Artificial Intelligence necessitates automated\napproaches to co-design neural network architecture and neural accelerators to\nmaximize system efficiency and address productivity challenges. To enable joint\noptimization of this vast space, there has been growing interest in\ndifferentiable NN-HW co-design. Fully differentiable co-design has reduced the\nresource requirements for discovering optimized NN-HW configurations, but fail\nto adapt to general hardware accelerator search spaces. This is due to the\nexistence of non-synthesizable (invalid) designs in the search space of many\nhardware accelerators. To enable efficient and realizable co-design of\nconfigurable hardware accelerators with arbitrary neural network search spaces,\nwe introduce RHNAS. RHNAS is a method that combines reinforcement learning for\nhardware optimization with differentiable neural architecture search. RHNAS\ndiscovers realizable NN-HW designs with 1.84x lower latency and 1.86x lower\nenergy-delay product (EDP) on ImageNet and 2.81x lower latency and 3.30x lower\nEDP on CIFAR-10 over the default hardware accelerator design.",
          "link": "http://arxiv.org/abs/2106.09180",
          "publishedOn": "2021-06-18T02:06:35.727Z",
          "wordCount": 604,
          "title": "RHNAS: Realizable Hardware and Neural Architecture Search. (arXiv:2106.09180v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonnaire_T/0/1/0/all/0/1\">Tony Bonnaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decelle_A/0/1/0/all/0/1\">Aur&#xe9;lien Decelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghanim_N/0/1/0/all/0/1\">Nabila Aghanim</a>",
          "description": "A regularized version of Mixture Models is proposed to learn a principal\ngraph from a distribution of $D$-dimensional data points. In the particular\ncase of manifold learning for ridge detection, we assume that the underlying\nmanifold can be modeled as a graph structure acting like a topological prior\nfor the Gaussian clusters turning the problem into a maximum a posteriori\nestimation. Parameters of the model are iteratively estimated through an\nExpectation-Maximization procedure making the learning of the structure\ncomputationally efficient with guaranteed convergence for any graph prior in a\npolynomial time. We also embed in the formalism a natural way to make the\nalgorithm robust to outliers of the pattern and heteroscedasticity of the\nmanifold sampling coherently with the graph structure. The method uses a graph\nprior given by the minimum spanning tree that we extend using random\nsub-samplings of the dataset to take into account cycles that can be observed\nin the spatial distribution.",
          "link": "http://arxiv.org/abs/2106.09035",
          "publishedOn": "2021-06-18T02:06:35.720Z",
          "wordCount": 605,
          "title": "Regularization of Mixture Models for Robust Principal Graph Learning. (arXiv:2106.09035v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jeremiah Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Abhijit Guha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhy_S/0/1/0/all/0/1\">Shreyas Padhy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>",
          "description": "Mahalanobis distance (MD) is a simple and popular post-processing method for\ndetecting out-of-distribution (OOD) inputs in neural networks. We analyze its\nfailure modes for near-OOD detection and propose a simple fix called relative\nMahalanobis distance (RMD) which improves performance and is more robust to\nhyperparameter choice. On a wide selection of challenging vision, language, and\nbiology OOD benchmarks (CIFAR-100 vs CIFAR-10, CLINC OOD intent detection,\nGenomics OOD), we show that RMD meaningfully improves upon MD performance (by\nup to 15% AUROC on genomics OOD).",
          "link": "http://arxiv.org/abs/2106.09022",
          "publishedOn": "2021-06-18T02:06:35.714Z",
          "wordCount": 523,
          "title": "A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection. (arXiv:2106.09022v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qun Li</a>",
          "description": "With the fast development of quantum computing and deep learning, quantum\nneural networks have attracted great attention recently. By leveraging the\npower of quantum computing, deep neural networks can potentially overcome\ncomputational power limitations in classic machine learning. However, when\nmultiple quantum machines wish to train a global model using the local data on\neach machine, it may be very difficult to copy the data into one machine and\ntrain the model. Therefore, a collaborative quantum neural network framework is\nnecessary. In this article, we borrow the core idea of federated learning to\npropose QuantumFed, a quantum federated learning framework to have multiple\nquantum nodes with local quantum data train a mode together. Our experiments\nshow the feasibility and robustness of our framework.",
          "link": "http://arxiv.org/abs/2106.09109",
          "publishedOn": "2021-06-18T02:06:35.707Z",
          "wordCount": 558,
          "title": "QuantumFed: A Federated Learning Framework for Collaborative Quantum Training. (arXiv:2106.09109v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Paul Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1\">Christoph H. Lampert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bickel_B/0/1/0/all/0/1\">Bernd Bickel</a>",
          "description": "Our goal in this work is to generate realistic videos given just one initial\nframe as input. Existing unsupervised approaches to this task do not consider\nthe fact that a video typically shows a 3D environment, and that this should\nremain coherent from frame to frame even as the camera and objects move. We\naddress this by developing a model that first estimates the latent 3D structure\nof the scene, including the segmentation of any moving objects. It then\npredicts future frames by simulating the object and camera dynamics, and\nrendering the resulting views. Importantly, it is trained end-to-end using only\nthe unsupervised objective of predicting future frames, without any 3D\ninformation nor segmentation annotations. Experiments on two challenging\ndatasets of natural videos show that our model can estimate 3D structure and\nmotion segmentation from a single frame, and hence generate plausible and\nvaried predictions.",
          "link": "http://arxiv.org/abs/2106.09051",
          "publishedOn": "2021-06-18T02:06:35.659Z",
          "wordCount": 598,
          "title": "Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure. (arXiv:2106.09051v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Annabi_L/0/1/0/all/0/1\">Louis Annabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitti_A/0/1/0/all/0/1\">Alexandre Pitti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quoy_M/0/1/0/all/0/1\">Mathias Quoy</a>",
          "description": "As a phenomenon in dynamical systems allowing autonomous switching between\nstable behaviors, chaotic itinerancy has gained interest in neurorobotics\nresearch. In this study, we draw a connection between this phenomenon and the\npredictive coding theory by showing how a recurrent neural network implementing\npredictive coding can generate neural trajectories similar to chaotic\nitinerancy in the presence of input noise. We propose two scenarios generating\nrandom and past-independent attractor switching trajectories using our model.",
          "link": "http://arxiv.org/abs/2106.08937",
          "publishedOn": "2021-06-17T16:16:41.906Z",
          "wordCount": 505,
          "title": "A Predictive Coding Account for Chaotic Itinerancy. (arXiv:2106.08937v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2104.11014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Min-Fong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao-Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Min-Hung Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yu-Syuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hsien-Kai Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Min Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hung-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jou_K/0/1/0/all/0/1\">Kevin Jou</a>",
          "description": "Network spaces have been known as a critical factor in both handcrafted\nnetwork designs or defining search spaces for Neural Architecture Search (NAS).\nHowever, an effective space involves tremendous prior knowledge and/or manual\neffort, and additional constraints are required to discover efficiency-aware\narchitectures. In this paper, we define a new problem, Network Space Search\n(NSS), as searching for favorable network spaces instead of a single\narchitecture. We propose an NSS method to directly search for efficient-aware\nnetwork spaces automatically, reducing the manual effort and immense cost in\ndiscovering satisfactory ones. The resultant network spaces, named Elite\nSpaces, are discovered from Expanded Search Space with minimal human expertise\nimposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front\nunder various complexity constraints and can be further served as NAS search\nspaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an\naveragely 2.3% lower error rate and 3.7% closer to target constraint than the\nbaseline with around 90% fewer samples required to find satisfactory networks).\nMoreover, our NSS approach is capable of searching for superior spaces in\nfuture unexplored spaces, revealing great potential in searching for network\nspaces automatically. Website:\nhttps://minhungchen.netlify.app/publication/nss/.",
          "link": "http://arxiv.org/abs/2104.11014",
          "publishedOn": "2021-06-17T15:44:17.182Z",
          "wordCount": 697,
          "title": "Network Space Search for Pareto-Efficient Spaces. (arXiv:2104.11014v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1\">Sophia Bano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casella_A/0/1/0/all/0/1\">Alessandro Casella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisco Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1\">Sara Moccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attilakos_G/0/1/0/all/0/1\">George Attilakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wimalasundera_R/0/1/0/all/0/1\">Ruwan Wimalasundera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paladini_D/0/1/0/all/0/1\">Dario Paladini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deprest_J/0/1/0/all/0/1\">Jan Deprest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momi_E/0/1/0/all/0/1\">Elena De Momi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattos_L/0/1/0/all/0/1\">Leonardo S. Mattos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>",
          "description": "Fetoscopy laser photocoagulation is a widely used procedure for the treatment\nof Twin-to-Twin Transfusion Syndrome (TTTS), that occur in mono-chorionic\nmultiple pregnancies due to placental vascular anastomoses. This procedure is\nparticularly challenging due to limited field of view, poor manoeuvrability of\nthe fetoscope, poor visibility due to fluid turbidity, variability in light\nsource, and unusual position of the placenta. This may lead to increased\nprocedural time and incomplete ablation, resulting in persistent TTTS.\nComputer-assisted intervention may help overcome these challenges by expanding\nthe fetoscopic field of view through video mosaicking and providing better\nvisualization of the vessel network. However, the research and development in\nthis domain remain limited due to unavailability of high-quality data to encode\nthe intra- and inter-procedure variability. Through the \\textit{Fetoscopic\nPlacental Vessel Segmentation and Registration (FetReg)} challenge, we present\na large-scale multi-centre dataset for the development of generalized and\nrobust semantic segmentation and video mosaicking algorithms for the fetal\nenvironment with a focus on creating drift-free mosaics from long duration\nfetoscopy videos. In this paper, we provide an overview of the FetReg dataset,\nchallenge tasks, evaluation metrics and baseline methods for both segmentation\nand registration. Baseline methods results on the FetReg dataset shows that our\ndataset poses interesting challenges, offering large opportunity for the\ncreation of novel methods and models through a community effort initiative\nguided by the FetReg challenge.",
          "link": "http://arxiv.org/abs/2106.05923",
          "publishedOn": "2021-06-17T15:44:17.153Z",
          "wordCount": 708,
          "title": "FetReg: Placental Vessel Segmentation and Registration in Fetoscopy Challenge Dataset. (arXiv:2106.05923v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05739",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Domingo_Enrich_C/0/1/0/all/0/1\">Carles Domingo-Enrich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1\">Youssef Mroueh</a>",
          "description": "Several works in implicit and explicit generative modeling empirically\nobserved that feature-learning discriminators outperform fixed-kernel\ndiscriminators in terms of the sample quality of the models. We provide\nseparation results between probability metrics with fixed-kernel and\nfeature-learning discriminators using the function classes $\\mathcal{F}_2$ and\n$\\mathcal{F}_1$ respectively, which were developed to study overparametrized\ntwo-layer neural networks. In particular, we construct pairs of distributions\nover hyper-spheres that can not be discriminated by fixed kernel\n$(\\mathcal{F}_2)$ integral probability metric (IPM) and Stein discrepancy (SD)\nin high dimensions, but that can be discriminated by their feature learning\n($\\mathcal{F}_1$) counterparts. To further study the separation we provide\nlinks between the $\\mathcal{F}_1$ and $\\mathcal{F}_2$ IPMs with sliced\nWasserstein distances. Our work suggests that fixed-kernel discriminators\nperform worse than their feature learning counterparts because their\ncorresponding metrics are weaker.",
          "link": "http://arxiv.org/abs/2106.05739",
          "publishedOn": "2021-06-17T15:44:17.143Z",
          "wordCount": 586,
          "title": "Separation Results between Fixed-Kernel and Feature-Learning Probability Metrics. (arXiv:2106.05739v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1\">Richard Antonello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1\">Javier Turek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1\">Vy Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander Huth</a>",
          "description": "How related are the representations learned by neural language models,\ntranslation models, and language tagging tasks? We answer this question by\nadapting an encoder-decoder transfer learning method from computer vision to\ninvestigate the structure among 100 different feature spaces extracted from\nhidden representations of various networks trained on language tasks. This\nmethod reveals a low-dimensional structure where language models and\ntranslation models smoothly interpolate between word embeddings, syntactic and\nsemantic tasks, and future word embeddings. We call this low-dimensional\nstructure a language representation embedding because it encodes the\nrelationships between representations needed to process language for a variety\nof NLP tasks. We find that this representation embedding can predict how well\neach individual feature space maps to human brain responses to natural language\nstimuli recorded using fMRI. Additionally, we find that the principal dimension\nof this structure can be used to create a metric which highlights the brain's\nnatural language processing hierarchy. This suggests that the embedding\ncaptures some part of the brain's natural language representation structure.",
          "link": "http://arxiv.org/abs/2106.05426",
          "publishedOn": "2021-06-17T15:44:17.131Z",
          "wordCount": 628,
          "title": "Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14193",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Wen-Hua Chen</a>",
          "description": "This paper presents stability analysis tools for model predictive control\n(MPC) with and without terminal weight. Stability analysis of MPC with a\nlimited horizon but without terminal weight is a long-standing open problem. By\nusing a modified value function as an Lyapunov function candidate and the\nprinciple of optimality, this paper establishes stability conditions for this\ntype of widely spread MPC algorithms. A new stability guaranteed MPC algorithm\nwithout terminal weight (MPCS) is presented. With the help of designing a new\nsublevel set defined by the value function of one-step ahead stage cost,\nconditions for checking its recursive feasibility and stability of the proposed\nMPC algorithm are presented. The new stability condition and the derived MPCS\novercome the difficulties arising in the existing terminal weight based MPC\nframework, including the need of searching a suitable terminal weight and\npossible poor performance caused by an inappropriate terminal weight. This work\nis further extended to MPC with a terminal weight for the completeness.\nNumerical examples are presented to demonstrate the effectiveness of the\nproposed tool, whereas the existing stability analysis tools are either not\napplicable or lead to quite conservative results. It shows that the proposed\ntools offer a number of mechanisms to achieve stability: adjusting state and/or\ncontrol weights, extending the length of horizon, and adding a simple extra\nconstraint on the first or second state in the optimisation.",
          "link": "http://arxiv.org/abs/2011.14193",
          "publishedOn": "2021-06-17T01:58:46.948Z",
          "wordCount": 688,
          "title": "Model Predictive Control with and without Terminal Weight: Stability and Algorithms. (arXiv:2011.14193v2 [eess.SY] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02331",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tanaka_K/0/1/0/all/0/1\">Keitaro Tanaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sawata_R/0/1/0/all/0/1\">Ryosuke Sawata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takahashi_S/0/1/0/all/0/1\">Shusuke Takahashi</a>",
          "description": "This paper presents a new deep clustering (DC) method called manifold-aware\nDC (M-DC) that can enhance hyperspace utilization more effectively than the\noriginal DC. The original DC has a limitation in that a pair of two speakers\nhas to be embedded having an orthogonal relationship due to its use of the\none-hot vector-based loss function, while our method derives a unique loss\nfunction aimed at maximizing the target angle in the hyperspace based on the\nnature of a regular simplex. Our proposed loss imposes a higher penalty than\nthe original DC when the speaker is assigned incorrectly. The change from DC to\nM-DC can be easily achieved by rewriting just one term in the loss function of\nDC, without any other modifications to the network architecture or model\nparameters. As such, our method has high practicability because it does not\naffect the original inference part. The experimental results show that the\nproposed method improves the performances of the original DC and its expansion\nmethod.",
          "link": "http://arxiv.org/abs/2106.02331",
          "publishedOn": "2021-06-17T01:58:46.936Z",
          "wordCount": 628,
          "title": "Manifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex. (arXiv:2106.02331v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galvez_R/0/1/0/all/0/1\">Rafa G&#xe1;lvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moonsamy_V/0/1/0/all/0/1\">Veelasha Moonsamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_C/0/1/0/all/0/1\">Claudia Diaz</a>",
          "description": "In this paper we present LiM (\"Less is More\"), a malware classification\nframework that leverages Federated Learning to detect and classify malicious\napps in a privacy-respecting manner. Information about newly installed apps is\nkept locally on users' devices, so that the provider cannot infer which apps\nwere installed by users. At the same time, input from all users is taken into\naccount in the federated learning process and they all benefit from better\nclassification performance. A key challenge of this setting is that users do\nnot have access to the ground truth (i.e. they cannot correctly identify\nwhether an app is malicious). To tackle this, LiM uses a safe semi-supervised\nensemble that maximizes classification accuracy with respect to a baseline\nclassifier trained by the service provider (i.e. the cloud). We implement LiM\nand show that the cloud server has F1 score of 95%, while clients have perfect\nrecall with only 1 false positive in >100 apps, using a dataset of 25K clean\napps and 25K malicious apps, 200 users and 50 rounds of federation.\nFurthermore, we conduct a security analysis and demonstrate that LiM is robust\nagainst both poisoning attacks by adversaries who control half of the clients,\nand inference attacks performed by an honest-but-curious cloud server. Further\nexperiments with MaMaDroid's dataset confirm resistance against poisoning\nattacks and a performance improvement due to the federation.",
          "link": "http://arxiv.org/abs/2007.08319",
          "publishedOn": "2021-06-17T01:58:46.932Z",
          "wordCount": 703,
          "title": "Less is More: A privacy-respecting Android malware classifier using Federated Learning. (arXiv:2007.08319v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1\">Damir Koren&#x10d;i&#x107;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ristov_S/0/1/0/all/0/1\">Strahil Ristov</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Repar_J/0/1/0/all/0/1\">Jelena Repar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a> (2) ((1) Rudjer Bo&#x161;kovi&#x107; Institute, Croatia, (2) University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)",
          "description": "Topic models are widely used unsupervised models of text capable of learning\ntopics - weighted lists of words and documents - from large collections of text\ndocuments. When topic models are used for discovery of topics in text\ncollections, a question that arises naturally is how well the model-induced\ntopics correspond to topics of interest to the analyst. In this paper we\nrevisit and extend a so far neglected approach to topic model evaluation based\non measuring topic coverage - computationally matching model topics with a set\nof reference topics that models are expected to uncover. The approach is well\nsuited for analyzing models' performance in topic discovery and for large-scale\nanalysis of both topic models and measures of model quality. We propose new\nmeasures of coverage and evaluate, in a series of experiments, different types\nof topic models on two distinct text domains for which interest for topic\ndiscovery exists. The experiments include evaluation of model quality, analysis\nof coverage of distinct topic categories, and the analysis of the relationship\nbetween coverage and other methods of topic model evaluation. The contributions\nof the paper include new measures of coverage, insights into both topic models\nand other methods of model evaluation, and the datasets and code for\nfacilitating future research of both topic coverage and other approaches to\ntopic model evaluation.",
          "link": "http://arxiv.org/abs/2012.06274",
          "publishedOn": "2021-06-17T01:58:46.925Z",
          "wordCount": 741,
          "title": "A Topic Coverage Approach to Evaluation of Topic Models. (arXiv:2012.06274v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09000",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_D/0/1/0/all/0/1\">Donglin Wang</a>",
          "description": "The objective of this study is to derive functional networks for the autism\nspectrum disorder (ASD) population using the group ICA and dictionary learning\nmodel together and to classify ASD and typically developing (TD) participants\nusing the functional connectivity calculated from the derived functional\nnetworks. In our experiments, the ASD functional networks were derived from\nresting-state functional magnetic resonance imaging (rs-fMRI) data. We\ndownloaded a total of 120 training samples, including 58 ASD and 62 TD\nparticipants, which were obtained from the public repository: Autism Brain\nImaging Data Exchange I (ABIDE I). Our methodology and results have five main\nparts. First, we utilize a group ICA model to extract functional networks from\nthe ASD group and rank the top 20 regions of interest (ROIs). Second, we\nutilize a dictionary learning model to extract functional networks from the ASD\ngroup and rank the top 20 ROIs. Third, we merged the 40 selected ROIs from the\ntwo models together as the ASD functional networks. Fourth, we generate three\ncorresponding masks based on the 20 selected ROIs from group ICA, the 20 ROIs\nselected from dictionary learning, and the 40 combined ROIs selected from both.\nFinally, we extract ROIs for all training samples using the above three masks,\nand the calculated functional connectivity was used as features for ASD and TD\nclassification. The classification results showed that the functional networks\nderived from ICA and dictionary learning together outperform those derived from\na single ICA model or a single dictionary learning model.",
          "link": "http://arxiv.org/abs/2106.09000",
          "publishedOn": "2021-06-17T01:58:46.920Z",
          "wordCount": 701,
          "title": "Deriving Autism Spectrum Disorder Functional Networks from RS-FMRI Data using Group ICA and Dictionary Learning. (arXiv:2106.09000v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdomo_J/0/1/0/all/0/1\">Juan C. Perdomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1\">Tijana Zrnic</a>",
          "description": "In performative prediction, predictions guide decision-making and hence can\ninfluence the distribution of future data. To date, work on performative\nprediction has focused on finding performatively stable models, which are the\nfixed points of repeated retraining. However, stable solutions can be far from\noptimal when evaluated in terms of the performative risk, the loss experienced\nby the decision maker when deploying a model. In this paper, we shift attention\nbeyond performative stability and focus on optimizing the performative risk\ndirectly. We identify a natural set of properties of the loss function and\nmodel-induced distribution shift under which the performative risk is convex, a\nproperty which does not follow from convexity of the loss alone. Furthermore,\nwe develop algorithms that leverage our structural assumptions to optimize the\nperformative risk with better sample efficiency than generic methods for\nderivative-free convex optimization.",
          "link": "http://arxiv.org/abs/2102.08570",
          "publishedOn": "2021-06-17T01:58:46.914Z",
          "wordCount": 596,
          "title": "Outside the Echo Chamber: Optimizing the Performative Risk. (arXiv:2102.08570v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Kendrick Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "Self-training algorithms, which train a model to fit pseudolabels predicted\nby another previously-learned model, have been very successful for learning\nwith unlabeled data using neural networks. However, the current theoretical\nunderstanding of self-training only applies to linear models. This work\nprovides a unified theoretical analysis of self-training with deep networks for\nsemi-supervised learning, unsupervised domain adaptation, and unsupervised\nlearning. At the core of our analysis is a simple but realistic \"expansion\"\nassumption, which states that a low probability subset of the data must expand\nto a neighborhood with large probability relative to the subset. We also assume\nthat neighborhoods of examples in different classes have minimal overlap. We\nprove that under these assumptions, the minimizers of population objectives\nbased on self-training and input-consistency regularization will achieve high\naccuracy with respect to ground-truth labels. By using off-the-shelf\ngeneralization bounds, we immediately convert this result to sample complexity\nguarantees for neural nets that are polynomial in the margin and Lipschitzness.\nOur results help explain the empirical successes of recently proposed\nself-training algorithms which use input consistency regularization.",
          "link": "http://arxiv.org/abs/2010.03622",
          "publishedOn": "2021-06-17T01:58:46.909Z",
          "wordCount": 659,
          "title": "Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data. (arXiv:2010.03622v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08936",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murn_L/0/1/0/all/0/1\">Luka Murn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blasi_S/0/1/0/all/0/1\">Saverio Blasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smeaton_A/0/1/0/all/0/1\">Alan F. Smeaton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>",
          "description": "The versatility of recent machine learning approaches makes them ideal for\nimprovement of next generation video compression solutions. Unfortunately,\nthese approaches typically bring significant increases in computational\ncomplexity and are difficult to interpret into explainable models, affecting\ntheir potential for implementation within practical video coding applications.\nThis paper introduces a novel explainable neural network-based inter-prediction\nscheme, to improve the interpolation of reference samples needed for fractional\nprecision motion compensation. The approach requires a single neural network to\nbe trained from which a full quarter-pixel interpolation filter set is derived,\nas the network is easily interpretable due to its linear structure. A novel\ntraining framework enables each network branch to resemble a specific\nfractional shift. This practical solution makes it very efficient to use\nalongside conventional video coding schemes. When implemented in the context of\nthe state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and\n2.25% BD-rate savings can be achieved on average for lower resolution sequences\nunder the random access, low-delay B and low-delay P configurations,\nrespectively, while the complexity of the learned interpolation schemes is\nsignificantly reduced compared to the interpolation with full CNNs.",
          "link": "http://arxiv.org/abs/2106.08936",
          "publishedOn": "2021-06-17T01:58:46.903Z",
          "wordCount": 664,
          "title": "Improved CNN-based Learning of Interpolation Filters for Low-Complexity Inter Prediction in Video Coding. (arXiv:2106.08936v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaopeng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_G/0/1/0/all/0/1\">Guy Jacobson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_R/0/1/0/all/0/1\">Rittwik Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wen-Ling Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talasila_M/0/1/0/all/0/1\">Manoop Talasila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aftab_S/0/1/0/all/0/1\">Syed Anwar Aftab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borcea_C/0/1/0/all/0/1\">Cristian Borcea</a>",
          "description": "Fine-grained location prediction on smart phones can be used to improve\napp/system performance. Application scenarios include video quality adaptation\nas a function of the 5G network quality at predicted user locations, and\naugmented reality apps that speed up content rendering based on predicted user\nlocations. Such use cases require prediction error in the same range as the GPS\nerror, and no existing works on location prediction can achieve this level of\naccuracy. We present a system for fine-grained location prediction (FGLP) of\nmobile users, based on GPS traces collected on the phones. FGLP has two\ncomponents: a federated learning framework and a prediction model. The\nframework runs on the phones of the users and also on a server that coordinates\nlearning from all users in the system. FGLP represents the user location data\nas relative points in an abstract 2D space, which enables learning across\ndifferent physical spaces. The model merges Bidirectional Long Short-Term\nMemory (BiLSTM) and Convolutional Neural Networks (CNN), where BiLSTM learns\nthe speed and direction of the mobile users, and CNN learns information such as\nuser movement preferences. FGLP uses federated learning to protect user privacy\nand reduce bandwidth consumption. Our experimental results, using a dataset\nwith over 600,000 users, demonstrate that FGLP outperforms baseline models in\nterms of prediction accuracy. We also demonstrate that FGLP works well in\nconjunction with transfer learning, which enables model reusability. Finally,\nbenchmark results on several types of Android phones demonstrate FGLP's\nfeasibility in real life.",
          "link": "http://arxiv.org/abs/2106.08946",
          "publishedOn": "2021-06-17T01:58:46.882Z",
          "wordCount": 690,
          "title": "FGLP: A Federated Fine-Grained Location Prediction System for Mobile Users. (arXiv:2106.08946v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Yang Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianping Shen</a>",
          "description": "Recently developed large pre-trained language models, e.g., BERT, have\nachieved remarkable performance in many downstream natural language processing\napplications. These pre-trained language models often contain hundreds of\nmillions of parameters and suffer from high computation and latency in\nreal-world applications. It is desirable to reduce the computation overhead of\nthe models for fast training and inference while keeping the model performance\nin downstream applications. Several lines of work utilize knowledge\ndistillation to compress the teacher model to a smaller student model. However,\nthey usually discard the teacher's knowledge when in inference. Differently, in\nthis paper, we propose RefBERT to leverage the knowledge learned from the\nteacher, i.e., facilitating the pre-computed BERT representation on the\nreference sample and compressing BERT into a smaller student model. To\nguarantee our proposal, we provide theoretical justification on the loss\nfunction and the usage of reference samples. Significantly, the theoretical\nresult shows that including the pre-computed teacher's representations on the\nreference samples indeed increases the mutual information in learning the\nstudent model. Finally, we conduct the empirical evaluation and show that our\nRefBERT can beat the vanilla TinyBERT over 8.1\\% and achieves more than 94\\% of\nthe performance of $\\BERTBASE$ on the GLUE benchmark. Meanwhile, RefBERT is\n7.4x smaller and 9.5x faster on inference than BERT$_{\\rm BASE}$.",
          "link": "http://arxiv.org/abs/2106.08898",
          "publishedOn": "2021-06-17T01:58:46.875Z",
          "wordCount": 652,
          "title": "RefBERT: Compressing BERT by Referencing to Pre-computed Representations. (arXiv:2106.08898v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gillen_S/0/1/0/all/0/1\">Sean Gillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byl_K/0/1/0/all/0/1\">Katie Byl</a>",
          "description": "A key limitation in using various modern methods of machine learning in\ndeveloping feedback control policies is the lack of appropriate methodologies\nto analyze their long-term dynamics, in terms of making any sort of guarantees\n(even statistically) about robustness. The central reasons for this are largely\ndue to the so-called curse of dimensionality, combined with the black-box\nnature of the resulting control policies themselves. This paper aims at the\nfirst of these issues. Although the full state space of a system may be quite\nlarge in dimensionality, it is a common feature of most model-based control\nmethods that the resulting closed-loop systems demonstrate dominant dynamics\nthat are rapidly driven to some lower-dimensional sub-space within. In this\nwork we argue that the dimensionality of this subspace is captured by tools\nfrom fractal geometry, namely various notions of a fractional dimension. We\nthen show that the dimensionality of trajectories induced by model free\nreinforcement learning agents can be influenced adding a post processing\nfunction to the agents reward signal. We verify that the dimensionality\nreduction is robust to noise being added to the system and show that that the\nmodified agents are more actually more robust to noise and push disturbances in\ngeneral for the systems we examined.",
          "link": "http://arxiv.org/abs/2012.11662",
          "publishedOn": "2021-06-17T01:58:46.865Z",
          "wordCount": 661,
          "title": "Explicitly Encouraging Low Fractional Dimensional Trajectories Via Reinforcement Learning. (arXiv:2012.11662v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "In label-noise learning, the transition matrix plays a key role in building\nstatistically consistent classifiers. Existing consistent estimators for the\ntransition matrix have been developed by exploiting anchor points. However, the\nanchor-point assumption is not always satisfied in real scenarios. In this\npaper, we propose an end-to-end framework for solving label-noise learning\nwithout anchor points, in which we simultaneously optimize two objectives: the\ncross entropy loss between the noisy label and the predicted probability by the\nneural network, and the volume of the simplex formed by the columns of the\ntransition matrix. Our proposed framework can identify the transition matrix if\nthe clean class-posterior probabilities are sufficiently scattered. This is by\nfar the mildest assumption under which the transition matrix is provably\nidentifiable and the learned classifier is statistically consistent.\nExperimental results on benchmark datasets demonstrate the effectiveness and\nrobustness of the proposed method.",
          "link": "http://arxiv.org/abs/2102.02400",
          "publishedOn": "2021-06-17T01:58:46.844Z",
          "wordCount": 607,
          "title": "Provably End-to-end Label-Noise Learning without Anchor Points. (arXiv:2102.02400v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09004",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Dietrich_F/0/1/0/all/0/1\">Felix Dietrich</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Makeev_A/0/1/0/all/0/1\">Alexei Makeev</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kevrekidis_G/0/1/0/all/0/1\">George Kevrekidis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Evangelou_N/0/1/0/all/0/1\">Nikolaos Evangelou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bertalan_T/0/1/0/all/0/1\">Tom Bertalan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Reich_S/0/1/0/all/0/1\">Sebastian Reich</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kevrekidis_I/0/1/0/all/0/1\">Ioannis G. Kevrekidis</a>",
          "description": "We identify effective stochastic differential equations (SDE) for coarse\nobservables of fine-grained particle- or agent-based simulations; these SDE\nthen provide coarse surrogate models of the fine scale dynamics. We approximate\nthe drift and diffusivity functions in these effective SDE through neural\nnetworks, which can be thought of as effective stochastic ResNets. The loss\nfunction is inspired by, and embodies, the structure of established stochastic\nnumerical integrators (here, Euler-Maruyama and Milstein); our approximations\ncan thus benefit from error analysis of these underlying numerical schemes.\nThey also lend themselves naturally to \"physics-informed\" gray-box\nidentification when approximate coarse models, such as mean field equations,\nare available. Our approach does not require long trajectories, works on\nscattered snapshot data, and is designed to naturally handle different time\nsteps per snapshot. We consider both the case where the coarse collective\nobservables are known in advance, as well as the case where they must be found\nin a data-driven manner.",
          "link": "http://arxiv.org/abs/2106.09004",
          "publishedOn": "2021-06-17T01:58:46.836Z",
          "wordCount": 607,
          "title": "Learning effective stochastic differential equations from microscopic simulations: combining stochastic numerics and deep learning. (arXiv:2106.09004v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Junhui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritov_Y/0/1/0/all/0/1\">Ya&#x27;acov Ritov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Linda Zhao</a>",
          "description": "Large-scale modern data often involves estimation and testing for\nhigh-dimensional unknown parameters. It is desirable to identify the sparse\nsignals, ``the needles in the haystack'', with accuracy and false discovery\ncontrol. However, the unprecedented complexity and heterogeneity in modern data\nstructure require new machine learning tools to effectively exploit\ncommonalities and to robustly adjust for both sparsity and heterogeneity. In\naddition, estimates for high-dimensional parameters often lack uncertainty\nquantification. In this paper, we propose a novel Spike-and-Nonparametric\nmixture prior (SNP) -- a spike to promote the sparsity and a nonparametric\nstructure to capture signals. In contrast to the state-of-the-art methods, the\nproposed methods solve the estimation and testing problem at once with several\nmerits: 1) an accurate sparsity estimation; 2) point estimates with\nshrinkage/soft-thresholding property; 3) credible intervals for uncertainty\nquantification; 4) an optimal multiple testing procedure that controls false\ndiscovery rate. Our method exhibits promising empirical performance on both\nsimulated data and a gene expression case study.",
          "link": "http://arxiv.org/abs/2106.08881",
          "publishedOn": "2021-06-17T01:58:46.823Z",
          "wordCount": 592,
          "title": "Nonparametric Empirical Bayes Estimation and Testing for Sparse and Heteroscedastic Signals. (arXiv:2106.08881v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tekgul_B/0/1/0/all/0/1\">Buse G.A. Tekgul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shelly Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchal_S/0/1/0/all/0/1\">Samuel Marchal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asokan_N/0/1/0/all/0/1\">N. Asokan</a>",
          "description": "Recent work has discovered that deep reinforcement learning (DRL) policies\nare vulnerable to adversarial examples. These attacks mislead the policy of DRL\nagents by perturbing the state of the environment observed by agents. They are\nfeasible in principle but too slow to fool DRL policies in real time. We\npropose a new attack to fool DRL policies that is both effective and efficient\nenough to be mounted in real time. We utilize the Universal Adversarial\nPerturbation (UAP) method to compute effective perturbations independent of the\nindividual inputs to which they are applied. Via an extensive evaluation using\nAtari 2600 games, we show that our technique is effective, as it fully degrades\nthe performance of both deterministic and stochastic policies (up to 100%, even\nwhen the $l_\\infty$ bound on the perturbation is as small as 0.005). We also\nshow that our attack is efficient, incurring an online computational cost of\n0.027ms on average. It is faster compared to the response time (0.6ms on\naverage) of agents with different DRL policies, and considerably faster than\nprior attacks (2.7ms on average). Furthermore, we demonstrate that known\ndefenses are ineffective against universal perturbations. We propose an\neffective detection technique which can form the basis for robust defenses\nagainst attacks based on universal perturbations.",
          "link": "http://arxiv.org/abs/2106.08746",
          "publishedOn": "2021-06-17T01:58:46.812Z",
          "wordCount": 646,
          "title": "Real-time Attacks Against Deep Reinforcement Learning Policies. (arXiv:2106.08746v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alva_S/0/1/0/all/0/1\">Sahan Suresh Alva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Counterfactuals, serving as one of the emerging type of model\ninterpretations, have recently received attention from both researchers and\npractitioners. Counterfactual explanations formalize the exploration of\n``what-if'' scenarios, and are an instance of example-based reasoning using a\nset of hypothetical data samples. Counterfactuals essentially show how the\nmodel decision alters with input perturbations. Existing methods for generating\ncounterfactuals are mainly algorithm-based, which are time-inefficient and\nassume the same counterfactual universe for different queries. To address these\nlimitations, we propose a Model-based Counterfactual Synthesizer (MCS)\nframework for interpreting machine learning models. We first analyze the\nmodel-based counterfactual process and construct a base synthesizer using a\nconditional generative adversarial net (CGAN). To better approximate the\ncounterfactual universe for those rare queries, we novelly employ the umbrella\nsampling technique to conduct the MCS framework training. Besides, we also\nenhance the MCS framework by incorporating the causal dependence among\nattributes with model inductive bias, and validate its design correctness from\nthe causality identification perspective. Experimental results on several\ndatasets demonstrate the effectiveness as well as efficiency of our proposed\nMCS framework, and verify the advantages compared with other alternatives.",
          "link": "http://arxiv.org/abs/2106.08971",
          "publishedOn": "2021-06-17T01:58:46.798Z",
          "wordCount": 614,
          "title": "Model-Based Counterfactual Synthesizer for Interpretation. (arXiv:2106.08971v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hengyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1\">Adam Lerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">David Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noam Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "The standard problem setting in Dec-POMDPs is self-play, where the goal is to\nfind a set of policies that play optimally together. Policies learned through\nself-play may adopt arbitrary conventions and implicitly rely on multi-step\nreasoning based on fragile assumptions about other agents' actions and thus\nfail when paired with humans or independently trained agents at test time. To\naddress this, we present off-belief learning (OBL). At each timestep OBL agents\nfollow a policy $\\pi_1$ that is optimized assuming past actions were taken by a\ngiven, fixed policy ($\\pi_0$), but assuming that future actions will be taken\nby $\\pi_1$. When $\\pi_0$ is uniform random, OBL converges to an optimal policy\nthat does not rely on inferences based on other agents' behavior (an optimal\ngrounded policy). OBL can be iterated in a hierarchy, where the optimal policy\nfrom one level becomes the input to the next, thereby introducing multi-level\ncognitive reasoning in a controlled manner. Unlike existing approaches, which\nmay converge to any equilibrium policy, OBL converges to a unique policy,\nmaking it suitable for zero-shot coordination (ZSC). OBL can be scaled to\nhigh-dimensional settings with a fictitious transition mechanism and shows\nstrong performance in both a toy-setting and the benchmark human-AI & ZSC\nproblem Hanabi.",
          "link": "http://arxiv.org/abs/2103.04000",
          "publishedOn": "2021-06-17T01:58:46.782Z",
          "wordCount": 657,
          "title": "Off-Belief Learning. (arXiv:2103.04000v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Arnob Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanujit Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Dibakar Ghosh</a>",
          "description": "The remarkable flexibility and adaptability of both deep learning models and\nensemble methods have led to the proliferation for their application in\nunderstanding many physical phenomena. Traditionally, these two techniques have\nlargely been treated as independent methodologies in practical applications.\nThis study develops an optimized ensemble deep learning (OEDL) framework\nwherein these two machine learning techniques are jointly used to achieve\nsynergistic improvements in model accuracy, stability, scalability, and\nreproducibility prompting a new wave of applications in the forecasting of\ndynamics. Unpredictability is considered as one of the key features of chaotic\ndynamics, so forecasting such dynamics of nonlinear systems is a relevant issue\nin the scientific community. It becomes more challenging when the prediction of\nextreme events is the focus issue for us. In this circumstance, the proposed\nOEDL model based on a best convex combination of feed-forward neural networks,\nreservoir computing, and long short-term memory can play a key role in\nadvancing predictions of dynamics consisting of extreme events. The combined\nframework can generate the best out-of-sample performance than the individual\ndeep learners and standard ensemble framework for both numerically simulated\nand real world data sets. We exhibit the outstanding performance of the OEDL\nframework for forecasting extreme events generated from Lienard-type system,\nprediction of COVID-19 cases in Brazil, dengue cases in San Juan, and sea\nsurface temperature in Nino 3.4 region.",
          "link": "http://arxiv.org/abs/2106.08968",
          "publishedOn": "2021-06-17T01:58:46.776Z",
          "wordCount": 710,
          "title": "Optimized ensemble deep learning framework for scalable forecasting of dynamics containing extreme events. (arXiv:2106.08968v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1\">Matthieu Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glanois_C/0/1/0/all/0/1\">Claire Glanois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddique_U/0/1/0/all/0/1\">Umer Siddique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_P/0/1/0/all/0/1\">Paul Weng</a>",
          "description": "We consider the problem of learning fair policies in (deep) cooperative\nmulti-agent reinforcement learning (MARL). We formalize it in a principled way\nas the problem of optimizing a welfare function that explicitly encodes two\nimportant aspects of fairness: efficiency and equity. As a solution method, we\npropose a novel neural network architecture, which is composed of two\nsub-networks specifically designed for taking into account the two aspects of\nfairness. In experiments, we demonstrate the importance of the two sub-networks\nfor fair optimization. Our overall approach is general as it can accommodate\nany (sub)differentiable welfare function. Therefore, it is compatible with\nvarious notions of fairness that have been proposed in the literature (e.g.,\nlexicographic maximin, generalized Gini social welfare function, proportional\nfairness). Our solution method is generic and can be implemented in various\nMARL settings: centralized training and decentralized execution, or fully\ndecentralized. Finally, we experimentally validate our approach in various\ndomains and show that it can perform much better than previous methods.",
          "link": "http://arxiv.org/abs/2012.09421",
          "publishedOn": "2021-06-17T01:58:46.767Z",
          "wordCount": 638,
          "title": "Learning Fair Policies in Decentralized Cooperative Multi-Agent Reinforcement Learning. (arXiv:2012.09421v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08928",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ennis_M/0/1/0/all/0/1\">Michaela Ennis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozachkov_L/0/1/0/all/0/1\">Leo Kozachkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slotine_J/0/1/0/all/0/1\">Jean-Jacques Slotine</a>",
          "description": "Advanced applications of modern machine learning will likely involve\ncombinations of trained networks, as are already used in spectacular systems\nsuch as DeepMind's AlphaGo. Recursively building such combinations in an\neffective and stable fashion while also allowing for continual refinement of\nthe individual networks - as nature does for biological networks - will require\nnew analysis tools. This paper takes a step in this direction by establishing\ncontraction properties of broad classes of nonlinear recurrent networks and\nneural ODEs, and showing how these quantified properties allow in turn to\nrecursively construct stable networks of networks in a systematic fashion. The\nresults can also be used to stably combine recurrent networks and physical\nsystems with quantified contraction properties. Similarly, they may be applied\nto modular computational models of cognition.",
          "link": "http://arxiv.org/abs/2106.08928",
          "publishedOn": "2021-06-17T01:58:46.760Z",
          "wordCount": 567,
          "title": "Recursive Construction of Stable Assemblies of Recurrent Neural Networks. (arXiv:2106.08928v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>",
          "description": "Video-grounded dialogue systems aim to integrate video understanding and\ndialogue understanding to generate responses that are relevant to both the\ndialogue and video context. Most existing approaches employ deep learning\nmodels and have achieved remarkable performance, given the relatively small\ndatasets available. However, the results are partly accomplished by exploiting\nbiases in the datasets rather than developing multimodal reasoning, resulting\nin limited generalization. In this paper, we propose a novel approach of\nCompositional Counterfactual Contrastive Learning ($C^3$) to develop\ncontrastive training between factual and counterfactual samples in\nvideo-grounded dialogues. Specifically, we design factual/counterfactual\nsampling based on the temporal steps in videos and tokens in dialogues and\npropose contrastive loss functions that exploit object-level or action-level\nvariance. Different from prior approaches, we focus on contrastive hidden state\nrepresentations among compositional output tokens to optimize the\nrepresentation space in a generation setting. We achieved promising performance\ngains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the\nbenefits of our approach in grounding video and dialogue context.",
          "link": "http://arxiv.org/abs/2106.08914",
          "publishedOn": "2021-06-17T01:58:46.725Z",
          "wordCount": 608,
          "title": "$C^3$: Compositional Counterfactual Constrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Nan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zepeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianfeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askari_H/0/1/0/all/0/1\">Hassan Askari</a>",
          "description": "Estimation of the longitudinal slip ratio of tires is important in boosting\nthe control performance of the vehicle under driving and braking conditions. In\nthis paper, the slip ratio is estimated using four machine learning algorithms\n(Neural Network, Gradient Boosting Machine, Random Forest and Support Vector\nMachine) based on the acceleration signals from the tri-axial MEMS\naccelerometers utilized in the intelligent tire system. The experimental data\nare collected through the MTS experimental platform. The corresponding\nacceleration signals within the tire contact patch are extracted after\nfiltering to be used for the training the aforesaid machine learning\nalgorithms. A comparison is provided between the implemented ML algorithms\nusing a 10-fold CV. NRMS errors in the CV results indicate that NN has the\nhighest accuracy in comparison with other techniques. The NRSM errors of NN,\nGBM, RF, and SVM are 2.59\\%, 3.30\\%, 4.21\\%, and 5.34\\%, respectively. Among\nthese techniques, GBM has a more stable results as it has the smallest output\nvariance. The present study with the fusion of intelligent tire system and\nmachine learning algorithms paves the way for the accurate estimation of tire\nslip ratio, which is critical for the development of reliable vehicle control\nalgorithms.",
          "link": "http://arxiv.org/abs/2106.08961",
          "publishedOn": "2021-06-17T01:58:46.709Z",
          "wordCount": 631,
          "title": "Intelligent Tire-Based Slip Ratio Estimation Using Different Machine Learning Algorithms. (arXiv:2106.08961v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beugnot_G/0/1/0/all/0/1\">Gaspard Beugnot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1\">Julien Mairal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "The theory of spectral filtering is a remarkable tool to understand the\nstatistical properties of learning with kernels. For least squares, it allows\nto derive various regularization schemes that yield faster convergence rates of\nthe excess risk than with Tikhonov regularization. This is typically achieved\nby leveraging classical assumptions called source and capacity conditions,\nwhich characterize the difficulty of the learning task. In order to understand\nestimators derived from other loss functions, Marteau-Ferey et al. have\nextended the theory of Tikhonov regularization to generalized self concordant\nloss functions (GSC), which contain, e.g., the logistic loss. In this paper, we\ngo a step further and show that fast and optimal rates can be achieved for GSC\nby using the iterated Tikhonov regularization scheme, which is intrinsically\nrelated to the proximal point method in optimization, and overcomes the\nlimitation of the classical Tikhonov regularization.",
          "link": "http://arxiv.org/abs/2106.08855",
          "publishedOn": "2021-06-17T01:58:46.703Z",
          "wordCount": 578,
          "title": "Beyond Tikhonov: Faster Learning with Self-Concordant Losses via Iterative Regularization. (arXiv:2106.08855v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08901",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Hopp_D/0/1/0/all/0/1\">Daniel Hopp</a>",
          "description": "Artificial neural networks (ANNs) have been the catalyst to numerous advances\nin a variety of fields and disciplines in recent years. Their impact on\neconomics, however, has been comparatively muted. One type of ANN, the long\nshort-term memory network (LSTM), is particularly wellsuited to deal with\neconomic time-series. Here, the architecture's performance and characteristics\nare evaluated in comparison with the dynamic factor model (DFM), currently a\npopular choice in the field of economic nowcasting. LSTMs are found to produce\nsuperior results to DFMs in the nowcasting of three separate variables; global\nmerchandise export values and volumes, and global services exports. Further\nadvantages include their ability to handle large numbers of input features in a\nvariety of time frequencies. A disadvantage is the inability to ascribe\ncontributions of input features to model outputs, common to all ANNs. In order\nto facilitate continued applied research of the methodology by avoiding the\nneed for any knowledge of deep-learning libraries, an accompanying Python\nlibrary was developed using PyTorch, https://pypi.org/project/nowcast-lstm/.",
          "link": "http://arxiv.org/abs/2106.08901",
          "publishedOn": "2021-06-17T01:58:46.697Z",
          "wordCount": 594,
          "title": "Economic Nowcasting with Long Short-Term Memory Artificial Neural Networks (LSTM). (arXiv:2106.08901v1 [econ.EM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Petri_G/0/1/0/all/0/1\">Guido Petri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanley_M/0/1/0/all/0/1\">Michael H. Stanley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hon_A/0/1/0/all/0/1\">Alec B. Hon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_A/0/1/0/all/0/1\">Alexander Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xenopoulos_P/0/1/0/all/0/1\">Peter Xenopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1\">Cl&#xe1;udio Silva</a>",
          "description": "Many esports use a pick and ban process to define the parameters of a match\nbefore it starts. In Counter-Strike: Global Offensive (CSGO) matches, two teams\nfirst pick and ban maps, or virtual worlds, to play. Teams typically ban and\npick maps based on a variety of factors, such as banning maps which they do not\npractice, or choosing maps based on the team's recent performance. We introduce\na contextual bandit framework to tackle the problem of map selection in CSGO\nand to investigate teams' pick and ban decision-making. Using a data set of\nover 3,500 CSGO matches and over 25,000 map selection decisions, we consider\ndifferent framings for the problem, different contexts, and different reward\nmetrics. We find that teams have suboptimal map choice policies with respect to\nboth picking and banning. We also define an approach for rewarding bans, which\nhas not been explored in the bandit setting, and find that incorporating ban\nrewards improves model performance. Finally, we determine that usage of our\nmodel could improve teams' predicted map win probability by up to 11% and raise\noverall match win probabilities by 19.8% for evenly-matched teams.",
          "link": "http://arxiv.org/abs/2106.08888",
          "publishedOn": "2021-06-17T01:58:46.673Z",
          "wordCount": 632,
          "title": "Bandit Modeling of Map Selection in Counter-Strike: Global Offensive. (arXiv:2106.08888v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1\">Peter Kairouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMahan_B/0/1/0/all/0/1\">Brendan McMahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_O/0/1/0/all/0/1\">Om Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakurta_A/0/1/0/all/0/1\">Abhradeep Thakurta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zheng Xu</a>",
          "description": "We consider training models with differential privacy (DP) using mini-batch\ngradients. The existing state-of-the-art, Differentially Private Stochastic\nGradient Descent (DP-SGD), requires privacy amplification by sampling or\nshuffling to obtain the best privacy/accuracy/computation trade-offs.\nUnfortunately, the precise requirements on exact sampling and shuffling can be\nhard to obtain in important practical scenarios, particularly federated\nlearning (FL). We design and analyze a DP variant of\nFollow-The-Regularized-Leader (DP-FTRL) that compares favorably (both\ntheoretically and empirically) to amplified DP-SGD, while allowing for much\nmore flexible data access patterns. DP-FTRL does not use any form of privacy\namplification.\n\nThe code is available at\nhttps://github.com/google-research/federated/tree/master/dp_ftrl and\nhttps://github.com/google-research/DP-FTRL .",
          "link": "http://arxiv.org/abs/2103.00039",
          "publishedOn": "2021-06-17T01:58:46.610Z",
          "wordCount": 572,
          "title": "Practical and Private (Deep) Learning without Sampling or Shuffling. (arXiv:2103.00039v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Souri_H/0/1/0/all/0/1\">Hossein Souri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1\">Liam Fowl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "As the curation of data for machine learning becomes increasingly automated,\ndataset tampering is a mounting threat. Backdoor attackers tamper with training\ndata to embed a vulnerability in models that are trained on that data. This\nvulnerability is then activated at inference time by placing a \"trigger\" into\nthe model's input. Typical backdoor attacks insert the trigger directly into\nthe training data, although the presence of such an attack may be visible upon\ninspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning\nwithout placing a trigger into the training data at all. However, this hidden\ntrigger attack is ineffective at poisoning neural networks trained from\nscratch. We develop a new hidden trigger attack, Sleeper Agent, which employs\ngradient matching, data selection, and target model re-training during the\ncrafting process. Sleeper Agent is the first hidden trigger backdoor attack to\nbe effective against neural networks trained from scratch. We demonstrate its\neffectiveness on ImageNet and in black-box settings. Our implementation code\ncan be found at https://github.com/hsouri/Sleeper-Agent.",
          "link": "http://arxiv.org/abs/2106.08970",
          "publishedOn": "2021-06-17T01:58:46.568Z",
          "wordCount": 613,
          "title": "Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch. (arXiv:2106.08970v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zengin_R/0/1/0/all/0/1\">Rahman Salim Zengin</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sezer_V/0/1/0/all/0/1\">Volkan Sezer</a> (1) ((1) Istanbul Technical University)",
          "description": "Voronoi tessellations are used to partition the Euclidean space into\npolyhedral regions, which are called Voronoi cells. Labeling the Voronoi cells\nwith the class information, we can map any classification problem into a\nVoronoi tessellation. In this way, the classification problem changes into a\nquery of just finding the enclosing Voronoi cell. In order to accomplish this\ntask, we have developed a new algorithm which generates a labeled Voronoi\ntessellation that partitions the training data into polyhedral regions and\nobtains interclass boundaries as an indirect result. It is called Supervised\nk-Voxels or in short Super-k. We are introducing Super-k as a foundational new\nalgorithm and opening the possibility of a new family of algorithms. In this\npaper, it is shown via comparisons on certain datasets that the Super-k\nalgorithm has the potential of providing comparable performance of the\nwell-known SVM family of algorithms with less complexity.",
          "link": "http://arxiv.org/abs/2012.15492",
          "publishedOn": "2021-06-17T01:58:46.497Z",
          "wordCount": 625,
          "title": "Super-k: A Piecewise Linear Classifier Based on Voronoi Tessellations. (arXiv:2012.15492v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.06706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_Y/0/1/0/all/0/1\">Ye Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>",
          "description": "Petabytes of data are generated each day by emerging Internet of Things\n(IoT), but only few of them can be finally collected and used for Machine\nLearning (ML) purposes due to the apprehension of data & privacy leakage, which\nseriously retarding ML's growth. To alleviate this problem, Federated learning\nis proposed to perform model training by multiple clients' combined data\nwithout the dataset sharing within the cluster. Nevertheless, federated\nlearning introduces massive communication overhead as the synchronized data in\neach epoch is of the same size as the model, and thereby leading to a low\ncommunication efficiency. Consequently, variant methods mainly focusing on the\ncommunication rounds reduction and data compression are proposed to reduce the\ncommunication overhead of federated learning. In this paper, we propose\nOverlap-FedAvg, a framework that parallels the model training phase with model\nuploading & downloading phase, so that the latter phase can be totally covered\nby the former phase. Compared to vanilla FedAvg, Overlap-FedAvg is further\ndeveloped with a hierarchical computing strategy, a data compensation mechanism\nand a nesterov accelerated gradients~(NAG) algorithm. Besides, Overlap-FedAvg\nis orthogonal to many other compression methods so that they can be applied\ntogether to maximize the utilization of the cluster. Furthermore, the\ntheoretical analysis is provided to prove the convergence of the proposed\nOverlap-FedAvg framework. Extensive experiments on both conventional and\nrecurrent tasks with multiple models and datasets also demonstrate that the\nproposed Overlap-FedAvg framework substantially boosts the federated learning\nprocess.",
          "link": "http://arxiv.org/abs/2012.06706",
          "publishedOn": "2021-06-17T01:58:46.455Z",
          "wordCount": 698,
          "title": "Communication-Efficient Federated Learning with Compensated Overlap-FedAvg. (arXiv:2012.06706v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Mingzhu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shaoqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>",
          "description": "User data confidentiality protection is becoming a rising challenge in the\npresent deep learning research. Without access to data, conventional\ndata-driven model compression faces a higher risk of performance degradation.\nRecently, some works propose to generate images from a specific pretrained\nmodel to serve as training data. However, the inversion process only utilizes\nbiased feature statistics stored in one model and is from low-dimension to\nhigh-dimension. As a consequence, it inevitably encounters the difficulties of\ngeneralizability and inexact inversion, which leads to unsatisfactory\nperformance. To address these problems, we propose MixMix based on two simple\nyet effective techniques: (1) Feature Mixing: utilizes various models to\nconstruct a universal feature space for generalized inversion; (2) Data Mixing:\nmixes the synthesized images and labels to generate exact label information. We\nprove the effectiveness of MixMix from both theoretical and empirical\nperspectives. Extensive experiments show that MixMix outperforms existing\nmethods on the mainstream compression tasks, including quantization, knowledge\ndistillation, and pruning. Specifically, MixMix achieves up to 4% and 20%\naccuracy uplift on quantization and pruning, respectively, compared to existing\ndata-free compression work.",
          "link": "http://arxiv.org/abs/2011.09899",
          "publishedOn": "2021-06-17T01:58:46.236Z",
          "wordCount": null,
          "title": "MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing. (arXiv:2011.09899v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grosnit_A/0/1/0/all/0/1\">Antoine Grosnit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutunov_R/0/1/0/all/0/1\">Rasul Tutunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maraval_A/0/1/0/all/0/1\">Alexandre Max Maraval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1\">Ryan-Rhys Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowen_Rivers_A/0/1/0/all/0/1\">Alexander I. Cowen-Rivers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1\">Wenlong Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhitang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jan Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bou_Ammar_H/0/1/0/all/0/1\">Haitham Bou-Ammar</a>",
          "description": "We introduce a method based on deep metric learning to perform Bayesian\noptimisation over high-dimensional, structured input spaces using variational\nautoencoders (VAEs). By extending ideas from supervised deep metric learning,\nwe address a longstanding problem in high-dimensional VAE Bayesian\noptimisation, namely how to enforce a discriminative latent space as an\ninductive bias. Importantly, we achieve such an inductive bias using just 1% of\nthe available labelled data relative to previous work, highlighting the sample\nefficiency of our approach. As a theoretical contribution, we present a proof\nof vanishing regret for our method. As an empirical contribution, we present\nstate-of-the-art results on real-world high-dimensional black-box optimisation\nproblems including property-guided molecule generation. It is the hope that the\nresults presented in this paper can act as a guiding principle for realising\neffective high-dimensional Bayesian optimisation.",
          "link": "http://arxiv.org/abs/2106.03609",
          "publishedOn": "2021-06-17T01:58:46.235Z",
          "wordCount": null,
          "title": "High-Dimensional Bayesian Optimisation with Variational Autoencoders and Deep Metric Learning. (arXiv:2106.03609v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01933",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Grieger_N/0/1/0/all/0/1\">Niklas Grieger</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Schwabedal_J/0/1/0/all/0/1\">Justus T. C. Schwabedal</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wendel_S/0/1/0/all/0/1\">Stefanie Wendel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ritze_Y/0/1/0/all/0/1\">Yvonne Ritze</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bialonski_S/0/1/0/all/0/1\">Stephan Bialonski</a>",
          "description": "Reliable automation of the labor-intensive manual task of scoring animal\nsleep can facilitate the analysis of long-term sleep studies. In recent years,\ndeep-learning-based systems, which learn optimal features from the data,\nincreased scoring accuracies for the classical sleep stages of Wake, REM, and\nNon-REM. Meanwhile, it has been recognized that the statistics of transitional\nstages such as pre-REM, found between Non-REM and REM, may hold additional\ninsight into the physiology of sleep and are now under vivid investigation. We\npropose a classification system based on a simple neural network architecture\nthat scores the classical stages as well as pre-REM sleep in mice. When\nrestricted to the classical stages, the optimized network showed\nstate-of-the-art classification performance with an out-of-sample F1 score of\n0.95 in male C57BL/6J mice. When unrestricted, the network showed lower F1\nscores on pre-REM (0.5) compared to the classical stages. The result is\ncomparable to previous attempts to score transitional stages in other species\nsuch as transition sleep in rats or N1 sleep in humans. Nevertheless, we\nobserved that the sequence of predictions including pre-REM typically\ntransitioned from Non-REM to REM reflecting sleep dynamics observed by human\nscorers. Our findings provide further evidence for the difficulty of scoring\ntransitional sleep stages, likely because such stages of sleep are\nunder-represented in typical data sets or show large inter-scorer variability.\nWe further provide our source code and an online platform to run predictions\nwith our trained network.",
          "link": "http://arxiv.org/abs/2105.01933",
          "publishedOn": "2021-06-17T01:58:46.222Z",
          "wordCount": 715,
          "title": "Automated scoring of pre-REM sleep in mice with deep learning. (arXiv:2105.01933v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Illing_B/0/1/0/all/0/1\">Bernd Illing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventura_J/0/1/0/all/0/1\">Jean Ventura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1\">Guillaume Bellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1\">Wulfram Gerstner</a>",
          "description": "Learning in the brain is poorly understood and learning rules that respect\nbiological constraints, yet yield deep hierarchical representations, are still\nunknown. Here, we propose a learning rule that takes inspiration from\nneuroscience and recent advances in self-supervised deep learning. Learning\nminimizes a simple layer-specific loss function and does not need to\nback-propagate error signals within or between layers. Instead, weight updates\nfollow a local, Hebbian, learning rule that only depends on pre- and\npost-synaptic neuronal activity, predictive dendritic input and widely\nbroadcasted modulation factors which are identical for large groups of neurons.\nThe learning rule applies contrastive predictive learning to a causal,\nbiological setting using saccades (i.e. rapid shifts in gaze direction). We\nfind that networks trained with this self-supervised and local rule build deep\nhierarchical representations of images, speech and video.",
          "link": "http://arxiv.org/abs/2010.08262",
          "publishedOn": "2021-06-17T01:58:46.215Z",
          "wordCount": null,
          "title": "Local plasticity rules can learn deep representations using self-supervised contrastive predictions. (arXiv:2010.08262v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Louboutin_M/0/1/0/all/0/1\">Mathias Louboutin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siahkoohi_A/0/1/0/all/0/1\">Ali Siahkoohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rongrong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrmann_F/0/1/0/all/0/1\">Felix J. Herrmann</a>",
          "description": "Thanks to the combination of state-of-the-art accelerators and highly\noptimized open software frameworks, there has been tremendous progress in the\nperformance of deep neural networks. While these developments have been\nresponsible for many breakthroughs, progress towards solving large-scale\nproblems, such as video encoding and semantic segmentation in 3D, is hampered\nbecause access to on-premise memory is often limited. Instead of relying on\n(optimal) checkpointing or invertibility of the network layers -- to recover\nthe activations during backpropagation -- we propose to approximate the\ngradient of convolutional layers in neural networks with a multi-channel\nrandomized trace estimation technique. Compared to other methods, this approach\nis simple, amenable to analyses, and leads to a greatly reduced memory\nfootprint. Even though the randomized trace estimation introduces stochasticity\nduring training, we argue that this is of little consequence as long as the\ninduced errors are of the same order as errors in the gradient due to the use\nof stochastic gradient descent. We discuss the performance of networks trained\nwith stochastic backpropagation and how the error can be controlled while\nmaximizing memory usage and minimizing computational overhead.",
          "link": "http://arxiv.org/abs/2106.06998",
          "publishedOn": "2021-06-17T01:58:46.215Z",
          "wordCount": null,
          "title": "Low-memory stochastic backpropagation with multi-channel randomized trace estimation. (arXiv:2106.06998v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.09991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fausti_F/0/1/0/all/0/1\">Fabrizio De Fausti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugliese_F/0/1/0/all/0/1\">Francesco Pugliese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zardetto_D/0/1/0/all/0/1\">Diego Zardetto</a>",
          "description": "In recent years, the interest in Big Data sources has been steadily growing\nwithin the Official Statistic community. The Italian National Institute of\nStatistics (Istat) is currently carrying out several Big Data pilot studies.\nOne of these studies, the ICT Big Data pilot, aims at exploiting massive\namounts of textual data automatically scraped from the websites of Italian\nenterprises in order to predict a set of target variables (e.g. e-commerce)\nthat are routinely observed by the traditional ICT Survey. In this paper, we\nshow that Deep Learning techniques can successfully address this problem.\nEssentially, we tackle a text classification task: an algorithm must learn to\ninfer whether an Italian enterprise performs e-commerce from the textual\ncontent of its website. To reach this goal, we developed a sophisticated\nprocessing pipeline and evaluated its performance through extensive\nexperiments. Our pipeline uses Convolutional Neural Networks and relies on Word\nEmbeddings to encode raw texts into grayscale images (i.e. normalized numeric\nmatrices). Web-scraped texts are huge and have very low signal to noise ratio:\nto overcome these issues, we adopted a framework known as False Positive\nReduction, which has seldom (if ever) been applied before to text\nclassification tasks. Several original contributions enable our processing\npipeline to reach good classification results. Empirical evidence shows that\nour proposal outperforms all the alternative Machine Learning solutions already\ntested in Istat for the same task.",
          "link": "http://arxiv.org/abs/1910.09991",
          "publishedOn": "2021-06-17T01:58:46.203Z",
          "wordCount": 714,
          "title": "Towards Automated Website Classification by Deep Learning. (arXiv:1910.09991v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.05978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lisboa_P/0/1/0/all/0/1\">Paulo J. G. Lisboa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Martorell_S/0/1/0/all/0/1\">Sandra Ortega-Martorell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cashman_S/0/1/0/all/0/1\">Sadie Cashman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olier_I/0/1/0/all/0/1\">Ivan Olier</a>",
          "description": "Among interpretable machine learning methods, the class of Generalised\nAdditive Neural Networks (GANNs) is referred to as Self-Explaining Neural\nNetworks (SENN) because of the linear dependence on explicit functions of the\ninputs. In binary classification this shows the precise weight that each input\ncontributes towards the logit. The nomogram is a graphical representation of\nthese weights. We show that functions of individual and pairs of variables can\nbe derived from a functional Analysis of Variance (ANOVA) representation,\nenabling an efficient feature selection to be carried by application of the\nlogistic Lasso. This process infers the structure of GANNs which otherwise\nneeds to be predefined. As this method is particularly suited for tabular data,\nit starts by fitting a generic flexible model, in this case a Multi-layer\nPerceptron (MLP) to which the ANOVA decomposition is applied. This has the\nfurther advantage that the resulting GANN can be replicated as a SENN, enabling\nfurther refinement of the univariate and bivariate component functions to take\nplace. The component functions are partial responses hence the SENN is a\npartial response network. The Partial Response Network (PRN) is equally as\ntransparent as a traditional logistic regression model, but capable of\nnon-linear classification with comparable or superior performance to the\noriginal MLP. In other words, the PRN is a fully interpretable representation\nof the MLP, at the level of univariate and bivariate effects. The performance\nof the PRN is shown to be competitive for benchmark data, against\nstate-of-the-art machine learning methods including GBM, SVM and Random\nForests. It is also compared with spline-based Sparse Additive Models (SAM)\nshowing that a semi-parametric representation of the GAM as a neural network\ncan be as effective as the SAM though less constrained by the need to set\nspline nodes.",
          "link": "http://arxiv.org/abs/1908.05978",
          "publishedOn": "2021-06-17T01:58:46.197Z",
          "wordCount": null,
          "title": "The Partial Response Network: a neural network nomogram. (arXiv:1908.05978v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grigsby_J/0/1/0/all/0/1\">Jake Grigsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jin Yong Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>",
          "description": "Model-free off-policy actor-critic methods are an efficient solution to\ncomplex continuous control tasks. However, these algorithms rely on a number of\ndesign tricks and many hyperparameters, making their applications to new\ndomains difficult and computationally expensive. This paper creates an\nevolutionary approach that automatically tunes these design decisions and\neliminates the RL-specific hyperparameters from the Soft Actor-Critic\nalgorithm. Our design is sample efficient and provides practical advantages\nover baseline approaches, including improved exploration, generalization over\nmultiple control frequencies, and a robust ensemble of high-performance\npolicies. Empirically, we show that our agent outperforms well-tuned\nhyperparameter settings in popular benchmarks from the DeepMind Control Suite.\nWe then apply it to new control tasks to find high-performance solutions with\nminimal compute and research effort.",
          "link": "http://arxiv.org/abs/2106.08918",
          "publishedOn": "2021-06-17T01:58:46.195Z",
          "wordCount": null,
          "title": "Towards Automatic Actor-Critic Solutions to Continuous Control. (arXiv:2106.08918v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1\">Mert Seker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannisto_A/0/1/0/all/0/1\">Anssi M&#xe4;nnist&#xf6;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1\">Jenni Raitoharju</a>",
          "description": "The COVID-19 virus has caused a global pandemic since March 2020. The World\nHealth Organization (WHO) has provided guidelines on how to reduce the spread\nof the virus and one of the most important measures is social distancing.\nMaintaining a minimum of one meter distance from other people is strongly\nsuggested to reduce the risk of infection. This has created a strong interest\nin monitoring the social distances either as a safety measure or to study how\nthe measures have affected human behavior and country-wise differences in this.\nThe need for automatic social distance estimation algorithms is evident, but\nthere is no suitable test benchmark for such algorithms. Collecting images with\nmeasured ground-truth pair-wise distances between all the people using\ndifferent camera settings is cumbersome. Furthermore, performance evaluation\nfor social distance estimation algorithms is not straightforward and there is\nno widely accepted evaluation protocol. In this paper, we provide a dataset of\nvarying images with measured pair-wise social distances under different camera\npositionings and focal length values. We suggest a performance evaluation\nprotocol and provide a benchmark to easily evaluate social distance estimation\nalgorithms. We also propose a method for automatic social distance estimation.\nOur method takes advantage of object detection and human pose estimation. It\ncan be applied on any single image as long as focal length and sensor size\ninformation are known. The results on our benchmark are encouraging with 92%\nhuman detection rate and only 28.9% average error in distance estimation among\nthe detected people.",
          "link": "http://arxiv.org/abs/2103.06759",
          "publishedOn": "2021-06-17T01:58:46.187Z",
          "wordCount": 774,
          "title": "Automatic Social Distance Estimation From Images: Performance Evaluation, Test Benchmark, and Algorithm. (arXiv:2103.06759v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lanlan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "Recent work introduced progressive network growing as a promising way to ease\nthe training for large GANs, but the model design and architecture-growing\nstrategy still remain under-explored and needs manual design for different\nimage data. In this paper, we propose a method to dynamically grow a GAN during\ntraining, optimizing the network architecture and its parameters together with\nautomation. The method embeds architecture search techniques as an interleaving\nstep with gradient-based training to periodically seek the optimal\narchitecture-growing strategy for the generator and discriminator. It enjoys\nthe benefits of both eased training because of progressive growing and improved\nperformance because of broader architecture design space. Experimental results\ndemonstrate new state-of-the-art of image generation. Observations in the\nsearch procedure also provide constructive insights into the GAN model design\nsuch as generator-discriminator balance and convolutional layer choices.",
          "link": "http://arxiv.org/abs/2106.08505",
          "publishedOn": "2021-06-17T01:58:46.180Z",
          "wordCount": 567,
          "title": "Dynamically Grown Generative Adversarial Networks. (arXiv:2106.08505v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonon_L/0/1/0/all/0/1\">Lukas Gonon</a>",
          "description": "This article investigates the use of random feature neural networks for\nlearning Kolmogorov partial (integro-)differential equations associated to\nBlack-Scholes and more general exponential L\\'evy models. Random feature neural\nnetworks are single-hidden-layer feedforward neural networks in which only the\noutput weights are trainable. This makes training particularly simple, but (a\npriori) reduces expressivity. Interestingly, this is not the case for\nBlack-Scholes type PDEs, as we show here. We derive bounds for the prediction\nerror of random neural networks for learning sufficiently non-degenerate\nBlack-Scholes type models. A full error analysis is provided and it is shown\nthat the derived bounds do not suffer from the curse of dimensionality. We also\ninvestigate an application of these results to basket options and validate the\nbounds numerically.\n\nThese results prove that neural networks are able to \\textit{learn} solutions\nto Black-Scholes type PDEs without the curse of dimensionality. In addition,\nthis provides an example of a relevant learning problem in which random feature\nneural networks are provably efficient.",
          "link": "http://arxiv.org/abs/2106.08900",
          "publishedOn": "2021-06-17T01:58:46.167Z",
          "wordCount": null,
          "title": "Random feature neural networks learn Black-Scholes type PDEs without curse of dimensionality. (arXiv:2106.08900v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Recently, transformation-based self-supervised learning has been applied to\ngenerative adversarial networks (GANs) to mitigate the catastrophic forgetting\nproblem of discriminator by learning stable representations. However, the\nseparate self-supervised tasks in existing self-supervised GANs cause an\ninconsistent goal with generative modeling due to the learning of the generator\nfrom their generator distribution-agnostic classifiers. To address this issue,\nwe propose a novel self-supervised GANs framework with label augmentation,\ni.e., augmenting the GAN labels (real or fake) with the self-supervised\npseudo-labels. In particular, the discriminator and the self-supervised\nclassifier are unified to learn a single task that predicts the augmented label\nsuch that the discriminator/classifier is aware of the generator distribution,\nwhile the generator tries to confuse the discriminator/classifier by optimizing\nthe discrepancy between the transformed real and generated distributions.\nTheoretically, we prove that the generator, at the equilibrium point, converges\nto replicate the data distribution. Empirically, we demonstrate that the\nproposed method significantly outperforms competitive baselines on both\ngenerative modeling and representation learning across benchmark datasets.",
          "link": "http://arxiv.org/abs/2106.08601",
          "publishedOn": "2021-06-17T01:58:46.166Z",
          "wordCount": 582,
          "title": "Self-supervised GANs with Label Augmentation. (arXiv:2106.08601v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Ching-Yu Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_A/0/1/0/all/0/1\">Alvin Wen-Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "This paper presents a novel system architecture that integrates blind source\nseparation with joint beat and downbeat tracking in musical audio signals. The\nsource separation module segregates the percussive and non-percussive\ncomponents of the input signal, over which beat and downbeat tracking are\nperformed separately and then the results are aggregated with a learnable\nfusion mechanism. This way, the system can adaptively determine how much the\ntracking result for an input signal should depend on the input's percussive or\nnon-percussive components. Evaluation on four testing sets that feature\ndifferent levels of presence of drum sounds shows that the new architecture\nconsistently outperforms the widely-adopted baseline architecture that does not\nemploy source separation.",
          "link": "http://arxiv.org/abs/2106.08685",
          "publishedOn": "2021-06-17T01:58:46.160Z",
          "wordCount": null,
          "title": "Drum-Aware Ensemble Architecture for Improved Joint Musical Beat and Downbeat Tracking. (arXiv:2106.08685v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Cong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yaochen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Youzhi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shenglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>",
          "description": "Molecular property prediction is gaining increasing attention due to its\ndiverse applications. One task of particular interests and importance is to\npredict quantum chemical properties without 3D equilibrium structures. This is\npractically favorable since obtaining 3D equilibrium structures requires\nextremely expensive calculations. In this work, we design a deep graph neural\nnetwork to predict quantum properties by directly learning from 2D molecular\ngraphs. In addition, we propose a 3D graph neural network to learn from\nlow-cost conformer sets, which can be obtained with open-source tools using an\naffordable budget. We employ our methods to participate in the 2021 KDD Cup on\nOGB Large-Scale Challenge (OGB-LSC), which aims to predict the HOMO-LUMO energy\ngap of molecules. Final evaluation results reveal that we are one of the\nwinners with a mean absolute error of 0.1235 on the holdout test set. Our\nimplementation is available as part of the MoleculeX package\n(https://github.com/divelab/MoleculeX).",
          "link": "http://arxiv.org/abs/2106.08551",
          "publishedOn": "2021-06-17T01:58:46.159Z",
          "wordCount": 606,
          "title": "Fast Quantum Property Prediction via Deeper 2D and 3D Graph Networks. (arXiv:2106.08551v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ro_J/0/1/0/all/0/1\">Jae Ro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1\">Mehryar Mohri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Ananda Theertha Suresh</a>",
          "description": "In distributed learning settings such as federated learning, the training\nalgorithm can be potentially biased towards different clients. Mohri et al.\n(2019) proposed a domain-agnostic learning algorithm, where the model is\noptimized for any target distribution formed by a mixture of the client\ndistributions in order to overcome this bias. They further proposed an\nalgorithm for the cross-silo federated learning setting, where the number of\nclients is small. We consider this problem in the cross-device setting, where\nthe number of clients is much larger. We propose a communication-efficient\ndistributed algorithm called Agnostic Federated Averaging (or AgnosticFedAvg)\nto minimize the domain-agnostic objective proposed in Mohri et al. (2019),\nwhich is amenable to other private mechanisms such as secure aggregation. We\nhighlight two types of naturally occurring domains in federated learning and\nargue that AgnosticFedAvg performs well on both. To demonstrate the practical\neffectiveness of AgnosticFedAvg, we report positive results for large-scale\nlanguage modeling tasks in both simulation and live experiments, where the\nlatter involves training language models for Spanish virtual keyboard for\nmillions of user devices.",
          "link": "http://arxiv.org/abs/2104.02748",
          "publishedOn": "2021-06-17T01:58:46.159Z",
          "wordCount": null,
          "title": "Communication-Efficient Agnostic Federated Averaging. (arXiv:2104.02748v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bedi_A/0/1/0/all/0/1\">Amrit Singh Bedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parayil_A/0/1/0/all/0/1\">Anjaly Parayil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengdi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppel_A/0/1/0/all/0/1\">Alec Koppel</a>",
          "description": "Reinforcement learning is a framework for interactive decision-making with\nincentives sequentially revealed across time without a system dynamics model.\nDue to its scaling to continuous spaces, we focus on policy search where one\niteratively improves a parameterized policy with stochastic policy gradient\n(PG) updates. In tabular Markov Decision Problems (MDPs), under persistent\nexploration and suitable parameterization, global optimality may be obtained.\nBy contrast, in continuous space, the non-convexity poses a pathological\nchallenge as evidenced by existing convergence results being mostly limited to\nstationarity or arbitrary local extrema. To close this gap, we step towards\npersistent exploration in continuous space through policy parameterizations\ndefined by distributions of heavier tails defined by tail-index parameter\nalpha, which increases the likelihood of jumping in state space. Doing so\ninvalidates smoothness conditions of the score function common to PG. Thus, we\nestablish how the convergence rate to stationarity depends on the policy's tail\nindex alpha, a Holder continuity parameter, integrability conditions, and an\nexploration tolerance parameter introduced here for the first time. Further, we\ncharacterize the dependence of the set of local maxima on the tail index\nthrough an exit and transition time analysis of a suitably defined Markov\nchain, identifying that policies associated with Levy Processes of a heavier\ntail converge to wider peaks. This phenomenon yields improved stability to\nperturbations in supervised learning, which we corroborate also manifests in\nimproved performance of policy search, especially when myopic and farsighted\nincentives are misaligned.",
          "link": "http://arxiv.org/abs/2106.08414",
          "publishedOn": "2021-06-17T01:58:46.152Z",
          "wordCount": null,
          "title": "On the Sample Complexity and Metastability of Heavy-tailed Policy Search in Continuous Control. (arXiv:2106.08414v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhaoyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Minghao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guodong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kehuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "Recent works have shown that interval bound propagation (IBP) can be used to\ntrain verifiably robust neural networks. Reseachers observe an intriguing\nphenomenon on these IBP trained networks: CROWN, a bounding method based on\ntight linear relaxation, often gives very loose bounds on these networks. We\nalso observe that most neurons become dead during the IBP training process,\nwhich could hurt the representation capability of the network. In this paper,\nwe study the relationship between IBP and CROWN, and prove that CROWN is always\ntighter than IBP when choosing appropriate bounding lines. We further propose a\nrelaxed version of CROWN, linear bound propagation (LBP), that can be used to\nverify large networks to obtain lower verified errors than IBP. We also design\na new activation function, parameterized ramp function (ParamRamp), which has\nmore diversity of neuron status than ReLU. We conduct extensive experiments on\nMNIST, CIFAR-10 and Tiny-ImageNet with ParamRamp activation and achieve\nstate-of-the-art verified robustness. Code and the appendix are available at\nhttps://github.com/ZhaoyangLyu/VerifiablyRobustNN.",
          "link": "http://arxiv.org/abs/2104.00447",
          "publishedOn": "2021-06-17T01:58:46.146Z",
          "wordCount": null,
          "title": "Towards Evaluating and Training Verifiably Robust Neural Networks. (arXiv:2104.00447v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sani_N/0/1/0/all/0/1\">Numair Sani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinsky_D/0/1/0/all/0/1\">Daniel Malinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shpitser_I/0/1/0/all/0/1\">Ilya Shpitser</a>",
          "description": "We propose to explain the behavior of black-box prediction methods (e.g.,\ndeep neural networks trained on image pixel data) using causal graphical\nmodels. Specifically, we explore learning the structure of a causal graph where\nthe nodes represent prediction outcomes along with a set of macro-level\n\"interpretable\" features, while allowing for arbitrary unmeasured confounding\namong these variables. The resulting graph may indicate which of the\ninterpretable features, if any, are possible causes of the prediction outcome\nand which may be merely associated with prediction outcomes due to confounding.\nThe approach is motivated by a counterfactual theory of causal explanation\nwherein good explanations point to factors that are \"difference-makers\" in an\ninterventionist sense. The resulting analysis may be useful in algorithm\nauditing and evaluation, by identifying features which make a causal difference\nto the algorithm's output.",
          "link": "http://arxiv.org/abs/2006.02482",
          "publishedOn": "2021-06-17T01:58:46.133Z",
          "wordCount": 606,
          "title": "Explaining the Behavior of Black-Box Prediction Algorithms with Causal Learning. (arXiv:2006.02482v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1902.04251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Seungki Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maglaras_C/0/1/0/all/0/1\">Costis Maglaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moallemi_C/0/1/0/all/0/1\">Ciamac C. Moallemi</a>",
          "description": "We consider a finite-horizon multi-armed bandit (MAB) problem in a Bayesian\nsetting, for which we propose an information relaxation sampling framework.\nWith this framework, we define an intuitive family of control policies that\ninclude Thompson sampling (TS) and the Bayesian optimal policy as endpoints.\nAnalogous to TS, which, at each decision epoch pulls an arm that is best with\nrespect to the randomly sampled parameters, our algorithms sample entire future\nreward realizations and take the corresponding best action. However, this is\ndone in the presence of \"penalties\" that seek to compensate for the\navailability of future information.\n\nWe develop several novel policies and performance bounds for MAB problems\nthat vary in terms of improving performance and increasing computational\ncomplexity between the two endpoints. Our policies can be viewed as natural\ngeneralizations of TS that simultaneously incorporate knowledge of the time\nhorizon and explicitly consider the exploration-exploitation trade-off. We\nprove associated structural results on performance bounds and suboptimality\ngaps. Numerical experiments suggest that this new class of policies perform\nwell, in particular in settings where the finite time horizon introduces\nsignificant exploration-exploitation tension into the problem. Finally,\ninspired by the finite-horizon Gittins index, we propose an index policy that\nbuilds on our framework that particularly outperforms the state-of-the-art\nalgorithms in our numerical experiments.",
          "link": "http://arxiv.org/abs/1902.04251",
          "publishedOn": "2021-06-17T01:58:46.116Z",
          "wordCount": 665,
          "title": "Thompson Sampling with Information Relaxation Penalties. (arXiv:1902.04251v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Price_I/0/1/0/all/0/1\">Ilan Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanner_J/0/1/0/all/0/1\">Jared Tanner</a>",
          "description": "That neural networks may be pruned to high sparsities and retain high\naccuracy is well established. Recent research efforts focus on pruning\nimmediately after initialization so as to allow the computational savings\nafforded by sparsity to extend to the training process. In this work, we\nintroduce a new `DCT plus Sparse' layer architecture, which maintains\ninformation propagation and trainability even with as little as 0.01% trainable\nkernel parameters remaining. We show that standard training of networks built\nwith these layers, and pruned at initialization, achieves state-of-the-art\naccuracy for extreme sparsities on a variety of benchmark network architectures\nand datasets. Moreover, these results are achieved using only simple heuristics\nto determine the locations of the trainable parameters in the network, and thus\nwithout having to initially store or compute with the full, unpruned network,\nas is required by competing prune-at-initialization algorithms. Switching from\nstandard sparse layers to DCT plus Sparse layers does not increase the storage\nfootprint of a network and incurs only a small additional computational\noverhead.",
          "link": "http://arxiv.org/abs/2102.07655",
          "publishedOn": "2021-06-17T01:58:46.110Z",
          "wordCount": 648,
          "title": "Dense for the Price of Sparse: Improved Performance of Sparsely Initialized Networks via a Subspace Offset. (arXiv:2102.07655v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_A/0/1/0/all/0/1\">Anthony Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1\">Max Paul Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resch_M/0/1/0/all/0/1\">Michael Resch</a>",
          "description": "With the increasing safety validation requirements for the release of a\nself-driving car, alternative approaches, such as simulation-based testing, are\nemerging in addition to conventional real-world testing. In order to rely on\nvirtual tests the employed sensor models have to be validated. For this reason,\nit is necessary to quantify the discrepancy between simulation and reality in\norder to determine whether a certain fidelity is sufficient for a desired\nintended use. There exists no sound method to measure this\nsimulation-to-reality gap of radar perception for autonomous driving. We\naddress this problem by introducing a multi-layered evaluation approach, which\nconsists of a combination of an explicit and an implicit sensor model\nevaluation. The former directly evaluates the realism of the synthetically\ngenerated sensor data, while the latter refers to an evaluation of a downstream\ntarget application. In order to demonstrate the method, we evaluated the\nfidelity of three typical radar model types (ideal, data-driven, ray\ntracing-based) and their applicability for virtually testing radar-based\nmulti-object tracking. We have shown the effectiveness of the proposed approach\nin terms of providing an in-depth sensor model assessment that renders existing\ndisparities visible and enables a realistic estimation of the overall model\nfidelity across different scenarios.",
          "link": "http://arxiv.org/abs/2106.08372",
          "publishedOn": "2021-06-17T01:58:46.104Z",
          "wordCount": 663,
          "title": "A Multi-Layered Approach for Measuring the Simulation-to-Reality Gap of Radar Perception for Autonomous Driving. (arXiv:2106.08372v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1\">Qi Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1\">Kunfeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinchao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuangzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shilian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoniu Yang</a>",
          "description": "Our digital world is full of time series and graphs which capture the various\naspects of many complex systems. Traditionally, there are respective methods in\nprocessing these two different types of data, e.g., Recurrent Neural Network\n(RNN) and Graph Neural Network (GNN), while in recent years, time series could\nbe mapped to graphs by using the techniques such as Visibility Graph (VG), so\nthat researchers can use graph algorithms to mine the knowledge in time series.\nSuch mapping methods establish a bridge between time series and graphs, and\nhave high potential to facilitate the analysis of various real-world time\nseries. However, the VG method and its variants are just based on fixed rules\nand thus lack of flexibility, largely limiting their application in reality. In\nthis paper, we propose an Adaptive Visibility Graph (AVG) algorithm that can\nadaptively map time series into graphs, based on which we further establish an\nend-to-end classification framework AVGNet, by utilizing GNN model DiffPool as\nthe classifier. We then adopt AVGNet for radio signal modulation classification\nwhich is an important task in the field of wireless communication. The\nsimulations validate that AVGNet outperforms a series of advanced deep learning\nmethods, achieving the state-of-the-art performance in this task.",
          "link": "http://arxiv.org/abs/2106.08564",
          "publishedOn": "2021-06-17T01:58:46.098Z",
          "wordCount": 642,
          "title": "Adaptive Visibility Graph Neural Network and It's Application in Modulation Classification. (arXiv:2106.08564v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.10415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huihan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Pre-trained language models have been successful on text classification\ntasks, but are prone to learning spurious correlations from biased datasets,\nand are thus vulnerable when making inferences in a new domain. Prior works\nreveal such spurious patterns via post-hoc explanation algorithms which compute\nthe importance of input features. Further, the model is regularized to align\nthe importance scores with human knowledge, so that the unintended model\nbehaviors are eliminated. However, such a regularization technique lacks\nflexibility and coverage, since only importance scores towards a pre-defined\nlist of features are adjusted, while more complex human knowledge such as\nfeature interaction and pattern generalization can hardly be incorporated. In\nthis work, we propose to refine a learned language model for a target domain by\ncollecting human-provided compositional explanations regarding observed biases.\nBy parsing these explanations into executable logic rules, the human-specified\nrefinement advice from a small set of explanations can be generalized to more\ntraining examples. We additionally introduce a regularization term allowing\nadjustments for both importance and interaction of features to better rectify\nmodel behavior. We demonstrate the effectiveness of the proposed approach on\ntwo text classification tasks by showing improved performance in target domain\nas well as improved model fairness after refinement.",
          "link": "http://arxiv.org/abs/2103.10415",
          "publishedOn": "2021-06-17T01:58:46.079Z",
          "wordCount": 663,
          "title": "Refining Language Models with Compositional Explanations. (arXiv:2103.10415v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07537",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Diamandis_T/0/1/0/all/0/1\">Theo Diamandis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fallah_A/0/1/0/all/0/1\">Alireza Fallah</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Farnia_F/0/1/0/all/0/1\">Farzan Farnia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ozdaglar_A/0/1/0/all/0/1\">Asuman Ozdaglar</a>",
          "description": "Multi-modal distributions are commonly used to model clustered data in\nstatistical learning tasks. In this paper, we consider the Mixed Linear\nRegression (MLR) problem. We propose an optimal transport-based framework for\nMLR problems, Wasserstein Mixed Linear Regression (WMLR), which minimizes the\nWasserstein distance between the learned and target mixture regression models.\nThrough a model-based duality analysis, WMLR reduces the underlying MLR task to\na nonconvex-concave minimax optimization problem, which can be provably solved\nto find a minimax stationary point by the Gradient Descent Ascent (GDA)\nalgorithm. In the special case of mixtures of two linear regression models, we\nshow that WMLR enjoys global convergence and generalization guarantees. We\nprove that WMLR's sample complexity grows linearly with the dimension of data.\nFinally, we discuss the application of WMLR to the federated learning task\nwhere the training samples are collected by multiple agents in a network.\nUnlike the Expectation Maximization algorithm, WMLR directly extends to the\ndistributed, federated learning setting. We support our theoretical results\nthrough several numerical experiments, which highlight our framework's ability\nto handle the federated learning setting with mixture models.",
          "link": "http://arxiv.org/abs/2106.07537",
          "publishedOn": "2021-06-17T01:58:46.072Z",
          "wordCount": 647,
          "title": "A Wasserstein Minimax Framework for Mixed Linear Regression. (arXiv:2106.07537v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boix_Adsera_E/0/1/0/all/0/1\">Enric Boix-Adsera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bresler_G/0/1/0/all/0/1\">Guy Bresler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1\">Frederic Koehler</a>",
          "description": "We consider the problem of learning a tree-structured Ising model from data,\nsuch that subsequent predictions computed using the model are accurate.\nConcretely, we aim to learn a model such that posteriors $P(X_i|X_S)$ for small\nsets of variables $S$ are accurate. Since its introduction more than 50 years\nago, the Chow-Liu algorithm, which efficiently computes the maximum likelihood\ntree, has been the benchmark algorithm for learning tree-structured graphical\nmodels. A bound on the sample complexity of the Chow-Liu algorithm with respect\nto the prediction-centric local total variation loss was shown in [BK19]. While\nthose results demonstrated that it is possible to learn a useful model even\nwhen recovering the true underlying graph is impossible, their bound depends on\nthe maximum strength of interactions and thus does not achieve the\ninformation-theoretic optimum. In this paper, we introduce a new algorithm that\ncarefully combines elements of the Chow-Liu algorithm with tree metric\nreconstruction methods to efficiently and optimally learn tree Ising models\nunder a prediction-centric loss. Our algorithm is robust to model\nmisspecification and adversarial corruptions. In contrast, we show that the\ncelebrated Chow-Liu algorithm can be arbitrarily suboptimal.",
          "link": "http://arxiv.org/abs/2106.03969",
          "publishedOn": "2021-06-17T01:58:46.066Z",
          "wordCount": 649,
          "title": "Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models. (arXiv:2106.03969v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08922",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moritz_N/0/1/0/all/0/1\">Niko Moritz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>",
          "description": "Pseudo-labeling (PL) has been shown to be effective in semi-supervised\nautomatic speech recognition (ASR), where a base model is self-trained with\npseudo-labels generated from unlabeled data. While PL can be further improved\nby iteratively updating pseudo-labels as the model evolves, most of the\nprevious approaches involve inefficient retraining of the model or intricate\ncontrol of the label update. We present momentum pseudo-labeling (MPL), a\nsimple yet effective strategy for semi-supervised ASR. MPL consists of a pair\nof online and offline models that interact and learn from each other, inspired\nby the mean teacher method. The online model is trained to predict\npseudo-labels generated on the fly by the offline model. The offline model\nmaintains a momentum-based moving average of the online model. MPL is performed\nin a single training process and the interaction between the two models\neffectively helps them reinforce each other to improve the ASR performance. We\napply MPL to an end-to-end ASR model based on the connectionist temporal\nclassification. The experimental results demonstrate that MPL effectively\nimproves over the base model and is scalable to different semi-supervised\nscenarios with varying amounts of data or domain mismatch.",
          "link": "http://arxiv.org/abs/2106.08922",
          "publishedOn": "2021-06-17T01:58:46.060Z",
          "wordCount": 630,
          "title": "Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition. (arXiv:2106.08922v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2001.09528",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Niharika Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1\">Alberto Olmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sailik Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manikonda_L/0/1/0/all/0/1\">Lydia Manikonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1\">Subbarao Kambhampati</a>",
          "description": "In this paper, we show that popular Generative Adversarial Networks (GANs)\nexacerbate biases along the axes of gender and skin tone when given a skewed\ndistribution of face-shots. While practitioners celebrate synthetic data\ngeneration using GANs as an economical way to augment data for training\ndata-hungry machine learning models, it is unclear whether they recognize the\nperils of such techniques when applied to real world datasets biased along\nlatent dimensions. Specifically, we show that (1) traditional GANs further skew\nthe distribution of a dataset consisting of engineering faculty headshots,\ngenerating minority modes less often and of worse quality and (2)\nimage-to-image translation (conditional) GANs also exacerbate biases by\nlightening skin color of non-white faces and transforming female facial\nfeatures to be masculine when generating faces of engineering professors. Thus,\nour study is meant to serve as a cautionary tale.",
          "link": "http://arxiv.org/abs/2001.09528",
          "publishedOn": "2021-06-17T01:58:46.054Z",
          "wordCount": 638,
          "title": "Imperfect ImaGANation: Implications of GANs Exacerbating Biases on Facial Data Augmentation and Snapchat Selfie Lenses. (arXiv:2001.09528v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_J/0/1/0/all/0/1\">Junayed Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1\">Fahim Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnob_R/0/1/0/all/0/1\">Raihan Islam Arnob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_K/0/1/0/all/0/1\">Kevin Moran</a>",
          "description": "Automated source code summarization is a popular software engineering\nresearch topic wherein machine translation models are employed to \"translate\"\ncode snippets into relevant natural language descriptions. Most evaluations of\nsuch models are conducted using automatic reference-based metrics. However,\ngiven the relatively large semantic gap between programming languages and\nnatural language, we argue that this line of research would benefit from a\nqualitative investigation into the various error modes of current\nstate-of-the-art models. Therefore, in this work, we perform both a\nquantitative and qualitative comparison of three recently proposed source code\nsummarization models. In our quantitative evaluation, we compare the models\nbased on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics,\nand in our qualitative evaluation, we perform a manual open-coding of the most\ncommon errors committed by the models when compared to ground truth captions.\nOur investigation reveals new insights into the relationship between\nmetric-based performance and model prediction errors grounded in an empirically\nderived error taxonomy that can be used to drive future research efforts",
          "link": "http://arxiv.org/abs/2106.08415",
          "publishedOn": "2021-06-17T01:58:46.039Z",
          "wordCount": 651,
          "title": "Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors. (arXiv:2106.08415v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2006.15368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandfonbrener_D/0/1/0/all/0/1\">David Brandfonbrener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1\">William F. Whitney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "Recent results in supervised learning suggest that while overparameterized\nmodels have the capacity to overfit, they in fact generalize quite well. We ask\nwhether the same phenomenon occurs for offline contextual bandits. Our results\nare mixed. Value-based algorithms benefit from the same generalization behavior\nas overparameterized supervised learning, but policy-based algorithms do not.\nWe show that this discrepancy is due to the \\emph{action-stability} of their\nobjectives. An objective is action-stable if there exists a prediction\n(action-value vector or action distribution) which is optimal no matter which\naction is observed. While value-based objectives are action-stable,\npolicy-based objectives are unstable. We formally prove upper bounds on the\nregret of overparameterized value-based learning and lower bounds on the regret\nfor policy-based algorithms. In our experiments with large neural networks,\nthis gap between action-stable value-based objectives and unstable policy-based\nobjectives leads to significant performance differences.",
          "link": "http://arxiv.org/abs/2006.15368",
          "publishedOn": "2021-06-17T01:58:46.033Z",
          "wordCount": 629,
          "title": "Offline Contextual Bandits with Overparameterized Models. (arXiv:2006.15368v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_K/0/1/0/all/0/1\">Kexin Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantini_D/0/1/0/all/0/1\">Dante Mantini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quanying Liu</a>",
          "description": "EEG source localization is an important technical issue in EEG analysis.\nDespite many numerical methods existed for EEG source localization, they all\nrely on strong priors and the deep sources are intractable. Here we propose a\ndeep learning framework using spatial basis function decomposition for EEG\nsource localization. This framework combines the edge sparsity prior and\nGaussian source basis, called Edge Sparse Basis Network (ESBN). The performance\nof ESBN is validated by both synthetic data and real EEG data during motor\ntasks. The results suggest that the supervised ESBN outperforms the traditional\nnumerical methods in synthetic data and the unsupervised fine-tuning provides\nmore focal and accurate localizations in real data. Our proposed deep learning\nframework can be extended to account for other source priors, and the real-time\nproperty of ESBN can facilitate the applications of EEG in brain-computer\ninterfaces and clinics.",
          "link": "http://arxiv.org/abs/2102.09188",
          "publishedOn": "2021-06-17T01:58:46.028Z",
          "wordCount": 618,
          "title": "Edge Sparse Basis Network: A Deep Learning Framework for EEG Source Localization. (arXiv:2102.09188v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08502",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Altschuler_J/0/1/0/all/0/1\">Jason M. Altschuler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chewi_S/0/1/0/all/0/1\">Sinho Chewi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gerber_P/0/1/0/all/0/1\">Patrik Gerber</a>, <a href=\"http://arxiv.org/find/math/1/au:+Stromme_A/0/1/0/all/0/1\">Austin J. Stromme</a>",
          "description": "We study first-order optimization algorithms for computing the barycenter of\nGaussian distributions with respect to the optimal transport metric. Although\nthe objective is geodesically non-convex, Riemannian GD empirically converges\nrapidly, in fact faster than off-the-shelf methods such as Euclidean GD and SDP\nsolvers. This stands in stark contrast to the best-known theoretical results\nfor Riemannian GD, which depend exponentially on the dimension. In this work,\nwe prove new geodesic convexity results which provide stronger control of the\niterates, yielding a dimension-free convergence rate. Our techniques also\nenable the analysis of two related notions of averaging, the\nentropically-regularized barycenter and the geometric median, providing the\nfirst convergence guarantees for Riemannian GD for these problems.",
          "link": "http://arxiv.org/abs/2106.08502",
          "publishedOn": "2021-06-17T01:58:46.022Z",
          "wordCount": 554,
          "title": "Averaging on the Bures-Wasserstein manifold: dimension-free convergence of gradient descent. (arXiv:2106.08502v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengli Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Weimin Ding</a>",
          "description": "Ensemble classifiers have been investigated by many in the artificial\nintelligence and machine learning community. Majority voting and weighted\nmajority voting are two commonly used combination schemes in ensemble learning.\nHowever, understanding of them is incomplete at best, with some properties even\nmisunderstood. In this paper, we present a group of properties of these two\nschemes formally under a dataset-level geometric framework. Two key factors,\nevery component base classifier's performance and dissimilarity between each\npair of component classifiers are evaluated by the same metric - the Euclidean\ndistance. Consequently, ensembling becomes a deterministic problem and the\nperformance of an ensemble can be calculated directly by a formula. We prove\nseveral theorems of interest and explain their implications for ensembles. In\nparticular, we compare and contrast the effect of the number of component\nclassifiers on these two types of ensemble schemes. Empirical investigation is\nalso conducted to verify the theoretical results when other metrics such as\naccuracy are used. We believe that the results from this paper are very useful\nfor us to understand the fundamental properties of these two combination\nschemes and the principles of ensemble classifiers in general. The results are\nalso helpful for us to investigate some issues in ensemble classifiers, such as\nensemble performance prediction, selecting a small number of base classifiers\nto obtain efficient and effective ensembles.",
          "link": "http://arxiv.org/abs/2106.08658",
          "publishedOn": "2021-06-17T01:58:46.016Z",
          "wordCount": 656,
          "title": "A Dataset-Level Geometric Framework for Ensemble Classifiers. (arXiv:2106.08658v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1\">Chirag Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Adversarial training has been the topic of dozens of studies and a leading\nmethod for defending against adversarial attacks. Yet, it remains largely\nunknown (a) how adversarially-robust ImageNet classifiers (R classifiers)\ngeneralize to out-of-distribution examples; and (b) how their generalization\ncapability relates to their hidden representations. In this paper, we perform a\nthorough, systematic study to answer these two questions across AlexNet,\nGoogLeNet, and ResNet-50 architectures. We found that while standard ImageNet\nclassifiers have a strong texture bias, their R counterparts rely heavily on\nshapes. Remarkably, adversarial training induces three simplicity biases into\nhidden neurons in the process of 'robustifying' the network. That is, each\nconvolutional neuron in R networks often changes to detecting (1) pixel-wise\nsmoother patterns i.e. a mechanism that blocks high-frequency noise from\npassing through the network; (2) more lower-level features i.e. textures and\ncolors (instead of objects); and (3) fewer types of inputs. Our findings reveal\nthe interesting mechanisms that made networks more adversarially robust and\nalso explain some recent findings. Our findings reveal the interesting\nmechanisms that made networks more adversarially robust and also explain some\nrecent findings e.g. why R networks benefit from much larger capacity (Xie and\nYuille, 2020) and can act as a strong image prior in image synthesis (Santurkar\net al., 2019).",
          "link": "http://arxiv.org/abs/2006.09373",
          "publishedOn": "2021-06-17T01:58:45.997Z",
          "wordCount": 693,
          "title": "The shape and simplicity biases of adversarially robust ImageNet-trained CNNs. (arXiv:2006.09373v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>",
          "description": "We investigate the training of sparse layers that use different parameters\nfor different inputs based on hashing in large Transformer models.\nSpecifically, we modify the feedforward layer to hash to different sets of\nweights depending on the current token, over all tokens in the sequence. We\nshow that this procedure either outperforms or is competitive with\nlearning-to-route mixture-of-expert methods such as Switch Transformers and\nBASE Layers, while requiring no routing parameters or extra terms in the\nobjective function such as a load balancing loss, and no sophisticated\nassignment algorithm. We study the performance of different hashing techniques,\nhash sizes and input features, and show that balanced and random hashes focused\non the most local features work best, compared to either learning clusters or\nusing longer-range context. We show our approach works well both on large\nlanguage modeling and dialogue tasks, and on downstream fine-tuning tasks.",
          "link": "http://arxiv.org/abs/2106.04426",
          "publishedOn": "2021-06-17T01:58:45.991Z",
          "wordCount": 587,
          "title": "Hash Layers For Large Sparse Models. (arXiv:2106.04426v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.02960",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Colin White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nolen_S/0/1/0/all/0/1\">Sam Nolen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savani_Y/0/1/0/all/0/1\">Yash Savani</a>",
          "description": "Neural architecture search (NAS) has seen a steep rise in interest over the\nlast few years. Many algorithms for NAS consist of searching through a space of\narchitectures by iteratively choosing an architecture, evaluating its\nperformance by training it, and using all prior evaluations to come up with the\nnext choice. The evaluation step is noisy - the final accuracy varies based on\nthe random initialization of the weights. Prior work has focused on devising\nnew search algorithms to handle this noise, rather than quantifying or\nunderstanding the level of noise in architecture evaluations. In this work, we\nshow that (1) the simplest hill-climbing algorithm is a powerful baseline for\nNAS, and (2), when the noise in popular NAS benchmark datasets is reduced to a\nminimum, hill-climbing to outperforms many popular state-of-the-art algorithms.\nWe further back up this observation by showing that the number of local minima\nis substantially reduced as the noise decreases, and by giving a theoretical\ncharacterization of the performance of local search in NAS. Based on our\nfindings, for NAS research we suggest (1) using local search as a baseline, and\n(2) denoising the training pipeline when possible.",
          "link": "http://arxiv.org/abs/2005.02960",
          "publishedOn": "2021-06-17T01:58:45.984Z",
          "wordCount": 654,
          "title": "Exploring the Loss Landscape in Neural Architecture Search. (arXiv:2005.02960v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Semret_N/0/1/0/all/0/1\">Nemo Semret</a>",
          "description": "We present a fully automated model for in-season crop yield prediction,\ndesigned to work where there is a dearth of sub-national \"ground truth\"\ninformation. Our approach relies primarily on satellite data and is\ncharacterized by careful feature engineering combined with a simple regression\nmodel. As such, it can work almost anywhere in the world. Applying it to 10\ndifferent crop-country pairs (5 cereals -- corn, wheat, sorghum, barley and\nmillet, in 2 countries -- Ethiopia and Kenya), we achieve RMSEs of 5\\%-10\\% for\npredictions 9 months into the year, and 7\\%-14\\% for predictions 3 months into\nthe year. The model outputs daily forecasts for the final yield of the current\nyear. It is trained using approximately 4 million data points for each\ncrop-country pair. These consist of: historical country-level annual yields,\ncrop calendars, crop cover, NDVI, temperature, rainfall, and\nevapotransporation.",
          "link": "http://arxiv.org/abs/2106.08720",
          "publishedOn": "2021-06-17T01:58:45.976Z",
          "wordCount": 569,
          "title": "Predicting crop yields with little ground truth: A simple statistical model for in-season forecasting. (arXiv:2106.08720v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khazraei_A/0/1/0/all/0/1\">Amir Khazraei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallyburton_S/0/1/0/all/0/1\">Spencer Hallyburton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qitong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1\">Miroslav Pajic</a>",
          "description": "This work focuses on the use of deep learning for vulnerability analysis of\ncyber-physical systems (CPS). Specifically, we consider a control architecture\nwidely used in CPS (e.g., robotics), where the low-level control is based on\ne.g., the extended Kalman filter (EKF) and an anomaly detector. To facilitate\nanalyzing the impact potential sensing attacks could have, our objective is to\ndevelop learning-enabled attack generators capable of designing stealthy\nattacks that maximally degrade system operation. We show how such problem can\nbe cast within a learning-based grey-box framework where parts of the runtime\ninformation are known to the attacker, and introduce two models based on\nfeed-forward neural networks (FNN); both models are trained offline, using a\ncost function that combines the attack effects on the estimation error and the\nresidual signal used for anomaly detection, so that the trained models are\ncapable of recursively generating such effective sensor attacks in real-time.\nThe effectiveness of the proposed methods is illustrated on several case\nstudies.",
          "link": "http://arxiv.org/abs/2103.06271",
          "publishedOn": "2021-06-17T01:58:45.969Z",
          "wordCount": 623,
          "title": "Learning-Based Vulnerability Analysis of Cyber-Physical Systems. (arXiv:2103.06271v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1\">Marian-Andrei Rizoiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soen_A/0/1/0/all/0/1\">Alexander Soen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shidi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Leanne Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderon_P/0/1/0/all/0/1\">Pio Calderon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Krishna Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lexing Xie</a>",
          "description": "This work builds a novel point process and tools to use the Hawkes process\nwith interval-censored data. Such data records the aggregated counts of events\nsolely during specific time intervals -- such as the number of patients\nadmitted to the hospital or the volume of vehicles passing traffic loop\ndetectors -- and not the exact occurrence time of the events. First, we\nestablish the Mean Behavior Poisson (MBP) process, a novel Poisson process with\na direct parameter correspondence to the popular self-exciting Hawkes process.\nThe event intensity function of the MBP is the expected intensity over all\npossible Hawkes realizations with the same parameter set. We fit MBP in the\ninterval-censored setting using an interval-censored Poisson log-likelihood\n(IC-LL). We use the parameter equivalence to uncover the parameters of the\nassociated Hawkes process. Second, we introduce two novel exogenous functions\nto distinguish the exogenous from the endogenous events. We propose the\nmulti-impulse exogenous function when the exogenous events are observed as\nevent time and the latent homogeneous Poisson process exogenous function when\nthe exogenous events are presented as interval-censored volumes. Third, we\nprovide several approximation methods to estimate the intensity and compensator\nfunction of MBP when no analytical solution exists. Fourth and finally, we\nconnect the interval-censored loss of MBP to a broader class of Bregman\ndivergence-based functions. Using the connection, we show that the current\nstate of the art in popularity estimation (Hawkes Intensity Process (HIP)\n(Rizoiu et al.,2017b)) is a particular case of the MBP process. We verify our\nmodels through empirical testing on synthetic data and real-world data. We find\nthat on real-world datasets that ourMBP process outperforms HIP for the task of\npopularity prediction.",
          "link": "http://arxiv.org/abs/2104.07932",
          "publishedOn": "2021-06-17T01:58:45.963Z",
          "wordCount": 739,
          "title": "Interval-censored Hawkes processes. (arXiv:2104.07932v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14471",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terry_J/0/1/0/all/0/1\">J. K. Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_B/0/1/0/all/0/1\">Benjamin Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grammel_N/0/1/0/all/0/1\">Nathaniel Grammel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_M/0/1/0/all/0/1\">Mario Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hari_A/0/1/0/all/0/1\">Ananth Hari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_R/0/1/0/all/0/1\">Ryan Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1\">Luis Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_R/0/1/0/all/0/1\">Rodrigo Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horsch_C/0/1/0/all/0/1\">Caroline Horsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dieffendahl_C/0/1/0/all/0/1\">Clemens Dieffendahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_N/0/1/0/all/0/1\">Niall L. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lokesh_Y/0/1/0/all/0/1\">Yashas Lokesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_P/0/1/0/all/0/1\">Praveen Ravi</a>",
          "description": "This paper introduces the PettingZoo library and the accompanying Agent\nEnvironment Cycle (\"AEC\") games model. PettingZoo is a library of diverse sets\nof multi-agent environments with a universal, elegant Python API. PettingZoo\nwas developed with the goal of accelerating research in Multi-Agent\nReinforcement Learning (\"MARL\"), by making work more interchangeable,\naccessible and reproducible akin to what OpenAI's Gym library did for\nsingle-agent reinforcement learning. PettingZoo's API, while inheriting many\nfeatures of Gym, is unique amongst MARL APIs in that it's based around the\nnovel AEC games model. We argue, in part through case studies on major problems\nin popular MARL environments, that the popular game models are poor conceptual\nmodels of the games commonly used with MARL, that they promote severe bugs that\nare hard to detect, and that the AEC games model addresses these problems.",
          "link": "http://arxiv.org/abs/2009.14471",
          "publishedOn": "2021-06-17T01:58:45.947Z",
          "wordCount": 649,
          "title": "PettingZoo: Gym for Multi-Agent Reinforcement Learning. (arXiv:2009.14471v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Ching-Yu Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1\">Joann Ching</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_W/0/1/0/all/0/1\">Wen-Yi Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Hua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_A/0/1/0/all/0/1\">Alvin Wen-Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Hsuan Yang</a>",
          "description": "Due to advances in deep learning, the performance of automatic beat and\ndownbeat tracking in musical audio signals has seen great improvement in recent\nyears. In training such deep learning based models, data augmentation has been\nfound an important technique. However, existing data augmentation methods for\nthis task mainly target at balancing the distribution of the training data with\nrespect to their tempo. In this paper, we investigate another approach for data\naugmentation, to account for the composition of the training data in terms of\nthe percussive and non-percussive sound sources. Specifically, we propose to\nemploy a blind drum separation model to segregate the drum and non-drum sounds\nfrom each training audio signal, filtering out training signals that are\ndrumless, and then use the obtained drum and non-drum stems to augment the\ntraining data. We report experiments on four completely unseen test sets,\nvalidating the effectiveness of the proposed method, and accordingly the\nimportance of drum sound composition in the training data for beat and downbeat\ntracking.",
          "link": "http://arxiv.org/abs/2106.08703",
          "publishedOn": "2021-06-17T01:58:45.941Z",
          "wordCount": null,
          "title": "Source Separation-based Data Augmentation for Improved Joint Beat and Downbeat Tracking. (arXiv:2106.08703v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2101.03735",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xie_W/0/1/0/all/0/1\">Wei Xie</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Martagan_T/0/1/0/all/0/1\">Tugce Martagan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Akcay_A/0/1/0/all/0/1\">Alp Akcay</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ravenstein_B/0/1/0/all/0/1\">Bram van Ravenstein</a>",
          "description": "In biopharmaceutical manufacturing, fermentation processes play a critical\nrole on productivity and profit. A fermentation process uses living cells with\ncomplex biological mechanisms, and this leads to high variability in the\nprocess outputs. By building on the biological mechanisms of protein and\nimpurity growth, we introduce a stochastic model to characterize the\naccumulation of the protein and impurity levels in the fermentation process.\nHowever, a common challenge in industry is the availability of only very\nlimited amount of data especially in the development and early stage of\nproduction. This adds an additional layer of uncertainty, referred to as model\nrisk, due to the difficulty of estimating the model parameters with limited\ndata. In this paper, we study the harvesting decision for a fermentation\nprocess under model risk. In particular, we adopt a Bayesian approach to update\nthe unknown parameters of the growth-rate distributions, and use the resulting\nposterior distributions to characterize the impact of model risk on\nfermentation output variability. The harvesting problem is formulated as a\nMarkov decision process model with knowledge states that summarize the\nposterior distributions and hence incorporate the model risk in\ndecision-making. The resulting model is solved by using a reinforcement\nlearning algorithm based on Bayesian sparse sampling. We provide analytical\nresults on the structure of the optimal policy and its objective function, and\nexplicitly study the impact of model risk on harvesting decisions. Our case\nstudies at MSD Animal Health demonstrate that the proposed model and solution\napproach improve the harvesting decisions in real life by achieving\nsubstantially higher average output from a fermentation batch along with lower\nbatch-to-batch variability.",
          "link": "http://arxiv.org/abs/2101.03735",
          "publishedOn": "2021-06-17T01:58:45.933Z",
          "wordCount": 727,
          "title": "Optimizing Biomanufacturing Harvesting Decisions under Limited Historical Data. (arXiv:2101.03735v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">SeongKu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Junyoung Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_W/0/1/0/all/0/1\">Wonbin Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hwanjo Yu</a>",
          "description": "Recommender Systems (RS) have employed knowledge distillation which is a\nmodel compression technique training a compact student model with the knowledge\ntransferred from a pre-trained large teacher model. Recent work has shown that\ntransferring knowledge from the teacher's intermediate layer significantly\nimproves the recommendation quality of the student. However, they transfer the\nknowledge of individual representation point-wise and thus have a limitation in\nthat primary information of RS lies in the relations in the representation\nspace. This paper proposes a new topology distillation approach that guides the\nstudent by transferring the topological structure built upon the relations in\nthe teacher space. We first observe that simply making the student learn the\nwhole topological structure is not always effective and even degrades the\nstudent's performance. We demonstrate that because the capacity of the student\nis highly limited compared to that of the teacher, learning the whole\ntopological structure is daunting for the student. To address this issue, we\npropose a novel method named Hierarchical Topology Distillation (HTD) which\ndistills the topology hierarchically to cope with the large capacity gap. Our\nextensive experiments on real-world datasets show that the proposed method\nsignificantly outperforms the state-of-the-art competitors. We also provide\nin-depth analyses to ascertain the benefit of distilling the topology for RS.",
          "link": "http://arxiv.org/abs/2106.08700",
          "publishedOn": "2021-06-17T01:58:45.925Z",
          "wordCount": 642,
          "title": "Topology Distillation for Recommender System. (arXiv:2106.08700v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aiguo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Nowadays, Graph Neural Networks (GNNs) following the Message Passing paradigm\nbecome the dominant way to learn on graphic data. Models in this paradigm have\nto spend extra space to look up adjacent nodes with adjacency matrices and\nextra time to aggregate multiple messages from adjacent nodes. To address this\nissue, we develop a method called LinkDist that distils self-knowledge from\nconnected node pairs into a Multi-Layer Perceptron (MLP) without the need to\naggregate messages. Experiment with 8 real-world datasets shows the MLP derived\nfrom LinkDist can predict the label of a node without knowing its adjacencies\nbut achieve comparable accuracy against GNNs in the contexts of semi- and\nfull-supervised node classification. Moreover, LinkDist benefits from its\nNon-Message Passing paradigm that we can also distil self-knowledge from\narbitrarily sampled node pairs in a contrastive way to further boost the\nperformance of LinkDist.",
          "link": "http://arxiv.org/abs/2106.08541",
          "publishedOn": "2021-06-17T01:58:45.914Z",
          "wordCount": 584,
          "title": "Distilling Self-Knowledge From Contrastive Links to Classify Graph Nodes Without Passing Messages. (arXiv:2106.08541v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08334",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Araz_J/0/1/0/all/0/1\">Jack Y. Araz</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Spannowsky_M/0/1/0/all/0/1\">Michael Spannowsky</a>",
          "description": "Tensor Networks are non-trivial representations of high-dimensional tensors,\noriginally designed to describe quantum many-body systems. We show that Tensor\nNetworks are ideal vehicles to connect quantum mechanical concepts to machine\nlearning techniques, thereby facilitating an improved interpretability of\nneural networks. This study presents the discrimination of top quark signal\nover QCD background processes using a Matrix Product State classifier. We show\nthat entanglement entropy can be used to interpret what a network learns, which\ncan be used to reduce the complexity of the network and feature space without\nloss of generality or performance. For the optimisation of the network, we\ncompare the Density Matrix Renormalization Group (DMRG) algorithm to stochastic\ngradient descent (SGD) and propose a joined training algorithm to harness the\nexplainability of DMRG with the efficiency of SGD.",
          "link": "http://arxiv.org/abs/2106.08334",
          "publishedOn": "2021-06-17T01:58:45.884Z",
          "wordCount": 586,
          "title": "Quantum-inspired event reconstruction with Tensor Networks: Matrix Product States. (arXiv:2106.08334v1 [hep-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2101.12616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Freeman_I/0/1/0/all/0/1\">Ido Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummert_A/0/1/0/all/0/1\">Anton Kummert</a>",
          "description": "The rising demand for Active Safety systems in automotive applications\nstresses the need for a reliable short to mid-term trajectory prediction.\nAnticipating the unfolding path of road users, one can act to increase the\noverall safety. In this work, we propose to train artificial neural networks\nfor movement understanding by predicting trajectories in their natural form, as\na function of time. Predicting polynomial coefficients allows us to increased\naccuracy and improve generalisation.",
          "link": "http://arxiv.org/abs/2101.12616",
          "publishedOn": "2021-06-17T01:58:45.875Z",
          "wordCount": 540,
          "title": "Polynomial Trajectory Predictions for Improved Learning Performance. (arXiv:2101.12616v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11970",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_R/0/1/0/all/0/1\">Rui Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1\">Yao Xie</a>",
          "description": "We develop a projected Wasserstein distance for the two-sample test, a\nfundamental problem in statistics and machine learning: given two sets of\nsamples, to determine whether they are from the same distribution. In\nparticular, we aim to circumvent the curse of dimensionality in Wasserstein\ndistance: when the dimension is high, it has diminishing testing power, which\nis inherently due to the slow concentration property of Wasserstein metrics in\nthe high dimension space. A key contribution is to couple optimal projection to\nfind the low dimensional linear mapping to maximize the Wasserstein distance\nbetween projected probability distributions. We characterize the theoretical\nproperty of the finite-sample convergence rate on IPMs and present practical\nalgorithms for computing this metric. Numerical examples validate our\ntheoretical results.",
          "link": "http://arxiv.org/abs/2010.11970",
          "publishedOn": "2021-06-17T01:58:45.869Z",
          "wordCount": 588,
          "title": "Two-sample Test using Projected Wasserstein Distance: Breaking the Curse of Dimensionality. (arXiv:2010.11970v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DInverno_G/0/1/0/all/0/1\">Giuseppe Alessio D&#x27;Inverno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchini_M/0/1/0/all/0/1\">Monica Bianchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampoli_M/0/1/0/all/0/1\">Maria Lucia Sampoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1\">Franco Scarselli</a>",
          "description": "Graph Neural Networks (GNNs) are a wide class of connectionist models for\ngraph processing. They perform an iterative message passing operation on each\nnode and its neighbors, to solve classification/ clustering tasks --- on some\nnodes or on the whole graph --- collecting all such messages, regardless of\ntheir order. Despite the differences among the various models belonging to this\nclass, most of them adopt the same computation scheme, based on a local\naggregation mechanism and, intuitively, the local computation framework is\nmainly responsible for the expressive power of GNNs. In this paper, we prove\nthat the Weisfeiler--Lehman test induces an equivalence relationship on the\ngraph nodes that exactly corresponds to the unfolding equivalence, defined on\nthe original GNN model. Therefore, the results on the expressive power of the\noriginal GNNs can be extended to general GNNs which, under mild conditions, can\nbe proved capable of approximating, in probability and up to any precision, any\nfunction on graphs that respects the unfolding equivalence.",
          "link": "http://arxiv.org/abs/2106.08992",
          "publishedOn": "2021-06-17T01:58:45.863Z",
          "wordCount": 611,
          "title": "An unifying point of view on expressive power of GNNs. (arXiv:2106.08992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.01357",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1\">Valentin De Bortoli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thornton_J/0/1/0/all/0/1\">James Thornton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heng_J/0/1/0/all/0/1\">Jeremy Heng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>",
          "description": "Progressively applying Gaussian noise transforms complex data distributions\nto approximately Gaussian. Reversing this dynamic defines a generative model.\nWhen the forward noising process is given by a Stochastic Differential Equation\n(SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the\nassociated reverse-time SDE may be estimated using score-matching. A limitation\nof this approach is that the forward-time SDE must be run for a sufficiently\nlong time for the final distribution to be approximately Gaussian. In contrast,\nsolving the Schr\\\"odinger Bridge problem (SB), i.e. an entropy-regularized\noptimal transport problem on path spaces, yields diffusions which generate\nsamples from the data distribution in finite time. We present Diffusion SB\n(DSB), an original approximation of the Iterative Proportional Fitting (IPF)\nprocedure to solve the SB problem, and provide theoretical analysis along with\ngenerative modeling experiments. The first DSB iteration recovers the\nmethodology proposed by Song et al. (2021), with the flexibility of using\nshorter time intervals, as subsequent DSB iterations reduce the discrepancy\nbetween the final-time marginal of the forward (resp. backward) SDE with\nrespect to the prior (resp. data) distribution. Beyond generative modeling, DSB\noffers a widely applicable computational optimal transport tool as the\ncontinuous state-space analogue of the popular Sinkhorn algorithm (Cuturi,\n2013).",
          "link": "http://arxiv.org/abs/2106.01357",
          "publishedOn": "2021-06-17T01:58:45.856Z",
          "wordCount": 662,
          "title": "Diffusion Schr\\\"odinger Bridge with Applications to Score-Based Generative Modeling. (arXiv:2106.01357v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08981",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Salazar_D/0/1/0/all/0/1\">Domingos S. P. Salazar</a>",
          "description": "Self-supervised learning (SSL) of energy based models has an intuitive\nrelation to equilibrium thermodynamics because the softmax layer, mapping\nenergies to probabilities, is a Gibbs distribution. However, in what way SSL is\na thermodynamic process? We show that some SSL paradigms behave as a\nthermodynamic composite system formed by representations and self-labels in\ncontact with a nonequilibrium reservoir. Moreover, this system is subjected to\nusual thermodynamic cycles, such as adiabatic expansion and isochoric heating,\nresulting in a generalized Gibbs ensemble (GGE). In this picture, we show that\nlearning is seen as a demon that operates in cycles using feedback measurements\nto extract negative work from the system. As applications, we examine some SSL\nalgorithms using this idea.",
          "link": "http://arxiv.org/abs/2106.08981",
          "publishedOn": "2021-06-17T01:58:45.847Z",
          "wordCount": 544,
          "title": "Nonequilibrium thermodynamics of self-supervised learning. (arXiv:2106.08981v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08352",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mohan_D/0/1/0/all/0/1\">Devang S Ram Mohan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_V/0/1/0/all/0/1\">Vivian Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teh_T/0/1/0/all/0/1\">Tian Huey Teh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Torresquintero_A/0/1/0/all/0/1\">Alexandra Torresquintero</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wallis_C/0/1/0/all/0/1\">Christopher G. R. Wallis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Staib_M/0/1/0/all/0/1\">Marlene Staib</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foglianti_L/0/1/0/all/0/1\">Lorenzo Foglianti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_J/0/1/0/all/0/1\">Jiameng Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_S/0/1/0/all/0/1\">Simon King</a>",
          "description": "Text does not fully specify the spoken form, so text-to-speech models must be\nable to learn from speech data that vary in ways not explained by the\ncorresponding text. One way to reduce the amount of unexplained variation in\ntraining data is to provide acoustic information as an additional learning\nsignal. When generating speech, modifying this acoustic information enables\nmultiple distinct renditions of a text to be produced.\n\nSince much of the unexplained variation is in the prosody, we propose a model\nthat generates speech explicitly conditioned on the three primary acoustic\ncorrelates of prosody: $F_{0}$, energy and duration. The model is flexible\nabout how the values of these features are specified: they can be externally\nprovided, or predicted from text, or predicted then subsequently modified.\n\nCompared to a model that employs a variational auto-encoder to learn\nunsupervised latent features, our model provides more interpretable,\ntemporally-precise, and disentangled control. When automatically predicting the\nacoustic features from text, it generates speech that is more natural than that\nfrom a Tacotron 2 model with reference encoder. Subsequent human-in-the-loop\nmodification of the predicted acoustic features can significantly further\nincrease naturalness.",
          "link": "http://arxiv.org/abs/2106.08352",
          "publishedOn": "2021-06-17T01:58:45.827Z",
          "wordCount": 656,
          "title": "Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis. (arXiv:2106.08352v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lily Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrault_A/0/1/0/all/0/1\">Andrew Perrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1\">Milind Tambe</a>",
          "description": "Green security domains feature defenders who plan patrols in the face of\nuncertainty about the adversarial behavior of poachers, illegal loggers, and\nillegal fishers. Importantly, the deterrence effect of patrols on adversaries'\nfuture behavior makes patrol planning a sequential decision-making problem.\nTherefore, we focus on robust sequential patrol planning for green security\nfollowing the minimax regret criterion, which has not been considered in the\nliterature. We formulate the problem as a game between the defender and nature\nwho controls the parameter values of the adversarial behavior and design an\nalgorithm MIRROR to find a robust policy. MIRROR uses two reinforcement\nlearning-based oracles and solves a restricted game considering limited\ndefender strategies and parameter values. We evaluate MIRROR on real-world\npoaching data.",
          "link": "http://arxiv.org/abs/2106.08413",
          "publishedOn": "2021-06-17T01:58:45.817Z",
          "wordCount": 573,
          "title": "Robust Reinforcement Learning Under Minimax Regret for Green Security. (arXiv:2106.08413v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadinejad_S/0/1/0/all/0/1\">Sara Mohammadinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1\">Jyotirmy V. Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenzi_L/0/1/0/all/0/1\">Laura Nenzi</a>",
          "description": "The Internet-of-Things, complex sensor networks, multi-agent cyber-physical\nsystems are all examples of spatially distributed systems that continuously\nevolve in time. Such systems generate huge amounts of spatio-temporal data, and\nsystem designers are often interested in analyzing and discovering structure\nwithin the data. There has been considerable interest in learning causal and\nlogical properties of temporal data using logics such as Signal Temporal Logic\n(STL); however, there is limited work on discovering such relations on\nspatio-temporal data. We propose the first set of algorithms for unsupervised\nlearning for spatio-temporal data. Our method does automatic feature extraction\nfrom the spatio-temporal data by projecting it onto the parameter space of a\nparametric spatio-temporal reach and escape logic (PSTREL). We propose an\nagglomerative hierarchical clustering technique that guarantees that each\ncluster satisfies a distinct STREL formula. We show that our method generates\nSTREL formulas of bounded description complexity using a novel decision-tree\napproach which generalizes previous unsupervised learning techniques for Signal\nTemporal Logic. We demonstrate the effectiveness of our approach on case\nstudies from diverse domains such as urban transportation, epidemiology, green\ninfrastructure, and air quality monitoring.",
          "link": "http://arxiv.org/abs/2106.08548",
          "publishedOn": "2021-06-17T01:58:45.810Z",
          "wordCount": 607,
          "title": "Mining Interpretable Spatio-temporal Logic Properties for Spatially Distributed Systems. (arXiv:2106.08548v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozbulak_U/0/1/0/all/0/1\">Utku Ozbulak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anzaku_E/0/1/0/all/0/1\">Esla Timothy Anzaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neve_W/0/1/0/all/0/1\">Wesley De Neve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messem_A/0/1/0/all/0/1\">Arnout Van Messem</a>",
          "description": "Although the adoption rate of deep neural networks (DNNs) has tremendously\nincreased in recent years, a solution for their vulnerability against\nadversarial examples has not yet been found. As a result, substantial research\nefforts are dedicated to fix this weakness, with many studies typically using a\nsubset of source images to generate adversarial examples, treating every image\nin this subset as equal. We demonstrate that, in fact, not every source image\nis equally suited for this kind of assessment. To do so, we devise a\nlarge-scale model-to-model transferability scenario for which we meticulously\nanalyze the properties of adversarial examples, generated from every suitable\nsource image in ImageNet by making use of two of the most frequently deployed\nattacks. In this transferability scenario, which involves seven distinct DNN\nmodels, including the recently proposed vision transformers, we reveal that it\nis possible to have a difference of up to $12.5\\%$ in model-to-model\ntransferability success, $1.01$ in average $L_2$ perturbation, and $0.03$\n($8/225$) in average $L_{\\infty}$ perturbation when $1,000$ source images are\nsampled randomly among all suitable candidates. We then take one of the first\nsteps in evaluating the robustness of images used to create adversarial\nexamples, proposing a number of simple but effective methods to identify\nunsuitable source images, thus making it possible to mitigate extreme cases in\nexperimentation and support high-quality benchmarking.",
          "link": "http://arxiv.org/abs/2106.07141",
          "publishedOn": "2021-06-17T01:58:45.797Z",
          "wordCount": 681,
          "title": "Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks. (arXiv:2106.07141v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruoming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>",
          "description": "Recently, linear regression models, such as EASE and SLIM, have shown to\noften produce rather competitive results against more sophisticated deep\nlearning models. On the other side, the (weighted) matrix factorization\napproaches have been popular choices for recommendation in the past and widely\nadopted in the industry. In this work, we aim to theoretically understand the\nrelationship between these two approaches, which are the cornerstones of\nmodel-based recommendations. Through the derivation and analysis of the\nclosed-form solutions for two basic regression and matrix factorization\napproaches, we found these two approaches are indeed inherently related but\nalso diverge in how they \"scale-down\" the singular values of the original\nuser-item interaction matrix. This analysis also helps resolve the questions\nrelated to the regularization parameter range and model complexities. We\nfurther introduce a new learning algorithm in searching (hyper)parameters for\nthe closed-form solution and utilize it to discover the nearby models of the\nexisting solutions. The experimental results demonstrate that the basic models\nand their closed-form solutions are indeed quite competitive against the\nstate-of-the-art models, thus, confirming the validity of studying the basic\nmodels. The effectiveness of exploring the nearby models are also\nexperimentally validated.",
          "link": "http://arxiv.org/abs/2105.12937",
          "publishedOn": "2021-06-17T01:58:45.779Z",
          "wordCount": 663,
          "title": "Towards a Better Understanding of Linear Models for Recommendation. (arXiv:2105.12937v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1\">Puoya Tabaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1\">Eli Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovi&#x107;</a>",
          "description": "Embedding methods for product spaces are powerful techniques for\nlow-distortion and low-dimensional representation of complex data structures.\nNevertheless, little is known regarding downstream learning and optimization\nproblems in such spaces. Here, we address the problem of linear classification\nin a product space form -- a mix of Euclidean, spherical, and hyperbolic\nspaces. First, we describe new formulations for linear classifiers on a\nRiemannian manifold using geodesics and Riemannian metrics which generalize\nstraight lines and inner products in vector spaces, respectively. Second, we\nprove that linear classifiers in $d$-dimensional space forms of any curvature\nhave the same expressive power, i.e., they can shatter exactly $d+1$ points.\nThird, we formalize linear classifiers in product space forms, describe the\nfirst corresponding perceptron and SVM classification algorithms, and establish\nrigorous convergence results for the former. We support our theoretical\nfindings with simulation results on several datasets, including synthetic data,\nCIFAR-100, MNIST, Omniglot, and single-cell RNA sequencing data. The results\nshow that learning methods applied to small-dimensional embeddings in product\nspace forms outperform their algorithmic counterparts in each space form.",
          "link": "http://arxiv.org/abs/2102.10204",
          "publishedOn": "2021-06-17T01:58:45.767Z",
          "wordCount": 630,
          "title": "Linear Classifiers in Product Space Forms. (arXiv:2102.10204v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00107",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wei_L/0/1/0/all/0/1\">Linchuan Wei</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gomez_A/0/1/0/all/0/1\">Andres Gomez</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kucukyavuz_S/0/1/0/all/0/1\">Simge Kucukyavuz</a>",
          "description": "Motivated by modern regression applications, in this paper, we study the\nconvexification of a class of convex optimization problems with indicator\nvariables and combinatorial constraints on the indicators. Unlike most of the\nprevious work on convexification of sparse regression problems, we\nsimultaneously consider the nonlinear non-separable objective, indicator\nvariables, and combinatorial constraints. Specifically, we give the convex hull\ndescription of the epigraph of the composition of a one-dimensional convex\nfunction and an affine function under arbitrary combinatorial constraints. As\nspecial cases of this result, we derive ideal convexifications for problems\nwith hierarchy, multi-collinearity, and sparsity constraints. Moreover, we also\ngive a short proof that for a separable objective function, the perspective\nreformulation is ideal independent from the constraints of the problem. Our\ncomputational experiments with regression problems under hierarchy constraints\non real datasets demonstrate the potential of the proposed approach in\nimproving the relaxation quality without significant computational overhead.",
          "link": "http://arxiv.org/abs/2007.00107",
          "publishedOn": "2021-06-17T01:58:45.759Z",
          "wordCount": 598,
          "title": "Ideal formulations for constrained convex optimization problems with indicator variables. (arXiv:2007.00107v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eden_T/0/1/0/all/0/1\">Talya Eden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1\">Piotr Indyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shyam Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinfeld_R/0/1/0/all/0/1\">Ronitt Rubinfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1\">Sandeep Silwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1\">Tal Wagner</a>",
          "description": "We consider the problem of estimating the number of distinct elements in a\nlarge data set (or, equivalently, the support size of the distribution induced\nby the data set) from a random sample of its elements. The problem occurs in\nmany applications, including biology, genomics, computer systems and\nlinguistics. A line of research spanning the last decade resulted in algorithms\nthat estimate the support up to $ \\pm \\varepsilon n$ from a sample of size\n$O(\\log^2(1/\\varepsilon) \\cdot n/\\log n)$, where $n$ is the data set size.\nUnfortunately, this bound is known to be tight, limiting further improvements\nto the complexity of this problem. In this paper we consider estimation\nalgorithms augmented with a machine-learning-based predictor that, given any\nelement, returns an estimation of its frequency. We show that if the predictor\nis correct up to a constant approximation factor, then the sample complexity\ncan be reduced significantly, to \\[ \\ \\log (1/\\varepsilon) \\cdot\nn^{1-\\Theta(1/\\log(1/\\varepsilon))}. \\] We evaluate the proposed algorithms on\na collection of data sets, using the neural-network based estimators from {Hsu\net al, ICLR'19} as predictors. Our experiments demonstrate substantial (up to\n3x) improvements in the estimation accuracy compared to the state of the art\nalgorithm.",
          "link": "http://arxiv.org/abs/2106.08396",
          "publishedOn": "2021-06-17T01:58:45.726Z",
          "wordCount": 646,
          "title": "Learning-based Support Estimation in Sublinear Time. (arXiv:2106.08396v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08866",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Vijaykumar_S/0/1/0/all/0/1\">Suhas Vijaykumar</a>",
          "description": "Offset Rademacher complexities have been shown to imply sharp, data-dependent\nupper bounds for the square loss in a broad class of problems including\nimproper statistical learning and online learning. We show that in the\nstatistical setting, the offset complexity upper bound can be generalized to\nany loss satisfying a certain uniform convexity condition. Amazingly, this\ncondition is shown to also capture exponential concavity and self-concordance,\nuniting several apparently disparate results. By a unified geometric argument,\nthese bounds translate directly to improper learning in a non-convex class\nusing Audibert's \"star algorithm.\" As applications, we recover the optimal\nrates for proper and improper learning with the $p$-loss, $1 < p < \\infty$ and\nshow that improper variants of empirical risk minimization can attain fast\nrates for logistic regression and other generalized linear models.",
          "link": "http://arxiv.org/abs/2105.08866",
          "publishedOn": "2021-06-17T01:58:45.720Z",
          "wordCount": 563,
          "title": "Localization, Convexity, and Star Aggregation. (arXiv:2105.08866v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Addad_V/0/1/0/all/0/1\">Vincent Cohen-Addad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lattanzi_S/0/1/0/all/0/1\">Silvio Lattanzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrovic_S/0/1/0/all/0/1\">Slobodan Mitrovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_Fard_A/0/1/0/all/0/1\">Ashkan Norouzi-Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parotsidis_N/0/1/0/all/0/1\">Nikos Parotsidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarnawski_J/0/1/0/all/0/1\">Jakub Tarnawski</a>",
          "description": "Correlation clustering is a central topic in unsupervised learning, with many\napplications in ML and data mining. In correlation clustering, one receives as\ninput a signed graph and the goal is to partition it to minimize the number of\ndisagreements. In this work we propose a massively parallel computation (MPC)\nalgorithm for this problem that is considerably faster than prior work. In\nparticular, our algorithm uses machines with memory sublinear in the number of\nnodes in the graph and returns a constant approximation while running only for\na constant number of rounds. To the best of our knowledge, our algorithm is the\nfirst that can provably approximate a clustering problem on graphs using only a\nconstant number of MPC rounds in the sublinear memory regime. We complement our\nanalysis with an experimental analysis of our techniques.",
          "link": "http://arxiv.org/abs/2106.08448",
          "publishedOn": "2021-06-17T01:58:45.700Z",
          "wordCount": 581,
          "title": "Correlation Clustering in Constant Many Parallel Rounds. (arXiv:2106.08448v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1\">Joni Korpihalkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1\">Tuomo Sipola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puuska_S/0/1/0/all/0/1\">Samir Puuska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1\">Tero Kokkonen</a>",
          "description": "Computer vision and machine learning can be used to automate various tasks in\ncancer diagnostic and detection. If an attacker can manipulate the automated\nprocessing, the results can be devastating and in the worst case lead to wrong\ndiagnosis and treatment. In this research, the goal is to demonstrate the use\nof one-pixel attacks in a real-life scenario with a real pathology dataset,\nTUPAC16, which consists of digitized whole-slide images. We attack against the\nIBM CODAIT's MAX breast cancer detector using adversarial images. These\nadversarial examples are found using differential evolution to perform the\none-pixel modification to the images in the dataset. The results indicate that\na minor one-pixel modification of a whole slide image under analysis can affect\nthe diagnosis by reversing the automatic diagnosis result. The attack poses a\nthreat from the cyber security perspective: the one-pixel method can be used as\nan attack vector by a motivated attacker.",
          "link": "http://arxiv.org/abs/2012.00517",
          "publishedOn": "2021-06-17T01:58:45.694Z",
          "wordCount": 632,
          "title": "One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer. (arXiv:2012.00517v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>",
          "description": "Attention mechanism of late has been quite popular in the computer vision\ncommunity. A lot of work has been done to improve the performance of the\nnetwork, although almost always it results in increased computational\ncomplexity. In this paper, we propose a new attention module that not only\nachieves the best performance but also has lesser parameters compared to most\nexisting models. Our attention module can easily be integrated with other\nconvolutional neural networks because of its lightweight nature. The proposed\nnetwork named Dual Multi Scale Attention Network (DMSANet) is comprised of two\nparts: the first part is used to extract features at various scales and\naggregate them, the second part uses spatial and channel attention modules in\nparallel to adaptively integrate local features with their global dependencies.\nWe benchmark our network performance for Image Classification on ImageNet\ndataset, Object Detection and Instance Segmentation both on MS COCO dataset.",
          "link": "http://arxiv.org/abs/2106.08382",
          "publishedOn": "2021-06-17T01:58:45.686Z",
          "wordCount": 583,
          "title": "DMSANet: Dual Multi Scale Attention Network. (arXiv:2106.08382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongpeng Li</a>",
          "description": "Online reinforcement learning (RL) has been widely applied in information\nprocessing scenarios, which usually exhibit much uncertainty due to the\nintrinsic randomness of channels and service demands. In this paper, we\nconsider an un-discounted RL in general Markov decision processes (MDPs) with\nboth endogeneous and exogeneous uncertainty, where both the rewards and state\ntransition probability are unknown to the RL agent and evolve with the time as\nlong as their respective variations do not exceed certain dynamic budget (i.e.,\nupper bound). We first develop a variation-aware Bernstein-based upper\nconfidence reinforcement learning (VB-UCRL), which we allow to restart\naccording to a schedule dependent on the variations. We successfully overcome\nthe challenges due to the exogeneous uncertainty and establish a regret bound\nof saving at most $\\sqrt{S}$ or $S^{\\frac{1}{6}}T^{\\frac{1}{12}}$ compared with\nthe latest results in the literature, where $S$ denotes the state size of the\nMDP and $T$ indicates the iteration index of learning steps.",
          "link": "http://arxiv.org/abs/2106.08477",
          "publishedOn": "2021-06-17T01:58:45.615Z",
          "wordCount": 595,
          "title": "Fundamental Limits of Reinforcement Learning in Environment with Endogeneous and Exogeneous Uncertainty. (arXiv:2106.08477v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02125",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chojnacka_R/0/1/0/all/0/1\">Roza Chojnacka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pelecanos_J/0/1/0/all/0/1\">Jason Pelecanos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>",
          "description": "In this paper, we describe SpeakerStew - a hybrid system to perform speaker\nverification on 46 languages. Two core ideas were explored in this system: (1)\nPooling training data of different languages together for multilingual\ngeneralization and reducing development cycles; (2) A novel triage mechanism\nbetween text-dependent and text-independent models to reduce runtime cost and\nexpected latency. To the best of our knowledge, this is the first study of\nspeaker verification systems at the scale of 46 languages. The problem is\nframed from the perspective of using a smart speaker device with interactions\nconsisting of a wake-up keyword (text-dependent) followed by a speech query\n(text-independent). Experimental evidence suggests that training on multiple\nlanguages can generalize to unseen varieties while maintaining performance on\nseen varieties. We also found that it can reduce computational requirements for\ntraining models by an order of magnitude. Furthermore, during model inference\non English data, we observe that leveraging a triage framework can reduce the\nnumber of calls to the more computationally expensive text-independent system\nby 73% (and reduce latency by 59%) while maintaining an EER no worse than the\ntext-independent setup.",
          "link": "http://arxiv.org/abs/2104.02125",
          "publishedOn": "2021-06-17T01:58:45.188Z",
          "wordCount": 670,
          "title": "SpeakerStew: Scaling to Many Languages with a Triaged Multilingual Text-Dependent and Text-Independent Speaker Verification System. (arXiv:2104.02125v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carmichael_Z/0/1/0/all/0/1\">Zachariah Carmichael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1\">Walter J. Scheirer</a>",
          "description": "Many applications of data-driven models demand transparency of decisions,\nespecially in health care, criminal justice, and other high-stakes\nenvironments. Modern trends in machine learning research have led to algorithms\nthat are increasingly intricate to the degree that they are considered to be\nblack boxes. In an effort to reduce the opacity of decisions, methods have been\nproposed to construe the inner workings of such models in a\nhuman-comprehensible manner. These post hoc techniques are described as being\nuniversal explainers - capable of faithfully augmenting decisions with\nalgorithmic insight. Unfortunately, there is little agreement about what\nconstitutes a \"good\" explanation. Moreover, current methods of explanation\nevaluation are derived from either subjective or proxy means. In this work, we\npropose a framework for the evaluation of post hoc explainers on ground truth\nthat is directly derived from the additive structure of a model. We demonstrate\nthe efficacy of the framework in understanding explainers by evaluating popular\nexplainers on thousands of synthetic and several real-world tasks. The\nframework unveils that explanations may be accurate but misattribute the\nimportance of individual features.",
          "link": "http://arxiv.org/abs/2106.08376",
          "publishedOn": "2021-06-17T01:58:45.163Z",
          "wordCount": 615,
          "title": "On the Objective Evaluation of Post Hoc Explainers. (arXiv:2106.08376v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finlay_C/0/1/0/all/0/1\">Chris Finlay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1\">Adam Oberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "Recent work has shown that Neural Ordinary Differential Equations (ODEs) can\nserve as generative models of images using the perspective of Continuous\nNormalizing Flows (CNFs). Such models offer exact likelihood calculation, and\ninvertible generation/density estimation. In this work we introduce a\nMulti-Resolution variant of such models (MRCNF), by characterizing the\nconditional distribution over the additional information required to generate a\nfine image that is consistent with the coarse image. We introduce a\ntransformation between resolutions that allows for no change in the log\nlikelihood. We show that this approach yields comparable likelihood values for\nvarious image datasets, with improved performance at higher resolutions, with\nfewer parameters, using only 1 GPU.",
          "link": "http://arxiv.org/abs/2106.08462",
          "publishedOn": "2021-06-17T01:58:45.157Z",
          "wordCount": 552,
          "title": "Multi-Resolution Continuous Normalizing Flows. (arXiv:2106.08462v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_Q/0/1/0/all/0/1\">Qicheng Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "There has been emerging interest to use transductive learning for adversarial\nrobustness (Goldwasser et al., NeurIPS 2020; Wu et al., ICML 2020). Compared to\ntraditional \"test-time\" defenses, these defense mechanisms \"dynamically\nretrain\" the model based on test time input via transductive learning; and\ntheoretically, attacking these defenses boils down to bilevel optimization,\nwhich seems to raise the difficulty for adaptive attacks. In this paper, we\nfirst formalize and analyze modeling aspects of transductive robustness. Then,\nwe propose the principle of attacking model space for solving bilevel attack\nobjectives, and present an instantiation of the principle which breaks previous\ntransductive defenses. These attacks thus point to significant difficulties in\nthe use of transductive learning to improve adversarial robustness. To this\nend, we present new theoretical and empirical evidence in support of the\nutility of transductive learning.",
          "link": "http://arxiv.org/abs/2106.08387",
          "publishedOn": "2021-06-17T01:58:45.149Z",
          "wordCount": 566,
          "title": "Towards Adversarial Robustness via Transductive Learning. (arXiv:2106.08387v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schrouff_J/0/1/0/all/0/1\">Jessica Schrouff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baur_S/0/1/0/all/0/1\">Sebastien Baur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Shaobo Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mincu_D/0/1/0/all/0/1\">Diana Mincu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loreaux_E/0/1/0/all/0/1\">Eric Loreaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanes_R/0/1/0/all/0/1\">Ralph Blanes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wexler_J/0/1/0/all/0/1\">James Wexler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Been Kim</a>",
          "description": "Interpretability techniques aim to provide the rationale behind a model's\ndecision, typically by explaining either an individual prediction (local\nexplanation, e.g. `why is this patient diagnosed with this condition') or a\nclass of predictions (global explanation, e.g. `why are patients diagnosed with\nthis condition in general'). While there are many methods focused on either\none, few frameworks can provide both local and global explanations in a\nconsistent manner. In this work, we combine two powerful existing techniques,\none local (Integrated Gradients, IG) and one global (Testing with Concept\nActivation Vectors), to provide local, and global concept-based explanations.\nWe first validate our idea using two synthetic datasets with a known ground\ntruth, and further demonstrate with a benchmark natural image dataset. We test\nour method with various concepts, target classes, model architectures and IG\nbaselines. We show that our method improves global explanations over TCAV when\ncompared to ground truth, and provides useful insights. We hope our work\nprovides a step towards building bridges between many existing local and global\nmethods to get the best of both worlds.",
          "link": "http://arxiv.org/abs/2106.08641",
          "publishedOn": "2021-06-17T01:58:45.134Z",
          "wordCount": 616,
          "title": "Best of both worlds: local and global explanations with human-understandable concepts. (arXiv:2106.08641v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08443",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>",
          "description": "This is a tutorial and survey paper on kernels, kernel methods, and related\nfields. We start with reviewing the history of kernels in functional analysis\nand machine learning. Then, Mercer kernel, Hilbert and Banach spaces,\nReproducing Kernel Hilbert Space (RKHS), Mercer's theorem and its proof,\nfrequently used kernels, kernel construction from distance metric, important\nclasses of kernels (including bounded, integrally positive definite, universal,\nstationary, and characteristic kernels), kernel centering and normalization,\nand eigenfunctions are explained in detail. Then, we introduce types of use of\nkernels in machine learning including kernel methods (such as kernel support\nvector machines), kernel learning by semi-definite programming, Hilbert-Schmidt\nindependence criterion, maximum mean discrepancy, kernel mean embedding, and\nkernel dimensionality reduction. We also cover rank and factorization of kernel\nmatrix as well as the approximation of eigenfunctions and kernels using the\nNystr{\\\"o}m method. This paper can be useful for various fields of science\nincluding machine learning, dimensionality reduction, functional analysis in\nmathematics, and mathematical physics in quantum mechanics.",
          "link": "http://arxiv.org/abs/2106.08443",
          "publishedOn": "2021-06-17T01:58:45.097Z",
          "wordCount": 633,
          "title": "Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr\\\"om Method, and Use of Kernels in Machine Learning: Tutorial and Survey. (arXiv:2106.08443v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Multi-task learning (MTL) aims to improve the generalization of several\nrelated tasks by learning them jointly. As a comparison, in addition to the\njoint training scheme, modern meta-learning allows unseen tasks with limited\nlabels during the test phase, in the hope of fast adaptation over them. Despite\nthe subtle difference between MTL and meta-learning in the problem formulation,\nboth learning paradigms share the same insight that the shared structure\nbetween existing training tasks could lead to better generalization and\nadaptation. In this paper, we take one important step further to understand the\nclose connection between these two learning paradigms, through both theoretical\nanalysis and empirical investigation. Theoretically, we first demonstrate that\nMTL shares the same optimization formulation with a class of gradient-based\nmeta-learning (GBML) algorithms. We then prove that for over-parameterized\nneural networks with sufficient depth, the learned predictive functions of MTL\nand GBML are close. In particular, this result implies that the predictions\ngiven by these two models are similar over the same unseen task. Empirically,\nwe corroborate our theoretical findings by showing that, with proper\nimplementation, MTL is competitive against state-of-the-art GBML algorithms on\na set of few-shot image classification benchmarks. Since existing GBML\nalgorithms often involve costly second-order bi-level optimization, our\nfirst-order MTL method is an order of magnitude faster on large-scale datasets\nsuch as mini-ImageNet. We believe this work could help bridge the gap between\nthese two learning paradigms, and provide a computationally efficient\nalternative to GBML that also supports fast task adaptation.",
          "link": "http://arxiv.org/abs/2106.09017",
          "publishedOn": "2021-06-17T01:58:45.091Z",
          "wordCount": 699,
          "title": "Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation. (arXiv:2106.09017v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafarnia_Jahromi_M/0/1/0/all/0/1\">Mehdi Jafarnia-Jahromi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rahul Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>",
          "description": "We introduce a generic template for developing regret minimization algorithms\nin the Stochastic Shortest Path (SSP) model, which achieves minimax optimal\nregret as long as certain properties are ensured. The key of our analysis is a\nnew technique called implicit finite-horizon approximation, which approximates\nthe SSP model by a finite-horizon counterpart only in the analysis without\nexplicit implementation. Using this template, we develop two new algorithms:\nthe first one is model-free (the first in the literature to our knowledge) and\nminimax optimal under strictly positive costs; the second one is model-based\nand minimax optimal even with zero-cost state-action pairs, matching the best\nexisting result from [Tarbouriech et al., 2021b]. Importantly, both algorithms\nadmit highly sparse updates, making them computationally more efficient than\nall existing algorithms. Moreover, both can be made completely parameter-free.",
          "link": "http://arxiv.org/abs/2106.08377",
          "publishedOn": "2021-06-17T01:58:45.086Z",
          "wordCount": 567,
          "title": "Implicit Finite-Horizon Approximation and Efficient Optimal Algorithms for Stochastic Shortest Path. (arXiv:2106.08377v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gottwald_M/0/1/0/all/0/1\">Martin Gottwald</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gronauer_S/0/1/0/all/0/1\">Sven Gronauer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Diepold_K/0/1/0/all/0/1\">Klaus Diepold</a> (1) ((1) Technical University of Munich, (2) fortiss)",
          "description": "Recent development of Deep Reinforcement Learning has demonstrated superior\nperformance of neural networks in solving challenging problems with large or\neven continuous state spaces. One specific approach is to deploy neural\nnetworks to approximate value functions by minimising the Mean Squared Bellman\nError function. Despite great successes of Deep Reinforcement Learning,\ndevelopment of reliable and efficient numerical algorithms to minimise the\nBellman Error is still of great scientific interest and practical demand. Such\na challenge is partially due to the underlying optimisation problem being\nhighly non-convex or using incorrect gradient information as done in\nSemi-Gradient algorithms. In this work, we analyse the Mean Squared Bellman\nError from a smooth optimisation perspective combined with a Residual Gradient\nformulation. Our contribution is two-fold.\n\nFirst, we analyse critical points of the error function and provide technical\ninsights on the optimisation procure and design choices for neural networks.\nWhen the existence of global minima is assumed and the objective fulfils\ncertain conditions we can eliminate suboptimal local minima when using\nover-parametrised neural networks. We can construct an efficient Approximate\nNewton's algorithm based on our analysis and confirm theoretical properties of\nthis algorithm such as being locally quadratically convergent to a global\nminimum numerically.\n\nSecond, we demonstrate feasibility and generalisation capabilities of the\nproposed algorithm empirically using continuous control problems and provide a\nnumerical verification of our critical point analysis. We outline the short\ncoming of Semi-Gradients. To benefit from an approximate Newton's algorithm\ncomplete derivatives of the Mean Squared Bellman error must be considered\nduring training.",
          "link": "http://arxiv.org/abs/2106.08774",
          "publishedOn": "2021-06-17T01:58:45.080Z",
          "wordCount": 700,
          "title": "Analysis and Optimisation of Bellman Residual Errors with Neural Function Approximation. (arXiv:2106.08774v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhengzheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Ziyue Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xuehai Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharejo_F/0/1/0/all/0/1\">Fayaz Ali Dharejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanchun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>",
          "description": "Data augmentation aims to generate new and synthetic features from the\noriginal data, which can identify a better representation of data and improve\nthe performance and generalizability of downstream tasks. However, data\naugmentation for graph-based models remains a challenging problem, as graph\ndata is more complex than traditional data, which consists of two features with\ndifferent properties: graph topology and node attributes. In this paper, we\nstudy the problem of graph data augmentation for Graph Convolutional Network\n(GCN) in the context of improving the node embeddings for semi-supervised node\nclassification. Specifically, we conduct cosine similarity based cross\noperation on the original features to create new graph features, including new\nnode attributes and new graph topologies, and we combine them as new pairwise\ninputs for specific GCNs. Then, we propose an attentional integrating model to\nweighted sum the hidden node embeddings encoded by these GCNs into the final\nnode embeddings. We also conduct a disparity constraint on these hidden node\nembeddings when training to ensure that non-redundant information is captured\nfrom different features. Experimental results on five real-world datasets show\nthat our method improves the classification accuracy with a clear margin (+2.5%\n- +84.2%) than the original GCN model.",
          "link": "http://arxiv.org/abs/2106.08848",
          "publishedOn": "2021-06-17T01:58:45.074Z",
          "wordCount": 654,
          "title": "Data Augmentation for Graph Convolutional Network on Semi-Supervised Classification. (arXiv:2106.08848v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Siddharth Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_M/0/1/0/all/0/1\">Minji Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1\">Kijung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faloutsos_C/0/1/0/all/0/1\">Christos Faloutsos</a>",
          "description": "Given a stream of graph edges from a dynamic graph, how can we assign anomaly\nscores to edges in an online manner, for the purpose of detecting unusual\nbehavior, using constant time and memory? Existing approaches aim to detect\nindividually surprising edges. In this work, we propose MIDAS, which focuses on\ndetecting microcluster anomalies, or suddenly arriving groups of suspiciously\nsimilar edges, such as lockstep behavior, including denial of service attacks\nin network traffic data. We further propose MIDAS-F, to solve the problem by\nwhich anomalies are incorporated into the algorithm's internal states, creating\na `poisoning' effect that can allow future anomalies to slip through\nundetected. MIDAS-F introduces two modifications: 1) We modify the anomaly\nscoring function, aiming to reduce the `poisoning' effect of newly arriving\nedges; 2) We introduce a conditional merge step, which updates the algorithm's\ndata structures after each time tick, but only if the anomaly score is below a\nthreshold value, also to reduce the `poisoning' effect. Experiments show that\nMIDAS-F has significantly higher accuracy than MIDAS. MIDAS has the following\nproperties: (a) it detects microcluster anomalies while providing theoretical\nguarantees about its false positive probability; (b) it is online, thus\nprocessing each edge in constant time and constant memory, and also processes\nthe data orders-of-magnitude faster than state-of-the-art approaches; (c) it\nprovides up to 62% higher ROC-AUC than state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2009.08452",
          "publishedOn": "2021-06-17T01:58:45.068Z",
          "wordCount": 696,
          "title": "Real-Time Anomaly Detection in Edge Streams. (arXiv:2009.08452v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1\">Daniele Cattaneo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1\">Matteo Vaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Loop closure detection is an essential component of Simultaneous Localization\nand Mapping (SLAM) systems, which reduces the drift accumulated over time. Over\nthe years, several deep learning approaches have been proposed to address this\ntask, however their performance has been subpar compared to handcrafted\ntechniques, especially while dealing with reverse loops. In this paper, we\nintroduce the novel LCDNet that effectively detects loop closures in LiDAR\npoint clouds by simultaneously identifying previously visited places and\nestimating the 6-DoF relative transformation between the current scan and the\nmap. LCDNet is composed of a shared encoder, a place recognition head that\nextracts global descriptors, and a relative pose head that estimates the\ntransformation between two point clouds. We introduce a novel relative pose\nhead based on the unbalanced optimal transport theory that we implement in a\ndifferentiable manner to allow for end-to-end training. Extensive evaluations\nof LCDNet on multiple real-world autonomous driving datasets show that our\napproach outperforms state-of-the-art loop closure detection and point cloud\nregistration techniques by a large margin, especially while dealing with\nreverse loops. Moreover, we integrate our proposed loop closure detection\napproach into a LiDAR SLAM library to provide a complete mapping system and\ndemonstrate the generalization ability using different sensor setup in an\nunseen city.",
          "link": "http://arxiv.org/abs/2103.05056",
          "publishedOn": "2021-06-17T01:58:45.014Z",
          "wordCount": 675,
          "title": "LCDNet: Deep Loop Closure Detection and Point Cloud Registration for LiDAR SLAM. (arXiv:2103.05056v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08419",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Digani_J/0/1/0/all/0/1\">Jagrit Digani</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hon_P/0/1/0/all/0/1\">Phillip Hon</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Davoyan_A/0/1/0/all/0/1\">Artur R. Davoyan</a>",
          "description": "Photonic inverse design has emerged as an indispensable engineering tool for\ncomplex optical systems. In many instances it is important to optimize for both\nmaterial and geometry configurations, which results in complex non-smooth\nsearch spaces with multiple local minima. Finding solutions approaching global\noptimum may present a computationally intractable task. Here, we develop a\nframework that allows expediting the search of solutions close to global\noptimum on complex optimization spaces. We study the way representative black\nbox optimization algorithms work, including genetic algorithm (GA), particle\nswarm optimization (PSO), simulated annealing (SA), and mesh adaptive direct\nsearch (NOMAD). We then propose and utilize a two-step approach that identifies\nbest performance algorithms on arbitrarily complex search spaces. We reveal a\nconnection between the search space complexity and algorithm performance and\nfind that PSO and NOMAD consistently deliver better performance for mixed\ninteger problems encountered in photonic inverse design, particularly with the\naccount of material combinations. Our results differ from a commonly\nanticipated advantage of GA. Our findings will foster more efficient design of\nphotonic systems with optimal performance.",
          "link": "http://arxiv.org/abs/2106.08419",
          "publishedOn": "2021-06-17T01:58:45.006Z",
          "wordCount": 616,
          "title": "A Framework for Discovering Optimal Solutions in Photonic Inverse Design. (arXiv:2106.08419v1 [physics.optics])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sewoong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_S/0/1/0/all/0/1\">Song Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "For deployment, neural architecture search should be hardware-aware, in order\nto satisfy the device-specific constraints (e.g., memory usage, latency and\nenergy consumption) and enhance the model efficiency. Existing methods on\nhardware-aware NAS collect a large number of samples (e.g., accuracy and\nlatency) from a target device, either builds a lookup table or a latency\nestimator. However, such approach is impractical in real-world scenarios as\nthere exist numerous devices with different hardware specifications, and\ncollecting samples from such a large number of devices will require prohibitive\ncomputational and monetary cost. To overcome such limitations, we propose\nHardware-adaptive Efficient Latency Predictor (HELP), which formulates the\ndevice-specific latency estimation problem as a meta-learning problem, such\nthat we can estimate the latency of a model's performance for a given task on\nan unseen device with a few samples. To this end, we introduce novel hardware\nembeddings to embed any devices considering them as black-box functions that\noutput latencies, and meta-learn the hardware-adaptive latency predictor in a\ndevice-dependent manner, using the hardware embeddings. We validate the\nproposed HELP for its latency estimation performance on unseen platforms, on\nwhich it achieves high estimation performance with as few as 10 measurement\nsamples, outperforming all relevant baselines. We also validate end-to-end NAS\nframeworks using HELP against ones without it, and show that it largely reduces\nthe total time cost of the base NAS method, in latency-constrained settings.",
          "link": "http://arxiv.org/abs/2106.08630",
          "publishedOn": "2021-06-17T01:58:44.993Z",
          "wordCount": 655,
          "title": "HELP: Hardware-Adaptive Efficient Latency Predictor for NAS via Meta-Learning. (arXiv:2106.08630v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shichong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moazeni_A/0/1/0/all/0/1\">Alireza Moazeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>",
          "description": "Deep generative models such as GANs have driven impressive advances in\nconditional image synthesis in recent years. A persistent challenge has been to\ngenerate diverse versions of output images from the same input image, due to\nthe problem of mode collapse: because only one ground truth output image is\ngiven per input image, only one mode of the conditional distribution is\nmodelled. In this paper, we focus on this problem of multimodal conditional\nimage synthesis and build on the recently proposed technique of Implicit\nMaximum Likelihood Estimation (IMLE). Prior IMLE-based methods required\ndifferent architectures for different tasks, which limit their applicability,\nand were lacking in fine details in the generated images. We propose CAM-Net, a\nunified architecture that can be applied to a broad range of tasks.\nAdditionally, it is capable of generating convincing high frequency details,\nachieving a reduction of the Frechet Inception Distance (FID) by up to 45.3%\ncompared to the baseline.",
          "link": "http://arxiv.org/abs/2106.09015",
          "publishedOn": "2021-06-17T01:58:44.978Z",
          "wordCount": 627,
          "title": "Cascading Modular Network (CAM-Net) for Multimodal Image Synthesis. (arXiv:2106.09015v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.12207",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lemhadri_I/0/1/0/all/0/1\">Ismael Lemhadri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ruan_F/0/1/0/all/0/1\">Feng Ruan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Abraham_L/0/1/0/all/0/1\">Louis Abraham</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tibshirani_R/0/1/0/all/0/1\">Robert Tibshirani</a>",
          "description": "Much work has been done recently to make neural networks more interpretable,\nand one obvious approach is to arrange for the network to use only a subset of\nthe available features. In linear models, Lasso (or $\\ell_1$-regularized)\nregression assigns zero weights to the most irrelevant or redundant features,\nand is widely used in data science. However the Lasso only applies to linear\nmodels. Here we introduce LassoNet, a neural network framework with global\nfeature selection. Our approach enforces a hierarchy: specifically a feature\ncan participate in a hidden unit only if its linear representative is active.\nUnlike other approaches to feature selection for neural nets, our method uses a\nmodified objective function with constraints, and so integrates feature\nselection with the parameter learning directly. As a result, it delivers an\nentire regularization path of solutions with a range of feature sparsity. On\nsystematic experiments, LassoNet significantly outperforms state-of-the-art\nmethods for feature selection and regression. The LassoNet method uses\nprojected proximal gradient descent, and generalizes directly to deep networks.\nIt can be implemented by adding just a few lines of code to a standard neural\nnetwork.",
          "link": "http://arxiv.org/abs/1907.12207",
          "publishedOn": "2021-06-17T01:58:44.962Z",
          "wordCount": 725,
          "title": "LassoNet: A Neural Network with Feature Sparsity. (arXiv:1907.12207v10 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yikai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xingyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Annie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Rong Ge</a>",
          "description": "Hessian captures important properties of the deep neural network loss\nlandscape. Previous works have observed low rank structure in the Hessians of\nneural networks. We make several new observations about the top eigenspace of\nlayer-wise Hessian: top eigenspaces for different models have surprisingly high\noverlap, and top eigenvectors form low rank matrices when they are reshaped\ninto the same shape as the corresponding weight matrix. Towards formally\nexplaining such structures of the Hessian, we show that the new eigenspace\nstructure can be explained by approximating the Hessian using Kronecker\nfactorization; we also prove the low rank structure for random data at random\ninitialization for over-parametrized two-layer neural nets. Our new\nunderstanding can explain why some of these structures become weaker when the\nnetwork is trained with batch normalization. The Kronecker factorization also\nleads to better explicit generalization bounds.",
          "link": "http://arxiv.org/abs/2010.04261",
          "publishedOn": "2021-06-17T01:58:44.943Z",
          "wordCount": 657,
          "title": "Dissecting Hessian: Understanding Common Structure of Hessian in Neural Networks. (arXiv:2010.04261v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04229",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Kasim_M/0/1/0/all/0/1\">Muhammad F. Kasim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Vinko_S/0/1/0/all/0/1\">Sam M. Vinko</a>",
          "description": "Improving the predictive capability of molecular properties in ab initio\nsimulations is essential for advanced material discovery. Despite recent\nprogress making use of machine learning, utilizing deep neural networks to\nimprove quantum chemistry modelling remains severely limited by the scarcity\nand heterogeneity of appropriate experimental data. Here we show how training a\nneural network to replace the exchange-correlation functional within a\nfully-differentiable three-dimensional Kohn-Sham density functional theory\n(DFT) framework can greatly improve simulation accuracy. Using only eight\nexperimental data points on diatomic molecules, our trained\nexchange-correlation networks enable improved prediction accuracy of\natomization energies across a collection of 104 molecules containing new bonds\nand atoms that are not present in the training dataset.",
          "link": "http://arxiv.org/abs/2102.04229",
          "publishedOn": "2021-06-17T01:58:44.918Z",
          "wordCount": 586,
          "title": "Learning the exchange-correlation functional from nature with fully differentiable density functional theory. (arXiv:2102.04229v4 [physics.chem-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05072",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Courts_J/0/1/0/all/0/1\">Jarrad Courts</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wills_A/0/1/0/all/0/1\">Adrian Wills</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1\">Thomas Sch&#xf6;n</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ninness_B/0/1/0/all/0/1\">Brett Ninness</a>",
          "description": "This paper considers parameter estimation for nonlinear state-space models,\nwhich is an important but challenging problem. We address this challenge by\nemploying a variational inference (VI) approach, which is a principled method\nthat has deep connections to maximum likelihood estimation. This VI approach\nultimately provides estimates of the model as solutions to an optimisation\nproblem, which is deterministic, tractable and can be solved using standard\noptimisation tools. A specialisation of this approach for systems with additive\nGaussian noise is also detailed. The proposed method is examined numerically on\na range of simulated and real examples focusing on the robustness to parameter\ninitialisation; additionally, favourable comparisons are performed against\nstate-of-the-art alternatives.",
          "link": "http://arxiv.org/abs/2012.05072",
          "publishedOn": "2021-06-17T01:58:44.902Z",
          "wordCount": 564,
          "title": "Variational System Identification for Nonlinear State-Space Models. (arXiv:2012.05072v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08541",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Xiu_Z/0/1/0/all/0/1\">Zidi Xiu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_M/0/1/0/all/0/1\">Michael Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Davis_C/0/1/0/all/0/1\">Connor Davis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goldstein_B/0/1/0/all/0/1\">Benjamin A. Goldstein</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Henao_R/0/1/0/all/0/1\">Ricardo Henao</a>",
          "description": "Combining the increasing availability and abundance of healthcare data and\nthe current advances in machine learning methods have created renewed\nopportunities to improve clinical decision support systems. However, in\nhealthcare risk prediction applications, the proportion of cases with the\ncondition (label) of interest is often very low relative to the available\nsample size. Though very prevalent in healthcare, such imbalanced\nclassification settings are also common and challenging in many other\nscenarios. So motivated, we propose a variational disentanglement approach to\nsemi-parametrically learn from rare events in heavily imbalanced classification\nproblems. Specifically, we leverage the imposed extreme-distribution behavior\non a latent space to extract information from low-prevalence events, and\ndevelop a robust prediction arm that joins the merits of the generalized\nadditive model and isotonic neural nets. Results on synthetic studies and\ndiverse real-world datasets, including mortality prediction on a COVID-19\ncohort, demonstrate that the proposed approach outperforms existing\nalternatives.",
          "link": "http://arxiv.org/abs/2009.08541",
          "publishedOn": "2021-06-17T01:58:44.852Z",
          "wordCount": 670,
          "title": "Variational Disentanglement for Rare Event Modeling. (arXiv:2009.08541v5 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01681",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_X/0/1/0/all/0/1\">Xinwei Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tang_H/0/1/0/all/0/1\">Haoyue Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Conventional supervised learning methods, especially deep ones, are found to\nbe sensitive to out-of-distribution (OOD) examples, largely because the learned\nrepresentation mixes the semantic factor with the variation factor due to their\ndomain-specific correlation, while only the semantic factor causes the output.\nTo address the problem, we propose a Causal Semantic Generative model (CSG)\nbased on a causal reasoning so that the two factors are modeled separately, and\ndevelop methods for OOD prediction from a single training domain, which is\ncommon and challenging. The methods are based on the causal invariance\nprinciple, with a novel design for both efficient learning and easy prediction.\nTheoretically, we prove that under certain conditions, CSG can identify the\nsemantic factor by fitting training data, and this semantic-identification\nguarantees the boundedness of OOD generalization error and the success of\nadaptation. Empirical study shows improved OOD performance over prevailing\nbaselines.",
          "link": "http://arxiv.org/abs/2011.01681",
          "publishedOn": "2021-06-17T01:58:44.840Z",
          "wordCount": 658,
          "title": "Learning Causal Semantic Representation for Out-of-Distribution Prediction. (arXiv:2011.01681v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.06148",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ahmadi_A/0/1/0/all/0/1\">Amir Ali Ahmadi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1\">Jeffrey Zhang</a>",
          "description": "We consider the notions of (i) critical points, (ii) second-order points,\n(iii) local minima, and (iv) strict local minima for multivariate polynomials.\nFor each type of point, and as a function of the degree of the polynomial, we\nstudy the complexity of deciding (1) if a given point is of that type, and (2)\nif a polynomial has a point of that type. Our results characterize the\ncomplexity of these two questions for all degrees left open by prior\nliterature. Our main contributions reveal that many of these questions turn out\nto be tractable for cubic polynomials. In particular, we present an\nefficiently-checkable necessary and sufficient condition for local minimality\nof a point for a cubic polynomial. We also show that a local minimum of a cubic\npolynomial can be efficiently found by solving semidefinite programs of size\nlinear in the number of variables. By contrast, we show that it is strongly\nNP-hard to decide if a cubic polynomial has a critical point. We also prove\nthat the set of second-order points of any cubic polynomial is a spectrahedron,\nand conversely that any spectrahedron is the projection of the set of\nsecond-order points of a cubic polynomial. In our final section, we briefly\npresent a potential application of finding local minima of cubic polynomials to\nthe design of a third-order Newton method.",
          "link": "http://arxiv.org/abs/2008.06148",
          "publishedOn": "2021-06-17T01:58:44.823Z",
          "wordCount": 683,
          "title": "Complexity aspects of local minima and related notions. (arXiv:2008.06148v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.09764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xianghong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoli Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "Variational autoencoders have been widely applied for natural language\ngeneration, however, there are two long-standing problems: information\nunder-representation and posterior collapse. The former arises from the fact\nthat only the last hidden state from the encoder is transformed to the latent\nspace, which is insufficient to summarize data. The latter comes as a result of\nthe imbalanced scale between the reconstruction loss and the KL divergence in\nthe objective function. To tackle these issues, in this paper we propose the\ndiscrete variational attention model with categorical distribution over the\nattention mechanism owing to the discrete nature in languages. Our approach is\ncombined with an auto-regressive prior to capture the sequential dependency\nfrom observations, which can enhance the latent space for language generation.\nMoreover, thanks to the property of discreteness, the training of our proposed\napproach does not suffer from posterior collapse. Furthermore, we carefully\nanalyze the superiority of discrete latent space over the continuous space with\nthe common Gaussian distribution. Extensive experiments on language generation\ndemonstrate superior advantages of our proposed approach in comparison with the\nstate-of-the-art counterparts.",
          "link": "http://arxiv.org/abs/2004.09764",
          "publishedOn": "2021-06-17T01:58:44.816Z",
          "wordCount": 665,
          "title": "Discrete Variational Attention Models for Language Generation. (arXiv:2004.09764v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kiyama_M/0/1/0/all/0/1\">Masato Kiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amagasaki_M/0/1/0/all/0/1\">Motoki Amagasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iida_M/0/1/0/all/0/1\">Masahiro Iida</a>",
          "description": "Quantization is used to speed up execution time and save power when runnning\nDeep neural networks (DNNs) on edge devices like AI chips. To investigate the\neffect of quantization, we need performing inference after quantizing the\nweights of DNN with 32-bit floating-point precision by a some bit width, and\nthen quantizing them back to 32-bit floating-point precision. This is because\nthe DNN library can only handle floating-point numbers. However, the accuracy\nof the emulation does not provide accurate precision. We need accurate\nprecision to detect overflow in MAC operations or to verify the operation on\nedge de vices. We have developed PyParch, a DNN library that executes quantized\nDNNs (QNNs) with exactly the same be havior as hardware. In this paper, we\ndescribe a new proposal and implementation of PyParch. As a result of the\nevaluation, the accuracy of QNNs with arbitrary bit widths can be estimated for\nla rge and complex DNNs such as YOLOv5, and the overflow can be detected. We\nevaluated the overhead of the emulation time and found that it was 5.6 times\nslower for QNN and 42\n\ntimes slower for QNN with overflow detection compared to the normal DNN\nexecution time.",
          "link": "http://arxiv.org/abs/2106.08892",
          "publishedOn": "2021-06-17T01:58:44.802Z",
          "wordCount": 625,
          "title": "Development of Quantized DNN Library for Exact Hardware Emulation. (arXiv:2106.08892v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03548",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kevin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>",
          "description": "The objective of lifelong reinforcement learning (RL) is to optimize agents\nwhich can continuously adapt and interact in changing environments. However,\ncurrent RL approaches fail drastically when environments are non-stationary and\ninteractions are non-episodic. We propose Lifelong Skill Planning (LiSP), an\nalgorithmic framework for non-episodic lifelong RL based on planning in an\nabstract space of higher-order skills. We learn the skills in an unsupervised\nmanner using intrinsic rewards and plan over the learned skills using a learned\ndynamics model. Moreover, our framework permits skill discovery even from\noffline data, thereby reducing the need for excessive real-world interactions.\nWe demonstrate empirically that LiSP successfully enables long-horizon planning\nand learns agents that can avoid catastrophic failures even in challenging\nnon-stationary and non-episodic environments derived from gridworld and MuJoCo\nbenchmarks.",
          "link": "http://arxiv.org/abs/2012.03548",
          "publishedOn": "2021-06-17T01:58:44.778Z",
          "wordCount": 606,
          "title": "Reset-Free Lifelong Learning with Skill-Space Planning. (arXiv:2012.03548v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08903",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Klicpera_J/0/1/0/all/0/1\">Johannes Klicpera</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Becker_F/0/1/0/all/0/1\">Florian Becker</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Effectively predicting molecular interactions has the potential to accelerate\nmolecular dynamics by multiple orders of magnitude and thus revolutionize\nchemical simulations. Graph neural networks (GNNs) have recently shown great\nsuccesses for this task, overtaking classical methods based on fixed molecular\nkernels. However, they still appear very limited from a theoretical\nperspective, since regular GNNs cannot distinguish certain types of graphs. In\nthis work we close this gap between theory and practice. We show that GNNs with\ndirected edge embeddings and two-hop message passing are indeed universal\napproximators for predictions that are invariant to global rotation and\ntranslation, and equivariant to permutation. We then leverage these insights\nand multiple structural improvements to propose the geometric message passing\nneural network (GemNet). We demonstrate the benefits of the proposed changes in\nmultiple ablation studies. GemNet outperforms previous models on the COLL and\nMD17 molecular dynamics datasets by 36%, performing especially well on the most\nchallenging molecules.",
          "link": "http://arxiv.org/abs/2106.08903",
          "publishedOn": "2021-06-17T01:58:44.766Z",
          "wordCount": 587,
          "title": "GemNet: Universal Directional Graph Neural Networks for Molecules. (arXiv:2106.08903v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chengchao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Youtan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xubin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>",
          "description": "Generative Adversarial Networks (GANs) have demonstrated unprecedented\nsuccess in various image generation tasks. The encouraging results, however,\ncome at the price of a cumbersome training process, during which the generator\nand discriminator are alternately updated in two stages. In this paper, we\ninvestigate a general training scheme that enables training GANs efficiently in\nonly one stage. Based on the adversarial losses of the generator and\ndiscriminator, we categorize GANs into two classes, Symmetric GANs and\nAsymmetric GANs, and introduce a novel gradient decomposition method to unify\nthe two, allowing us to train both classes in one stage and hence alleviate the\ntraining effort. We also computationally analyze the efficiency of the proposed\nmethod, and empirically demonstrate that, the proposed method yields a solid\n$1.5\\times$ acceleration across various datasets and network architectures.\nFurthermore, we show that the proposed method is readily applicable to other\nadversarial-training scenarios, such as data-free knowledge distillation. The\ncode is available at https://github.com/zju-vipa/OSGAN.",
          "link": "http://arxiv.org/abs/2103.00430",
          "publishedOn": "2021-06-17T01:58:44.759Z",
          "wordCount": 645,
          "title": "Training Generative Adversarial Networks in One Stage. (arXiv:2103.00430v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1806.04823",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nekipelov_D/0/1/0/all/0/1\">Denis Nekipelov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Semenova_V/0/1/0/all/0/1\">Vira Semenova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Syrgkanis_V/0/1/0/all/0/1\">Vasilis Syrgkanis</a>",
          "description": "This paper proposes a Lasso-type estimator for a high-dimensional sparse\nparameter identified by a single index conditional moment restriction (CMR). In\naddition to this parameter, the moment function can also depend on a nuisance\nfunction, such as the propensity score or the conditional choice probability,\nwhich we estimate by modern machine learning tools. We first adjust the moment\nfunction so that the gradient of the future loss function is insensitive\n(formally, Neyman-orthogonal) with respect to the first-stage regularization\nbias, preserving the single index property. We then take the loss function to\nbe an indefinite integral of the adjusted moment function with respect to the\nsingle index. The proposed Lasso estimator converges at the oracle rate, where\nthe oracle knows the nuisance function and solves only the parametric problem.\nWe demonstrate our method by estimating the short-term heterogeneous impact of\nConnecticut's Jobs First welfare reform experiment on women's welfare\nparticipation decision.",
          "link": "http://arxiv.org/abs/1806.04823",
          "publishedOn": "2021-06-17T01:58:44.719Z",
          "wordCount": 649,
          "title": "Regularized Orthogonal Machine Learning for Nonlinear Semiparametric Models. (arXiv:1806.04823v7 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fellows_M/0/1/0/all/0/1\">Matthew Fellows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartikainen_K/0/1/0/all/0/1\">Kristian Hartikainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1\">Shimon Whiteson</a>",
          "description": "We introduce a novel perspective on Bayesian reinforcement learning (RL);\nwhereas existing approaches infer a posterior over the transition distribution\nor Q-function, we characterise the uncertainty in the Bellman operator. Our\nBayesian Bellman operator (BBO) framework is motivated by the insight that when\nbootstrapping is introduced, model-free approaches actually infer a posterior\nover Bellman operators, not value functions. In this paper, we use BBO to\nprovide a rigorous theoretical analysis of model-free Bayesian RL to better\nunderstand its relationshipto established frequentist RL methodologies. We\nprove that Bayesian solutions are consistent with frequentist RL solutions,\neven when approximate inference isused, and derive conditions for which\nconvergence properties hold. Empirically, we demonstrate that algorithms\nderived from the BBO framework have sophisticated deep exploration properties\nthat enable them to solve continuous control tasks at which state-of-the-art\nregularised actor-critic algorithms fail catastrophically",
          "link": "http://arxiv.org/abs/2106.05012",
          "publishedOn": "2021-06-17T01:58:44.690Z",
          "wordCount": 576,
          "title": "Bayesian Bellman Operators. (arXiv:2106.05012v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.05350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1\">Sai Praneeth Karimireddy</a>",
          "description": "We analyze (stochastic) gradient descent (SGD) with delayed updates on smooth\nquasi-convex and non-convex functions and derive concise, non-asymptotic,\nconvergence rates. We show that the rate of convergence in all cases consists\nof two terms: (i) a stochastic term which is not affected by the delay, and\n(ii) a higher order deterministic term which is only linearly slowed down by\nthe delay. Thus, in the presence of noise, the effects of the delay become\nnegligible after a few iterations and the algorithm converges at the same\noptimal rate as standard SGD. This result extends a line of research that\nshowed similar results in the asymptotic regime or for strongly-convex\nquadratic functions only. We further show similar results for SGD with more\nintricate form of delayed gradients---compressed gradients under error\ncompensation and for local~SGD where multiple workers perform local steps\nbefore communicating with each other. In all of these settings, we improve upon\nthe best known rates. These results show that SGD is robust to compressed\nand/or delayed stochastic gradient updates. This is in particular important for\ndistributed parallel implementations, where asynchronous and communication\nefficient methods are the key to achieve linear speedups for optimization with\nmultiple devices.",
          "link": "http://arxiv.org/abs/1909.05350",
          "publishedOn": "2021-06-17T01:58:44.681Z",
          "wordCount": 704,
          "title": "The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication. (arXiv:1909.05350v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Menghani_G/0/1/0/all/0/1\">Gaurav Menghani</a>",
          "description": "Deep Learning has revolutionized the fields of computer vision, natural\nlanguage understanding, speech recognition, information retrieval and more.\nHowever, with the progressive improvements in deep learning models, their\nnumber of parameters, latency, resources required to train, etc. have all have\nincreased significantly. Consequently, it has become important to pay attention\nto these footprint metrics of a model as well, not just its quality. We present\nand motivate the problem of efficiency in deep learning, followed by a thorough\nsurvey of the five core areas of model efficiency (spanning modeling\ntechniques, infrastructure, and hardware) and the seminal work there. We also\npresent an experiment-based guide along with code, for practitioners to\noptimize their model training and deployment. We believe this is the first\ncomprehensive survey in the efficient deep learning space that covers the\nlandscape of model efficiency from modeling techniques to hardware support. Our\nhope is that this survey would provide the reader with the mental model and the\nnecessary understanding of the field to apply generic efficiency techniques to\nimmediately get significant improvements, and also equip them with ideas for\nfurther research and experimentation to achieve additional gains.",
          "link": "http://arxiv.org/abs/2106.08962",
          "publishedOn": "2021-06-17T01:58:44.674Z",
          "wordCount": 619,
          "title": "Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better. (arXiv:2106.08962v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1\">Apostolos Modas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xompero_A/0/1/0/all/0/1\">Alessio Xompero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Matilla_R/0/1/0/all/0/1\">Ricardo Sanchez-Matilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>",
          "description": "We investigate the problem of classifying - from a single image - the level\nof content in a cup or a drinking glass. This problem is made challenging by\nseveral ambiguities caused by transparencies, shape variations and partial\nocclusions, and by the availability of only small training datasets. In this\npaper, we tackle this problem with an appropriate strategy for transfer\nlearning. Specifically, we use adversarial training in a generic source dataset\nand then refine the training with a task-specific dataset. We also discuss and\nexperimentally evaluate several training strategies and their combination on a\nrange of container types of the CORSMAL Containers Manipulation dataset. We\nshow that transfer learning with adversarial training in the source domain\nconsistently improves the classification accuracy on the test set and limits\nthe overfitting of the classifier to specific features of the training data.",
          "link": "http://arxiv.org/abs/2102.04057",
          "publishedOn": "2021-06-17T01:58:44.599Z",
          "wordCount": 616,
          "title": "Improving filling level classification with adversarial training. (arXiv:2102.04057v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.04284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>",
          "description": "For linear classifiers, the relationship between (normalized) output margin\nand generalization is captured in a clear and simple bound -- a large output\nmargin implies good generalization. Unfortunately, for deep models, this\nrelationship is less clear: existing analyses of the output margin give\ncomplicated bounds which sometimes depend exponentially on depth. In this work,\nwe propose to instead analyze a new notion of margin, which we call the\n\"all-layer margin.\" Our analysis reveals that the all-layer margin has a clear\nand direct relationship with generalization for deep models. This enables the\nfollowing concrete applications of the all-layer margin: 1) by analyzing the\nall-layer margin, we obtain tighter generalization bounds for neural nets which\ndepend on Jacobian and hidden layer norms and remove the exponential dependency\non depth 2) our neural net results easily translate to the adversarially robust\nsetting, giving the first direct analysis of robust test error for deep\nnetworks, and 3) we present a theoretically inspired training algorithm for\nincreasing the all-layer margin. Our algorithm improves both clean and\nadversarially robust test performance over strong baselines in practice.",
          "link": "http://arxiv.org/abs/1910.04284",
          "publishedOn": "2021-06-17T01:58:44.517Z",
          "wordCount": 692,
          "title": "Improved Sample Complexities for Deep Networks and Robust Classification via an All-Layer Margin. (arXiv:1910.04284v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04709",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lanfredi_R/0/1/0/all/0/1\">Ricardo Bigolin Lanfredi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schroeder_J/0/1/0/all/0/1\">Joyce D. Schroeder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tasdizen_T/0/1/0/all/0/1\">Tolga Tasdizen</a>",
          "description": "Adversarial training, especially projected gradient descent (PGD), has been a\nsuccessful approach for improving robustness against adversarial attacks. After\nadversarial training, gradients of models with respect to their inputs have a\npreferential direction. However, the direction of alignment is not\nmathematically well established, making it difficult to evaluate\nquantitatively. We propose a novel definition of this direction as the\ndirection of the vector pointing toward the closest point of the support of the\nclosest inaccurate class in decision space. To evaluate the alignment with this\ndirection after adversarial training, we apply a metric that uses generative\nadversarial networks to produce the smallest residual needed to change the\nclass present in the image. We show that PGD-trained models have a higher\nalignment than the baseline according to our definition, that our metric\npresents higher alignment values than a competing metric formulation, and that\nenforcing this alignment increases the robustness of models.",
          "link": "http://arxiv.org/abs/2009.04709",
          "publishedOn": "2021-06-17T01:58:44.507Z",
          "wordCount": 693,
          "title": "Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent. (arXiv:2009.04709v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1\">Timoleon Moraitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1\">Abu Sebastian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eleftheriou_E/0/1/0/all/0/1\">Evangelos Eleftheriou</a> (IBM Research - Zurich)",
          "description": "Biological neurons and their in-silico emulations for neuromorphic artificial\nintelligence (AI) use extraordinarily energy-efficient mechanisms, such as\nspike-based communication and local synaptic plasticity. It remains unclear\nwhether these neuronal mechanisms only offer efficiency or also underlie the\nsuperiority of biological intelligence. Here, we prove rigorously that, indeed,\nthe Bayes-optimal prediction and inference of randomly but continuously\ntransforming environments, a common natural setting, relies on short-term\nspike-timing-dependent plasticity, a hallmark of biological synapses. Further,\nthis dynamic Bayesian inference through plasticity enables circuits of the\ncerebral cortex in simulations to recognize previously unseen, highly distorted\ndynamic stimuli. Strikingly, this also introduces a biologically-modelled AI,\nthe first to overcome multiple limitations of deep learning and outperform\nartificial neural networks in a visual task. The cortical-like network is\nspiking and event-based, trained only with unsupervised and local plasticity,\non a small, narrow, and static training dataset, but achieves recognition of\nunseen, transformed, and dynamic data better than deep neural networks with\ncontinuous activations, trained with supervised backpropagation on the\ntransforming data. These results link short-term plasticity to high-level\ncortical function, suggest optimality of natural intelligence for natural\nenvironments, and repurpose neuromorphic AI from mere efficiency to\ncomputational supremacy altogether.",
          "link": "http://arxiv.org/abs/2009.06808",
          "publishedOn": "2021-06-17T01:58:44.499Z",
          "wordCount": 687,
          "title": "Optimality of short-term synaptic plasticity in modelling certain dynamic environments. (arXiv:2009.06808v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1\">Virginia Smith</a>",
          "description": "Fairness and robustness are two important concerns for federated learning\nsystems. In this work, we identify that robustness to data and model poisoning\nattacks and fairness, measured as the uniformity of performance across devices,\nare competing constraints in statistically heterogeneous networks. To address\nthese constraints, we propose employing a simple, general framework for\npersonalized federated learning, Ditto, that can inherently provide fairness\nand robustness benefits, and develop a scalable solver for it. Theoretically,\nwe analyze the ability of Ditto to achieve fairness and robustness\nsimultaneously on a class of linear problems. Empirically, across a suite of\nfederated datasets, we show that Ditto not only achieves competitive\nperformance relative to recent personalization methods, but also enables more\naccurate, robust, and fair models relative to state-of-the-art fair or robust\nbaselines.",
          "link": "http://arxiv.org/abs/2012.04221",
          "publishedOn": "2021-06-17T01:58:44.492Z",
          "wordCount": 598,
          "title": "Ditto: Fair and Robust Federated Learning Through Personalization. (arXiv:2012.04221v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yajing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoxian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1\">Bruno Lepri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1\">Marco De Nadai</a>",
          "description": "Image-to-Image (I2I) multi-domain translation models are usually evaluated\nalso using the quality of their semantic interpolation results. However,\nstate-of-the-art models frequently show abrupt changes in the image appearance\nduring interpolation, and usually perform poorly in interpolations across\ndomains. In this paper, we propose a new training protocol based on three\nspecific losses which help a translation network to learn a smooth and\ndisentangled latent style space in which: 1) Both intra- and inter-domain\ninterpolations correspond to gradual changes in the generated images and 2) The\ncontent of the source image is better preserved during the translation.\nMoreover, we propose a novel evaluation metric to properly measure the\nsmoothness of latent style space of I2I translation models. The proposed method\ncan be plugged into existing translation approaches, and our extensive\nexperiments on different datasets show that it can significantly boost the\nquality of the generated images and the graduality of the interpolations.",
          "link": "http://arxiv.org/abs/2106.09016",
          "publishedOn": "2021-06-17T01:58:44.486Z",
          "wordCount": 606,
          "title": "Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation. (arXiv:2106.09016v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08929",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Glaser_P/0/1/0/all/0/1\">Pierre Glaser</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Arbel_M/0/1/0/all/0/1\">Michael Arbel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1\">Arthur Gretton</a>",
          "description": "We study the gradient flow for a relaxed approximation to the\nKullback-Leibler (KL) divergence between a moving source and a fixed target\ndistribution. This approximation, termed the KALE (KL approximate lower-bound\nestimator), solves a regularized version of the Fenchel dual problem defining\nthe KL over a restricted class of functions. When using a Reproducing Kernel\nHilbert Space (RKHS) to define the function class, we show that the KALE\ncontinuously interpolates between the KL and the Maximum Mean Discrepancy\n(MMD). Like the MMD and other Integral Probability Metrics, the KALE remains\nwell defined for mutually singular distributions. Nonetheless, the KALE\ninherits from the limiting KL a greater sensitivity to mismatch in the support\nof the distributions, compared with the MMD. These two properties make the KALE\ngradient flow particularly well suited when the target distribution is\nsupported on a low-dimensional manifold. Under an assumption of sufficient\nsmoothness of the trajectories, we show the global convergence of the KALE\nflow. We propose a particle implementation of the flow given initial samples\nfrom the source and the target distribution, which we use to empirically\nconfirm the KALE's properties.",
          "link": "http://arxiv.org/abs/2106.08929",
          "publishedOn": "2021-06-17T01:58:44.471Z",
          "wordCount": 618,
          "title": "KALE Flow: A Relaxed KL Gradient Flow for Probabilities with Disjoint Support. (arXiv:2106.08929v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2004.07119",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Guo_X/0/1/0/all/0/1\">Xiaojie Guo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tadepalli_S/0/1/0/all/0/1\">Sivani Tadepalli</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shehu_A/0/1/0/all/0/1\">Amarda Shehu</a>",
          "description": "Much scientific enquiry across disciplines is founded upon a mechanistic\ntreatment of dynamic systems that ties form to function. A highly visible\ninstance of this is in molecular biology, where an important goal is to\ndetermine functionally-relevant forms/structures that a protein molecule\nemploys to interact with molecular partners in the living cell. This goal is\ntypically pursued under the umbrella of stochastic optimization with algorithms\nthat optimize a scoring function. Research repeatedly shows that current\nscoring function, though steadily improving, correlate weakly with molecular\nactivity. Inspired by recent momentum in generative deep learning, this paper\nproposes and evaluates an alternative approach to generating\nfunctionally-relevant three-dimensional structures of a protein. Though\ntypically deep generative models struggle with highly-structured data, the work\npresented here circumvents this challenge via graph-generative models. A\ncomprehensive evaluation of several deep architectures shows the promise of\ngenerative models in directly revealing the latent space for sampling novel\ntertiary structures, as well as in highlighting axes/factors that carry\nstructural meaning and open the black box often associated with deep models.\nThe work presented here is a first step towards interpretative, deep generative\nmodels becoming viable and informative complementary approaches to protein\nstructure prediction.",
          "link": "http://arxiv.org/abs/2004.07119",
          "publishedOn": "2021-06-17T01:58:44.465Z",
          "wordCount": 648,
          "title": "Generating Tertiary Protein Structures via an Interpretative Variational Autoencoder. (arXiv:2004.07119v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rappeport_H/0/1/0/all/0/1\">Hagai Rappeport</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisman_I/0/1/0/all/0/1\">Irit Levin Reisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tishby_N/0/1/0/all/0/1\">Naftali Tishby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaban_N/0/1/0/all/0/1\">Nathalie Q. Balaban</a>",
          "description": "Many complex phenomena, from weather systems to heartbeat rhythm patterns,\nare effectively modeled as low-dimensional dynamical systems. Such systems may\nbehave chaotically under certain conditions, and so the ability to detect chaos\nbased on empirical measurement is an important step in characterizing and\npredicting these processes. Classifying a system as chaotic usually requires\nestimating its largest Lyapunov exponent, which quantifies the average rate of\nconvergence or divergence of initially close trajectories in state space, and\nfor which a positive value is generally accepted as an operational definition\nof chaos. Estimating the largest Lyapunov exponent from observations of a\nprocess is especially challenging in systems affected by dynamical noise, which\nis the case for many models of real-world processes, in particular models of\nbiological systems. We describe a novel method for estimating the largest\nLyapunov exponent from data, based on training Deep Learning models on\nsynthetically generated trajectories, and demonstrate that this method yields\naccurate and noise-robust predictions given relatively short inputs and across\na range of different dynamical systems. Our method is unique in that it can\nanalyze tree-shaped data, a ubiquitous topology in biological settings, and\nspecifically in dynamics over lineages of cells or organisms. We also\ncharacterize the types of input information extracted by our models for their\npredictions, allowing for a deeper understanding into the different ways by\nwhich chaos can be analyzed in different topologies.",
          "link": "http://arxiv.org/abs/2106.08956",
          "publishedOn": "2021-06-17T01:58:44.458Z",
          "wordCount": 660,
          "title": "Detecting chaos in lineage-trees: A deep learning approach. (arXiv:2106.08956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.01670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1\">Kale-ab Tessera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1\">Benjamin Rosman</a>",
          "description": "Training sparse networks to converge to the same performance as dense neural\narchitectures has proven to be elusive. Recent work suggests that\ninitialization is the key. However, while this direction of research has had\nsome success, focusing on initialization alone appears to be inadequate. In\nthis paper, we take a broader view of training sparse networks and consider the\nrole of regularization, optimization, and architecture choices on sparse\nmodels. We propose a simple experimental framework, Same Capacity Sparse vs\nDense Comparison (SC-SDC), that allows for a fair comparison of sparse and\ndense networks. Furthermore, we propose a new measure of gradient flow,\nEffective Gradient Flow (EGF), that better correlates to performance in sparse\nnetworks. Using top-line metrics, SC-SDC and EGF, we show that default choices\nof optimizers, activation functions and regularizers used for dense networks\ncan disadvantage sparse networks. Based upon these findings, we show that\ngradient flow in sparse networks can be improved by reconsidering aspects of\nthe architecture design and the training regime. Our work suggests that\ninitialization is only one piece of the puzzle and taking a wider view of\ntailoring optimization to sparse networks yields promising results.",
          "link": "http://arxiv.org/abs/2102.01670",
          "publishedOn": "2021-06-17T01:58:44.439Z",
          "wordCount": 655,
          "title": "Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization. (arXiv:2102.01670v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_M/0/1/0/all/0/1\">Michael Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Charline Le Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1\">Sheheryar Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupont_E/0/1/0/all/0/1\">Emilien Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjik Kim</a>",
          "description": "Group equivariant neural networks are used as building blocks of group\ninvariant neural networks, which have been shown to improve generalisation\nperformance and data efficiency through principled parameter sharing. Such\nworks have mostly focused on group equivariant convolutions, building on the\nresult that group equivariant linear maps are necessarily convolutions. In this\nwork, we extend the scope of the literature to self-attention, that is emerging\nas a prominent building block of deep learning models. We propose the\nLieTransformer, an architecture composed of LieSelfAttention layers that are\nequivariant to arbitrary Lie groups and their discrete subgroups. We\ndemonstrate the generality of our approach by showing experimental results that\nare competitive to baseline methods on a wide range of tasks: shape counting on\npoint clouds, molecular property regression and modelling particle trajectories\nunder Hamiltonian dynamics.",
          "link": "http://arxiv.org/abs/2012.10885",
          "publishedOn": "2021-06-17T01:58:44.433Z",
          "wordCount": 613,
          "title": "LieTransformer: Equivariant self-attention for Lie Groups. (arXiv:2012.10885v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08750",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyu Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhao Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ren_T/0/1/0/all/0/1\">Tongzheng Ren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Recent years have witnessed an upsurge of interest in employing flexible\nmachine learning models for instrumental variable (IV) regression, but the\ndevelopment of uncertainty quantification methodology is still lacking. In this\nwork we present a scalable quasi-Bayesian procedure for IV regression, building\nupon the recently developed kernelized IV models. Contrary to Bayesian modeling\nfor IV, our approach does not require additional assumptions on the data\ngenerating process, and leads to a scalable approximate inference algorithm\nwith time cost comparable to the corresponding point estimation methods. Our\nalgorithm can be further extended to work with neural network models. We\nanalyze the theoretical properties of the proposed quasi-posterior, and\ndemonstrate through empirical evaluation the competitive performance of our\nmethod.",
          "link": "http://arxiv.org/abs/2106.08750",
          "publishedOn": "2021-06-17T01:58:44.353Z",
          "wordCount": 548,
          "title": "Scalable Quasi-Bayesian Inference for Instrumental Variable Regression. (arXiv:2106.08750v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiatai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longbo Huang</a>",
          "description": "We propose Banker-OMD, a novel framework generalizing the classical Online\nMirror Descent (OMD) technique in online learning algorithm design. Banker-OMD\nallows algorithms to robustly handle delayed feedback, and offers a general\nmethodology for achieving $\\tilde{O}(\\sqrt{T} + \\sqrt{D})$-style regret bounds\nin various delayed-feedback online learning tasks, where $T$ is the time\nhorizon length and $D$ is the total feedback delay. We demonstrate the power of\nBanker-OMD with applications to three important bandit scenarios with delayed\nfeedback, including delayed adversarial Multi-armed bandits (MAB), delayed\nadversarial linear bandits, and a novel delayed best-of-both-worlds MAB\nsetting. Banker-OMD achieves nearly-optimal performance in all the three\nsettings. In particular, it leads to the first delayed adversarial linear\nbandit algorithm achieving $\\tilde{O}(\\text{poly}(n)(\\sqrt{T} + \\sqrt{D}))$\nregret.",
          "link": "http://arxiv.org/abs/2106.08943",
          "publishedOn": "2021-06-17T01:58:44.340Z",
          "wordCount": 530,
          "title": "Banker Online Mirror Descent. (arXiv:2106.08943v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1\">Puoya Tabaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>",
          "description": "Many data analysis problems can be cast as distance geometry problems in\n\\emph{space forms} -- Euclidean, spherical, or hyperbolic spaces. Often,\nabsolute distance measurements are often unreliable or simply unavailable and\nonly proxies to absolute distances in the form of similarities are available.\nHence we ask the following: Given only \\emph{comparisons} of similarities\namongst a set of entities, what can be said about the geometry of the\nunderlying space form? To study this question, we introduce the notions of the\n\\textit{ordinal capacity} of a target space form and \\emph{ordinal spread} of\nthe similarity measurements. The latter is an indicator of complex patterns in\nthe measurements, while the former quantifies the capacity of a space form to\naccommodate a set of measurements with a specific ordinal spread profile. We\nprove that the ordinal capacity of a space form is related to its dimension and\nthe sign of its curvature. This leads to a lower bound on the Euclidean and\nspherical embedding dimension of what we term similarity graphs. More\nimportantly, we show that the statistical behavior of the ordinal spread random\nvariables defined on a similarity graph can be used to identify its underlying\nspace form. We support our theoretical claims with experiments on weighted\ntrees, single-cell RNA expression data and spherical cartographic measurements.",
          "link": "http://arxiv.org/abs/2006.09858",
          "publishedOn": "2021-06-17T01:58:44.335Z",
          "wordCount": 675,
          "title": "Geometry of Similarity Comparisons. (arXiv:2006.09858v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>",
          "description": "Weak supervision has shown promising results in many natural language\nprocessing tasks, such as Named Entity Recognition (NER). Existing work mainly\nfocuses on learning deep NER models only with weak supervision, i.e., without\nany human annotation, and shows that by merely using weakly labeled data, one\ncan achieve good performance, though still underperforms fully supervised NER\nwith manually/strongly labeled data. In this paper, we consider a more\npractical scenario, where we have both a small amount of strongly labeled data\nand a large amount of weakly labeled data. Unfortunately, we observe that\nweakly labeled data does not necessarily improve, or even deteriorate the model\nperformance (due to the extensive noise in the weak labels) when we train deep\nNER models over a simple or weighted combination of the strongly labeled and\nweakly labeled data. To address this issue, we propose a new multi-stage\ncomputational framework -- NEEDLE with three essential ingredients: (1) weak\nlabel completion, (2) noise-aware loss function, and (3) final fine-tuning over\nthe strongly labeled data. Through experiments on E-commerce query NER and\nBiomedical NER, we demonstrate that NEEDLE can effectively suppress the noise\nof the weak labels and outperforms existing methods. In particular, we achieve\nnew SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,\nBC5CDR-disease 90.69, NCBI-disease 92.28.",
          "link": "http://arxiv.org/abs/2106.08977",
          "publishedOn": "2021-06-17T01:58:44.321Z",
          "wordCount": 669,
          "title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>",
          "description": "Real-world applications of machine learning tools in high-stakes domains are\noften regulated to be fair, in the sense that the predicted target should\nsatisfy some quantitative notion of parity with respect to a protected\nattribute. However, the exact tradeoff between fairness and accuracy with a\nreal-valued target is not clear. In this paper, we characterize the inherent\ntradeoff between statistical parity and accuracy in the regression setting by\nproviding a lower bound on the error of any fair regressor. Our lower bound is\nsharp, algorithm-independent, and admits a simple interpretation: when the\nmoments of the target differ between groups, any fair algorithm has to make a\nlarge error on at least one of the groups. We further extend this result to\ngive a lower bound on the joint error of any (approximately) fair algorithm,\nusing the Wasserstein distance to measure the quality of the approximation. On\nthe upside, we establish the first connection between individual fairness,\naccuracy parity, and the Wasserstein distance by showing that if a regressor is\nindividually fair, it also approximately verifies the accuracy parity, where\nthe gap is given by the Wasserstein distance between the two groups. Inspired\nby our theoretical results, we develop a practical algorithm for fair\nregression through the lens of representation learning, and conduct experiments\non a real-world dataset to corroborate our findings.",
          "link": "http://arxiv.org/abs/2106.08812",
          "publishedOn": "2021-06-17T01:58:44.313Z",
          "wordCount": 646,
          "title": "Costs and Benefits of Wasserstein Fair Regression. (arXiv:2106.08812v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1\">Zahra Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1\">Nandan Kumar Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1\">Brandon Reagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddharth Garg</a>",
          "description": "The simultaneous rise of machine learning as a service and concerns over user\nprivacy have increasingly motivated the need for private inference (PI). While\nrecent work demonstrates PI is possible using cryptographic primitives, the\ncomputational overheads render it impractical. The community is largely\nunprepared to address these overheads, as the source of slowdown in PI stems\nfrom the ReLU operator whereas optimizations for plaintext inference focus on\noptimizing FLOPs. In this paper we re-think the ReLU computation and propose\noptimizations for PI tailored to properties of neural networks. Specifically,\nwe reformulate ReLU as an approximate sign test and introduce a novel\ntruncation method for the sign test that significantly reduces the cost per\nReLU. These optimizations result in a specific type of stochastic ReLU. The key\nobservation is that the stochastic fault behavior is well suited for the\nfault-tolerant properties of neural network inference. Thus, we provide\nsignificant savings without impacting accuracy. We collectively call the\noptimizations Circa and demonstrate improvements of up to 4.7x storage and 3x\nruntime over baseline implementations; we further show that Circa can be used\non top of recent PI optimizations to obtain 1.8x additional speedup.",
          "link": "http://arxiv.org/abs/2106.08475",
          "publishedOn": "2021-06-17T01:58:44.271Z",
          "wordCount": 619,
          "title": "Circa: Stochastic ReLUs for Private Deep Learning. (arXiv:2106.08475v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaomeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1\">Michael Potter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gaurav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Chan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1\">V. Ratna Saripalli</a>",
          "description": "It is no secret amongst deep learning researchers that finding the right data\naugmentation strategy during training can mean the difference between a\nstate-of-the-art result and a run-of-the-mill ranking. To that end, the\ncommunity has seen many efforts to automate the process of finding the perfect\naugmentation procedure for any task at hand. Unfortunately, even recent\ncutting-edge methods bring massive computational overhead, requiring as many as\n100 full model trainings to settle on an ideal configuration. We show how to\nachieve even better performance in just 7: with Random Unidimensional\nAugmentation. Source code is available at https://github.com/fastestimator/RUA",
          "link": "http://arxiv.org/abs/2106.08756",
          "publishedOn": "2021-06-17T01:58:44.265Z",
          "wordCount": 521,
          "title": "Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08445",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dietrich_M/0/1/0/all/0/1\">Maximilian Dietrich</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Seidlitz_S/0/1/0/all/0/1\">Silvia Seidlitz</a> (2, 3), <a href=\"http://arxiv.org/find/cs/1/au:+Schreck_N/0/1/0/all/0/1\">Nicholas Schreck</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1\">Manuel Wiesenfarth</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Godau_P/0/1/0/all/0/1\">Patrick Godau</a> (2, 3), <a href=\"http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu Tizabi</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Sellner_J/0/1/0/all/0/1\">Jan Sellner</a> (2, 3), <a href=\"http://arxiv.org/find/cs/1/au:+Marx_S/0/1/0/all/0/1\">Sebastian Marx</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Knodler_S/0/1/0/all/0/1\">Samuel Kn&#xf6;dler</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Allers_M/0/1/0/all/0/1\">Michael M. Allers</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Ayala_L/0/1/0/all/0/1\">Leonardo Ayala</a> (2, 7), <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_K/0/1/0/all/0/1\">Karsten Schmidt</a> (8), <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_T/0/1/0/all/0/1\">Thorsten Brenner</a> (8), <a href=\"http://arxiv.org/find/cs/1/au:+Studier_Fischer_A/0/1/0/all/0/1\">Alexander Studier-Fischer</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Nickel_F/0/1/0/all/0/1\">Felix Nickel</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1\">Beat P. M&#xfc;ller-Stich</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_Schneider_A/0/1/0/all/0/1\">Annette Kopp-Schneider</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Weigand_M/0/1/0/all/0/1\">Markus A. Weigand</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a> (2, 6, 7) ((1) Department of Anesthesiology, Heidelberg University Hospital, Heidelberg, Germany, (2) Division of Computer Assisted Medical Interventions, German Cancer Research Center (DKFZ), Heidelberg, Germany, (3) HIDSS4Health - Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany (4) Division of Biostatistics, German Cancer Research Center (DKFZ), Heidelberg, Germany, (5) Department of General, Visceral, and Transplantation Surgery, Heidelberg University Hospital, Heidelberg, Germany, (6) Faculty of Mathematics and Computer Science, Heidelberg University, Heidelberg, Germany, (7) Medical Faculty, Heidelberg University, Heidelberg, Germany, (8) Department of Anesthesiology and Intensive Care Medicine, University Hospital Essen, University Duisburg-Essen, Essen, Germany)",
          "description": "Sepsis is a leading cause of mortality and critical illness worldwide. While\nrobust biomarkers for early diagnosis are still missing, recent work indicates\nthat hyperspectral imaging (HSI) has the potential to overcome this bottleneck\nby monitoring microcirculatory alterations. Automated machine learning-based\ndiagnosis of sepsis based on HSI data, however, has not been explored to date.\nGiven this gap in the literature, we leveraged an existing data set to (1)\ninvestigate whether HSI-based automated diagnosis of sepsis is possible and (2)\nput forth a list of possible confounders relevant for HSI-based tissue\nclassification. While we were able to classify sepsis with an accuracy of over\n$98\\,\\%$ using the existing data, our research also revealed several subject-,\ntherapy- and imaging-related confounders that may lead to an overestimation of\nalgorithm performance when not balanced across the patient groups. We conclude\nthat further prospective studies, carefully designed with respect to these\nconfounders, are necessary to confirm the preliminary results obtained in this\nstudy.",
          "link": "http://arxiv.org/abs/2106.08445",
          "publishedOn": "2021-06-17T01:58:44.256Z",
          "wordCount": 770,
          "title": "Machine learning-based analysis of hyperspectral images for automated sepsis diagnosis. (arXiv:2106.08445v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Theerthagiri_P/0/1/0/all/0/1\">Prasannavenkatesan Theerthagiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+J_V/0/1/0/all/0/1\">Vidya J</a>",
          "description": "Cardiovascular diseases (CVDs) are one of the most common chronic illnesses\nthat affect peoples health. Early detection of CVDs can reduce mortality rates\nby preventing or reducing the severity of the disease. Machine learning\nalgorithms are a promising method for identifying risk factors. This paper\nproposes a proposed recursive feature elimination-based gradient boosting\n(RFE-GB) algorithm in order to obtain accurate heart disease prediction. The\npatients health record with important CVD features has been analyzed for the\nevaluation of the results. Several other machine learning methods were also\nused to build the prediction model, and the results were compared with the\nproposed model. The results of this proposed model infer that the combined\nrecursive feature elimination and gradient boosting algorithm achieves the\nhighest accuracy (89.7 %). Further, with an area under the curve of 0.84, the\nproposed RFE-GB algorithm was found superior and had obtained a substantial\ngain over other techniques. Thus, the proposed RFE-GB algorithm will serve as a\nprominent model for CVD estimation and treatment.",
          "link": "http://arxiv.org/abs/2106.08889",
          "publishedOn": "2021-06-17T01:58:44.248Z",
          "wordCount": 597,
          "title": "Cardiovascular Disease Prediction using Recursive Feature Elimination and Gradient Boosting Classification Techniques. (arXiv:2106.08889v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Igor Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>",
          "description": "Episodic memory lets reinforcement learning algorithms remember and exploit\npromising experience from the past to improve agent performance. Previous works\non memory mechanisms show benefits of using episodic-based data structures for\ndiscrete action problems in terms of sample-efficiency. The application of\nepisodic memory for continuous control with a large action space is not\ntrivial. Our study aims to answer the question: can episodic memory be used to\nimprove agent's performance in continuous control? Our proposed algorithm\ncombines episodic memory with Actor-Critic architecture by modifying critic's\nobjective. We further improve performance by introducing episodic-based replay\nbuffer prioritization. We evaluate our algorithm on OpenAI gym domains and show\ngreater sample-efficiency compared with the state-of-the art model-free\noff-policy algorithms.",
          "link": "http://arxiv.org/abs/2106.08832",
          "publishedOn": "2021-06-17T01:58:44.243Z",
          "wordCount": 545,
          "title": "Solving Continuous Control with Episodic Memory. (arXiv:2106.08832v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Anish Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1\">Abolfazl Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prateek Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1\">Sujay Sanghavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit S. Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "Geometric median (\\textsc{Gm}) is a classical method in statistics for\nachieving a robust estimation of the uncorrupted data; under gross corruption,\nit achieves the optimal breakdown point of 0.5. However, its computational\ncomplexity makes it infeasible for robustifying stochastic gradient descent\n(SGD) for high-dimensional optimization problems. In this paper, we show that\nby applying \\textsc{Gm} to only a judiciously chosen block of coordinates at a\ntime and using a memory mechanism, one can retain the breakdown point of 0.5\nfor smooth non-convex problems, with non-asymptotic convergence rates\ncomparable to the SGD with \\textsc{Gm}.",
          "link": "http://arxiv.org/abs/2106.08882",
          "publishedOn": "2021-06-17T01:58:44.227Z",
          "wordCount": 545,
          "title": "Robust Training in High Dimensions via Block Coordinate Geometric Median Descent. (arXiv:2106.08882v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sapkota_S/0/1/0/all/0/1\">Suman Sapkota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1\">Binod Bhattarai</a>",
          "description": "In this paper, we present a novel method to constrain invexity on Neural\nNetworks (NN). Invex functions ensure every stationary point is global minima.\nHence, gradient descent commenced from any point will lead to the global\nminima. Another advantage of invexity on NN is to divide data space locally\ninto two connected sets with a highly non-linear decision boundary by simply\nthresholding the output. To this end, we formulate a universal invex function\napproximator and employ it to enforce invexity in NN. We call it Input Invex\nNeural Networks (II-NN). We first fit data with a known invex function,\nfollowed by modification with a NN, compare the direction of the gradient and\npenalize the direction of gradient on NN if it contradicts with the direction\nof reference invex function. In order to penalize the direction of the gradient\nwe perform Gradient Clipped Gradient Penalty (GC-GP). We applied our method to\nthe existing NNs for both image classification and regression tasks. From the\nextensive empirical and qualitative experiments, we observe that our method\ngives the performance similar to ordinary NN yet having invexity. Our method\noutperforms linear NN and Input Convex Neural Network (ICNN) with a large\nmargin. We publish our code and implementation details at github.",
          "link": "http://arxiv.org/abs/2106.08748",
          "publishedOn": "2021-06-17T01:58:44.221Z",
          "wordCount": 621,
          "title": "Input Invex Neural Network. (arXiv:2106.08748v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olin_Ammentorp_W/0/1/0/all/0/1\">Wilkie Olin-Ammentorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazhenov_M/0/1/0/all/0/1\">Maxim Bazhenov</a>",
          "description": "Despite rapid progress, current deep learning methods face a number of\ncritical challenges. These include high energy consumption, catastrophic\nforgetting, dependance on global losses, and an inability to reason\nsymbolically. By combining concepts from information bottleneck theory and\nvector-symbolic architectures, we propose and implement a novel information\nprocessing architecture, the 'Bridge network.' We show this architecture\nprovides unique advantages which can address the problem of global losses and\ncatastrophic forgetting. Furthermore, we argue that it provides a further basis\nfor increasing energy efficiency of execution and the ability to reason\nsymbolically.",
          "link": "http://arxiv.org/abs/2106.08446",
          "publishedOn": "2021-06-17T01:58:44.216Z",
          "wordCount": 503,
          "title": "Bridge Networks. (arXiv:2106.08446v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blier_L/0/1/0/all/0/1\">L&#xe9;onard Blier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ollivier_Y/0/1/0/all/0/1\">Yann Ollivier</a>",
          "description": "In multi-goal reinforcement learning (RL) settings, the reward for each goal\nis sparse, and located in a small neighborhood of the goal. In large dimension,\nthe probability of reaching a reward vanishes and the agent receives little\nlearning signal. Methods such as Hindsight Experience Replay (HER) tackle this\nissue by also learning from realized but unplanned-for goals. But HER is known\nto introduce bias, and can converge to low-return policies by overestimating\nchancy outcomes. First, we vindicate HER by proving that it is actually\nunbiased in deterministic environments, such as many optimal control settings.\nNext, for stochastic environments in continuous spaces, we tackle sparse\nrewards by directly taking the infinitely sparse reward limit. We fully\nformalize the problem of multi-goal RL with infinitely sparse Dirac rewards at\neach goal. We introduce unbiased deep Q-learning and actor-critic algorithms\nthat can handle such infinitely sparse rewards, and test them in toy\nenvironments.",
          "link": "http://arxiv.org/abs/2106.08863",
          "publishedOn": "2021-06-17T01:58:44.210Z",
          "wordCount": 568,
          "title": "Unbiased Methods for Multi-Goal Reinforcement Learning. (arXiv:2106.08863v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohtashami_A/0/1/0/all/0/1\">Amirkeivan Mohtashami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U. Stich</a>",
          "description": "For deploying deep learning models to lower end devices, it is necessary to\ntrain less resource-demanding variants of state-of-the-art architectures. This\ndoes not eliminate the need for more expensive models as they have a higher\nperformance. In order to avoid training two separate models, we show that it is\npossible to train neural networks in such a way that a predefined 'core'\nsubnetwork can be split-off from the trained full network with remarkable good\nperformance. We extend on prior methods that focused only on core networks of\nsmaller width, while we focus on supporting arbitrary core network\narchitectures. Our proposed training scheme switches consecutively between\noptimizing only the core part of the network and the full one. The accuracy of\nthe full model remains comparable, while the core network achieves better\nperformance than when it is trained in isolation. In particular, we show that\ntraining a Transformer with a low-rank core gives a low-rank model with\nsuperior performance than when training the low-rank model alone. We analyze\nour training scheme theoretically, and show its convergence under assumptions\nthat are either standard or practically justified. Moreover, we show that the\ndeveloped theoretical framework allows analyzing many other partial training\nschemes for neural networks.",
          "link": "http://arxiv.org/abs/2106.08895",
          "publishedOn": "2021-06-17T01:58:44.205Z",
          "wordCount": 623,
          "title": "Simultaneous Training of Partially Masked Neural Networks. (arXiv:2106.08895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gast_N/0/1/0/all/0/1\">Nicolas Gast</a> (POLARIS), <a href=\"http://arxiv.org/find/cs/1/au:+Gaujal_B/0/1/0/all/0/1\">Bruno Gaujal</a> (POLARIS), <a href=\"http://arxiv.org/find/cs/1/au:+Khun_K/0/1/0/all/0/1\">Kimang Khun</a> (POLARIS)",
          "description": "We study learning algorithms for the classical Markovian bandit problem with\ndiscount. We explain how to adapt PSRL [24] and UCRL2 [2] to exploit the\nproblem structure. These variants are called MB-PSRL and MB-UCRL2. While the\nregret bound and runtime of vanilla implementations of PSRL and UCRL2 are\nexponential in the number of bandits, we show that the episodic regret of\nMB-PSRL and MB-UCRL2 is(S $\\sqrt$ nK) where K is the number of episodes, n is\nthe number of bandits and S is the number of states of each bandit (the exact\nbound in S, n and K is given in the paper). Up to a factor $\\sqrt$ S, this\nmatches the lower bound of $\\Omega$($\\sqrt$ SnK) that we also derive in the\npaper. MB-PSRL is also computationally efficient: its runtime is linear in the\nnumber of bandits. We further show that this linear runtime cannot be achieved\nby adapting classical non-Bayesian algorithms such as UCRL2 or UCBVI to\nMarkovian bandit problems. Finally, we perform numerical experiments that\nconfirm that MB-PSRL outperforms other existing algorithms in practice, both in\nterms of regret and of computation time.",
          "link": "http://arxiv.org/abs/2106.08771",
          "publishedOn": "2021-06-17T01:58:44.190Z",
          "wordCount": 626,
          "title": "Reinforcement Learning for Markovian Bandits: Is Posterior Sampling more Scalable than Optimism?. (arXiv:2106.08771v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08519",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Qian_K/0/1/0/all/0/1\">Kaizhi Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cox_D/0/1/0/all/0/1\">David Cox</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>",
          "description": "Prosody plays an important role in characterizing the style of a speaker or\nan emotion, but most non-parallel voice or emotion style transfer algorithms do\nnot convert any prosody information. Two major components of prosody are pitch\nand rhythm. Disentangling the prosody information, particularly the rhythm\ncomponent, from the speech is challenging because it involves breaking the\nsynchrony between the input speech and the disentangled speech representation.\nAs a result, most existing prosody style transfer algorithms would need to rely\non some form of text transcriptions to identify the content information, which\nconfines their application to high-resource languages only. Recently,\nSpeechSplit has made sizeable progress towards unsupervised prosody style\ntransfer, but it is unable to extract high-level global prosody style in an\nunsupervised manner. In this paper, we propose AutoPST, which can disentangle\nglobal prosody style from speech without relying on any text transcriptions.\nAutoPST is an Autoencoder-based Prosody Style Transfer framework with a\nthorough rhythm removal module guided by the self-expressive representation\nlearning. Experiments on different style transfer tasks show that AutoPST can\neffectively convert prosody that correctly reflects the styles of the target\ndomains.",
          "link": "http://arxiv.org/abs/2106.08519",
          "publishedOn": "2021-06-17T01:58:44.183Z",
          "wordCount": 629,
          "title": "Global Rhythm Style Transfer Without Text Transcriptions. (arXiv:2106.08519v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2007.10817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Foo_L/0/1/0/all/0/1\">Lin Geng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>",
          "description": "We consider the problem of segmenting cell nuclei instances from Hematoxylin\nand Eosin (H&E) stains with dot annotations only. While most recent works focus\non improving the segmentation quality, this is usually insufficient for\ninstance segmentation of cell instances clustered together or with a small\nsize. In this work, we propose a simple two-step post-processing procedure,\nSplit and Expand, that directly improves the conversion of segmentation maps to\ninstances. In the splitting step, we generate fine-grained cell instances from\nthe segmentation map with the guidance of cell-center predictions. For the\nexpansion step, we utilize Layer-wise Relevance Propagation (LRP) explanation\nresults to add small cells that are not captured in the segmentation map.\nAlthough we additionally train an output head to predict cell-centers, the\npost-processing procedure itself is not explicitly trained and is executed at\ninference-time only. A feature re-weighting loss based on LRP is proposed to\nimprove our method even further. We test our procedure on the MoNuSeg and TNBC\ndatasets and show quantitatively and qualitatively that our proposed method\nimproves object-level metrics substantially.",
          "link": "http://arxiv.org/abs/2007.10817",
          "publishedOn": "2021-06-17T01:58:44.177Z",
          "wordCount": 649,
          "title": "Split and Expand: An inference-time improvement for Weakly Supervised Cell Instance Segmentation. (arXiv:2007.10817v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuzhou Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_S/0/1/0/all/0/1\">Senlin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yitian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bo An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "Can we learn a multi-class classifier from only data of a single class? We\nshow that without any assumptions on the loss functions, models, and\noptimizers, we can successfully learn a multi-class classifier from only data\nof a single class with a rigorous consistency guarantee when confidences (i.e.,\nthe class-posterior probabilities for all the classes) are available.\nSpecifically, we propose an empirical risk minimization framework that is\nloss-/model-/optimizer-independent. Instead of constructing a boundary between\nthe given class and other classes, our method can conduct discriminative\nclassification between all the classes even if no data from the other classes\nare provided. We further theoretically and experimentally show that our method\ncan be Bayes-consistent with a simple modification even if the provided\nconfidences are highly noisy. Then, we provide an extension of our method for\nthe case where data from a subset of all the classes are available.\nExperimental results demonstrate the effectiveness of our methods.",
          "link": "http://arxiv.org/abs/2106.08864",
          "publishedOn": "2021-06-17T01:58:44.172Z",
          "wordCount": 592,
          "title": "Multi-Class Classification from Single-Class Data with Confidences. (arXiv:2106.08864v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boes_W/0/1/0/all/0/1\">Wim Boes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rompaey_R/0/1/0/all/0/1\">Robbe Van Rompaey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verwimp_L/0/1/0/all/0/1\">Lyan Verwimp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelemans_J/0/1/0/all/0/1\">Joris Pelemans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wambacq_P/0/1/0/all/0/1\">Patrick Wambacq</a>",
          "description": "We inspect the long-term learning ability of Long Short-Term Memory language\nmodels (LSTM LMs) by evaluating a contextual extension based on the Continuous\nBag-of-Words (CBOW) model for both sentence- and discourse-level LSTM LMs and\nby analyzing its performance. We evaluate on text and speech. Sentence-level\nmodels using the long-term contextual module perform comparably to vanilla\ndiscourse-level LSTM LMs. On the other hand, the extension does not provide\ngains for discourse-level models. These findings indicate that discourse-level\nLSTM LMs already rely on contextual information to perform long-term learning.",
          "link": "http://arxiv.org/abs/2106.08927",
          "publishedOn": "2021-06-17T01:58:44.166Z",
          "wordCount": 540,
          "title": "On the long-term learning ability of LSTM LMs. (arXiv:2106.08927v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pappas_D/0/1/0/all/0/1\">Dimitris Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>",
          "description": "Question answering (QA) systems for large document collections typically use\npipelines that (i) retrieve possibly relevant documents, (ii) re-rank them,\n(iii) rank paragraphs or other snippets of the top-ranked documents, and (iv)\nselect spans of the top-ranked snippets as exact answers. Pipelines are\nconceptually simple, but errors propagate from one component to the next,\nwithout later components being able to revise earlier decisions. We present an\narchitecture for joint document and snippet ranking, the two middle stages,\nwhich leverages the intuition that relevant documents have good snippets and\ngood snippets come from relevant documents. The architecture is general and can\nbe used with any neural text relevance ranker. We experiment with two main\ninstantiations of the architecture, based on POSIT-DRMM (PDRMM) and a\nBERT-based ranker. Experiments on biomedical data from BIOASQ show that our\njoint models vastly outperform the pipelines in snippet retrieval, the main\ngoal for QA, with fewer trainable parameters, also remaining competitive in\ndocument retrieval. Furthermore, our joint PDRMM-based model is competitive\nwith BERT-based models, despite using orders of magnitude fewer parameters.\nThese claims are also supported by human evaluation on two test batches of\nBIOASQ. To test our key findings on another dataset, we modified the Natural\nQuestions dataset so that it can also be used for document and snippet\nretrieval. Our joint PDRMM-based model again outperforms the corresponding\npipeline in snippet retrieval on the modified Natural Questions dataset, even\nthough it performs worse than the pipeline in document retrieval. We make our\ncode and the modified Natural Questions dataset publicly available.",
          "link": "http://arxiv.org/abs/2106.08908",
          "publishedOn": "2021-06-17T01:58:44.150Z",
          "wordCount": 713,
          "title": "A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections. (arXiv:2106.08908v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08870",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Marinho_E/0/1/0/all/0/1\">Eraldo Pereira Marinho</a>",
          "description": "A PCA-based, machine learning version of the SPH method is proposed. In the\npresent scheme, the smoothing tensor is computed to have their eigenvalues\nproportional to the covariance's principal components, using a modified octree\ndata structure, which allows the fast estimation of the anisotropic\nself-regulating kNN. Each SPH particle is the center of such an optimal kNN\ncluster, i.e., the one whose covariance tensor allows the find of the kNN\ncluster itself according to the Mahalanobis metric. Such machine learning\nconstitutes a fixed point problem. The definitive (self-regulating) kNN cluster\ndefines the smoothing volume, or properly saying, the smoothing ellipsoid,\nrequired to perform the anisotropic interpolation. Thus, the smoothing kernel\nhas an ellipsoidal profile, which changes how the kernel gradients are\ncomputed. As an application, it was performed the simulation of collapse and\nfragmentation of a non-magnetic, rotating gaseous sphere. An interesting\noutcome was the formation of protostars in the disc fragmentation, shown to be\nmuch more persistent and much more abundant in the anisotropic simulation than\nin the isotropic case.",
          "link": "http://arxiv.org/abs/2106.08870",
          "publishedOn": "2021-06-17T01:58:44.144Z",
          "wordCount": 604,
          "title": "Covariance-based smoothed particle hydrodynamics. A machine-learning application to simulating disc fragmentation. (arXiv:2106.08870v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1\">Tristan Karch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1\">Laetitia Teodorescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1\">Cl&#xe9;ment Moulin-Frier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Language is an interface to the outside world. In order for embodied agents\nto use it, language must be grounded in other, sensorimotor modalities. While\nthere is an extended literature studying how machines can learn grounded\nlanguage, the topic of how to learn spatio-temporal linguistic concepts is\nstill largely uncharted. To make progress in this direction, we here introduce\na novel spatio-temporal language grounding task where the goal is to learn the\nmeaning of spatio-temporal descriptions of behavioral traces of an embodied\nagent. This is achieved by training a truth function that predicts if a\ndescription matches a given history of observations. The descriptions involve\ntime-extended predicates in past and present tense as well as spatio-temporal\nreferences to objects in the scene. To study the role of architectural biases\nin this task, we train several models including multimodal Transformer\narchitectures; the latter implement different attention computations between\nwords and objects across space and time. We test models on two classes of\ngeneralization: 1) generalization to randomly held-out sentences; 2)\ngeneralization to grammar primitives. We observe that maintaining object\nidentity in the attention computation of our Transformers is instrumental to\nachieving good performance on generalization overall, and that summarizing\nobject traces in a single token has little influence on performance. We then\ndiscuss how this opens new perspectives for language-guided autonomous embodied\nagents. We also release our code under open-source license as well as\npretrained models and datasets to encourage the wider community to build upon\nand extend our work in the future.",
          "link": "http://arxiv.org/abs/2106.08858",
          "publishedOn": "2021-06-17T01:58:44.138Z",
          "wordCount": 687,
          "title": "Grounding Spatio-Temporal Language with Transformers. (arXiv:2106.08858v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1\">Jiquan Ngiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Benjamin Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Vijay Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hao-Tien Lewis Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1\">Jeffrey Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1\">Alex Bewley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1\">Ashish Venugopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1\">David Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1\">Ben Sapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jonathon Shlens</a>",
          "description": "Predicting the future motion of multiple agents is necessary for planning in\ndynamic environments. This task is challenging for autonomous driving since\nagents (e.g., vehicles and pedestrians) and their associated behaviors may be\ndiverse and influence each other. Most prior work has focused on first\npredicting independent futures for each agent based on all past motion, and\nthen planning against these independent predictions. However, planning against\nfixed predictions can suffer from the inability to represent the future\ninteraction possibilities between different agents, leading to sub-optimal\nplanning. In this work, we formulate a model for predicting the behavior of all\nagents jointly in real-world driving environments in a unified manner. Inspired\nby recent language modeling approaches, we use a masking strategy as the query\nto our model, enabling one to invoke a single model to predict agent behavior\nin many ways, such as potentially conditioned on the goal or full future\ntrajectory of the autonomous vehicle or the behavior of other agents in the\nenvironment. Our model architecture fuses heterogeneous world state in a\nunified Transformer architecture by employing attention across road elements,\nagent interactions and time steps. We evaluate our approach on autonomous\ndriving datasets for behavior prediction, and achieve state-of-the-art\nperformance. Our work demonstrates that formulating the problem of behavior\nprediction in a unified architecture with a masking strategy may allow us to\nhave a single model that can perform multiple motion prediction and planning\nrelated tasks effectively.",
          "link": "http://arxiv.org/abs/2106.08417",
          "publishedOn": "2021-06-17T01:58:44.132Z",
          "wordCount": 703,
          "title": "Scene Transformer: A unified multi-task model for behavior prediction and planning. (arXiv:2106.08417v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fu-Ming Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1\">Austin Huang</a>",
          "description": "Reducing computation cost, inference latency, and memory footprint of neural\nnetworks are frequently cited as research motivations for pruning and sparsity.\nHowever, operationalizing those benefits and understanding the end-to-end\neffect of algorithm design and regularization on the runtime execution is not\noften examined in depth.\n\nHere we apply structured and unstructured pruning to attention weights of\ntransformer blocks of the BERT language model, while also expanding block\nsparse representation (BSR) operations in the TVM compiler. Integration of BSR\noperations enables the TVM runtime execution to leverage structured pattern\nsparsity induced by model regularization.\n\nThis integrated view of pruning algorithms enables us to study relationships\nbetween modeling decisions and their direct impact on sparsity-enhanced\nexecution. Our main findings are: 1) we validate that performance benefits of\nstructured sparsity block regularization must be enabled by the BSR\naugmentations to TVM, with 4x speedup relative to vanilla PyTorch and 2.2x\nspeedup relative to standard TVM compilation (without expanded BSR support). 2)\nfor BERT attention weights, the end-to-end optimal block sparsity shape in this\nCPU inference context is not a square block (as in \\cite{gray2017gpu}) but\nrather a linear 32x1 block 3) the relationship between performance and block\nsize / shape is is suggestive of how model regularization parameters interact\nwith task scheduler optimizations resulting in the observed end-to-end\nperformance.",
          "link": "http://arxiv.org/abs/2106.08846",
          "publishedOn": "2021-06-17T01:58:44.126Z",
          "wordCount": 657,
          "title": "Algorithm to Compilation Codesign: An Integrated View of Neural Network Sparsity. (arXiv:2106.08846v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1\">Ilias Diakonikolas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1\">Daniel M. Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kongsgaard_D/0/1/0/all/0/1\">Daniel Kongsgaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1\">Kevin Tian</a>",
          "description": "We study the problem of list-decodable mean estimation, where an adversary\ncan corrupt a majority of the dataset. Specifically, we are given a set $T$ of\n$n$ points in $\\mathbb{R}^d$ and a parameter $0< \\alpha <\\frac 1 2$ such that\nan $\\alpha$-fraction of the points in $T$ are i.i.d. samples from a\nwell-behaved distribution $\\mathcal{D}$ and the remaining $(1-\\alpha)$-fraction\nof the points are arbitrary. The goal is to output a small list of vectors at\nleast one of which is close to the mean of $\\mathcal{D}$. As our main\ncontribution, we develop new algorithms for list-decodable mean estimation,\nachieving nearly-optimal statistical guarantees, with running time $n^{1 +\no(1)} d$. All prior algorithms for this problem had additional polynomial\nfactors in $\\frac 1 \\alpha$. As a corollary, we obtain the first almost-linear\ntime algorithms for clustering mixtures of $k$ separated well-behaved\ndistributions, nearly-matching the statistical guarantees of spectral methods.\nPrior clustering algorithms inherently relied on an application of $k$-PCA,\nthereby incurring runtimes of $\\Omega(n d k)$. This marks the first runtime\nimprovement for this basic statistical problem in nearly two decades.\n\nThe starting point of our approach is a novel and simpler near-linear time\nrobust mean estimation algorithm in the $\\alpha \\to 1$ regime, based on a\none-shot matrix multiplicative weights-inspired potential decrease. We\ncrucially leverage this new algorithmic framework in the context of the\niterative multi-filtering technique of Diakonikolas et. al. '18, '20, providing\na method to simultaneously cluster and downsample points using one-dimensional\nprojections --- thus, bypassing the $k$-PCA subroutines required by prior\nalgorithms.",
          "link": "http://arxiv.org/abs/2106.08537",
          "publishedOn": "2021-06-17T01:58:44.118Z",
          "wordCount": 704,
          "title": "Clustering Mixture Models in Almost-Linear Time via List-Decodable Mean Estimation. (arXiv:2106.08537v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_Hannemann_M/0/1/0/all/0/1\">Matthias M&#xfc;ller-Hannemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckert_R/0/1/0/all/0/1\">Ralf R&#xfc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiewe_A/0/1/0/all/0/1\">Alexander Schiewe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schobel_A/0/1/0/all/0/1\">Anita Sch&#xf6;bel</a>",
          "description": "The planning of attractive and cost efficient public transport systems is a\nhighly complex optimization process involving many steps. Integrating\nrobustness from a passenger's point of view makes the task even more\nchallenging. With numerous different definitions of robustness in literature, a\nreal-world acceptable evaluation of the robustness of a public transport system\nis to simulate its performance under a large number of possible scenarios.\nUnfortunately, this is computationally very expensive. In this paper, we\ntherefore explore a new way of such a scenario-based robustness approximation\nby using methods from machine learning. We achieve a fast approach with a very\nhigh accuracy by gathering a subset of key features of a public transport\nsystem and its passenger demand and training an artificial neural network to\nlearn the outcome of a given set of robustness tests. The network is then able\nto predict the robustness of untrained instances with high accuracy using only\nits key features, allowing for a robustness oracle for transport planners that\napproximates the robustness in constant time. Such an oracle can be used as\nblack box to increase the robustness within a local search framework for\nintegrated public transportation planning. In computational experiments with\ndifferent benchmark instances we demonstrate an excellent quality of our\npredictions.",
          "link": "http://arxiv.org/abs/2106.08967",
          "publishedOn": "2021-06-17T01:58:44.102Z",
          "wordCount": 635,
          "title": "Estimating the Robustness of Public Transport Systems Using Machine Learning. (arXiv:2106.08967v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsaregorodtsev_A/0/1/0/all/0/1\">Alexander Tsaregorodtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>",
          "description": "We present an automated data augmentation approach for image classification.\nWe formulate the problem as Monte Carlo sampling where our goal is to\napproximate the optimal augmentation policies. We propose a particle filtering\nformulation to find optimal augmentation policies and their schedules during\nmodel training. Our performance measurement procedure relies on a validation\nsubset of our training set, while the policy transition model depends on a\nGaussian prior and an optional augmentation velocity parameter. In our\nexperiments, we show that our formulation for automated augmentation reaches\npromising results on CIFAR-10, CIFAR-100, and ImageNet datasets using the\nstandard network architectures for this problem. By comparing with the related\nwork, we also show that our method reaches a balance between the computational\ncost of policy search and the model performance.",
          "link": "http://arxiv.org/abs/2106.08693",
          "publishedOn": "2021-06-17T01:58:44.096Z",
          "wordCount": 555,
          "title": "ParticleAugment: Sampling-Based Data Augmentation. (arXiv:2106.08693v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhongjie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1\">Martin Trapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skryagin_A/0/1/0/all/0/1\">Arseny Skryagin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "Inspired by recent advances in the field of expert-based approximations of\nGaussian processes (GPs), we present an expert-based approach to large-scale\nmulti-output regression using single-output GP experts. Employing a deeply\nstructured mixture of single-output GPs encoded via a probabilistic circuit\nallows us to capture correlations between multiple output dimensions\naccurately. By recursively partitioning the covariate space and the output\nspace, posterior inference in our model reduces to inference on single-output\nGP experts, which only need to be conditioned on a small subset of the\nobservations. We show that inference can be performed exactly and efficiently\nin our model, that it can capture correlations between output dimensions and,\nhence, often outperforms approaches that do not incorporate inter-output\ncorrelations, as demonstrated on several data sets in terms of the negative log\npredictive density.",
          "link": "http://arxiv.org/abs/2106.08687",
          "publishedOn": "2021-06-17T01:58:44.089Z",
          "wordCount": 567,
          "title": "Leveraging Probabilistic Circuits for Nonparametric Multi-Output Regression. (arXiv:2106.08687v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Soriano_D/0/1/0/all/0/1\">David Garcia-Soriano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1\">Francesco Bonchi</a>",
          "description": "We study a novel problem of fairness in ranking aimed at minimizing the\namount of individual unfairness introduced when enforcing group-fairness\nconstraints. Our proposal is rooted in the distributional maxmin fairness\ntheory, which uses randomization to maximize the expected satisfaction of the\nworst-off individuals. We devise an exact polynomial-time algorithm to find\nmaxmin-fair distributions of general search problems (including, but not\nlimited to, ranking), and show that our algorithm can produce rankings which,\nwhile satisfying the given group-fairness constraints, ensure that the maximum\npossible value is brought to individuals.",
          "link": "http://arxiv.org/abs/2106.08652",
          "publishedOn": "2021-06-17T01:58:44.072Z",
          "wordCount": 523,
          "title": "Maxmin-Fair Ranking: Individual Fairness under Group-Fairness Constraints. (arXiv:2106.08652v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08891",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Edmonds_A/0/1/0/all/0/1\">Andrew Edmonds</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Brown_D/0/1/0/all/0/1\">David Brown</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Vinas_L/0/1/0/all/0/1\">Luciano Vinas</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pagan_S/0/1/0/all/0/1\">Samantha Pagan</a>",
          "description": "We describe the use of machine learning algorithms to select high-quality\nmeasurements for the Mu2e experiment. This technique is important for\nexperiments with backgrounds that arise due to measurement errors. The\nalgorithms use multiple pieces of ancillary information that are sensitive to\nmeasurement quality to separate high-quality and low-quality measurements.",
          "link": "http://arxiv.org/abs/2106.08891",
          "publishedOn": "2021-06-17T01:58:44.065Z",
          "wordCount": 495,
          "title": "Using Machine Learning to Select High-Quality Measurements. (arXiv:2106.08891v1 [physics.data-an])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08706",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pandey_L/0/1/0/all/0/1\">Laxmi Pandey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arif_A/0/1/0/all/0/1\">Ahmed Sabbir Arif</a>",
          "description": "Speech sounds of spoken language are obtained by varying configuration of the\narticulators surrounding the vocal tract. They contain abundant information\nthat can be utilized to better understand the underlying mechanism of human\nspeech production. We propose a novel deep neural network-based learning\nframework that understands acoustic information in the variable-length sequence\nof vocal tract shaping during speech production, captured by real-time magnetic\nresonance imaging (rtMRI), and translate it into text. The proposed framework\ncomprises of spatiotemporal convolutions, a recurrent network, and the\nconnectionist temporal classification loss, trained entirely end-to-end. On the\nUSC-TIMIT corpus, the model achieved a 40.6% PER at sentence-level, much better\ncompared to the existing models. To the best of our knowledge, this is the\nfirst study that demonstrates the recognition of entire spoken sentence based\non an individual's articulatory motions captured by rtMRI video. We also\nperformed an analysis of variations in the geometry of articulation in each\nsub-regions of the vocal tract (i.e., pharyngeal, velar and dorsal, hard\npalate, labial constriction region) with respect to different emotions and\ngenders. Results suggest that each sub-regions distortion is affected by both\nemotion and gender.",
          "link": "http://arxiv.org/abs/2106.08706",
          "publishedOn": "2021-06-17T01:58:44.057Z",
          "wordCount": 656,
          "title": "Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in Real-Time MRI. (arXiv:2106.08706v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jinshuo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>",
          "description": "Characterizing the privacy degradation over compositions, i.e., privacy\naccounting, is a fundamental topic in differential privacy (DP) with many\napplications to differentially private machine learning and federated learning.\n\nWe propose a unification of recent advances (Renyi DP, privacy profiles,\n$f$-DP and the PLD formalism) via the characteristic function ($\\phi$-function)\nof a certain ``worst-case'' privacy loss random variable.\n\nWe show that our approach allows natural adaptive composition like Renyi DP,\n\nprovides exactly tight privacy accounting like PLD, and can be (often\nlosslessly) converted to privacy profile and $f$-DP, thus providing\n$(\\epsilon,\\delta)$-DP guarantees and interpretable tradeoff functions.\nAlgorithmically, we propose an analytical Fourier accountant that represents\nthe complex logarithm of $\\phi$-functions symbolically and uses Gaussian\nquadrature for numerical computation. On several popular DP mechanisms and\ntheir subsampled counterparts, we demonstrate the flexibility and tightness of\nour approach in theory and experiments.",
          "link": "http://arxiv.org/abs/2106.08567",
          "publishedOn": "2021-06-17T01:58:44.035Z",
          "wordCount": 565,
          "title": "Optimal Accounting of Differential Privacy via Characteristic Function. (arXiv:2106.08567v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jacky Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kit-Yung Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lik-Hang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_P/0/1/0/all/0/1\">Pan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiang Su</a>",
          "description": "Mobile Augmented Reality (MAR) integrates computer-generated virtual objects\nwith physical environments for mobile devices. MAR systems enable users to\ninteract with MAR devices, such as smartphones and head-worn wearables, and\nperforms seamless transitions from the physical world to a mixed world with\ndigital entities. These MAR systems support user experiences by using MAR\ndevices to provide universal accessibility to digital contents. Over the past\n20 years, a number of MAR systems have been developed, however, the studies and\ndesign of MAR frameworks have not yet been systematically reviewed from the\nperspective of user-centric design. This article presents the first effort of\nsurveying existing MAR frameworks (count: 37) and further discusses the latest\nstudies on MAR through a top-down approach: 1) MAR applications; 2) MAR\nvisualisation techniques adaptive to user mobility and contexts; 3) systematic\nevaluation of MAR frameworks including supported platforms and corresponding\nfeatures such as tracking, feature extraction plus sensing capabilities; and 4)\nunderlying machine learning approaches supporting intelligent operations within\nMAR systems. Finally, we summarise the development of emerging research fields,\ncurrent state-of-the-art, and discuss the important open challenges and\npossible theoretical and technical directions. This survey aims to benefit both\nresearchers and MAR system developers alike.",
          "link": "http://arxiv.org/abs/2106.08710",
          "publishedOn": "2021-06-17T01:58:44.026Z",
          "wordCount": 658,
          "title": "Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence. (arXiv:2106.08710v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1\">Mario Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eloy_C/0/1/0/all/0/1\">Christophe Eloy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1\">Matthieu Wyart</a>",
          "description": "Reinforcement learning is made much more complex when the agent's observation\nis partial or noisy. This case corresponds to a partially observable Markov\ndecision process (POMDP). One strategy to seek good performance in POMDPs is to\nendow the agent with a finite memory, whose update is governed by the policy.\nHowever, policy optimization is non-convex in that case and can lead to poor\ntraining performance for random initialization. The performance can be\nempirically improved by constraining the memory architecture, then sacrificing\noptimality to facilitate training. Here we study this trade-off in the two-arm\nbandit problem, and compare two extreme cases: (i) the random access memory\nwhere any transitions between $M$ memory states are allowed and (ii) a fixed\nmemory where the agent can access its last $m$ actions and rewards. For (i),\nthe probability $q$ to play the worst arm is known to be exponentially small in\n$M$ for the optimal policy. Our main result is to show that similar performance\ncan be reached for (ii) as well, despite the simplicity of the memory\narchitecture: using a conjecture on Gray-ordered binary necklaces, we find\npolicies for which $q$ is exponentially small in $2^m$ i.e. $q\\sim\\alpha^{2^m}$\nfor some $\\alpha < 1$. Interestingly, we observe empirically that training from\nrandom initialization leads to very poor results for (i), and significantly\nbetter results for (ii).",
          "link": "http://arxiv.org/abs/2106.08849",
          "publishedOn": "2021-06-17T01:58:44.013Z",
          "wordCount": 647,
          "title": "How memory architecture affects performance and learning in simple POMDPs. (arXiv:2106.08849v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xianghong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoli Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "Variational autoencoders (VAEs) have been widely applied for text modeling.\nIn practice, however, they are troubled by two challenges: information\nunderrepresentation and posterior collapse. The former arises as only the last\nhidden state of LSTM encoder is transformed into the latent space, which is\ngenerally insufficient to summarize the data. The latter is a long-standing\nproblem during the training of VAEs as the optimization is trapped to a\ndisastrous local optimum. In this paper, we propose Discrete Auto-regressive\nVariational Attention Model (DAVAM) to address the challenges. Specifically, we\nintroduce an auto-regressive variational attention approach to enrich the\nlatent space by effectively capturing the semantic dependency from the input.\nWe further design discrete latent space for the variational attention and\nmathematically show that our model is free from posterior collapse. Extensive\nexperiments on language modeling tasks demonstrate the superiority of DAVAM\nagainst several VAE counterparts.",
          "link": "http://arxiv.org/abs/2106.08571",
          "publishedOn": "2021-06-17T01:58:44.008Z",
          "wordCount": 579,
          "title": "Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2106.08571v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08619",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Favero_A/0/1/0/all/0/1\">Alessandro Favero</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cagnetta_F/0/1/0/all/0/1\">Francesco Cagnetta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wyart_M/0/1/0/all/0/1\">Matthieu Wyart</a>",
          "description": "Convolutional neural networks perform a local and translationally-invariant\ntreatment of the data: quantifying which of these two aspects is central to\ntheir success remains a challenge. We study this problem within a\nteacher-student framework for kernel regression, using `convolutional' kernels\ninspired by the neural tangent kernel of simple convolutional architectures of\ngiven filter size. Using heuristic methods from physics, we find in the\nridgeless case that locality is key in determining the learning curve exponent\n$\\beta$ (that relates the test error $\\epsilon_t\\sim P^{-\\beta}$ to the size of\nthe training set $P$), whereas translational invariance is not. In particular,\nif the filter size of the teacher $t$ is smaller than that of the student $s$,\n$\\beta$ is a function of $s$ only and does not depend on the input dimension.\nWe confirm our predictions on $\\beta$ empirically. Theoretically, in some cases\n(including when teacher and student are equal) it can be shown that this\nprediction is an upper bound on performance. We conclude by proving, using a\nnatural universality assumption, that performing kernel regression with a ridge\nthat decreases with the size of the training set leads to similar learning\ncurve exponents to those we obtain in the ridgeless case.",
          "link": "http://arxiv.org/abs/2106.08619",
          "publishedOn": "2021-06-17T01:58:43.990Z",
          "wordCount": 642,
          "title": "Locality defeats the curse of dimensionality in convolutional teacher-student scenarios. (arXiv:2106.08619v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06806",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Carderera_A/0/1/0/all/0/1\">Alejandro Carderera</a>, <a href=\"http://arxiv.org/find/math/1/au:+Diakonikolas_J/0/1/0/all/0/1\">Jelena Diakonikolas</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lin_C/0/1/0/all/0/1\">Cheuk Yin Lin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pokutta_S/0/1/0/all/0/1\">Sebastian Pokutta</a>",
          "description": "Projection-free conditional gradient (CG) methods are the algorithms of\nchoice for constrained optimization setups in which projections are often\ncomputationally prohibitive but linear optimization over the constraint set\nremains computationally feasible. Unlike in projection-based methods, globally\naccelerated convergence rates are in general unattainable for CG. However, a\nvery recent work on Locally accelerated CG (LaCG) has demonstrated that local\nacceleration for CG is possible for many settings of interest. The main\ndownside of LaCG is that it requires knowledge of the smoothness and strong\nconvexity parameters of the objective function. We remove this limitation by\nintroducing a novel, Parameter-Free Locally accelerated CG (PF-LaCG) algorithm,\nfor which we provide rigorous convergence guarantees. Our theoretical results\nare complemented by numerical experiments, which demonstrate local acceleration\nand showcase the practical improvements of PF-LaCG over non-accelerated\nalgorithms, both in terms of iteration count and wall-clock time.",
          "link": "http://arxiv.org/abs/2102.06806",
          "publishedOn": "2021-06-17T01:58:43.962Z",
          "wordCount": 589,
          "title": "Parameter-free Locally Accelerated Conditional Gradients. (arXiv:2102.06806v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Li-Ming Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Haowen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1\">Albert Y.S. Lam</a>",
          "description": "Out-of-scope intent detection is of practical importance in task-oriented\ndialogue systems. Since the distribution of outlier utterances is arbitrary and\nunknown in the training stage, existing methods commonly rely on strong\nassumptions on data distribution such as mixture of Gaussians to make\ninference, resulting in either complex multi-step training procedures or\nhand-crafted rules such as confidence threshold selection for outlier\ndetection. In this paper, we propose a simple yet effective method to train an\nout-of-scope intent classifier in a fully end-to-end manner by simulating the\ntest scenario in training, which requires no assumption on data distribution\nand no additional post-processing or threshold setting. Specifically, we\nconstruct a set of pseudo outliers in the training stage, by generating\nsynthetic outliers using inliner features via self-supervision and sampling\nout-of-scope sentences from easily available open-domain datasets. The pseudo\noutliers are used to train a discriminative classifier that can be directly\napplied to and generalize well on the test task. We evaluate our method\nextensively on four benchmark dialogue datasets and observe significant\nimprovements over state-of-the-art approaches. Our code has been released at\nhttps://github.com/liam0949/DCLOOS.",
          "link": "http://arxiv.org/abs/2106.08616",
          "publishedOn": "2021-06-17T01:58:43.945Z",
          "wordCount": 618,
          "title": "Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training. (arXiv:2106.08616v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sanjay Singh</a>",
          "description": "With language models being deployed increasingly in the real world, it is\nessential to address the issue of the fairness of their outputs. The word\nembedding representations of these language models often implicitly draw\nunwanted associations that form a social bias within the model. The nature of\ngendered languages like Hindi, poses an additional problem to the\nquantification and mitigation of bias, owing to the change in the form of the\nwords in the sentence, based on the gender of the subject. Additionally, there\nis sparse work done in the realm of measuring and debiasing systems for Indic\nlanguages. In our work, we attempt to evaluate and quantify the gender bias\nwithin a Hindi-English machine translation system. We implement a modified\nversion of the existing TGBI metric based on the grammatical considerations for\nHindi. We also compare and contrast the resulting bias measurements across\nmultiple metrics for pre-trained embeddings and the ones learned by our machine\ntranslation model.",
          "link": "http://arxiv.org/abs/2106.08680",
          "publishedOn": "2021-06-17T01:58:43.939Z",
          "wordCount": 587,
          "title": "Evaluating Gender Bias in Hindi-English Machine Translation. (arXiv:2106.08680v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>",
          "description": "Dropout is attracting intensive research interest in deep learning as an\nefficient approach to prevent overfitting. Recently incorporating structural\ninformation when deciding which units to drop out produced promising results\ncomparing to methods that ignore the structural information. However, a major\nissue of the existing work is that it failed to differentiate among instances\nwhen constructing the dropout architecture. This can be a significant\ndeficiency for many applications. To solve this issue, we propose\nConstructivism learning for instance-dependent Dropout Architecture (CODA),\nwhich is inspired from a philosophical theory, constructivism learning.\nSpecially, based on the theory we have designed a better drop out technique,\nUniform Process Mixture Models, using a Bayesian nonparametric method Uniform\nprocess. We have evaluated our proposed method on 5 real-world datasets and\ncompared the performance with other state-of-the-art dropout techniques. The\nexperimental results demonstrated the effectiveness of CODA.",
          "link": "http://arxiv.org/abs/2106.08444",
          "publishedOn": "2021-06-17T01:58:43.933Z",
          "wordCount": 559,
          "title": "CODA: Constructivism Learning for Instance-Dependent Dropout Architecture Construction. (arXiv:2106.08444v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08678",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Sim_A/0/1/0/all/0/1\">Aaron Sim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wiatrak_M/0/1/0/all/0/1\">Maciej Wiatrak</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Brayne_A/0/1/0/all/0/1\">Angus Brayne</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Creed_P/0/1/0/all/0/1\">P&#xe1;id&#xed; Creed</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Paliwal_S/0/1/0/all/0/1\">Saee Paliwal</a>",
          "description": "The inductive biases of graph representation learning algorithms are often\nencoded in the background geometry of their embedding space. In this paper, we\nshow that general directed graphs can be effectively represented by an\nembedding model that combines three components: a pseudo-Riemannian metric\nstructure, a non-trivial global topology, and a unique likelihood function that\nexplicitly incorporates a preferred direction in embedding space. We\ndemonstrate the representational capabilities of this method by applying it to\nthe task of link prediction on a series of synthetic and real directed graphs\nfrom natural language applications and biology. In particular, we show that\nlow-dimensional cylindrical Minkowski and anti-de Sitter spacetimes can produce\nequal or better graph representations than curved Riemannian manifolds of\nhigher dimensions.",
          "link": "http://arxiv.org/abs/2106.08678",
          "publishedOn": "2021-06-17T01:58:43.880Z",
          "wordCount": 554,
          "title": "Directed Graph Embeddings in Pseudo-Riemannian Manifolds. (arXiv:2106.08678v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1\">Ankur Moitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mossel_E/0/1/0/all/0/1\">Elchanan Mossel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandon_C/0/1/0/all/0/1\">Colin Sandon</a>",
          "description": "In this work, we study the computational complexity of determining whether a\nmachine learning model that perfectly fits the training data will generalizes\nto unseen data. In particular, we study the power of a malicious agent whose\ngoal is to construct a model g that fits its training data and nothing else,\nbut is indistinguishable from an accurate model f. We say that g strongly\nspoofs f if no polynomial-time algorithm can tell them apart. If instead we\nrestrict to algorithms that run in $n^c$ time for some fixed $c$, we say that g\nc-weakly spoofs f. Our main results are\n\n1. Under cryptographic assumptions, strong spoofing is possible and 2. For\nany c> 0, c-weak spoofing is possible unconditionally\n\nWhile the assumption of a malicious agent is an extreme scenario (hopefully\ncompanies training large models are not malicious), we believe that it sheds\nlight on the inherent difficulties of blindly trusting large proprietary models\nor data.",
          "link": "http://arxiv.org/abs/2106.08393",
          "publishedOn": "2021-06-17T01:58:43.865Z",
          "wordCount": 590,
          "title": "Spoofing Generalization: When Can't You Trust Proprietary Models?. (arXiv:2106.08393v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abrate_C/0/1/0/all/0/1\">Carlo Abrate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1\">Francesco Bonchi</a>",
          "description": "Training graph classifiers able to distinguish between healthy brains and\ndysfunctional ones, can help identifying substructures associated to specific\ncognitive phenotypes. However, the mere predictive power of the graph\nclassifier is of limited interest to the neuroscientists, which have plenty of\ntools for the diagnosis of specific mental disorders. What matters is the\ninterpretation of the model, as it can provide novel insights and new\nhypotheses.\n\nIn this paper we propose \\emph{counterfactual graphs} as a way to produce\nlocal post-hoc explanations of any black-box graph classifier. Given a graph\nand a black-box, a counterfactual is a graph which, while having high\nstructural similarity with the original graph, is classified by the black-box\nin a different class. We propose and empirically compare several strategies for\ncounterfactual graph search. Our experiments against a white-box classifier\nwith known optimal counterfactual, show that our methods, although heuristic,\ncan produce counterfactuals very close to the optimal one. Finally, we show how\nto use counterfactual graphs to build global explanations correctly capturing\nthe behaviour of different black-box classifiers and providing interesting\ninsights for the neuroscientists.",
          "link": "http://arxiv.org/abs/2106.08640",
          "publishedOn": "2021-06-17T01:58:43.845Z",
          "wordCount": 613,
          "title": "Counterfactual Graphs for Explainable Classification of Brain Networks. (arXiv:2106.08640v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xingquan Zhu</a>",
          "description": "Hospital readmission prediction is a study to learn models from historical\nmedical data to predict probability of a patient returning to hospital in a\ncertain period, 30 or 90 days, after the discharge. The motivation is to help\nhealth providers deliver better treatment and post-discharge strategies, lower\nthe hospital readmission rate, and eventually reduce the medical costs. Due to\ninherent complexity of diseases and healthcare ecosystems, modeling hospital\nreadmission is facing many challenges. By now, a variety of methods have been\ndeveloped, but existing literature fails to deliver a complete picture to\nanswer some fundamental questions, such as what are the main challenges and\nsolutions in modeling hospital readmission; what are typical features/models\nused for readmission prediction; how to achieve meaningful and transparent\npredictions for decision making; and what are possible conflicts when deploying\npredictive approaches for real-world usages. In this paper, we systematically\nreview computational models for hospital readmission prediction, and propose a\ntaxonomy of challenges featuring four main categories: (1) data variety and\ncomplexity; (2) data imbalance, locality and privacy; (3) model\ninterpretability; and (4) model implementation. The review summarizes methods\nin each category, and highlights technical solutions proposed to address the\nchallenges. In addition, a review of datasets and resources available for\nhospital readmission modeling also provides firsthand materials to support\nresearchers and practitioners to design new approaches for effective and\nefficient hospital readmission prediction.",
          "link": "http://arxiv.org/abs/2106.08488",
          "publishedOn": "2021-06-17T01:58:43.743Z",
          "wordCount": 649,
          "title": "Predictive Modeling of Hospital Readmission: Challenges and Solutions. (arXiv:2106.08488v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rando_M/0/1/0/all/0/1\">Marco Rando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carratino_L/0/1/0/all/0/1\">Luigi Carratino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villa_S/0/1/0/all/0/1\">Silvia Villa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosasco_L/0/1/0/all/0/1\">Lorenzo Rosasco</a>",
          "description": "Gaussian process optimization is a successful class of algorithms (e.g.\nGP-UCB) to optimize a black-box function through sequential evaluations.\nHowever, when the domain of the function is continuous, Gaussian process\noptimization has to either rely on a fixed discretization of the space, or\nsolve a non-convex optimization subproblem at each evaluation. The first\napproach can negatively affect performance, while the second one puts a heavy\ncomputational burden on the algorithm. A third option, that only recently has\nbeen theoretically studied, is to adaptively discretize the function domain.\nEven though this approach avoids the extra non-convex optimization costs, the\noverall computational complexity is still prohibitive. An algorithm such as\nGP-UCB has a runtime of $O(T^4)$, where $T$ is the number of iterations. In\nthis paper, we introduce Ada-BKB (Adaptive Budgeted Kernelized Bandit), a\nno-regret Gaussian process optimization algorithm for functions on continuous\ndomains, that provably runs in $O(T^2 d_\\text{eff}^2)$, where $d_\\text{eff}$ is\nthe effective dimension of the explored space, and which is typically much\nsmaller than $T$. We corroborate our findings with experiments on synthetic\nnon-convex functions and on the real-world problem of hyper-parameter\noptimization.",
          "link": "http://arxiv.org/abs/2106.08598",
          "publishedOn": "2021-06-17T01:58:43.738Z",
          "wordCount": 619,
          "title": "Ada-BKB: Scalable Gaussian Process Optimization on Continuous Domain by Adaptive Discretization. (arXiv:2106.08598v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dezhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qiujin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihan Huang</a>",
          "description": "Anomalous diffusion, which shows a deviation of transport dynamics from the\nframework of standard Brownian motion, is involved in the evolution of various\nphysical, chemical, biological, and economic systems. The study of such random\nprocesses is of fundamental importance in unveiling the physical properties of\nrandom walkers and complex systems. However, classical methods to characterize\nanomalous diffusion are often disqualified for individual short trajectories,\nleading to the launch of the Anomalous Diffusion (AnDi) Challenge. This\nchallenge aims at objectively assessing and comparing new approaches for single\ntrajectory characterization, with respect to three different aspects: the\ninference of the anomalous diffusion exponent; the classification of the\ndiffusion model; and the segmentation of trajectories. In this article, to\naddress the inference and classification tasks in the challenge, we develop a\nWaveNet-based deep neural network (WADNet) by combining a modified WaveNet\nencoder with long short-term memory networks, without any prior knowledge of\nanomalous diffusion. As the performance of our model has surpassed the current\n1st places in the challenge leaderboard on both two tasks for all dimensions (6\nsubtasks), WADNet could be the part of state-of-the-art techniques to decode\nthe AnDi database. Our method presents a benchmark for future research, and\ncould accelerate the development of a versatile tool for the characterization\nof anomalous diffusion.",
          "link": "http://arxiv.org/abs/2106.08887",
          "publishedOn": "2021-06-17T01:58:43.631Z",
          "wordCount": 663,
          "title": "WaveNet-Based Deep Neural Networks for the Characterization of Anomalous Diffusion (WADNet). (arXiv:2106.08887v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1\">Dipankar Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mukur Gupta</a>",
          "description": "The training of deep learning models poses vast challenges of including\nparameter tuning and ordering of training data. Significant research has been\ndone in Curriculum learning for optimizing the sequence of training data.\nRecent works have focused on using complex reinforcement learning techniques to\nfind the optimal data ordering strategy to maximize learning for a given\nnetwork. In this paper, we present a simple and efficient technique based on\ncontinuous optimization. We call this new approach Training Sequence\nOptimization (TSO). There are three critical components in our proposed\napproach: (a) An encoder network maps/embeds training sequence into continuous\nspace. (b) A predictor network uses the continuous representation of a strategy\nas input and predicts the accuracy for fixed network architecture. (c) A\ndecoder further maps a continuous representation of a strategy to the ordered\ntraining dataset. The performance predictor and encoder enable us to perform\ngradient-based optimization in the continuous space to find the embedding of\noptimal training data ordering with potentially better accuracy. Experiments\nshow that we can gain 2AP with our generated optimal curriculum strategy over\nthe random strategy using the CIFAR-100 dataset and have better boosts than the\nstate of the art CL algorithms. We do an ablation study varying the\narchitecture, dataset and sample sizes showcasing our approach's robustness.",
          "link": "http://arxiv.org/abs/2106.08569",
          "publishedOn": "2021-06-17T01:58:43.593Z",
          "wordCount": 641,
          "title": "TSO: Curriculum Generation using continuous optimization. (arXiv:2106.08569v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhili Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roosta_F/0/1/0/all/0/1\">Fred Roosta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>",
          "description": "A variety of dimensionality reduction techniques have been applied for\ncomputations involving large matrices. The underlying matrix is randomly\ncompressed into a smaller one, while approximately retaining many of its\noriginal properties. As a result, much of the expensive computation can be\nperformed on the small matrix. The sketching of positive semidefinite (PSD)\nmatrices is well understood, but there are many applications where the related\nmatrices are not PSD, including Hessian matrices in non-convex optimization and\ncovariance matrices in regression applications involving complex numbers. In\nthis paper, we present novel dimensionality reduction methods for non-PSD\nmatrices, as well as their ``square-roots\", which involve matrices with complex\nentries. We show how these techniques can be used for multiple downstream\ntasks. In particular, we show how to use the proposed matrix sketching\ntechniques for both convex and non-convex optimization, $\\ell_p$-regression for\nevery $1 \\leq p \\leq \\infty$, and vector-matrix-vector queries.",
          "link": "http://arxiv.org/abs/2106.08544",
          "publishedOn": "2021-06-17T01:58:43.570Z",
          "wordCount": 578,
          "title": "Non-PSD Matrix Sketching with Applications to Regression and Optimization. (arXiv:2106.08544v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Melis_D/0/1/0/all/0/1\">David Alvarez-Melis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusi_N/0/1/0/all/0/1\">Nicol&#xf2; Fusi</a>",
          "description": "Various machine learning tasks, from generative modeling to domain\nadaptation, revolve around the concept of dataset transformation and\nmanipulation. While various methods exist for transforming unlabeled datasets,\nprincipled methods to do so for labeled (e.g., classification) datasets are\nmissing. In this work, we propose a novel framework for dataset transformation,\nwhich we cast as optimization over data-generating joint probability\ndistributions. We approach this class of problems through Wasserstein gradient\nflows in probability space, and derive practical and efficient particle-based\nmethods for a flexible but well-behaved class of objective functions. Through\nvarious experiments, we show that this framework can be used to impose\nconstraints on classification datasets, adapt them for transfer learning, or to\nre-purpose fixed or black-box models to classify ---with high accuracy---\npreviously unseen datasets.",
          "link": "http://arxiv.org/abs/2010.12760",
          "publishedOn": "2021-06-17T01:58:43.563Z",
          "wordCount": 580,
          "title": "Dataset Dynamics via Gradient Flows in Probability Space. (arXiv:2010.12760v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grosse_J/0/1/0/all/0/1\">Julia Grosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1\">Philipp Hennig</a>",
          "description": "Exciting contemporary machine learning problems have recently been phrased in\nthe classic formalism of tree search -- most famously, the game of Go.\nInterestingly, the state-space underlying these sequential decision-making\nproblems often posses a more general latent structure than can be captured by a\ntree. In this work, we develop a probabilistic framework to exploit a search\nspace's latent structure and thereby share information across the search tree.\nThe method is based on a combination of approximate inference in jointly\nGaussian models for the explored part of the problem, and an abstraction for\nthe unexplored part that imposes a reduction of complexity ad hoc. We\nempirically find our algorithm to compare favorably to existing\nnon-probabilistic alternatives in Tic-Tac-Toe and a feature selection\napplication.",
          "link": "http://arxiv.org/abs/2106.08717",
          "publishedOn": "2021-06-17T01:58:43.543Z",
          "wordCount": 552,
          "title": "Probabilistic DAG Search. (arXiv:2106.08717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaomeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1\">Michael Potter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Chan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gaurav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1\">V. Ratna Saripalli</a>",
          "description": "There is a parameter ubiquitous throughout the deep learning world: learning\nrate. There is likewise a ubiquitous question: what should that learning rate\nbe? The true answer to this question is often tedious and time consuming to\nobtain, and a great deal of arcane knowledge has accumulated in recent years\nover how to pick and modify learning rates to achieve optimal training\nperformance. Moreover, the long hours spent carefully crafting the perfect\nlearning rate can come to nothing the moment your network architecture,\noptimizer, dataset, or initial conditions change ever so slightly. But it need\nnot be this way. We propose a new answer to the great learning rate question:\nthe Autonomous Learning Rate Controller. Find it at\nhttps://github.com/fastestimator/ARC",
          "link": "http://arxiv.org/abs/2106.08767",
          "publishedOn": "2021-06-17T01:58:43.471Z",
          "wordCount": 555,
          "title": "To Raise or Not To Raise: The Autonomous Learning Rate Question. (arXiv:2106.08767v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunxin Liu</a>",
          "description": "The knowledge of a deep learning model may be transferred to a student model,\nleading to intellectual property infringement or vulnerability propagation.\nDetecting such knowledge reuse is nontrivial because the suspect models may not\nbe white-box accessible and/or may serve different tasks. In this paper, we\npropose ModelDiff, a testing-based approach to deep learning model similarity\ncomparison. Instead of directly comparing the weights, activations, or outputs\nof two models, we compare their behavioral patterns on the same set of test\ninputs. Specifically, the behavioral pattern of a model is represented as a\ndecision distance vector (DDV), in which each element is the distance between\nthe model's reactions to a pair of inputs. The knowledge similarity between two\nmodels is measured with the cosine similarity between their DDVs. To evaluate\nModelDiff, we created a benchmark that contains 144 pairs of models that cover\nmost popular model reuse methods, including transfer learning, model\ncompression, and model stealing. Our method achieved 91.7% correctness on the\nbenchmark, which demonstrates the effectiveness of using ModelDiff for model\nreuse detection. A study on mobile deep learning apps has shown the feasibility\nof ModelDiff on real-world models.",
          "link": "http://arxiv.org/abs/2106.08890",
          "publishedOn": "2021-06-17T01:58:43.444Z",
          "wordCount": 631,
          "title": "ModelDiff: Testing-Based DNN Similarity Comparison for Model Reuse Detection. (arXiv:2106.08890v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08532",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyeoncheol Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1\">Youngrock Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_E/0/1/0/all/0/1\">Eunjoo Jeon</a>",
          "description": "Explaining the foundations for predictions obtained from graph neural\nnetworks (GNNs) is critical for credible use of GNN models for real-world\nproblems. Owing to the rapid growth of GNN applications, recent progress in\nexplaining predictions from GNNs, such as sensitivity analysis, perturbation\nmethods, and attribution methods, showed great opportunities and possibilities\nfor explaining GNN predictions. In this study, we propose a method to improve\nthe explanation quality of node classification tasks that can be applied in a\npost hoc manner through aggregation of auxiliary explanations from important\nneighboring nodes, named SEEN. Applying SEEN does not require modification of a\ngraph and can be used with diverse explainability techniques due to its\nindependent mechanism. Experiments on matching motif-participating nodes from a\ngiven graph show great improvement in explanation accuracy of up to 12.71% and\ndemonstrate the correlation between the auxiliary explanations and the enhanced\nexplanation accuracy through leveraging their contributions. SEEN provides a\nsimple but effective method to enhance the explanation quality of GNN model\noutputs, and this method is applicable in combination with most explainability\ntechniques.",
          "link": "http://arxiv.org/abs/2106.08532",
          "publishedOn": "2021-06-17T01:58:43.437Z",
          "wordCount": 609,
          "title": "SEEN: Sharpening Explanations for Graph Neural Networks using Explanations from Neighborhoods. (arXiv:2106.08532v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08486",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chuah_W/0/1/0/all/0/1\">WeiQin Chuah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1\">Ruwan Tennakoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1\">Alireza Bab-Hadiashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1\">David Suter</a>",
          "description": "Learning-based stereo matching and depth estimation networks currently excel\non public benchmarks with impressive results. However, state-of-the-art\nnetworks often fail to generalize from synthetic imagery to more challenging\nreal data domains. This paper is an attempt to uncover hidden secrets of\nachieving domain robustness and in particular, discovering the important\ningredients of generalization success of stereo matching networks by analyzing\nthe effect of synthetic image learning on real data performance. We provide\nevidence that demonstrates that learning of features in the synthetic domain by\na stereo matching network is heavily influenced by two \"shortcuts\" presented in\nthe synthetic data: (1) identical local statistics (RGB colour features)\nbetween matching pixels in the synthetic stereo images and (2) lack of realism\nin synthetic textures on 3D objects simulated in game engines. We will show\nthat by removing such shortcuts, we can achieve domain robustness in the\nstate-of-the-art stereo matching frameworks and produce a remarkable\nperformance on multiple realistic datasets, despite the fact that the networks\nwere trained on synthetic data, only. Our experimental results point to the\nfact that eliminating shortcuts from the synthetic data is key to achieve\ndomain-invariant generalization between synthetic and real data domains.",
          "link": "http://arxiv.org/abs/2106.08486",
          "publishedOn": "2021-06-17T01:58:43.414Z",
          "wordCount": 640,
          "title": "Achieving Domain Robustness in Stereo Matching Networks by Removing Shortcut Learning. (arXiv:2106.08486v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rabin_M/0/1/0/all/0/1\">Md Rafiqul Islam Rabin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Aftab Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1\">Vincent J. Hellendoorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipour_M/0/1/0/all/0/1\">Mohammad Amin Alipour</a>",
          "description": "Deep Neural Networks (DNN) are increasingly commonly used in software\nengineering and code intelligence tasks. These are powerful tools that are\ncapable of learning highly generalizable patterns from large datasets through\nmillions of parameters. At the same time, training DNNs means walking a knife's\nedges, because their large capacity also renders them prone to memorizing data\npoints. While traditionally thought of as an aspect of over-training, recent\nwork suggests that the memorization risk manifests especially strongly when the\ntraining datasets are noisy and memorization is the only recourse.\nUnfortunately, most code intelligence tasks rely on rather noise-prone and\nrepetitive data sources, such as GitHub, which, due to their sheer size, cannot\nbe manually inspected and evaluated. We evaluate the memorization and\ngeneralization tendencies in neural code intelligence models through a case\nstudy across several benchmarks and model families by leveraging established\napproaches from other fields that use DNNs, such as introducing targeted noise\ninto the training dataset. In addition to reinforcing prior general findings\nabout the extent of memorization in DNNs, our results shed light on the impact\nof noisy dataset in training.",
          "link": "http://arxiv.org/abs/2106.08704",
          "publishedOn": "2021-06-17T01:58:43.361Z",
          "wordCount": 625,
          "title": "Memorization and Generalization in Neural Code Intelligence Models. (arXiv:2106.08704v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2006.06863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linnan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_R/0/1/0/all/0/1\">Rodrigo Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tian Guo</a>",
          "description": "Efficient evaluation of a network architecture drawn from a large search\nspace remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS\nevaluates each architecture by training from scratch, which gives the true\nperformance but is extremely time-consuming. Recently, one-shot NAS\nsubstantially reduces the computation cost by training only one supernetwork,\na.k.a. supernet, to approximate the performance of every architecture in the\nsearch space via weight-sharing. However, the performance estimation can be\nvery inaccurate due to the co-adaption among operations. In this paper, we\npropose few-shot NAS that uses multiple supernetworks, called sub-supernet,\neach covering different regions of the search space to alleviate the undesired\nco-adaption. Compared to one-shot NAS, few-shot NAS improves the accuracy of\narchitecture evaluation with a small increase of evaluation cost. With only up\nto 7 sub-supernets, few-shot NAS establishes new SoTAs: on ImageNet, it finds\nmodels that reach 80.5% top-1 accuracy at 600 MB FLOPS and 77.5% top-1 accuracy\nat 238 MFLOPS; on CIFAR10, it reaches 98.72% top-1 accuracy without using extra\ndata or transfer learning. In Auto-GAN, few-shot NAS outperforms the previously\npublished results by up to 20%. Extensive experiments show that few-shot NAS\nsignificantly improves various one-shot methods, including 4 gradient-based and\n6 search-based methods on 3 different tasks in NasBench-201 and\nNasBench1-shot-1.",
          "link": "http://arxiv.org/abs/2006.06863",
          "publishedOn": "2021-06-17T01:58:43.267Z",
          "wordCount": 719,
          "title": "Few-shot Neural Architecture Search. (arXiv:2006.06863v8 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">KunYang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>",
          "description": "With the rapid development of storage and computing power on mobile devices,\nit becomes critical and popular to deploy models on devices to save onerous\ncommunication latencies and to capture real-time features. While quite a lot of\nworks have explored to facilitate on-device learning and inference, most of\nthem focus on dealing with response delay or privacy protection. Little has\nbeen done to model the collaboration between the device and the cloud modeling\nand benefit both sides jointly. To bridge this gap, we are among the first\nattempts to study the Device-Cloud Collaborative Learning (DCCL) framework.\nSpecifically, we propose a novel MetaPatch learning approach on the device side\nto efficiently achieve \"thousands of people with thousands of models\" given a\ncentralized cloud model. Then, with billions of updated personalized device\nmodels, we propose a \"model-over-models\" distillation algorithm, namely\nMoMoDistill, to update the centralized cloud model. Our extensive experiments\nover a range of datasets with different settings demonstrate the effectiveness\nof such collaboration on both cloud and devices, especially its superiority to\nmodel long-tailed users.",
          "link": "http://arxiv.org/abs/2104.06624",
          "publishedOn": "2021-06-17T01:58:43.240Z",
          "wordCount": 668,
          "title": "Device-Cloud Collaborative Learning for Recommendation. (arXiv:2104.06624v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_R/0/1/0/all/0/1\">Ramin Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naeim_A/0/1/0/all/0/1\">Arash Naeim</a>",
          "description": "A data science task can be deemed as making sense of the data or testing a\nhypothesis about it. The conclusions inferred from data can greatly guide us to\nmake informative decisions. Big data has enabled us to carry out countless\nprediction tasks in conjunction with machine learning, such as identifying high\nrisk patients suffering from a certain disease and taking preventable measures.\nHowever, healthcare practitioners are not content with mere predictions - they\nare also interested in the cause-effect relation between input features and\nclinical outcomes. Understanding such relations will help doctors treat\npatients and reduce the risk effectively. Causality is typically identified by\nrandomized controlled trials. Often such trials are not feasible when\nscientists and researchers turn to observational studies and attempt to draw\ninferences. However, observational studies may also be affected by selection\nand/or confounding biases that can result in wrong causal conclusions. In this\nchapter, we will try to highlight some of the drawbacks that may arise in\ntraditional machine learning and statistical approaches to analyze the\nobservational data, particularly in the healthcare data analytics domain. We\nwill discuss causal inference and ways to discover the cause-effect from\nobservational studies in healthcare domain. Moreover, we will demonstrate the\napplications of causal inference in tackling some common machine learning\nissues such as missing data and model transportability. Finally, we will\ndiscuss the possibility of integrating reinforcement learning with causality as\na way to counter confounding bias.",
          "link": "http://arxiv.org/abs/2105.04655",
          "publishedOn": "2021-06-17T01:58:43.226Z",
          "wordCount": 717,
          "title": "Causal Inference in medicine and in health policy, a summary. (arXiv:2105.04655v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.07978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_A/0/1/0/all/0/1\">A. H. Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">A. Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karstoft_H/0/1/0/all/0/1\">H. Karstoft</a>",
          "description": "Forecasting the formation and development of clouds is a central element of\nmodern weather forecasting systems. Incorrect clouds forecasts can lead to\nmajor uncertainty in the overall accuracy of weather forecasts due to their\nintrinsic role in the Earth's climate system. Few studies have tackled this\nchallenging problem from a machine learning point-of-view due to a shortage of\nhigh-resolution datasets with many historical observations globally. In this\npaper, we present a novel satellite-based dataset called ``CloudCast''. It\nconsists of 70,080 images with 10 different cloud types for multiple layers of\nthe atmosphere annotated on a pixel level. The spatial resolution of the\ndataset is 928 x 1530 pixels (3x3 km per pixel) with 15-min intervals between\nframes for the period 2017-01-01 to 2018-12-31. All frames are centered and\nprojected over Europe. To supplement the dataset, we conduct an evaluation\nstudy with current state-of-the-art video prediction methods such as\nconvolutional long short-term memory networks, generative adversarial networks,\nand optical flow-based extrapolation methods. As the evaluation of video\nprediction is difficult in practice, we aim for a thorough evaluation in the\nspatial and temporal domain. Our benchmark models show promising results but\nwith ample room for improvement. This is the first publicly available\nglobal-scale dataset with high-resolution cloud types on a high temporal\ngranularity to the authors' best knowledge.",
          "link": "http://arxiv.org/abs/2007.07978",
          "publishedOn": "2021-06-17T01:58:43.191Z",
          "wordCount": 711,
          "title": "CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds. (arXiv:2007.07978v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xingyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tianju Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusinkiewicz_S/0/1/0/all/0/1\">Szymon M. Rusinkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_R/0/1/0/all/0/1\">Ryan P. Adams</a>",
          "description": "In design, fabrication, and control problems, we are often faced with the\ntask of synthesis, in which we must generate an object or configuration that\nsatisfies a set of constraints while maximizing one or more objective\nfunctions. The synthesis problem is typically characterized by a physical\nprocess in which many different realizations may achieve the goal. This\nmany-to-one map presents challenges to the supervised learning of feed-forward\nsynthesis, as the set of viable designs may have a complex structure. In\naddition, the non-differentiable nature of many physical simulations prevents\ndirect optimization. We address both of these problems with a two-stage neural\nnetwork architecture that we may consider to be an autoencoder. We first learn\nthe decoder: a differentiable surrogate that approximates the many-to-one\nphysical realization process. We then learn the encoder, which maps from goal\nto design, while using the fixed decoder to evaluate the quality of the\nrealization. We evaluate the approach on two case studies: extruder path\nplanning in additive manufacturing and constrained soft robot inverse\nkinematics. We compare our approach to direct optimization of design using the\nlearned surrogate, and to supervised learning of the synthesis problem. We find\nthat our approach produces higher quality solutions than supervised learning,\nwhile being competitive in quality with direct optimization, at a greatly\nreduced computational cost.",
          "link": "http://arxiv.org/abs/2106.09019",
          "publishedOn": "2021-06-17T01:58:43.178Z",
          "wordCount": 656,
          "title": "Amortized Synthesis of Constrained Configurations Using a Differentiable Surrogate. (arXiv:2106.09019v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "Tabular data are ubiquitous for the widespread applications of tables and\nhence have attracted the attention of researchers to extract underlying\ninformation. One of the critical problems in mining tabular data is how to\nunderstand their inherent semantic structures automatically. Existing studies\ntypically adopt Convolutional Neural Network (CNN) to model the spatial\ninformation of tabular structures yet ignore more diverse relational\ninformation between cells, such as the hierarchical and paratactic\nrelationships. To simultaneously extract spatial and relational information\nfrom tables, we propose a novel neural network architecture, TabularNet. The\nspatial encoder of TabularNet utilizes the row/column-level Pooling and the\nBidirectional Gated Recurrent Unit (Bi-GRU) to capture statistical information\nand local positional correlation, respectively. For relational information, we\ndesign a new graph construction method based on the WordNet tree and adopt a\nGraph Convolutional Network (GCN) based encoder that focuses on the\nhierarchical and paratactic relationships between cells. Our neural network\narchitecture can be a unified neural backbone for different understanding tasks\nand utilized in a multitask scenario. We conduct extensive experiments on three\nclassification tasks with two real-world spreadsheet data sets, and the results\ndemonstrate the effectiveness of our proposed TabularNet over state-of-the-art\nbaselines.",
          "link": "http://arxiv.org/abs/2106.03096",
          "publishedOn": "2021-06-17T01:58:43.170Z",
          "wordCount": 667,
          "title": "TabularNet: A Neural Network Architecture for Understanding Semantic Structures of Tabular Data. (arXiv:2106.03096v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08902",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1\">Avishek Ghosh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sankararaman_A/0/1/0/all/0/1\">Abishek Sankararaman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>",
          "description": "We consider the problem of minimizing regret in an $N$ agent heterogeneous\nstochastic linear bandits framework, where the agents (users) are similar but\nnot all identical. We model user heterogeneity using two popularly used ideas\nin practice; (i) A clustering framework where users are partitioned into groups\nwith users in the same group being identical to each other, but different\nacross groups, and (ii) a personalization framework where no two users are\nnecessarily identical, but a user's parameters are close to that of the\npopulation average. In the clustered users' setup, we propose a novel\nalgorithm, based on successive refinement of cluster identities and regret\nminimization. We show that, for any agent, the regret scales as\n$\\mathcal{O}(\\sqrt{T/N})$, if the agent is in a `well separated' cluster, or\nscales as $\\mathcal{O}(T^{\\frac{1}{2} + \\varepsilon}/(N)^{\\frac{1}{2}\n-\\varepsilon})$ if its cluster is not well separated, where $\\varepsilon$ is\npositive and arbitrarily close to $0$. Our algorithm is adaptive to the cluster\nseparation, and is parameter free -- it does not need to know the number of\nclusters, separation and cluster size, yet the regret guarantee adapts to the\ninherent complexity. In the personalization framework, we introduce a natural\nalgorithm where, the personal bandit instances are initialized with the\nestimates of the global average model. We show that, an agent $i$ whose\nparameter deviates from the population average by $\\epsilon_i$, attains a\nregret scaling of $\\widetilde{O}(\\epsilon_i\\sqrt{T})$. This demonstrates that\nif the user representations are close (small $\\epsilon_i)$, the resulting\nregret is low, and vice-versa. The results are empirically validated and we\nobserve superior performance of our adaptive algorithms over non-adaptive\nbaselines.",
          "link": "http://arxiv.org/abs/2106.08902",
          "publishedOn": "2021-06-17T01:58:43.142Z",
          "wordCount": 698,
          "title": "Collaborative Learning and Personalization in Multi-Agent Stochastic Linear Bandits. (arXiv:2106.08902v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1\">Ayan Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1\">Himanshu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1\">Michal Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>",
          "description": "State-of-the-art transformer models use pairwise dot-product based\nself-attention, which comes at a computational cost quadratic in the input\nsequence length. In this paper, we investigate the global structure of\nattention scores computed using this dot product mechanism on a typical\ndistribution of inputs, and study the principal components of their variation.\nThrough eigen analysis of full attention score matrices, as well as of their\nindividual rows, we find that most of the variation among attention scores lie\nin a low-dimensional eigenspace. Moreover, we find significant overlap between\nthese eigenspaces for different layers and even different transformer models.\nBased on this, we propose to compute scores only for a partial subset of token\npairs, and use them to estimate scores for the remaining pairs. Beyond\ninvestigating the accuracy of reconstructing attention scores themselves, we\ninvestigate training transformer models that employ these approximations, and\nanalyze the effect on overall accuracy. Our analysis and the proposed method\nprovide insights into how to balance the benefits of exact pair-wise attention\nand its significant computational expense.",
          "link": "http://arxiv.org/abs/2106.08823",
          "publishedOn": "2021-06-17T01:58:43.128Z",
          "wordCount": 606,
          "title": "Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation. (arXiv:2106.08823v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.02077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1\">Akshay Rangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1\">Mohan M. Trivedi</a>",
          "description": "A driver's gaze is critical for determining their attention, state,\nsituational awareness, and readiness to take over control from partially\nautomated vehicles. Estimating the gaze direction is the most obvious way to\ngauge a driver's state under ideal conditions when limited to using\nnon-intrusive imaging sensors. Unfortunately, the vehicular environment\nintroduces a variety of challenges that are usually unaccounted for - harsh\nillumination, nighttime conditions, and reflective eyeglasses. Relying on head\npose alone under such conditions can prove to be unreliable and erroneous. In\nthis study, we offer solutions to address these problems encountered in the\nreal world. To solve issues with lighting, we demonstrate that using an\ninfrared camera with suitable equalization and normalization suffices. To\nhandle eyeglasses and their corresponding artifacts, we adopt image-to-image\ntranslation using generative adversarial networks to pre-process images prior\nto gaze estimation. Our proposed Gaze Preserving CycleGAN (GPCycleGAN) is\ntrained to preserve the driver's gaze while removing potential eyeglasses from\nface images. GPCycleGAN is based on the well-known CycleGAN approach - with the\naddition of a gaze classifier and a gaze consistency loss for additional\nsupervision. Our approach exhibits improved performance, interpretability,\nrobustness and superior qualitative results on challenging real-world datasets.",
          "link": "http://arxiv.org/abs/2002.02077",
          "publishedOn": "2021-06-17T01:58:43.122Z",
          "wordCount": 699,
          "title": "Gaze Preserving CycleGANs for Eyeglass Removal & Persistent Gaze Estimation. (arXiv:2002.02077v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laage_G/0/1/0/all/0/1\">Greta Laage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frejinger_E/0/1/0/all/0/1\">Emma Frejinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1\">Andrea Lodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1\">Guillaume Rabusseau</a>",
          "description": "Airlines and other industries have been making use of sophisticated Revenue\nManagement Systems to maximize revenue for decades. While improving the\ndifferent components of these systems has been the focus of numerous studies,\nestimating the impact of such improvements on the revenue has been overlooked\nin the literature despite its practical importance. Indeed, quantifying the\nbenefit of a change in a system serves as support for investment decisions.\nThis is a challenging problem as it corresponds to the difference between the\ngenerated value and the value that would have been generated keeping the system\nas before. The latter is not observable. Moreover, the expected impact can be\nsmall in relative value. In this paper, we cast the problem as counterfactual\nprediction of unobserved revenue. The impact on revenue is then the difference\nbetween the observed and the estimated revenue. The originality of this work\nlies in the innovative application of econometric methods proposed for\nmacroeconomic applications to a new problem setting. Broadly applicable, the\napproach benefits from only requiring revenue data observed for\norigin-destination pairs in the network of the airline at each day, before and\nafter a change in the system is applied. We report results using real\nlarge-scale data from Air Canada. We compare a deep neural network\ncounterfactual predictions model with econometric models. They achieve\nrespectively 1% and 1.1% of error on the counterfactual revenue predictions,\nand allow to accurately estimate small impacts (in the order of 2%).",
          "link": "http://arxiv.org/abs/2101.10249",
          "publishedOn": "2021-06-17T01:58:43.115Z",
          "wordCount": 718,
          "title": "Assessing the Impact: Does an Improvement to a Revenue Management System Lead to an Improved Revenue?. (arXiv:2101.10249v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08909",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brandfonbrener_D/0/1/0/all/0/1\">David Brandfonbrener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1\">William F. Whitney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "Most prior approaches to offline reinforcement learning (RL) have taken an\niterative actor-critic approach involving off-policy evaluation. In this paper\nwe show that simply doing one step of constrained/regularized policy\nimprovement using an on-policy Q estimate of the behavior policy performs\nsurprisingly well. This one-step algorithm beats the previously reported\nresults of iterative algorithms on a large portion of the D4RL benchmark. The\nsimple one-step baseline achieves this strong performance without many of the\ntricks used by previously proposed iterative algorithms and is more robust to\nhyperparameters. We argue that the relatively poor performance of iterative\napproaches is a result of the high variance inherent in doing off-policy\nevaluation and magnified by the repeated optimization of policies against those\nhigh-variance estimates. In addition, we hypothesize that the strong\nperformance of the one-step algorithm is due to a combination of favorable\nstructure in the environment and behavior policy.",
          "link": "http://arxiv.org/abs/2106.08909",
          "publishedOn": "2021-06-17T01:58:43.108Z",
          "wordCount": 571,
          "title": "Offline RL Without Off-Policy Evaluation. (arXiv:2106.08909v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nencka_A/0/1/0/all/0/1\">Andrew S. Nencka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherafati_M/0/1/0/all/0/1\">Mohammad Sherafati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_T/0/1/0/all/0/1\">Timothy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolat_P/0/1/0/all/0/1\">Parag Tolat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_K/0/1/0/all/0/1\">Kevin M. Koch</a>",
          "description": "Purpose: This study evaluates the effectiveness and impact of automated\norder-based protocol assignment for magnetic resonance imaging (MRI) exams\nusing natural language processing (NLP) and deep learning (DL).\n\nMethods: NLP tools were applied to retrospectively process orders from over\n116,000 MRI exams with 200 unique sub-specialized protocols (\"Local\" protocol\nclass). Separate DL models were trained on 70\\% of the processed data for\n\"Local\" protocols as well as 93 American College of Radiology (\"ACR\") protocols\nand 48 \"General\" protocols. The DL Models were assessed in an \"auto-protocoling\n(AP)\" inference mode which returns the top recommendation and in a \"clinical\ndecision support (CDS)\" inference mode which returns up to 10 protocols for\nradiologist review. The accuracy of each protocol recommendation was computed\nand analyzed based on the difference between the normalized output score of the\ncorresponding neural net for the top two recommendations.\n\nResults: The top predicted protocol in AP mode was correct for 82.8%, 73.8%,\nand 69.3% of the test cases for \"General\", \"ACR\", and \"Local\" protocol classes,\nrespectively. Higher levels of accuracy over 96% were obtained for all protocol\nclasses in CDS mode. However, at current validation performance levels, the\nproposed models offer modest, positive, financial impact on large-scale imaging\nnetworks.\n\nConclusions: DL-based protocol automation is feasible and can be tuned to\nroute substantial fractions of exams for auto-protocoling, with higher accuracy\nwith more general protocols. Economic analyses of the tested algorithms\nindicate that improved algorithm performance is required to yield a practical\nexam auto-protocoling tool for sub-specialized imaging exams.",
          "link": "http://arxiv.org/abs/2106.08963",
          "publishedOn": "2021-06-17T01:58:43.094Z",
          "wordCount": 699,
          "title": "Deep-learning based Tools for Automated Protocol Definition of Advanced Diagnostic Imaging Exams. (arXiv:2106.08963v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_A/0/1/0/all/0/1\">Arthur M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangras_V/0/1/0/all/0/1\">Vishal Rangras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnittker_G/0/1/0/all/0/1\">Georg Schnittker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waldmann_M/0/1/0/all/0/1\">Michael Waldmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friesen_M/0/1/0/all/0/1\">Maxim Friesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferfers_T/0/1/0/all/0/1\">Tobias Ferfers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreckenberg_L/0/1/0/all/0/1\">Lukas Schreckenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hufen_F/0/1/0/all/0/1\">Florian Hufen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jasperneite_J/0/1/0/all/0/1\">J&#xfc;rgen Jasperneite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1\">Marco Wiering</a>",
          "description": "Sub-optimal control policies in intersection traffic signal controllers (TSC)\ncontribute to congestion and lead to negative effects on human health and the\nenvironment. Reinforcement learning (RL) for traffic signal control is a\npromising approach to design better control policies and has attracted\nconsiderable research interest in recent years. However, most work done in this\narea used simplified simulation environments of traffic scenarios to train\nRL-based TSC. To deploy RL in real-world traffic systems, the gap between\nsimplified simulation environments and real-world applications has to be\nclosed. Therefore, we propose LemgoRL, a benchmark tool to train RL agents as\nTSC in a realistic simulation environment of Lemgo, a medium-sized town in\nGermany. In addition to the realistic simulation model, LemgoRL encompasses a\ntraffic signal logic unit that ensures compliance with all regulatory and\nsafety requirements. LemgoRL offers the same interface as the well-known OpenAI\ngym toolkit to enable easy deployment in existing research work. Our benchmark\ntool drives the development of RL algorithms towards real-world applications.\nWe provide LemgoRL as an open-source tool at https://github.com/rl-ina/lemgorl.",
          "link": "http://arxiv.org/abs/2103.16223",
          "publishedOn": "2021-06-17T01:58:43.086Z",
          "wordCount": 667,
          "title": "LemgoRL: An open-source Benchmark Tool to Train Reinforcement Learning Agents for Traffic Signal Control in a real-world simulation scenario. (arXiv:2103.16223v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Amulya Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nitin Gupta</a>",
          "description": "An outlier is an observation or a data point that is far from rest of the\ndata points in a given dataset or we can be said that an outlier is away from\nthe center of mass of observations. Presence of outliers can skew statistical\nmeasures and data distributions which can lead to misleading representation of\nthe underlying data and relationships. It is seen that the removal of outliers\nfrom the training dataset before modeling can give better predictions. With the\nadvancement of machine learning, the outlier detection models are also\nadvancing at a good pace. The goal of this work is to highlight and compare\nsome of the existing outlier detection techniques for the data scientists to\nuse that information for outlier algorithm selection while building a machine\nlearning model.",
          "link": "http://arxiv.org/abs/2106.08779",
          "publishedOn": "2021-06-17T01:58:43.074Z",
          "wordCount": 555,
          "title": "Comparison of Outlier Detection Techniques for Structured Data. (arXiv:2106.08779v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>",
          "description": "Dependent Dirichlet processes (DDP) have been widely applied to model data\nfrom distributions over collections of measures which are correlated in some\nway. On the other hand, in recent years, increasing research efforts in machine\nlearning and data mining have been dedicated to dealing with data involving\ninteractions from two or more factors. However, few researchers have addressed\nthe heterogeneous relationship in data brought by modulation of multiple\nfactors using techniques of DDP. In this paper, we propose a novel technique,\nMultiLinear Dirichlet Processes (MLDP), to constructing DDPs by combining DP\nwith a state-of-the-art factor analysis technique, multilinear factor analyzers\n(MLFA). We have evaluated MLDP on real-word data sets for different\napplications and have achieved state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.08852",
          "publishedOn": "2021-06-17T01:58:43.069Z",
          "wordCount": 525,
          "title": "Multilinear Dirichlet Processes. (arXiv:2106.08852v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meyer_A/0/1/0/all/0/1\">Angela Meyer</a>",
          "description": "Wind power is seeing a strong growth around the world. At the same time,\nshrinking profit margins in the energy markets let wind farm managers explore\noptions for cost reductions in the turbine operation and maintenance.\nSensor-based condition monitoring facilitates remote diagnostics of turbine\nsubsystems, enabling faster responses when unforeseen maintenance is required.\nCondition monitoring with data from the turbines' supervisory control and data\nacquisition (SCADA) systems was proposed and SCADA-based fault detection and\ndiagnosis approaches introduced based on single-task normal operation models of\nturbine state variables. As the number of SCADA channels has grown strongly,\nthousands of independent single-target models are in place today for monitoring\na single turbine. Multi-target learning was recently proposed to limit the\nnumber of models. This study applied multi-target neural networks to the task\nof early fault detection in drive-train components. The accuracy and delay of\ndetecting gear bearing faults were compared to state-of-the-art single-target\napproaches. We found that multi-target multi-layer perceptrons (MLPs) detected\nfaults at least as early and in many cases earlier than single-target MLPs. The\nmulti-target MLPs could detect faults up to several days earlier than the\nsingle-target models. This can deliver a significant advantage in the planning\nand performance of maintenance work. At the same time, the multi-target MLPs\nachieved the same level of prediction stability.",
          "link": "http://arxiv.org/abs/2106.08957",
          "publishedOn": "2021-06-17T01:58:43.063Z",
          "wordCount": 637,
          "title": "Early fault detection with multi-target neural networks. (arXiv:2106.08957v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08990",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Matthews_S/0/1/0/all/0/1\">Spencer Matthews</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hartman_B/0/1/0/all/0/1\">Brian Hartman</a>",
          "description": "Two-part models are important to and used throughout insurance and actuarial\nscience. Since insurance is required for registering a car, obtaining a\nmortgage, and participating in certain businesses, it is especially important\nthat the models which price insurance policies are fair and non-discriminatory.\nBlack box models can make it very difficult to know which covariates are\ninfluencing the results. SHAP values enable interpretation of various black box\nmodels, but little progress has been made in two-part models. In this paper, we\npropose mSHAP (or multiplicative SHAP), a method for computing SHAP values of\ntwo-part models using the SHAP values of the individual models. This method\nwill allow for the predictions of two-part models to be explained at an\nindividual observation level. After developing mSHAP, we perform an in-depth\nsimulation study. Although the kernelSHAP algorithm is also capable of\ncomputing approximate SHAP values for a two-part model, a comparison with our\nmethod demonstrates that mSHAP is exponentially faster. Ultimately, we apply\nmSHAP to a two-part ratemaking model for personal auto property damage\ninsurance coverage. Additionally, an R package (mshap) is available to easily\nimplement the method in a wide variety of applications.",
          "link": "http://arxiv.org/abs/2106.08990",
          "publishedOn": "2021-06-17T01:58:43.043Z",
          "wordCount": 610,
          "title": "mSHAP: SHAP Values for Two-Part Models. (arXiv:2106.08990v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cascante_Bonilla_P/0/1/0/all/0/1\">Paola Cascante-Bonilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1\">Arshdeep Sekhon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>",
          "description": "Convolutional neural networks for visual recognition require large amounts of\ntraining samples and usually benefit from data augmentation. This paper\nproposes PatchMix, a data augmentation method that creates new samples by\ncomposing patches from pairs of images in a grid-like pattern. These new\nsamples' ground truth labels are set as proportional to the number of patches\nfrom each image. We then add a set of additional losses at the patch-level to\nregularize and to encourage good representations at both the patch and image\nlevels. A ResNet-50 model trained on ImageNet using PatchMix exhibits superior\ntransfer learning capabilities across a wide array of benchmarks. Although\nPatchMix can rely on random pairings and random grid-like patterns for mixing,\nwe explore evolutionary search as a guiding strategy to discover optimal\ngrid-like patterns and image pairing jointly. For this purpose, we conceive a\nfitness function that bypasses the need to re-train a model to evaluate each\nchoice. In this way, PatchMix outperforms a base model on CIFAR-10 (+1.91),\nCIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16) by significant\nmargins, also outperforming previous state-of-the-art pairwise augmentation\nstrategies.",
          "link": "http://arxiv.org/abs/2106.09011",
          "publishedOn": "2021-06-17T01:58:43.037Z",
          "wordCount": 619,
          "title": "Evolving Image Compositions for Feature Representation Learning. (arXiv:2106.09011v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_A/0/1/0/all/0/1\">Alessio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huichapa_T/0/1/0/all/0/1\">Thaide Huichapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spoletini_P/0/1/0/all/0/1\">Paola Spoletini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novielli_N/0/1/0/all/0/1\">Nicole Novielli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fucci_D/0/1/0/all/0/1\">Davide Fucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girardi_D/0/1/0/all/0/1\">Daniela Girardi</a>",
          "description": "Capturing users engagement is crucial for gathering feedback about the\nfeatures of a software product. In a market-driven context, current approaches\nto collect and analyze users feedback are based on techniques leveraging\ninformation extracted from product reviews and social media. These approaches\nare hardly applicable in bespoke software development, or in contexts in which\none needs to gather information from specific users. In such cases, companies\nneed to resort to face-to-face interviews to get feedback on their products. In\nthis paper, we propose to utilize biometric data, in terms of physiological and\nvoice features, to complement interviews with information about the engagement\nof the user on the discussed product-relevant topics. We evaluate our approach\nby interviewing users while gathering their physiological data (i.e.,\nbiofeedback) using an Empatica E4 wristband, and capturing their voice through\nthe default audio-recorder of a common laptop. Our results show that we can\npredict users' engagement by training supervised machine learning algorithms on\nbiometric data, and that voice features alone can be sufficiently effective.\nThe performance of the prediction algorithms is maximised when pre-processing\nthe training data with the synthetic minority oversampling technique (SMOTE).\nThe results of our work suggest that biofeedback and voice analysis can be used\nto facilitate prioritization of requirements oriented to product improvement,\nand to steer the interview based on users' engagement. Furthermore, the usage\nof voice features can be particularly helpful for emotion-aware requirements\nelicitation in remote communication, either performed by human analysts or\nvoice-based chatbots.",
          "link": "http://arxiv.org/abs/2104.02410",
          "publishedOn": "2021-06-17T01:58:43.031Z",
          "wordCount": 765,
          "title": "Using Voice and Biofeedback to Predict User Engagement during Requirements Interviews. (arXiv:2104.02410v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mottini_A/0/1/0/all/0/1\">Alejandro Mottini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1\">Jaime Lorenzo-Trueba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlapati_S/0/1/0/all/0/1\">Sri Vishnu Kumar Karlapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drugman_T/0/1/0/all/0/1\">Thomas Drugman</a>",
          "description": "Voice Conversion (VC) is a technique that aims to transform the\nnon-linguistic information of a source utterance to change the perceived\nidentity of the speaker. While there is a rich literature on VC, most proposed\nmethods are trained and evaluated on clean speech recordings. However, many\nacoustic environments are noisy and reverberant, severely restricting the\napplicability of popular VC methods to such scenarios. To address this\nlimitation, we propose Voicy, a new VC framework particularly tailored for\nnoisy speech. Our method, which is inspired by the de-noising auto-encoders\nframework, is comprised of four encoders (speaker, content, phonetic and\nacoustic-ASR) and one decoder. Importantly, Voicy is capable of performing\nnon-parallel zero-shot VC, an important requirement for any VC system that\nneeds to work on speakers not seen during training. We have validated our\napproach using a noisy reverberant version of the LibriSpeech dataset.\nExperimental results show that Voicy outperforms other tested VC techniques in\nterms of naturalness and target speaker similarity in noisy reverberant\nenvironments.",
          "link": "http://arxiv.org/abs/2106.08873",
          "publishedOn": "2021-06-17T01:58:43.023Z",
          "wordCount": 610,
          "title": "Voicy: Zero-Shot Non-Parallel Voice Conversion in Noisy Reverberant Environments. (arXiv:2106.08873v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhiyuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Knowledge Graph (KG) alignment aims at finding equivalent entities and\nrelations (i.e., mappings) between two KGs. The existing approaches utilize\neither reasoning-based or semantic embedding-based techniques, but few studies\nexplore their combination. In this demonstration, we present PRASEMap, an\nunsupervised KG alignment system that iteratively computes the Mappings with\nboth Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.\nPRASEMap can support various embedding-based KG alignment approaches as the SE\nmodule, and enables easy human computer interaction that additionally provides\nan option for users to feed the mapping annotations back to the system for\nbetter results. The demonstration showcases these features via a stand-alone\nWeb application with user friendly interfaces.",
          "link": "http://arxiv.org/abs/2106.08801",
          "publishedOn": "2021-06-17T01:58:43.005Z",
          "wordCount": 552,
          "title": "PRASEMap: A Probabilistic Reasoning and Semantic Embedding based Knowledge Graph Alignment System. (arXiv:2106.08801v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1\">Karishma Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>",
          "description": "Vaccine hesitancy and misinformation on social media has increased concerns\nabout COVID-19 vaccine uptake required to achieve herd immunity and overcome\nthe pandemic. However anti-science and political misinformation and\nconspiracies have been rampant throughout the pandemic. For COVID-19 vaccines,\nwe investigate misinformation and conspiracy campaigns and their characteristic\nbehaviours. We identify whether coordinated efforts are used to promote\nmisinformation in vaccine related discussions, and find accounts coordinately\npromoting a `Great Reset' conspiracy group promoting vaccine related\nmisinformation and strong anti-vaccine and anti-social messages such as boycott\nvaccine passports, no lock-downs and masks. We characterize other\nmisinformation communities from the information diffusion structure, and study\nthe large anti-vaccine misinformation community and smaller anti-vaccine\ncommunities, including a far-right anti-vaccine conspiracy group. In comparison\nwith the mainstream and health news, left-leaning group, which are more\npro-vaccine, the right-leaning group is influenced more by the anti-vaccine and\nfar-right misinformation/conspiracy communities. The misinformation communities\nare more vocal either specific to the vaccine discussion or political\ndiscussion, and we find other differences in the characteristic behaviours of\ndifferent communities. Lastly, we investigate misinformation narratives and\ntactics of information distortion that can increase vaccine hesitancy, using\ntopic modeling and comparison with reported vaccine side-effects (VAERS)\nfinding rarer side-effects are more frequently discussed on social media.",
          "link": "http://arxiv.org/abs/2106.08423",
          "publishedOn": "2021-06-17T01:58:42.999Z",
          "wordCount": 693,
          "title": "COVID-19 Vaccines: Characterizing Misinformation Campaigns and Vaccine Hesitancy on Twitter. (arXiv:2106.08423v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaroop_S/0/1/0/all/0/1\">Siddharth Swaroop</a>",
          "description": "Humans and animals have a natural ability to quickly adapt to their\nsurroundings, but machine-learning models, when subjected to changes, often\nrequire a complete retraining from scratch. We present Knowledge-adaptation\npriors (K-priors) to reduce the cost of retraining by enabling quick and\naccurate adaptation for a wide-variety of tasks and models. This is made\npossible by a combination of weight and function-space priors to reconstruct\nthe gradients of the past, which recovers and generalizes many existing, but\nseemingly-unrelated, adaptation strategies. Training with simple first-order\ngradient methods can often recover the exact retrained model to an arbitrary\naccuracy by choosing a sufficiently large memory of the past data. Empirical\nresults confirm that the adaptation can be cheap and accurate, and a promising\nalternative to retraining.",
          "link": "http://arxiv.org/abs/2106.08769",
          "publishedOn": "2021-06-17T01:58:42.993Z",
          "wordCount": 542,
          "title": "Knowledge-Adaptation Priors. (arXiv:2106.08769v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08597",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Ning Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kairouz_P/0/1/0/all/0/1\">Peter Kairouz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ozgur_A/0/1/0/all/0/1\">Ayfer &#xd6;zg&#xfc;r</a>",
          "description": "We consider the problem of estimating a $d$-dimensional $s$-sparse discrete\ndistribution from its samples observed under a $b$-bit communication\nconstraint. The best-known previous result on $\\ell_2$ estimation error for\nthis problem is $O\\left( \\frac{s\\log\\left( {d}/{s}\\right)}{n2^b}\\right)$.\nSurprisingly, we show that when sample size $n$ exceeds a minimum threshold\n$n^*(s, d, b)$, we can achieve an $\\ell_2$ estimation error of $O\\left(\n\\frac{s}{n2^b}\\right)$. This implies that when $n>n^*(s, d, b)$ the convergence\nrate does not depend on the ambient dimension $d$ and is the same as knowing\nthe support of the distribution beforehand.\n\nWe next ask the question: ``what is the minimum $n^*(s, d, b)$ that allows\ndimension-free convergence?''. To upper bound $n^*(s, d, b)$, we develop novel\nlocalization schemes to accurately and efficiently localize the unknown\nsupport. For the non-interactive setting, we show that $n^*(s, d, b) = O\\left(\n\\min \\left( {d^2\\log^2 d}/{2^b}, {s^4\\log^2 d}/{2^b}\\right) \\right)$. Moreover,\nwe connect the problem with non-adaptive group testing and obtain a\npolynomial-time estimation scheme when $n = \\tilde{\\Omega}\\left({s^4\\log^4\nd}/{2^b}\\right)$. This group testing based scheme is adaptive to the sparsity\nparameter $s$, and hence can be applied without knowing it. For the interactive\nsetting, we propose a novel tree-based estimation scheme and show that the\nminimum sample-size needed to achieve dimension-free convergence can be further\nreduced to $n^*(s, d, b) = \\tilde{O}\\left( {s^2\\log^2 d}/{2^b} \\right)$.",
          "link": "http://arxiv.org/abs/2106.08597",
          "publishedOn": "2021-06-17T01:58:42.986Z",
          "wordCount": 651,
          "title": "Breaking The Dimension Dependence in Sparse Distribution Estimation under Communication Constraints. (arXiv:2106.08597v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hjelm_D/0/1/0/all/0/1\">Devon Hjelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>",
          "description": "Piecewise linear neural networks can be split into subfunctions, each with\nits own activation pattern, domain, and empirical error. Empirical error for\nthe full network can be written as an expectation over empirical error of\nsubfunctions. Constructing a generalization bound on subfunction empirical\nerror indicates that the more densely a subfunction is surrounded by training\nsamples in representation space, the more reliable its predictions are.\nFurther, it suggests that models with fewer activation regions generalize\nbetter, and models that abstract knowledge to a greater degree generalize\nbetter, all else equal. We propose not only a theoretical framework to reason\nabout subfunction error bounds but also a pragmatic way of approximately\nevaluating it, which we apply to predicting which samples the network will not\nsuccessfully generalize to. We test our method on detection of\nmisclassification and out-of-distribution samples, finding that it performs\ncompetitively in both cases. In short, some network activation patterns are\nassociated with higher reliability than others, and these can be identified\nusing subfunction error bounds.",
          "link": "http://arxiv.org/abs/2106.08365",
          "publishedOn": "2021-06-17T01:58:42.980Z",
          "wordCount": 604,
          "title": "Predicting Unreliable Predictions by Shattering a Neural Network. (arXiv:2106.08365v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1\">Kinjal Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hunsberger_E/0/1/0/all/0/1\">Eric Hunsberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batir_S/0/1/0/all/0/1\">Sean Batir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eliasmith_C/0/1/0/all/0/1\">Chris Eliasmith</a>",
          "description": "We seek to investigate the scalability of neuromorphic computing for computer\nvision, with the objective of replicating non-neuromorphic performance on\ncomputer vision tasks while reducing power consumption. We convert the deep\nArtificial Neural Network (ANN) architecture U-Net to a Spiking Neural Network\n(SNN) architecture using the Nengo framework. Both rate-based and spike-based\nmodels are trained and optimized for benchmarking performance and power, using\na modified version of the ISBI 2D EM Segmentation dataset consisting of\nmicroscope images of cells. We propose a partitioning method to optimize\ninter-chip communication to improve speed and energy efficiency when deploying\nmulti-chip networks on the Loihi neuromorphic chip. We explore the advantages\nof regularizing firing rates of Loihi neurons for converting ANN to SNN with\nminimum accuracy loss and optimized energy consumption. We propose a percentile\nbased regularization loss function to limit the spiking rate of the neuron\nbetween a desired range. The SNN is converted directly from the corresponding\nANN, and demonstrates similar semantic segmentation as the ANN using the same\nnumber of neurons and weights. However, the neuromorphic implementation on the\nIntel Loihi neuromorphic chip is over 2x more energy-efficient than\nconventional hardware (CPU, GPU) when running online (one image at a time).\nThese power improvements are achieved without sacrificing the task performance\naccuracy of the network, and when all weights (Loihi, CPU, and GPU networks)\nare quantized to 8 bits.",
          "link": "http://arxiv.org/abs/2106.08921",
          "publishedOn": "2021-06-17T01:58:42.973Z",
          "wordCount": 660,
          "title": "A Spiking Neural Network for Image Segmentation. (arXiv:2106.08921v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08489",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Subramanian_M/0/1/0/all/0/1\">Megha Subramanian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tipireddy_R/0/1/0/all/0/1\">Ramakrishna Tipireddy</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chatterjee_S/0/1/0/all/0/1\">Samrat Chatterjee</a>",
          "description": "Nonlinear dynamical systems such as Lorenz63 equations are known to be\nchaotic in nature and sensitive to initial conditions. As a result, a small\nperturbation in the initial conditions results in deviation in state trajectory\nafter a few time steps. The algorithms and computational resources needed to\naccurately identify the system states vary depending on whether the solution is\nin transition region or not. We refer to the transition and non-transition\nregions as unstable and stable regions respectively. We label a system state to\nbe stable if it's immediate past and future states reside in the same regime.\nHowever, at a given time step we don't have the prior knowledge about whether\nsystem is in stable or unstable region. In this paper, we develop and train a\nfeed forward (multi-layer perceptron) Neural Network to classify the system\nstates of a Lorenz system as stable and unstable. We pose this task as a\nsupervised learning problem where we train the neural network on Lorenz system\nwhich have states labeled as stable or unstable. We then test the ability of\nthe neural network models to identify the stable and unstable states on a\ndifferent Lorenz system that is generated using different initial conditions.\nWe also evaluate the classification performance in the mismatched case i.e.,\nwhen the initial conditions for training and validation data are sampled from\ndifferent intervals. We show that certain normalization schemes can greatly\nimprove the performance of neural networks in especially these mismatched\nscenarios. The classification framework developed in the paper can be a\npreprocessor for a larger context of sequential decision making framework where\nthe decision making is performed based on observed stable or unstable states.",
          "link": "http://arxiv.org/abs/2106.08489",
          "publishedOn": "2021-06-17T01:58:42.956Z",
          "wordCount": 707,
          "title": "Lorenz System State Stability Identification using Neural Networks. (arXiv:2106.08489v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghari_P/0/1/0/all/0/1\">Pouya M Ghari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanning Shen</a>",
          "description": "Online learning with expert advice is widely used in various machine learning\ntasks. It considers the problem where a learner chooses one from a set of\nexperts to take advice and make a decision. In many learning problems, experts\nmay be related, henceforth the learner can observe the losses associated with a\nsubset of experts that are related to the chosen one. In this context, the\nrelationship among experts can be captured by a feedback graph, which can be\nused to assist the learner's decision making. However, in practice, the nominal\nfeedback graph often entails uncertainties, which renders it impossible to\nreveal the actual relationship among experts. To cope with this challenge, the\npresent work studies various cases of potential uncertainties, and develops\nnovel online learning algorithms to deal with uncertainties while making use of\nthe uncertain feedback graph. The proposed algorithms are proved to enjoy\nsublinear regret under mild conditions. Experiments on real datasets are\npresented to demonstrate the effectiveness of the novel algorithms.",
          "link": "http://arxiv.org/abs/2106.08441",
          "publishedOn": "2021-06-17T01:58:42.942Z",
          "wordCount": 587,
          "title": "Online Learning with Uncertain Feedback Graphs. (arXiv:2106.08441v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fursov_I/0/1/0/all/0/1\">Ivan Fursov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morozov_M/0/1/0/all/0/1\">Matvey Morozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaploukhaya_N/0/1/0/all/0/1\">Nina Kaploukhaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovtun_E/0/1/0/all/0/1\">Elizaveta Kovtun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_Castro_R/0/1/0/all/0/1\">Rodrigo Rivera-Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusev_G/0/1/0/all/0/1\">Gleb Gusev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babaev_D/0/1/0/all/0/1\">Dmitry Babaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kireev_I/0/1/0/all/0/1\">Ivan Kireev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1\">Alexey Zaytsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "Machine learning models using transaction records as inputs are popular among\nfinancial institutions. The most efficient models use deep-learning\narchitectures similar to those in the NLP community, posing a challenge due to\ntheir tremendous number of parameters and limited robustness. In particular,\ndeep-learning models are vulnerable to adversarial attacks: a little change in\nthe input harms the model's output.\n\nIn this work, we examine adversarial attacks on transaction records data and\ndefences from these attacks. The transaction records data have a different\nstructure than the canonical NLP or time series data, as neighbouring records\nare less connected than words in sentences, and each record consists of both\ndiscrete merchant code and continuous transaction amount. We consider a\nblack-box attack scenario, where the attack doesn't know the true decision\nmodel, and pay special attention to adding transaction tokens to the end of a\nsequence. These limitations provide more realistic scenario, previously\nunexplored in NLP world.\n\nThe proposed adversarial attacks and the respective defences demonstrate\nremarkable performance using relevant datasets from the financial industry. Our\nresults show that a couple of generated transactions are sufficient to fool a\ndeep-learning model. Further, we improve model robustness via adversarial\ntraining or separate adversarial examples detection. This work shows that\nembedding protection from adversarial attacks improves model robustness,\nallowing a wider adoption of deep models for transaction records in banking and\nfinance.",
          "link": "http://arxiv.org/abs/2106.08361",
          "publishedOn": "2021-06-17T01:58:42.936Z",
          "wordCount": 669,
          "title": "Adversarial Attacks on Deep Models for Financial Transaction Records. (arXiv:2106.08361v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_W/0/1/0/all/0/1\">Waddah Saeed</a>",
          "description": "Short Message Service (SMS) is a very popular service used for communication\nby mobile users. However, this popular service can be abused by executing\nillegal activities and influencing security risks. Nowadays, many automatic\nmachine learning (AutoML) tools exist which can help domain experts and lay\nusers to build high-quality ML models with little or no machine learning\nknowledge. In this work, a classification performance comparison was conducted\nbetween three automatic ML tools for SMS spam message filtering. These tools\nare mljar-supervised AutoML, H2O AutoML, and Tree-based Pipeline Optimization\nTool (TPOT) AutoML. Experimental results showed that ensemble models achieved\nthe best classification performance. The Stacked Ensemble model, which was\nbuilt using H2O AutoML, achieved the best performance in terms of Log Loss\n(0.8370), true positive (1088/1116), and true negative (281/287) metrics. There\nis a 19.05\\% improvement in Log Loss with respect to TPOT AutoML and 10.53\\%\nimprovement with respect to mljar-supervised AutoML. The satisfactory filtering\nperformance achieved with AutoML tools provides a potential application for\nAutoML tools to automatically determine the best ML model that can perform best\nfor SMS spam message filtering.",
          "link": "http://arxiv.org/abs/2106.08671",
          "publishedOn": "2021-06-17T01:58:42.886Z",
          "wordCount": 610,
          "title": "Comparison of Automated Machine Learning Tools for SMS Spam Message Filtering. (arXiv:2106.08671v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velmurugan_M/0/1/0/all/0/1\">Mythreyi Velmurugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Chun Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_C/0/1/0/all/0/1\">Catarina Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindhgatta_R/0/1/0/all/0/1\">Renuka Sindhgatta</a>",
          "description": "Although modern machine learning and deep learning methods allow for complex\nand in-depth data analytics, the predictive models generated by these methods\nare often highly complex, and lack transparency. Explainable AI (XAI) methods\nare used to improve the interpretability of these complex models, and in doing\nso improve transparency. However, the inherent fitness of these explainable\nmethods can be hard to evaluate. In particular, methods to evaluate the\nfidelity of the explanation to the underlying black box require further\ndevelopment, especially for tabular data. In this paper, we (a) propose a three\nphase approach to developing an evaluation method; (b) adapt an existing\nevaluation method primarily for image and text data to evaluate models trained\non tabular data; and (c) evaluate two popular explainable methods using this\nevaluation method. Our evaluations suggest that the internal mechanism of the\nunderlying predictive model, the internal mechanism of the explainable method\nused and model and data complexity all affect explanation fidelity. Given that\nexplanation fidelity is so sensitive to context and tools and data used, we\ncould not clearly identify any specific explainable method as being superior to\nanother.",
          "link": "http://arxiv.org/abs/2106.08492",
          "publishedOn": "2021-06-17T01:58:42.871Z",
          "wordCount": 616,
          "title": "Developing a Fidelity Evaluation Approach for Interpretable Machine Learning. (arXiv:2106.08492v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08814",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Raymaekers_J/0/1/0/all/0/1\">Jakob Raymaekers</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rousseeuw_P/0/1/0/all/0/1\">Peter J. Rousseeuw</a>",
          "description": "Classification by neural nets and by tree-based methods are powerful tools of\nmachine learning. There exist interesting visualizations of the inner workings\nof these and other classifiers. Here we pursue a different goal, which is to\nvisualize the cases being classified, either in training data or in test data.\nAn important aspect is whether a case has been classified to its given class\n(label) or whether the classifier wants to assign it to different class. This\nis reflected in the (conditional and posterior) probability of the alternative\nclass (PAC). A high PAC indicates label bias, i.e. the possibility that the\ncase was mislabeled. The PAC is used to construct a silhouette plot which is\nsimilar in spirit to the silhouette plot for cluster analysis (Rousseeuw,\n1987). The average silhouette width can be used to compare different\nclassifications of the same dataset. We will also draw quasi residual plots of\nthe PAC versus a data feature, which may lead to more insight in the data. One\nof these data features is how far each case lies from its given class. The\ngraphical displays are illustrated and interpreted on benchmark data sets\ncontaining images, mixed features, and tweets.",
          "link": "http://arxiv.org/abs/2106.08814",
          "publishedOn": "2021-06-17T01:58:42.820Z",
          "wordCount": 627,
          "title": "Silhouettes and quasi residual plots for neural nets and tree-based classifiers. (arXiv:2106.08814v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boopathy_A/0/1/0/all/0/1\">Akhilan Boopathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiete_I/0/1/0/all/0/1\">Ila Fiete</a>",
          "description": "Recent works have examined how deep neural networks, which can solve a\nvariety of difficult problems, incorporate the statistics of training data to\nachieve their success. However, existing results have been established only in\nlimited settings. In this work, we derive the layerwise weight dynamics of\ninfinite-width neural networks with nonlinear activations trained by gradient\ndescent. We show theoretically that weight updates are aligned with input\ncorrelations from intermediate layers weighted by error, and demonstrate\nempirically that the result also holds in finite-width wide networks. The\nalignment result allows us to formulate backpropagation-free learning rules,\nnamed Align-zero and Align-ada, that theoretically achieve the same alignment\nas backpropagation. Finally, we test these learning rules on benchmark problems\nin feedforward and recurrent neural networks and demonstrate, in wide networks,\ncomparable performance to backpropagation.",
          "link": "http://arxiv.org/abs/2106.08453",
          "publishedOn": "2021-06-17T01:58:42.765Z",
          "wordCount": 569,
          "title": "Gradient-trained Weights in Wide Neural Networks Align Layerwise to Error-scaled Input Correlations. (arXiv:2106.08453v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chanti_D/0/1/0/all/0/1\">Dawood Al Chanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mateus_D/0/1/0/all/0/1\">Diana Mateus</a>",
          "description": "This paper addresses the domain shift problem for segmentation. As a\nsolution, we propose OLVA, a novel and lightweight unsupervised domain\nadaptation method based on a Variational Auto-Encoder (VAE) and Optimal\nTransport (OT) theory. Thanks to the VAE, our model learns a shared\ncross-domain latent space that follows a normal distribution, which reduces the\ndomain shift. To guarantee valid segmentations, our shared latent space is\ndesigned to model the shape rather than the intensity variations. We further\nrely on an OT loss to match and align the remaining discrepancy between the two\ndomains in the latent space. We demonstrate OLVA's effectiveness for the\nsegmentation of multiple cardiac structures on the public Multi-Modality Whole\nHeart Segmentation (MM-WHS) dataset, where the source domain consists of\nannotated 3D MR images and the unlabelled target domain of 3D CTs. Our results\nshow remarkable improvements with an additional margin of 12.5\\% dice score\nover concurrent generative training approaches.",
          "link": "http://arxiv.org/abs/2106.08188",
          "publishedOn": "2021-06-16T01:21:12.922Z",
          "wordCount": 600,
          "title": "Optimal Latent Vector Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation. (arXiv:2106.08188v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "In spoken conversational question answering (SCQA), the answer to the\ncorresponding question is generated by retrieving and then analyzing a fixed\nspoken document, including multi-part conversations. Most SCQA systems have\nconsidered only retrieving information from ordered utterances. However, the\nsequential order of dialogue is important to build a robust spoken\nconversational question answering system, and the changes of utterances order\nmay severely result in low-quality and incoherent corpora. To this end, we\nintroduce a self-supervised learning approach, including incoherence\ndiscrimination, insertion detection, and question prediction, to explicitly\ncapture the coreference resolution and dialogue coherence among spoken\ndocuments. Specifically, we design a joint learning framework where the\nauxiliary self-supervised tasks can enable the pre-trained SCQA systems towards\nmore coherent and meaningful spoken dialogue learning. We also utilize the\nproposed self-supervised learning tasks to capture intra-sentence coherence.\nExperimental results demonstrate that our proposed method provides more\ncoherent, meaningful, and appropriate responses, yielding superior performance\ngains compared to the original pre-trained language models. Our method achieves\nstate-of-the-art results on the Spoken-CoQA dataset.",
          "link": "http://arxiv.org/abs/2106.02182",
          "publishedOn": "2021-06-16T01:21:12.915Z",
          "wordCount": 630,
          "title": "Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yandong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuanming Shi</a>",
          "description": "In this paper, we consider decentralized federated learning (FL) over\nwireless networks, where over-the-air computation (AirComp) is adopted to\nfacilitate the local model consensus in a device-to-device (D2D) communication\nmanner. However, the AirComp-based consensus phase brings the additive noise in\neach algorithm iterate and the consensus needs to be robust to wireless network\ntopology changes, which introduce a coupled and novel challenge of establishing\nthe convergence for wireless decentralized FL algorithm. To facilitate\nconsensus phase, we propose an AirComp-based DSGD with gradient tracking and\nvariance reduction (DSGT-VR) algorithm, where both precoding and decoding\nstrategies are developed for D2D communication. Furthermore, we prove that the\nproposed algorithm converges linearly and establish the optimality gap for\nstrongly convex and smooth loss functions, taking into account the channel\nfading and noise. The theoretical result shows that the additional error bound\nin the optimality gap depends on the number of devices. Extensive simulations\nverify the theoretical results and show that the proposed algorithm outperforms\nother benchmark decentralized FL algorithms over wireless networks.",
          "link": "http://arxiv.org/abs/2106.08011",
          "publishedOn": "2021-06-16T01:21:12.908Z",
          "wordCount": 597,
          "title": "Over-the-Air Decentralized Federated Learning. (arXiv:2106.08011v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kangning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yiqiu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1\">Jakub Ch&#x142;&#x119;dowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>",
          "description": "In the last few years, deep learning classifiers have shown promising results\nin image-based medical diagnosis. However, interpreting the outputs of these\nmodels remains a challenge. In cancer diagnosis, interpretability can be\nachieved by localizing the region of the input image responsible for the\noutput, i.e. the location of a lesion. Alternatively, segmentation or detection\nmodels can be trained with pixel-wise annotations indicating the locations of\nmalignant lesions. Unfortunately, acquiring such labels is labor-intensive and\nrequires medical expertise. To overcome this difficulty, weakly-supervised\nlocalization can be utilized. These methods allow neural network classifiers to\noutput saliency maps highlighting the regions of the input most relevant to the\nclassification task (e.g. malignant lesions in mammograms) using only\nimage-level labels (e.g. whether the patient has cancer or not) during\ntraining. When applied to high-resolution images, existing methods produce\nlow-resolution saliency maps. This is problematic in applications in which\nsuspicious lesions are small in relation to the image size. In this work, we\nintroduce a novel neural network architecture to perform weakly-supervised\nsegmentation of high-resolution images. The proposed model selects regions of\ninterest via coarse-level localization, and then performs fine-grained\nsegmentation of those regions. We apply this model to breast cancer diagnosis\nwith screening mammography, and validate it on a large clinically-realistic\ndataset. Measured by Dice similarity score, our approach outperforms existing\nmethods by a large margin in terms of localization performance of benign and\nmalignant lesions, relatively improving the performance by 39.6% and 20.0%,\nrespectively. Code and the weights of some of the models are available at\nhttps://github.com/nyukat/GLAM",
          "link": "http://arxiv.org/abs/2106.07049",
          "publishedOn": "2021-06-16T01:21:12.902Z",
          "wordCount": 734,
          "title": "Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis. (arXiv:2106.07049v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alakuijala_M/0/1/0/all/0/1\">Minttu Alakuijala</a> (WILLOW, Thoth), <a href=\"http://arxiv.org/find/cs/1/au:+Dulac_Arnold_G/0/1/0/all/0/1\">Gabriel Dulac-Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1\">Julien Mairal</a> (Thoth), <a href=\"http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1\">Jean Ponce</a> (WILLOW), <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>",
          "description": "Residual reinforcement learning (RL) has been proposed as a way to solve\nchallenging robotic tasks by adapting control actions from a conventional\nfeedback controller to maximize a reward signal. We extend the residual\nformulation to learn from visual inputs and sparse rewards using\ndemonstrations. Learning from images, proprioceptive inputs and a sparse\ntask-completion reward relaxes the requirement of accessing full state\nfeatures, such as object and target positions. In addition, replacing the base\ncontroller with a policy learned from demonstrations removes the dependency on\na hand-engineered controller in favour of a dataset of demonstrations, which\ncan be provided by non-experts. Our experimental evaluation on simulated\nmanipulation tasks on a 6-DoF UR5 arm and a 28-DoF dexterous hand demonstrates\nthat residual RL from demonstrations is able to generalize to unseen\nenvironment conditions more flexibly than either behavioral cloning or RL\nfine-tuning, and is capable of solving high-dimensional, sparse-reward tasks\nout of reach for RL from scratch.",
          "link": "http://arxiv.org/abs/2106.08050",
          "publishedOn": "2021-06-16T01:21:12.895Z",
          "wordCount": 582,
          "title": "Residual Reinforcement Learning from Demonstrations. (arXiv:2106.08050v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rizzo_M/0/1/0/all/0/1\">Matteo Rizzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conati_C/0/1/0/all/0/1\">Cristina Conati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1\">Daesik Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hui Hu</a>",
          "description": "Computational Colour Constancy (CCC) consists of estimating the colour of one\nor more illuminants in a scene and using them to remove unwanted chromatic\ndistortions. Much research has focused on illuminant estimation for CCC on\nsingle images, with few attempts of leveraging the temporal information\nintrinsic in sequences of correlated images (e.g., the frames in a video), a\ntask known as Temporal Colour Constancy (TCC). The state-of-the-art for TCC is\nTCCNet, a deep-learning architecture that uses a ConvLSTM for aggregating the\nencodings produced by CNN submodules for each image in a sequence. We extend\nthis architecture with different models obtained by (i) substituting the TCCNet\nsubmodules with C4, the state-of-the-art method for CCC targeting images; (ii)\nadding a cascading strategy to perform an iterative improvement of the estimate\nof the illuminant. We tested our models on the recently released TCC benchmark\nand achieved results that surpass the state-of-the-art. Analyzing the impact of\nthe number of frames involved in illuminant estimation on performance, we show\nthat it is possible to reduce inference time by training the models on few\nselected frames from the sequences while retaining comparable accuracy.",
          "link": "http://arxiv.org/abs/2106.07955",
          "publishedOn": "2021-06-16T01:21:12.878Z",
          "wordCount": 614,
          "title": "Cascading Convolutional Temporal Colour Constancy. (arXiv:2106.07955v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gianinazzi_L/0/1/0/all/0/1\">Lukas Gianinazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fries_M/0/1/0/all/0/1\">Maximilian Fries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dryden_N/0/1/0/all/0/1\">Nikoli Dryden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Nun_T/0/1/0/all/0/1\">Tal Ben-Nun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besta_M/0/1/0/all/0/1\">Maciej Besta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1\">Torsten Hoefler</a>",
          "description": "We present a graph neural network to learn graph coloring heuristics using\nreinforcement learning. Our learned deterministic heuristics give better\nsolutions than classical degree-based greedy heuristics and only take seconds\nto evaluate on graphs with tens of thousands of vertices. As our approach is\nbased on policy-gradients, it also learns a probabilistic policy as well. These\nprobabilistic policies outperform all greedy coloring baselines and a machine\nlearning baseline. Our approach generalizes several previous machine-learning\nframeworks, which applied to problems like minimum vertex cover. We also\ndemonstrate that our approach outperforms two greedy heuristics on minimum\nvertex cover.",
          "link": "http://arxiv.org/abs/2106.03594",
          "publishedOn": "2021-06-16T01:21:12.871Z",
          "wordCount": 540,
          "title": "Learning Combinatorial Node Labeling Algorithms. (arXiv:2106.03594v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaoliang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jianzhong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Chenglu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Rongshan Yu</a>",
          "description": "Fairness has emerged as a critical problem in federated learning (FL). In\nthis work, we identify a cause of unfairness in FL -- conflicting gradients\nwith large differences in the magnitudes. To address this issue, we propose the\nfederated fair averaging (FedFV) algorithm to mitigate potential conflicts\namong clients before averaging their gradients. We first use the cosine\nsimilarity to detect gradient conflicts, and then iteratively eliminate such\nconflicts by modifying both the direction and the magnitude of the gradients.\nWe further show the theoretical foundation of FedFV to mitigate the issue\nconflicting gradients and converge to Pareto stationary solutions. Extensive\nexperiments on a suite of federated datasets confirm that FedFV compares\nfavorably against state-of-the-art methods in terms of fairness, accuracy and\nefficiency.",
          "link": "http://arxiv.org/abs/2104.14937",
          "publishedOn": "2021-06-16T01:21:12.865Z",
          "wordCount": 595,
          "title": "Federated Learning with Fair Averaging. (arXiv:2104.14937v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jourdan_T/0/1/0/all/0/1\">Th&#xe9;o Jourdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutet_A/0/1/0/all/0/1\">Antoine Boutet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frindel_C/0/1/0/all/0/1\">Carole Frindel</a>",
          "description": "Federated Learning (FL) is a collaborative scheme to train a learning model\nacross multiple participants without sharing data. While FL is a clear step\nforward towards enforcing users' privacy, different inference attacks have been\ndeveloped. In this paper, we quantify the utility and privacy trade-off of a FL\nscheme using private personalized layers. While this scheme has been proposed\nas local adaptation to improve the accuracy of the model through local\npersonalization, it has also the advantage to minimize the information about\nthe model exchanged with the server. However, the privacy of such a scheme has\nnever been quantified. Our evaluations using motion sensor dataset show that\npersonalized layers speed up the convergence of the model and slightly improve\nthe accuracy for all users compared to a standard FL scheme while better\npreventing both attribute and membership inferences compared to a FL scheme\nusing local differential privacy.",
          "link": "http://arxiv.org/abs/2106.08060",
          "publishedOn": "2021-06-16T01:21:12.859Z",
          "wordCount": 581,
          "title": "Privacy Assessment of Federated Learning using Private Personalized Layers. (arXiv:2106.08060v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>",
          "description": "Transformers have achieved great success in many artificial intelligence\nfields, such as natural language processing, computer vision, and audio\nprocessing. Therefore, it is natural to attract lots of interest from academic\nand industry researchers. Up to the present, a great variety of Transformer\nvariants (a.k.a. X-formers) have been proposed, however, a systematic and\ncomprehensive literature review on these Transformer variants is still missing.\nIn this survey, we provide a comprehensive review of various X-formers. We\nfirst briefly introduce the vanilla Transformer and then propose a new taxonomy\nof X-formers. Next, we introduce the various X-formers from three perspectives:\narchitectural modification, pre-training, and applications. Finally, we outline\nsome potential directions for future research.",
          "link": "http://arxiv.org/abs/2106.04554",
          "publishedOn": "2021-06-16T01:21:12.833Z",
          "wordCount": 554,
          "title": "A Survey of Transformers. (arXiv:2106.04554v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10306",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pfohl_S/0/1/0/all/0/1\">Stephen R. Pfohl</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Foryciarz_A/0/1/0/all/0/1\">Agata Foryciarz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shah_N/0/1/0/all/0/1\">Nigam H. Shah</a>",
          "description": "The use of machine learning to guide clinical decision making has the\npotential to worsen existing health disparities. Several recent works frame the\nproblem as that of algorithmic fairness, a framework that has attracted\nconsiderable attention and criticism. However, the appropriateness of this\nframework is unclear due to both ethical as well as technical considerations,\nthe latter of which include trade-offs between measures of fairness and model\nperformance that are not well-understood for predictive models of clinical\noutcomes. To inform the ongoing debate, we conduct an empirical study to\ncharacterize the impact of penalizing group fairness violations on an array of\nmeasures of model performance and group fairness. We repeat the analyses across\nmultiple observational healthcare databases, clinical outcomes, and sensitive\nattributes. We find that procedures that penalize differences between the\ndistributions of predictions across groups induce nearly-universal degradation\nof multiple performance metrics within groups. On examining the secondary\nimpact of these procedures, we observe heterogeneity of the effect of these\nprocedures on measures of fairness in calibration and ranking across\nexperimental conditions. Beyond the reported trade-offs, we emphasize that\nanalyses of algorithmic fairness in healthcare lack the contextual grounding\nand causal awareness necessary to reason about the mechanisms that lead to\nhealth disparities, as well as about the potential of algorithmic fairness\nmethods to counteract those mechanisms. In light of these limitations, we\nencourage researchers building predictive models for clinical use to step\noutside the algorithmic fairness frame and engage critically with the broader\nsociotechnical context surrounding the use of machine learning in healthcare.",
          "link": "http://arxiv.org/abs/2007.10306",
          "publishedOn": "2021-06-16T01:21:12.815Z",
          "wordCount": 754,
          "title": "An Empirical Characterization of Fair Machine Learning For Clinical Risk Prediction. (arXiv:2007.10306v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08161",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1\">Adam Foster</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vezer_A/0/1/0/all/0/1\">&#xc1;rpi Vez&#xe9;r</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Glastonbury_C/0/1/0/all/0/1\">Craig A Glastonbury</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Creed_P/0/1/0/all/0/1\">P&#xe1;id&#xed; Creed</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Abujudeh_S/0/1/0/all/0/1\">Sam Abujudeh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sim_A/0/1/0/all/0/1\">Aaron Sim</a>",
          "description": "Learning meaningful representations of data that can address challenges such\nas batch effect correction, data integration and counterfactual inference is a\ncentral problem in many domains including computational biology. Adopting a\nConditional VAE framework, we identify the mathematical principle that unites\nthese challenges: learning a representation that is marginally independent of a\ncondition variable. We therefore propose the Contrastive Mixture of Posteriors\n(CoMP) method that uses a novel misalignment penalty to enforce this\nindependence. This penalty is defined in terms of mixtures of the variational\nposteriors themselves, unlike prior work which uses external discrepancy\nmeasures such as MMD to ensure independence in latent space. We show that CoMP\nhas attractive theoretical properties compared to previous approaches,\nespecially when there is complex global structure in latent space. We further\ndemonstrate state of the art performance on a number of real-world problems,\nincluding the challenging tasks of aligning human tumour samples with cancer\ncell-lines and performing counterfactual inference on single-cell RNA\nsequencing data. Incidentally, we find parallels with the fair representation\nlearning literature, and demonstrate CoMP has competitive performance in\nlearning fair yet expressive latent representations.",
          "link": "http://arxiv.org/abs/2106.08161",
          "publishedOn": "2021-06-16T01:21:12.807Z",
          "wordCount": 627,
          "title": "Contrastive Mixture of Posteriors for Counterfactual Inference, Data Integration and Fairness. (arXiv:2106.08161v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1\">Sean Papay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>",
          "description": "In structured prediction, a major challenge for models is to represent the\ninterdependencies within their output structures. For the common case where\noutputs are structured as a sequence, linear-chain conditional random fields\n(CRFs) are a widely used model class which can learn local dependencies in\noutput sequences. However, the CRF's Markov assumption makes it impossible for\nthese models to capture nonlocal dependencies, and standard CRFs are unable to\nrespect nonlocal constraints of the data (such as global arity constraints on\noutput labels). We present a generalization of CRFs that can enforce a broad\nclass of constraints, including nonlocal ones, by specifying the space of\npossible output structures as a regular language $\\mathcal{L}$. The resulting\nregular-constrained CRF (RegCCRF) has the same formal properties as a standard\nCRF, but assigns zero probability to all label sequences not in $\\mathcal{L}$.\nNotably, RegCCRFs can incorporate their constraints during training, while\nrelated models only enforce constraints during decoding. We prove that\nconstrained training is never worse than constrained decoding, and show using\nsynthetic data that it can be substantially better in practice. Additionally,\nwe demonstrate a practical benefit on downstream tasks by incorporating a\nRegCCRF into a deep neural model for semantic role labeling, exceeding\nstate-of-the-art results on a standard dataset.",
          "link": "http://arxiv.org/abs/2106.07306",
          "publishedOn": "2021-06-16T01:21:12.800Z",
          "wordCount": 646,
          "title": "Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04651",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ponnoprat_D/0/1/0/all/0/1\">Donlapark Ponnoprat</a>",
          "description": "The Wasserstein distance provides a notion of dissimilarities between\nprobability measures, which has recent applications in learning of structured\ndata with varying size such as images and text documents. In this work, we\nanalyze the $k$-nearest neighbor classifier ($k$-NN) under the Wasserstein\ndistance and establish the universal consistency on families of distributions.\nUsing previous known results on the consistency of the $k$-NN classifier on\ninfinite dimensional metric spaces, it suffices to show that the families is a\ncountable union of finite dimension sets. As a result, we show that the $k$-NN\nclassifier is universally consistent on spaces of finitely supported measures,\nthe space of Gaussian measures, and the space of measures with finite wavelet\ndensities. In addition, we give a counterexample to show that the universal\nconsistency does not hold on $\\mathcal{W}_p((0,1))$.",
          "link": "http://arxiv.org/abs/2009.04651",
          "publishedOn": "2021-06-16T01:21:12.794Z",
          "wordCount": 591,
          "title": "Universal consistency of Wasserstein $k$-NN classifier. (arXiv:2009.04651v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1\">Ivan Grubi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1\">Marin Or&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>",
          "description": "Semi-supervised learning is especially interesting in the dense prediction\ncontext due to high cost of pixel-level ground truth. Unfortunately, most such\napproaches are evaluated on outdated architectures which hamper research due to\nvery slow training and high requirements on GPU RAM. We address this concern by\npresenting a simple and effective baseline which works very well both on\nstandard and efficient architectures. Our baseline is based on one-way\nconsistency and non-linear geometric and photometric perturbations. We show\nadvantage of perturbing only the student branch and present a plausible\nexplanation of such behaviour. Experiments on Cityscapes and CIFAR-10\ndemonstrate competitive performance with respect to prior work.",
          "link": "http://arxiv.org/abs/2106.07075",
          "publishedOn": "2021-06-16T01:21:12.771Z",
          "wordCount": 558,
          "title": "A baseline for semi-supervised learning of efficient semantic segmentation models. (arXiv:2106.07075v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08048",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Tseytlin_B/0/1/0/all/0/1\">Boris Tseytlin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Makarov_I/0/1/0/all/0/1\">Ilya Makarov</a>",
          "description": "During a long-running pandemic a pathogen can mutate, producing new strains\nwith different epidemiological parameters. Existing approaches to epidemic\nmodelling only consider one virus strain. We have developed a modified SEIR\nmodel to simulate multiple virus strains within the same population. As a case\nstudy, we investigate the potential effects of SARS-CoV-2 strain B.1.1.7 on the\ncity of Moscow. Our analysis indicates a high risk of a new wave of infections\nin September-October 2021 with up to 35 000 daily infections at peak. We\nopen-source our code and data.",
          "link": "http://arxiv.org/abs/2106.08048",
          "publishedOn": "2021-06-16T01:21:12.744Z",
          "wordCount": 575,
          "title": "Epidemic modelling of multiple virus strains:a case study of SARS-CoV-2 B.1.1.7 in Moscow. (arXiv:2106.08048v1 [q-bio.PE])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biktairov_Y/0/1/0/all/0/1\">Yuriy Biktairov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stebelev_M/0/1/0/all/0/1\">Maxim Stebelev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudenko_I/0/1/0/all/0/1\">Irina Rudenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shliazhko_O/0/1/0/all/0/1\">Oleh Shliazhko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangel_B/0/1/0/all/0/1\">Boris Yangel</a>",
          "description": "Predicting the motion of agents such as pedestrians or human-driven vehicles\nis one of the most critical problems in the autonomous driving domain. The\noverall safety of driving and the comfort of a passenger directly depend on its\nsuccessful solution. The motion prediction problem also remains one of the most\nchallenging problems in autonomous driving engineering, mainly due to high\nvariance of the possible agent's future behavior given a situation. The two\nphenomena responsible for the said variance are the multimodality caused by the\nuncertainty of the agent's intent (e.g., turn right or move forward) and\nuncertainty in the realization of a given intent (e.g., which lane to turn\ninto). To be useful within a real-time autonomous driving pipeline, a motion\nprediction system must provide efficient ways to describe and quantify this\nuncertainty, such as computing posterior modes and their probabilities or\nestimating density at the point corresponding to a given trajectory. It also\nshould not put substantial density on physically impossible trajectories, as\nthey can confuse the system processing the predictions. In this paper, we\nintroduce the PRANK method, which satisfies these requirements. PRANK takes\nrasterized bird-eye images of agent's surroundings as an input and extracts\nfeatures of the scene with a convolutional neural network. It then produces the\nconditional distribution of agent's trajectories plausible in the given scene.\nThe key contribution of PRANK is a way to represent that distribution using\nnearest-neighbor methods in latent trajectory space, which allows for efficient\ninference in real time. We evaluate PRANK on the in-house and Argoverse\ndatasets, where it shows competitive results.",
          "link": "http://arxiv.org/abs/2010.12007",
          "publishedOn": "2021-06-16T01:21:12.602Z",
          "wordCount": 715,
          "title": "PRANK: motion Prediction based on RANKing. (arXiv:2010.12007v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_V/0/1/0/all/0/1\">Vinu Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1\">Shoaib Ahmed Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskara_A/0/1/0/all/0/1\">Aditya Bhaskara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_G/0/1/0/all/0/1\">Ganesh Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_S/0/1/0/all/0/1\">Saurav Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garland_M/0/1/0/all/0/1\">Michael Garland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sheraz Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>",
          "description": "With the rise in edge-computing devices, there has been an increasing demand\nto deploy energy and resource-efficient models. A large body of research has\nbeen devoted to developing methods that can reduce the size of the model\nconsiderably without affecting the standard metrics such as top-1 accuracy.\nHowever, these pruning approaches tend to result in a significant mismatch in\nother metrics such as fairness across classes and explainability. To combat\nsuch misalignment, we propose a novel multi-part loss function inspired by the\nknowledge-distillation literature. Through extensive experiments, we\ndemonstrate the effectiveness of our approach across different compression\nalgorithms, architectures, tasks as well as datasets. In particular, we obtain\nup to $4.1\\times$ reduction in the number of prediction mismatches between the\ncompressed and reference models, and up to $5.7\\times$ in cases where the\nreference model makes the correct prediction; all while making no changes to\nthe compression algorithm, and minor modifications to the loss function.\nFurthermore, we demonstrate how inducing simple alignment between the\npredictions of the models naturally improves the alignment on other metrics\nincluding fairness and attributions. Our framework can thus serve as a simple\nplug-and-play component for compression algorithms in the future.",
          "link": "http://arxiv.org/abs/2012.01604",
          "publishedOn": "2021-06-16T01:21:12.595Z",
          "wordCount": 673,
          "title": "Going Beyond Classification Accuracy Metrics in Model Compression. (arXiv:2012.01604v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02298",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shuo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lining Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yifan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoqiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1\">Kun Gai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kuang-chih Lee</a>",
          "description": "Modern online advertising systems inevitably rely on personalization methods,\nsuch as click-through rate (CTR) prediction. Recent progress in CTR prediction\nenjoys the rich representation capabilities of deep learning and achieves great\nsuccess in large-scale industrial applications. However, these methods can\nsuffer from lack of exploration. Another line of prior work addresses the\nexploration-exploitation trade-off problem with contextual bandit methods,\nwhich are recently less studied in the industry due to the difficulty in\nextending their flexibility with deep models. In this paper, we propose a novel\nDeep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on\nGaussian processes, which can provide predictive uncertainty estimations while\nmaintaining the flexibility of deep neural networks. DUAL can be easily\nimplemented on existing models and deployed in real-time systems with minimal\nextra computational overhead. By linking the predictive uncertainty estimation\nability of DUAL to well-known bandit algorithms, we further present DUAL-based\nAd-ranking strategies to boost up long-term utilities such as the social\nwelfare in advertising systems. Experimental results on several public datasets\ndemonstrate the effectiveness of our methods. Remarkably, an online A/B test\ndeployed in the Alibaba display advertising platform shows an 8.2% social\nwelfare improvement and an 8.0% revenue lift.",
          "link": "http://arxiv.org/abs/2012.02298",
          "publishedOn": "2021-06-16T01:21:12.587Z",
          "wordCount": 675,
          "title": "Exploration in Online Advertising Systems with Deep Uncertainty-Aware Learning. (arXiv:2012.02298v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13511",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yuan_B/0/1/0/all/0/1\">Bowen Yuan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yu-Sheng Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Quan_P/0/1/0/all/0/1\">Pengrui Quan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lin_C/0/1/0/all/0/1\">Chih-Jen Lin</a>",
          "description": "We study the problem of learning similarity by using nonlinear embedding\nmodels (e.g., neural networks) from all possible pairs. This problem is\nwell-known for its difficulty of training with the extreme number of pairs. For\nthe special case of using linear embeddings, many studies have addressed this\nissue of handling all pairs by considering certain loss functions and\ndeveloping efficient optimization algorithms. This paper aims to extend results\nfor general nonlinear embeddings. First, we finish detailed derivations and\nprovide clean formulations for efficiently calculating some building blocks of\noptimization algorithms such as function, gradient evaluation, and\nHessian-vector product. The result enables the use of many optimization methods\nfor extreme similarity learning with nonlinear embeddings. Second, we study\nsome optimization methods in detail. Due to the use of nonlinear embeddings,\nimplementation issues different from linear cases are addressed. In the end,\nsome methods are shown to be highly efficient for extreme similarity learning\nwith nonlinear embeddings.",
          "link": "http://arxiv.org/abs/2010.13511",
          "publishedOn": "2021-06-16T01:21:12.532Z",
          "wordCount": 616,
          "title": "Efficient Optimization Methods for Extreme Similarity Learning with Nonlinear Embeddings. (arXiv:2010.13511v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_O/0/1/0/all/0/1\">Omid Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raut_P/0/1/0/all/0/1\">Prasanna Raut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazel_M/0/1/0/all/0/1\">Maryam Fazel</a>",
          "description": "In this paper, we consider an online optimization problem over $T$ rounds\nwhere at each step $t\\in[T]$, the algorithm chooses an action $x_t$ from the\nfixed convex and compact domain set $\\mathcal{K}$. A utility function\n$f_t(\\cdot)$ is then revealed and the algorithm receives the payoff $f_t(x_t)$.\nThis problem has been previously studied under the assumption that the\nutilities are adversarially chosen monotone DR-submodular functions and\n$\\mathcal{O}(\\sqrt{T})$ regret bounds have been derived. We first characterize\nthe class of strongly DR-submodular functions and then, we derive regret bounds\nfor the following new online settings: $(1)$ $\\{f_t\\}_{t=1}^T$ are monotone\nstrongly DR-submodular and chosen adversarially, $(2)$ $\\{f_t\\}_{t=1}^T$ are\nmonotone submodular (while the average $\\frac{1}{T}\\sum_{t=1}^T f_t$ is\nstrongly DR-submodular) and chosen by an adversary but they arrive in a\nuniformly random order, $(3)$ $\\{f_t\\}_{t=1}^T$ are drawn i.i.d. from some\nunknown distribution $f_t\\sim \\mathcal{D}$ where the expected function\n$f(\\cdot)=\\mathbb{E}_{f_t\\sim\\mathcal{D}}[f_t(\\cdot)]$ is monotone\nDR-submodular. For $(1)$, we obtain the first logarithmic regret bounds. In\nterms of the second framework, we show that it is possible to obtain similar\nlogarithmic bounds with high probability. Finally, for the i.i.d. model, we\nprovide algorithms with $\\tilde{\\mathcal{O}}(\\sqrt{T})$ stochastic regret\nbound, both in expectation and with high probability. Experimental results\ndemonstrate that our algorithms outperform the previous techniques in the\naforementioned three settings.",
          "link": "http://arxiv.org/abs/2106.07836",
          "publishedOn": "2021-06-16T01:21:11.497Z",
          "wordCount": 641,
          "title": "Improved Regret Bounds for Online Submodular Maximization. (arXiv:2106.07836v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05441",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kaiser_M/0/1/0/all/0/1\">Marcus Kaiser</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sipos_M/0/1/0/all/0/1\">Maksim Sipos</a>",
          "description": "Causal Discovery methods aim to identify a DAG structure that represents\ncausal relationships from observational data. In this article, we stress that\nit is important to test such methods for robustness in practical settings. As\nour main example, we analyze the NOTEARS method, for which we demonstrate a\nlack of scale-invariance. We show that NOTEARS is a method that aims to\nidentify a parsimonious DAG from the data that explains the residual variance.\nWe conclude that NOTEARS is not suitable for identifying truly causal\nrelationships from the data.",
          "link": "http://arxiv.org/abs/2104.05441",
          "publishedOn": "2021-06-16T01:21:11.489Z",
          "wordCount": 537,
          "title": "Unsuitability of NOTEARS for Causal Graph Discovery. (arXiv:2104.05441v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutt_A/0/1/0/all/0/1\">Arkopal Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lokhov_A/0/1/0/all/0/1\">Andrey Y. Lokhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuffray_M/0/1/0/all/0/1\">Marc Vuffray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_S/0/1/0/all/0/1\">Sidhant Misra</a>",
          "description": "The usual setting for learning the structure and parameters of a graphical\nmodel assumes the availability of independent samples produced from the\ncorresponding multivariate probability distribution. However, for many models\nthe mixing time of the respective Markov chain can be very large and i.i.d.\nsamples may not be obtained. We study the problem of reconstructing binary\ngraphical models from correlated samples produced by a dynamical process, which\nis natural in many applications. We analyze the sample complexity of two\nestimators that are based on the interaction screening objective and the\nconditional likelihood loss. We observe that for samples coming from a\ndynamical process far from equilibrium, the sample complexity reduces\nexponentially compared to a dynamical process that mixes quickly.",
          "link": "http://arxiv.org/abs/2104.00995",
          "publishedOn": "2021-06-16T01:21:11.483Z",
          "wordCount": 604,
          "title": "Exponential Reduction in Sample Complexity with Learning of Ising Model Dynamics. (arXiv:2104.00995v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.10119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenghui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1\">Barnabas Poczos</a>",
          "description": "Program synthesis has emerged as a successful approach to the image parsing\ntask. Most prior works rely on a two-step scheme involving supervised\npretraining of a Seq2Seq model with synthetic programs followed by\nreinforcement learning (RL) for fine-tuning with real reference images. Fully\nunsupervised approaches promise to train the model directly on the target\nimages without requiring curated pretraining datasets. However, they struggle\nwith the inherent sparsity of meaningful programs in the search space. In this\npaper, we present the first unsupervised algorithm capable of parsing\nconstructive solid geometry (CSG) images into context-free grammar (CFG)\nwithout pretraining via non-differentiable renderer. To tackle the\n\\emph{non-Markovian} sparse reward problem, we combine three key ingredients --\n(i) a grammar-encoded tree LSTM ensuring program validity (ii) entropy\nregularization and (iii) sampling without replacement from the CFG syntax tree.\nEmpirically, our algorithm recovers meaningful programs in large search spaces\n(up to $3.8 \\times 10^{28}$). Further, even though our approach is fully\nunsupervised, it generalizes better than supervised methods on the synthetic 2D\nCSG dataset. On the 2D computer aided design (CAD) dataset, our approach\nsignificantly outperforms the supervised pretrained model and is competitive to\nthe refined model.",
          "link": "http://arxiv.org/abs/2001.10119",
          "publishedOn": "2021-06-16T01:21:11.476Z",
          "wordCount": 657,
          "title": "Unsupervised Program Synthesis for Images By Sampling Without Replacement. (arXiv:2001.10119v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1\">Markus Wulfmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">Dushyant Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafner_R/0/1/0/all/0/1\">Roland Hafner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampe_T/0/1/0/all/0/1\">Thomas Lampe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdolmaleki_A/0/1/0/all/0/1\">Abbas Abdolmaleki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertweck_T/0/1/0/all/0/1\">Tim Hertweck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neunert_M/0/1/0/all/0/1\">Michael Neunert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tirumala_D/0/1/0/all/0/1\">Dhruva Tirumala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegel_N/0/1/0/all/0/1\">Noah Siegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1\">Nicolas Heess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1\">Martin Riedmiller</a>",
          "description": "We introduce Hindsight Off-policy Options (HO2), a data-efficient option\nlearning algorithm. Given any trajectory, HO2 infers likely option choices and\nbackpropagates through the dynamic programming inference procedure to robustly\ntrain all policy components off-policy and end-to-end. The approach outperforms\nexisting option learning methods on common benchmarks. To better understand the\noption framework and disentangle benefits from both temporal and action\nabstraction, we evaluate ablations with flat policies and mixture policies with\ncomparable optimization. The results highlight the importance of both types of\nabstraction as well as off-policy training and trust-region constraints,\nparticularly in challenging, simulated 3D robot manipulation tasks from raw\npixel inputs. Finally, we intuitively adapt the inference step to investigate\nthe effect of increased temporal abstraction on training with pre-trained\noptions and from scratch.",
          "link": "http://arxiv.org/abs/2007.15588",
          "publishedOn": "2021-06-16T01:21:11.448Z",
          "wordCount": 605,
          "title": "Data-efficient Hindsight Off-policy Option Learning. (arXiv:2007.15588v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial robustness of deep neural networks is an extensively studied\nproblem in the literature and various methods have been proposed to defend\nagainst adversarial images. However, only a handful of defense methods have\nbeen developed for defending against attacked videos. In this paper, we propose\na novel Over-and-Under complete restoration network for Defending against\nadversarial videos (OUDefend). Most restoration networks adopt an\nencoder-decoder architecture that first shrinks spatial dimension then expands\nit back. This approach learns undercomplete representations, which have large\nreceptive fields to collect global information but overlooks local details. On\nthe other hand, overcomplete representations have opposite properties. Hence,\nOUDefend is designed to balance local and global features by learning those two\nrepresentations. We attach OUDefend to target video recognition models as a\nfeature restoration block and train the entire network end-to-end. Experimental\nresults show that the defenses focusing on images may be ineffective to videos,\nwhile OUDefend enhances robustness against different types of adversarial\nvideos, ranging from additive attacks, multiplicative attacks to physically\nrealizable attacks. Code: https://github.com/shaoyuanlo/OUDefend",
          "link": "http://arxiv.org/abs/2012.04262",
          "publishedOn": "2021-06-16T01:21:11.441Z",
          "wordCount": 647,
          "title": "Overcomplete Representations Against Adversarial Videos. (arXiv:2012.04262v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seidl_P/0/1/0/all/0/1\">Philipp Seidl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renz_P/0/1/0/all/0/1\">Philipp Renz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyubankova_N/0/1/0/all/0/1\">Natalia Dyubankova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_P/0/1/0/all/0/1\">Paulo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verhoeven_J/0/1/0/all/0/1\">Jonas Verhoeven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segler_M/0/1/0/all/0/1\">Marwin Segler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">J&#xf6;rg K. Wegner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1\">Sepp Hochreiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1\">G&#xfc;nter Klambauer</a>",
          "description": "Finding synthesis routes for molecules of interest is an essential step in\nthe discovery of new drugs and materials. To find such routes,\ncomputer-assisted synthesis planning (CASP) methods are employed which rely on\na model of chemical reactivity. In this study, we model single-step\nretrosynthesis in a template-based approach using modern Hopfield networks\n(MHNs). We adapt MHNs to associate different modalities, reaction templates and\nmolecules, which allows the model to leverage structural information about\nreaction templates. This approach significantly improves the performance of\ntemplate relevance prediction, especially for templates with few or zero\ntraining examples. With inference speed several times faster than that of\nbaseline methods, we improve predictive performance for top-k exact match\naccuracy for $\\mathrm{k}\\geq5$ in the retrosynthesis benchmark USPTO-50k.",
          "link": "http://arxiv.org/abs/2104.03279",
          "publishedOn": "2021-06-16T01:21:11.434Z",
          "wordCount": 620,
          "title": "Modern Hopfield Networks for Few- and Zero-Shot Reaction Template Prediction. (arXiv:2104.03279v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1\">Sekitoshi Kanai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1\">Masanori Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_H/0/1/0/all/0/1\">Hiroshi Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamanaka_Y/0/1/0/all/0/1\">Yuki Yamanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ida_Y/0/1/0/all/0/1\">Yasutoshi Ida</a>",
          "description": "Deep neural networks are vulnerable to adversarial attacks. Recent studies\nabout adversarial robustness focus on the loss landscape in the parameter space\nsince it is related to optimization and generalization performance. These\nstudies conclude that the difficulty of adversarial training is caused by the\nnon-smoothness of the loss function: i.e., its gradient is not Lipschitz\ncontinuous. However, this analysis ignores the dependence of adversarial\nattacks on model parameters. Since adversarial attacks are optimized for\nmodels, they should depend on the parameters. Considering this dependence, we\nanalyze the smoothness of the loss function of adversarial training using the\noptimal attacks for the model parameter in more detail. We reveal that the\nconstraint of adversarial attacks is one cause of the non-smoothness and that\nthe smoothness depends on the types of the constraints. Specifically, the\n$L_\\infty$ constraint can cause non-smoothness more than the $L_2$ constraint.\nMoreover, our analysis implies that if we flatten the loss function with\nrespect to input data, the Lipschitz constant of the gradient of adversarial\nloss tends to increase. To address the non-smoothness, we show that EntropySGD\nsmoothens the non-smooth loss and improves the performance of adversarial\ntraining.",
          "link": "http://arxiv.org/abs/2103.01400",
          "publishedOn": "2021-06-16T01:21:11.427Z",
          "wordCount": 674,
          "title": "Smoothness Analysis of Adversarial Training. (arXiv:2103.01400v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.11743",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Whang_J/0/1/0/all/0/1\">Jay Whang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lindgren_E/0/1/0/all/0/1\">Erik M. Lindgren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>",
          "description": "Given an inverse problem with a normalizing flow prior, we wish to estimate\nthe distribution of the underlying signal conditioned on the observations. We\napproach this problem as a task of conditional inference on the pre-trained\nunconditional flow model. We first establish that this is computationally hard\nfor a large class of flow models. Motivated by this, we propose a framework for\napproximate inference that estimates the target conditional as a composition of\ntwo flow models. This formulation leads to a stable variational inference\ntraining procedure that avoids adversarial training. Our method is evaluated on\na variety of inverse problems and is shown to produce high-quality samples with\nuncertainty quantification. We further demonstrate that our approach can be\namortized for zero-shot inference.",
          "link": "http://arxiv.org/abs/2002.11743",
          "publishedOn": "2021-06-16T01:21:11.421Z",
          "wordCount": 578,
          "title": "Composing Normalizing Flows for Inverse Problems. (arXiv:2002.11743v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15727",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yueqi Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_Y/0/1/0/all/0/1\">Yoonho Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Basu_P/0/1/0/all/0/1\">Pallab Basu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1\">Juho Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Paninski_L/0/1/0/all/0/1\">Liam Paninski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pakman_A/0/1/0/all/0/1\">Ari Pakman</a>",
          "description": "Learning community structures in graphs has broad applications across\nscientific domains. While graph neural networks (GNNs) have been successful in\nencoding graph structures, existing GNN-based methods for community detection\nare limited by requiring knowledge of the number of communities in advance, in\naddition to lacking a proper probabilistic formulation to handle uncertainty.\nWe propose a simple framework for amortized community detection, which\naddresses both of these issues by combining the expressive power of GNNs with\nrecent methods for amortized clustering. Our models consist of a graph\nrepresentation backbone that extracts structural information and an amortized\nclustering network that naturally handles variable numbers of clusters. Both\ncomponents combine into well-defined models of the posterior distribution of\ngraph communities and are jointly optimized given labeled graphs. At inference\ntime, the models yield parallel samples from the posterior of community labels,\nquantifying uncertainty in a principled way. We evaluate several models from\nour framework on synthetic and real datasets and demonstrate superior\nperformance to previous methods. As a separate contribution, we extend recent\namortized probabilistic clustering architectures by adding attention modules,\nwhich yield further improvements on community detection tasks.",
          "link": "http://arxiv.org/abs/2010.15727",
          "publishedOn": "2021-06-16T01:21:11.383Z",
          "wordCount": 645,
          "title": "Amortized Probabilistic Detection of Communities in Graphs. (arXiv:2010.15727v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1\">Marten van Dijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nhuong V. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Toan N. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Lam M. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Ha Nguyen</a>",
          "description": "In the context of DP-SGD each round communicates a local SGD update which\nleaks some new information about the underlying local data set to the outside\nworld. In order to provide privacy, Gaussian noise is added to local SGD\nupdates. However, privacy leakage still aggregates over multiple training\nrounds. Therefore, in order to control privacy leakage over an increasing\nnumber of training rounds, we need to increase the added Gaussian noise per\nlocal SGD update. This dependence of the amount of Gaussian noise $\\sigma$ on\nthe number of training rounds $T$ may impose an impractical upper bound on $T$\n(because $\\sigma$ cannot be too large) leading to a low accuracy global model\n(because the global model receives too few local SGD updates). DP-SGD much less\ncompetitive compared to other existing privacy techniques.\n\nWe show for the first time that for $(\\epsilon,\\delta)$-differential privacy\n$\\sigma$ can be chosen equal to $\\sqrt{2(\\epsilon +\\ln(1/\\delta))/\\epsilon}$\nregardless the total number of training rounds $T$. In other words, $\\sigma$\ndoes not depend on $T$ anymore (and aggregation of privacy leakage increases to\na limit). This important discovery brings DP-SGD to practice because $\\sigma$\ncan remain small to make the trained model have high accuracy even for large\n$T$ as usually happens in practice.",
          "link": "http://arxiv.org/abs/2102.09030",
          "publishedOn": "2021-06-16T01:21:11.364Z",
          "wordCount": 706,
          "title": "Bringing Differential Private SGD to Practice: On the Independence of Gaussian Noise and the Number of Training Rounds. (arXiv:2102.09030v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.11231",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mekkaoui_K/0/1/0/all/0/1\">Khaoula El Mekkaoui</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mesquita_D/0/1/0/all/0/1\">Diego Mesquita</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Blomstedt_P/0/1/0/all/0/1\">Paul Blomstedt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kaski_S/0/1/0/all/0/1\">Samuel Kaski</a>",
          "description": "Stochastic gradient MCMC methods, such as stochastic gradient Langevin\ndynamics (SGLD), employ fast but noisy gradient estimates to enable large-scale\nposterior sampling. Although we can easily extend SGLD to distributed settings,\nit suffers from two issues when applied to federated non-IID data. First, the\nvariance of these estimates increases significantly. Second, delaying\ncommunication causes the Markov chains to diverge from the true posterior even\nfor very simple models. To alleviate both these problems, we propose conducive\ngradients, a simple mechanism that combines local likelihood approximations to\ncorrect gradient updates. Notably, conducive gradients are easy to compute, and\nsince we only calculate the approximations once, they incur negligible\noverhead. We apply conducive gradients to distributed stochastic gradient\nLangevin dynamics (DSGLD) and call the resulting method federated stochastic\ngradient Langevin dynamics (FSGLD). We demonstrate that our approach can handle\ndelayed communication rounds, converging to the target posterior in cases where\nDSGLD fails. We also show that FSGLD outperforms DSGLD for non-IID federated\ndata with experiments on metric learning and neural networks.",
          "link": "http://arxiv.org/abs/2004.11231",
          "publishedOn": "2021-06-16T01:21:11.356Z",
          "wordCount": 624,
          "title": "Federated Stochastic Gradient Langevin Dynamics. (arXiv:2004.11231v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">C.-H. Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_M/0/1/0/all/0/1\">Mohit Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Y.-C. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Quan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_T/0/1/0/all/0/1\">Tomoaki Yoshinaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1\">Tomokazu Murakami</a>",
          "description": "Physical processes, camera movement, and unpredictable environmental\nconditions like the presence of dust can induce noise and artifacts in video\nfeeds. We observe that popular unsupervised MOT methods are dependent on\nnoise-free inputs. We show that the addition of a small amount of artificial\nrandom noise causes a sharp degradation in model performance on benchmark\nmetrics. We resolve this problem by introducing a robust unsupervised\nmulti-object tracking (MOT) model: AttU-Net. The proposed single-head attention\nmodel helps limit the negative impact of noise by learning visual\nrepresentations at different segment scales. AttU-Net shows better unsupervised\nMOT tracking performance over variational inference-based state-of-the-art\nbaselines. We evaluate our method in the MNIST-MOT and the Atari game video\nbenchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''\nwhich consists of moving Japanese characters and ``Fashion-MNIST MOT'' to\nvalidate the effectiveness of the MOT models.",
          "link": "http://arxiv.org/abs/2105.10005",
          "publishedOn": "2021-06-16T01:21:11.349Z",
          "wordCount": 644,
          "title": "Robust Unsupervised Multi-Object Tracking in Noisy Environments. (arXiv:2105.10005v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11436",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Robey_A/0/1/0/all/0/1\">Alexander Robey</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1\">Hamed Hassani</a>",
          "description": "Despite remarkable success in a variety of applications, it is well-known\nthat deep learning can fail catastrophically when presented with\nout-of-distribution data. Toward addressing this challenge, we consider the\ndomain generalization problem, wherein predictors are trained using data drawn\nfrom a family of related training domains and then evaluated on a distinct and\nunseen test domain. We show that under a natural model of data generation and a\nconcomitant invariance condition, the domain generalization problem is\nequivalent to an infinite-dimensional constrained statistical learning problem;\nthis problem forms the basis of our approach, which we call Model-Based Domain\nGeneralization. Due to the inherent challenges in solving constrained\noptimization problems in deep learning, we exploit nonconvex duality theory to\ndevelop unconstrained relaxations of this statistical problem with tight bounds\non the duality gap. Based on this theoretical motivation, we propose a novel\ndomain generalization algorithm with convergence guarantees. In our\nexperiments, we report improvements of up to 30 percentage points over\nstate-of-the-art domain generalization baselines on several benchmarks\nincluding ColoredMNIST, Camelyon17-WILDS, FMoW-WILDS, and PACS.",
          "link": "http://arxiv.org/abs/2102.11436",
          "publishedOn": "2021-06-16T01:21:11.342Z",
          "wordCount": 610,
          "title": "Model-Based Domain Generalization. (arXiv:2102.11436v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.14180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Congliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haozhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>",
          "description": "In this paper, we present a distributed variant of adaptive stochastic\ngradient method for training deep neural networks in the parameter-server\nmodel. To reduce the communication cost among the workers and server, we\nincorporate two types of quantization schemes, i.e., gradient quantization and\nweight quantization, into the proposed distributed Adam. Besides, to reduce the\nbias introduced by quantization operations, we propose an error-feedback\ntechnique to compensate for the quantized gradient. Theoretically, in the\nstochastic nonconvex setting, we show that the distributed adaptive gradient\nmethod with gradient quantization and error-feedback converges to the\nfirst-order stationary point, and that the distributed adaptive gradient method\nwith weight quantization and error-feedback converges to the point related to\nthe quantized level under both the single-worker and multi-worker modes. At\nlast, we apply the proposed distributed adaptive gradient methods to train deep\nneural networks. Experimental results demonstrate the efficacy of our methods.",
          "link": "http://arxiv.org/abs/2004.14180",
          "publishedOn": "2021-06-16T01:21:11.321Z",
          "wordCount": 621,
          "title": "Quantized Adam with Error Feedback. (arXiv:2004.14180v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1\">Yangjun Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1\">Karen Ullrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severo_D/0/1/0/all/0/1\">Daniel Severo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Townsend_J/0/1/0/all/0/1\">James Townsend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1\">Ashish Khisti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makhzani_A/0/1/0/all/0/1\">Alireza Makhzani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1\">Chris J. Maddison</a>",
          "description": "Latent variable models have been successfully applied in lossless compression\nwith the bits-back coding algorithm. However, bits-back suffers from an\nincrease in the bitrate equal to the KL divergence between the approximate\nposterior and the true posterior. In this paper, we show how to remove this gap\nasymptotically by deriving bits-back coding algorithms from tighter variational\nbounds. The key idea is to exploit extended space representations of Monte\nCarlo estimators of the marginal likelihood. Naively applied, our schemes would\nrequire more initial bits than the standard bits-back coder, but we show how to\ndrastically reduce this additional cost with couplings in the latent space.\nWhen parallel architectures can be exploited, our coders can achieve better\nrates than bits-back with little additional cost. We demonstrate improved\nlossless compression rates in a variety of settings, especially in\nout-of-distribution or sequential data compression.",
          "link": "http://arxiv.org/abs/2102.11086",
          "publishedOn": "2021-06-16T01:21:11.305Z",
          "wordCount": 621,
          "title": "Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding. (arXiv:2102.11086v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.14215",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Russell_R/0/1/0/all/0/1\">Rebecca L. Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reale_C/0/1/0/all/0/1\">Christopher Reale</a>",
          "description": "Deep learning has the potential to dramatically impact navigation and\ntracking state estimation problems critical to autonomous vehicles and\nrobotics. Measurement uncertainties in state estimation systems based on Kalman\nand other Bayes filters are typically assumed to be a fixed covariance matrix.\nThis assumption is risky, particularly for \"black box\" deep learning models, in\nwhich uncertainty can vary dramatically and unexpectedly. Accurate\nquantification of multivariate uncertainty will allow for the full potential of\ndeep learning to be used more safely and reliably in these applications. We\nshow how to model multivariate uncertainty for regression problems with neural\nnetworks, incorporating both aleatoric and epistemic sources of heteroscedastic\nuncertainty. We train a deep uncertainty covariance matrix model in two ways:\ndirectly using a multivariate Gaussian density loss function, and indirectly\nusing end-to-end training through a Kalman filter. We experimentally show in a\nvisual tracking problem the large impact that accurate multivariate uncertainty\nquantification can have on Kalman filter performance for both in-domain and\nout-of-domain evaluation data. We additionally show in a challenging visual\nodometry problem how end-to-end filter training can allow uncertainty\npredictions to compensate for filter weaknesses.",
          "link": "http://arxiv.org/abs/1910.14215",
          "publishedOn": "2021-06-16T01:21:11.298Z",
          "wordCount": 657,
          "title": "Multivariate Uncertainty in Deep Learning. (arXiv:1910.14215v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gillenwater_J/0/1/0/all/0/1\">Jennifer Gillenwater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_M/0/1/0/all/0/1\">Matthew Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulesza_A/0/1/0/all/0/1\">Alex Kulesza</a>",
          "description": "Quantiles are often used for summarizing and understanding data. If that data\nis sensitive, it may be necessary to compute quantiles in a way that is\ndifferentially private, providing theoretical guarantees that the result does\nnot reveal private information. However, when multiple quantiles are needed,\nexisting differentially private algorithms fare poorly: they either compute\nquantiles individually, splitting the privacy budget, or summarize the entire\ndistribution, wasting effort. In either case the result is reduced accuracy. In\nthis work we propose an instance of the exponential mechanism that\nsimultaneously estimates exactly $m$ quantiles from $n$ data points while\nguaranteeing differential privacy. The utility function is carefully structured\nto allow for an efficient implementation that returns estimates of all $m$\nquantiles in time $O(mn\\log(n) + m^2n)$. Experiments show that our method\nsignificantly outperforms the current state of the art on both real and\nsynthetic data while remaining efficient enough to be practical.",
          "link": "http://arxiv.org/abs/2102.08244",
          "publishedOn": "2021-06-16T01:21:11.291Z",
          "wordCount": 609,
          "title": "Differentially Private Quantiles. (arXiv:2102.08244v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafarnia_Jahromi_M/0/1/0/all/0/1\">Mehdi Jafarnia-Jahromi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rahul Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayyar_A/0/1/0/all/0/1\">Ashutosh Nayyar</a>",
          "description": "Solving Partially Observable Markov Decision Processes (POMDPs) is hard.\nLearning optimal controllers for POMDPs when the model is unknown is harder.\nOnline learning of optimal controllers for unknown POMDPs, which requires\nefficient learning using regret-minimizing algorithms that effectively tradeoff\nexploration and exploitation, is even harder, and no solution exists currently.\nIn this paper, we consider infinite-horizon average-cost POMDPs with unknown\ntransition model, though a known observation model. We propose a natural\nposterior sampling-based reinforcement learning algorithm (PSRL-POMDP) and show\nthat it achieves a regret bound of $O(\\log T)$, where $T$ is the time horizon,\nwhen the parameter set is finite. In the general case (continuous parameter\nset), we show that the algorithm achieves $O (T^{2/3})$ regret under two\ntechnical assumptions. To the best of our knowledge, this is the first online\nRL algorithm for POMDPs and has sub-linear regret.",
          "link": "http://arxiv.org/abs/2102.12661",
          "publishedOn": "2021-06-16T01:21:11.275Z",
          "wordCount": 587,
          "title": "Online Learning for Unknown Partially Observable MDPs. (arXiv:2102.12661v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pande_H/0/1/0/all/0/1\">Harshit Pande</a>",
          "description": "Click-through rate (CTR) prediction models are common in many online\napplications such as digital advertising and recommender systems. Field-Aware\nFactorization Machine (FFM) and Field-weighted Factorization Machine (FwFM) are\nstate-of-the-art among the shallow models for CTR prediction. Recently, many\ndeep learning-based models have also been proposed. Among deeper models,\nDeepFM, xDeepFM, AutoInt+, and FiBiNet are state-of-the-art models. The deeper\nmodels combine a core architectural component, which learns explicit feature\ninteractions, with a deep neural network (DNN) component. We propose a novel\nshallow Field-Embedded Factorization Machine (FEFM) and its deep counterpart\nDeep Field-Embedded Factorization Machine (DeepFEFM). FEFM learns symmetric\nmatrix embeddings for each field pair along with the usual single vector\nembeddings for each feature. FEFM has significantly lower model complexity than\nFFM and roughly the same complexity as FwFM. FEFM also has insightful\nmathematical properties about important fields and field interactions. DeepFEFM\ncombines the FEFM interaction vectors learned by the FEFM component with a DNN\nand is thus able to learn higher order interactions. We conducted comprehensive\nexperiments over a wide range of hyperparameters on two large publicly\navailable real-world datasets. When comparing test AUC and log loss, the\nresults show that FEFM and DeepFEFM outperform the existing state-of-the-art\nshallow and deep models for CTR prediction tasks. We have made the code of FEFM\nand DeepFEFM available in the DeepCTR library\n(https://github.com/shenweichen/DeepCTR).",
          "link": "http://arxiv.org/abs/2009.09931",
          "publishedOn": "2021-06-16T01:21:11.264Z",
          "wordCount": 675,
          "title": "Field-Embedded Factorization Machines for Click-through rate prediction. (arXiv:2009.09931v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.08898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tong_T/0/1/0/all/0/1\">Tian Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1\">Yuejie Chi</a>",
          "description": "Low-rank matrix estimation is a canonical problem that finds numerous\napplications in signal processing, machine learning and imaging science. A\npopular approach in practice is to factorize the matrix into two compact\nlow-rank factors, and then optimize these factors directly via simple iterative\nmethods such as gradient descent and alternating minimization. Despite\nnonconvexity, recent literatures have shown that these simple heuristics in\nfact achieve linear convergence when initialized properly for a growing number\nof problems of interest. However, upon closer examination, existing approaches\ncan still be computationally expensive especially for ill-conditioned matrices:\nthe convergence rate of gradient descent depends linearly on the condition\nnumber of the low-rank matrix, while the per-iteration cost of alternating\nminimization is often prohibitive for large matrices. The goal of this paper is\nto set forth a competitive algorithmic approach dubbed Scaled Gradient Descent\n(ScaledGD) which can be viewed as pre-conditioned or diagonally-scaled gradient\ndescent, where the pre-conditioners are adaptive and iteration-varying with a\nminimal computational overhead. With tailored variants for low-rank matrix\nsensing, robust principal component analysis and matrix completion, we\ntheoretically show that ScaledGD achieves the best of both worlds: it converges\nlinearly at a rate independent of the condition number of the low-rank matrix\nsimilar as alternating minimization, while maintaining the low per-iteration\ncost of gradient descent. Our analysis is also applicable to general loss\nfunctions that are restricted strongly convex and smooth over low-rank\nmatrices. To the best of our knowledge, ScaledGD is the first algorithm that\nprovably has such properties over a wide range of low-rank matrix estimation\ntasks.",
          "link": "http://arxiv.org/abs/2005.08898",
          "publishedOn": "2021-06-16T01:21:11.246Z",
          "wordCount": 758,
          "title": "Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent. (arXiv:2005.08898v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.06837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1\">Nan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaochun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Duo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Dongrui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhimin Tang</a>",
          "description": "Many meta-learning methods are proposed for few-shot detection. However,\nprevious most methods have two main problems, poor detection APs, and strong\nbias because of imbalance and insufficient datasets. Previous works mainly\nalleviate these issues by additional datasets, multi-relation attention\nmechanisms and sub-modules. However, they require more cost. In this work, for\nmeta-learning, we find that the main challenges focus on related or irrelevant\nsemantic features between categories. Therefore, based on semantic features, we\npropose a Top-C classification loss (i.e., TCL-C) for classification task and a\ncategory-based grouping mechanism for category-based meta-features obtained by\nthe meta-model. The TCL-C exploits the true-label prediction and the most\nlikely C-1 false classification predictions to improve detection performance on\nfew-shot classes. According to similar appearance (i.e., visual appearance,\nshape, and limbs etc.) and environment in which objects often appear, the\ncategory-based grouping mechanism splits categories into disjoint groups to\nmake similar semantic features more compact between categories within a group\nand obtain more significant difference between groups, alleviating the strong\nbias problem and further improving detection APs. The whole training consists\nof the base model and the fine-tuning phases. According to grouping mechanism,\nwe group the meta-features vectors obtained by meta-model, so that the\ndistribution difference between groups is obvious, and the one within each\ngroup is less. Extensive experiments on Pascal VOC dataset demonstrate that\nours which combines the TCL-C with category-based grouping significantly\noutperforms previous state-of-the-art methods for few-shot detection. Compared\nwith previous competitive baseline, ours improves detection APs by almost 4%\nfor few-shot detection.",
          "link": "http://arxiv.org/abs/2007.06837",
          "publishedOn": "2021-06-16T01:21:11.214Z",
          "wordCount": 762,
          "title": "Top-Related Meta-Learning Method for Few-Shot Object Detection. (arXiv:2007.06837v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aiguo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Bei Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>",
          "description": "Conventional Supervised Learning approaches focus on the mapping from input\nfeatures to output labels. After training, the learnt models alone are adapted\nonto testing features to predict testing labels in isolation, with training\ndata wasted and their associations ignored. To take full advantage of the vast\nnumber of training data and their associations, we propose a novel learning\nparadigm called Memory-Associated Differential (MAD) Learning. We first\nintroduce an additional component called Memory to memorize all the training\ndata. Then we learn the differences of labels as well as the associations of\nfeatures in the combination of a differential equation and some sampling\nmethods. Finally, in the evaluating phase, we predict unknown labels by\ninferencing from the memorized facts plus the learnt differences and\nassociations in a geometrically meaningful manner. We gently build this theory\nin unary situations and apply it on Image Recognition, then extend it into Link\nPrediction as a binary situation, in which our method outperforms strong\nstate-of-the-art baselines on ogbl-ddi dataset.",
          "link": "http://arxiv.org/abs/2102.05246",
          "publishedOn": "2021-06-16T01:21:11.201Z",
          "wordCount": 617,
          "title": "Memory-Associated Differential Learning. (arXiv:2102.05246v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10246",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alemohammad_S/0/1/0/all/0/1\">Sina Alemohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard Baraniuk</a>",
          "description": "The study of deep neural networks (DNNs) in the infinite-width limit, via the\nso-called neural tangent kernel (NTK) approach, has provided new insights into\nthe dynamics of learning, generalization, and the impact of initialization. One\nkey DNN architecture remains to be kernelized, namely, the recurrent neural\nnetwork (RNN). In this paper we introduce and study the Recurrent Neural\nTangent Kernel (RNTK), which provides new insights into the behavior of\noverparametrized RNNs. A key property of the RNTK should greatly benefit\npractitioners is its ability to compare inputs of different length. To this\nend, we characterize how the RNTK weights different time steps to form its\noutput under different initialization parameters and nonlinearity choices. A\nsynthetic and 56 real-world data experiments demonstrate that the RNTK offers\nsignificant performance gains over other kernels, including standard NTKs,\nacross a wide array of data sets.",
          "link": "http://arxiv.org/abs/2006.10246",
          "publishedOn": "2021-06-16T01:21:11.176Z",
          "wordCount": 609,
          "title": "The Recurrent Neural Tangent Kernel. (arXiv:2006.10246v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barik_A/0/1/0/all/0/1\">Adarsh Barik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1\">Jean Honorio</a>",
          "description": "In this paper, we study the problem of fair sparse regression on a biased\ndataset where bias depends upon a hidden binary attribute. The presence of a\nhidden attribute adds an extra layer of complexity to the problem by combining\nsparse regression and clustering with unknown binary labels. The corresponding\noptimization problem is combinatorial, but we propose a novel relaxation of it\nas an \\emph{invex} optimization problem. To the best of our knowledge, this is\nthe first invex relaxation for a combinatorial problem. We show that the\ninclusion of the debiasing/fairness constraint in our model has no adverse\neffect on the performance. Rather, it enables the recovery of the hidden\nattribute. The support of our recovered regression parameter vector matches\nexactly with the true parameter vector. Moreover, we simultaneously solve the\nclustering problem by recovering the exact value of the hidden attribute for\neach sample. Our method uses carefully constructed primal dual witnesses to\nprovide theoretical guarantees for the combinatorial problem. To that end, we\nshow that the sample complexity of our method is logarithmic in terms of the\ndimension of the regression parameter vector.",
          "link": "http://arxiv.org/abs/2102.09704",
          "publishedOn": "2021-06-16T01:21:11.157Z",
          "wordCount": 640,
          "title": "Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem. (arXiv:2102.09704v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haitian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>",
          "description": "We present the Open Predicate Query Language (OPQL); a method for\nconstructing a virtual KB (VKB) trained entirely from text. Large Knowledge\nBases (KBs) are indispensable for a wide-range of industry applications such as\nquestion answering and recommendation. Typically, KBs encode world knowledge in\na structured, readily accessible form derived from laborious human annotation\nefforts. Unfortunately, while they are extremely high precision, KBs are\ninevitably highly incomplete and automated methods for enriching them are far\ntoo inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set\nof relation mentions in a way that naturally enables reasoning and can be\ntrained without any structured supervision. We demonstrate that OPQL\noutperforms prior VKB methods on two different KB reasoning tasks and,\nadditionally, can be used as an external memory integrated into a language\nmodel (OPQL-LM) leading to improvements on two open-domain question answering\ntasks.",
          "link": "http://arxiv.org/abs/2102.07043",
          "publishedOn": "2021-06-16T01:21:11.151Z",
          "wordCount": 625,
          "title": "Reasoning Over Virtual Knowledge Bases With Open Predicate Relations. (arXiv:2102.07043v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12589",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaskirat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Generation of stroke-based non-photorealistic imagery, is an important\nproblem in the computer vision community. As an endeavor in this direction,\nsubstantial recent research efforts have been focused on teaching machines \"how\nto paint\", in a manner similar to a human painter. However, the applicability\nof previous methods has been limited to datasets with little variation in\nposition, scale and saliency of the foreground object. As a consequence, we\nfind that these methods struggle to cover the granularity and diversity\npossessed by real world images. To this end, we propose a Semantic Guidance\npipeline with 1) a bi-level painting procedure for learning the distinction\nbetween foreground and background brush strokes at training time. 2) We also\nintroduce invariance to the position and scale of the foreground object through\na neural alignment model, which combines object localization and spatial\ntransformer networks in an end to end manner, to zoom into a particular\nsemantic instance. 3) The distinguishing features of the in-focus object are\nthen amplified by maximizing a novel guided backpropagation based focus reward.\nThe proposed agent does not require any supervision on human stroke-data and\nsuccessfully handles variations in foreground object attributes, thus,\nproducing much higher quality canvases for the CUB-200 Birds and Stanford\nCars-196 datasets. Finally, we demonstrate the further efficacy of our method\non complex datasets with multiple foreground object instances by evaluating an\nextension of our method on the challenging Virtual-KITTI dataset. Source code\nand models are available at https://github.com/1jsingh/semantic-guidance.",
          "link": "http://arxiv.org/abs/2011.12589",
          "publishedOn": "2021-06-16T01:21:11.144Z",
          "wordCount": 728,
          "title": "Combining Semantic Guidance and Deep Reinforcement Learning For Generating Human Level Paintings. (arXiv:2011.12589v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vorbach_C/0/1/0/all/0/1\">Charles Vorbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1\">Mathias Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "Imitation learning enables high-fidelity, vision-based learning of policies\nwithin rich, photorealistic environments. However, such techniques often rely\non traditional discrete-time neural models and face difficulties in\ngeneralizing to domain shifts by failing to account for the causal\nrelationships between the agent and the environment. In this paper, we propose\na theoretical and experimental framework for learning causal representations\nusing continuous-time neural networks, specifically over their discrete-time\ncounterparts. We evaluate our method in the context of visual-control learning\nof drones over a series of complex tasks, ranging from short- and long-term\nnavigation, to chasing static and dynamic objects through photorealistic\nenvironments. Our results demonstrate that causal continuous-time deep models\ncan perform robust navigation tasks, where advanced recurrent models fail.\nThese models learn complex causal control representations directly from raw\nvisual inputs and scale to solve a variety of tasks using imitation learning.",
          "link": "http://arxiv.org/abs/2106.08314",
          "publishedOn": "2021-06-16T01:21:11.123Z",
          "wordCount": 578,
          "title": "Causal Navigation by Continuous-time Neural Networks. (arXiv:2106.08314v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.13970",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rikhye_R/0/1/0/all/0/1\">Rajeev Rikhye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Q/0/1/0/all/0/1\">Qiao Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yiteng/0/1/0/all/0/1\">Yiteng</a> (Arden) <a href=\"http://arxiv.org/find/eess/1/au:+Huang/0/1/0/all/0/1\">Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narayanan_A/0/1/0/all/0/1\">Arun Narayanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McGraw_I/0/1/0/all/0/1\">Ian McGraw</a>",
          "description": "In this paper, we introduce a streaming keyphrase detection system that can\nbe easily customized to accurately detect any phrase composed of words from a\nlarge vocabulary. The system is implemented with an end-to-end trained\nautomatic speech recognition (ASR) model and a text-independent speaker\nverification model. To address the challenge of detecting these keyphrases\nunder various noisy conditions, a speaker separation model is added to the\nfeature frontend of the speaker verification model, and an adaptive noise\ncancellation (ANC) algorithm is included to exploit cross-microphone noise\ncoherence. Our experiments show that the text-independent speaker verification\nmodel largely reduces the false triggering rate of the keyphrase detection,\nwhile the speaker separation model and adaptive noise cancellation largely\nreduce false rejections.",
          "link": "http://arxiv.org/abs/2104.13970",
          "publishedOn": "2021-06-16T01:21:11.104Z",
          "wordCount": 583,
          "title": "Personalized Keyphrase Detection using Speaker and Environment Information. (arXiv:2104.13970v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02095",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1\">Aleksandr Beknazaryan</a>",
          "description": "We provide an entropy bound for the spaces of path norm regularized neural\nnetworks with piecewise linear activation functions, such as the ReLU and the\nabsolute value functions. This bound generalizes the known entropy bound for\nthe spaces of linear functions on $\\mathbb{R}^d$. Keeping the path norm\ntogether with the depth, width and the weights of networks to have logarithmic\ndependence on $1/\\varepsilon$, we $\\varepsilon$-approximate functions that are\nanalytic on certain regions of $\\mathbb{C}^d$.",
          "link": "http://arxiv.org/abs/2104.02095",
          "publishedOn": "2021-06-16T01:21:11.045Z",
          "wordCount": 524,
          "title": "Analytic function approximation by path norm regularized deep networks. (arXiv:2104.02095v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.04177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_A/0/1/0/all/0/1\">Ainesh Bakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chepurko_N/0/1/0/all/0/1\">Nadiia Chepurko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1\">David P. Woodruff</a>",
          "description": "Recently, Musco and Woodruff (FOCS, 2017) showed that given an $n \\times n$\npositive semidefinite (PSD) matrix $A$, it is possible to compute a\n$(1+\\epsilon)$-approximate relative-error low-rank approximation to $A$ by\nquerying $O(nk/\\epsilon^{2.5})$ entries of $A$ in time $O(nk/\\epsilon^{2.5} +n\nk^{\\omega-1}/\\epsilon^{2(\\omega-1)})$. They also showed that any relative-error\nlow-rank approximation algorithm must query $\\Omega(nk/\\epsilon)$ entries of\n$A$, this gap has since remained open. Our main result is to resolve this\nquestion by obtaining an optimal algorithm that queries $O(nk/\\epsilon)$\nentries of $A$ and outputs a relative-error low-rank approximation in\n$O(n(k/\\epsilon)^{\\omega-1})$ time. Note, our running time improves that of\nMusco and Woodruff, and matches the information-theoretic lower bound if the\nmatrix-multiplication exponent $\\omega$ is $2$.\n\nWe then extend our techniques to negative-type distance matrices. Bakshi and\nWoodruff (NeurIPS, 2018) showed a bi-criteria, relative-error low-rank\napproximation which queries $O(nk/\\epsilon^{2.5})$ entries and outputs a\nrank-$(k+4)$ matrix. We show that the bi-criteria guarantee is not necessary\nand obtain an $O(nk/\\epsilon)$ query algorithm, which is optimal. Our algorithm\napplies to all distance matrices that arise from metrics satisfying\nnegative-type inequalities, including $\\ell_1, \\ell_2,$ spherical metrics and\nhypermetrics.\n\nNext, we introduce a new robust low-rank approximation model which captures\nPSD matrices that have been corrupted with noise. While a sample complexity\nlower bound precludes sublinear algorithms for arbitrary PSD matrices, we\nprovide the first sublinear time and query algorithms when the corruption on\nthe diagonal entries is bounded. As a special case, we show sample-optimal\nsublinear time algorithms for low-rank approximation of correlation matrices\ncorrupted by noise.",
          "link": "http://arxiv.org/abs/1912.04177",
          "publishedOn": "2021-06-16T01:21:11.035Z",
          "wordCount": 745,
          "title": "Robust and Sample Optimal Algorithms for PSD Low-Rank Approximation. (arXiv:1912.04177v5 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.07687",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhichuang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruimin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Long Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mislove_A/0/1/0/all/0/1\">Alan Mislove</a>",
          "description": "On-device machine learning (ML) is quickly gaining popularity among mobile\napps. It allows offline model inference while preserving user privacy. However,\nML models, considered as core intellectual properties of model owners, are now\nstored on billions of untrusted devices and subject to potential thefts. Leaked\nmodels can cause both severe financial loss and security consequences. This\npaper presents the first empirical study of ML model protection on mobile\ndevices. Our study aims to answer three open questions with quantitative\nevidence: How widely is model protection used in apps? How robust are existing\nmodel protection techniques? What impacts can (stolen) models incur? To that\nend, we built a simple app analysis pipeline and analyzed 46,753 popular apps\ncollected from the US and Chinese app markets. We identified 1,468 ML apps\nspanning all popular app categories. We found that, alarmingly, 41% of ML apps\ndo not protect their models at all, which can be trivially stolen from app\npackages. Even for those apps that use model protection or encryption, we were\nable to extract the models from 66% of them via unsophisticated dynamic\nanalysis techniques. The extracted models are mostly commercial products and\nused for face recognition, liveness detection, ID/bank card recognition, and\nmalware detection. We quantitatively estimated the potential financial and\nsecurity impact of a leaked model, which can amount to millions of dollars for\ndifferent stakeholders. Our study reveals that on-device models are currently\nat high risk of being leaked; attackers are highly motivated to steal such\nmodels. Drawn from our large-scale study, we report our insights into this\nemerging security problem and discuss the technical challenges, hoping to\ninspire future research on robust and practical model protection for mobile\ndevices.",
          "link": "http://arxiv.org/abs/2002.07687",
          "publishedOn": "2021-06-16T01:21:11.018Z",
          "wordCount": 758,
          "title": "Mind Your Weight(s): A Large-scale Study on Insufficient Machine Learning Model Protection in Mobile Apps. (arXiv:2002.07687v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.",
          "link": "http://arxiv.org/abs/2010.11066",
          "publishedOn": "2021-06-16T01:21:11.011Z",
          "wordCount": 616,
          "title": "Contextualized Attention-based Knowledge Transfer for Spoken Conversational Question Answering. (arXiv:2010.11066v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1903.05631",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Haoteng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhanxing Zhu</a>",
          "description": "The spatio-temporal graph learning is becoming an increasingly important\nobject of graph study. Many application domains involve highly dynamic graphs\nwhere temporal information is crucial, e.g. traffic networks and financial\ntransaction graphs. Despite the constant progress made on learning structured\ndata, there is still a lack of effective means to extract dynamic complex\nfeatures from spatio-temporal structures. Particularly, conventional models\nsuch as convolutional networks or recurrent neural networks are incapable of\nrevealing the temporal patterns in short or long terms and exploring the\nspatial properties in local or global scope from spatio-temporal graphs\nsimultaneously. To tackle this problem, we design a novel multi-scale\narchitecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series\nmodeling. In this U-shaped network, a paired sampling operation is proposed in\nspacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in\nspatial from its deterministic partition while abstracts multi-resolution\ntemporal dependencies through dilated recurrent skip connections; based on\nprevious settings in the downsampling, the unpooling (ST-Unpool) restores the\noriginal structure of spatio-temporal graphs and resumes regular intervals\nwithin graph sequences. Experiments on spatio-temporal prediction tasks\ndemonstrate that our model effectively captures comprehensive features in\nmultiple scales and achieves substantial improvements over mainstream methods\non several real-world datasets.",
          "link": "http://arxiv.org/abs/1903.05631",
          "publishedOn": "2021-06-16T01:21:11.004Z",
          "wordCount": 657,
          "title": "ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series Modeling. (arXiv:1903.05631v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1\">Sabato Marco Siniscalchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>",
          "description": "We propose using an adversarial autoencoder (AAE) to replace generative\nadversarial network (GAN) in the private aggregation of teacher ensembles\n(PATE), a solution for ensuring differential privacy in speech applications.\nThe AAE architecture allows us to obtain good synthetic speech leveraging upon\na discriminative training of latent vectors. Such synthetic speech is used to\nbuild a privacy-preserving classifier when non-sensitive data is not\nsufficiently available in the public domain. This classifier follows the PATE\nscheme that uses an ensemble of noisy outputs to label the synthetic samples\nand guarantee $\\varepsilon$-differential privacy (DP) on its derived\nclassifiers. Our proposed framework thus consists of an AAE-based generator and\na PATE-based classifier (PATE-AAE). Evaluated on the Google Speech Commands\nDataset Version II, the proposed PATE-AAE improves the average classification\naccuracy by +$2.11\\%$ and +$6.60\\%$, respectively, when compared with\nalternative privacy-preserving solutions, namely PATE-GAN and DP-GAN, while\nmaintaining a strong level of privacy target at $\\varepsilon$=0.01 with a fixed\n$\\delta$=10$^{-5}$.",
          "link": "http://arxiv.org/abs/2104.01271",
          "publishedOn": "2021-06-16T01:21:10.997Z",
          "wordCount": 646,
          "title": "PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification. (arXiv:2104.01271v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dar_Y/0/1/0/all/0/1\">Yehuda Dar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>",
          "description": "We study the transfer learning process between two linear regression\nproblems. An important and timely special case is when the regressors are\noverparameterized and perfectly interpolate their training data. We examine a\nparameter transfer mechanism whereby a subset of the parameters of the target\ntask solution are constrained to the values learned for a related source task.\nWe analytically characterize the generalization error of the target task in\nterms of the salient factors in the transfer learning architecture, i.e., the\nnumber of examples available, the number of (free) parameters in each of the\ntasks, the number of parameters transferred from the source to target task, and\nthe correlation between the two tasks. Our non-asymptotic analysis shows that\nthe generalization error of the target task follows a two-dimensional double\ndescent trend (with respect to the number of free parameters in each of the\ntasks) that is controlled by the transfer learning factors. Our analysis points\nto specific cases where the transfer of parameters is beneficial. Specifically,\nwe show that transferring a specific set of parameters that generalizes well on\nthe respective part of the source task can soften the demand on the task\ncorrelation level that is required for successful transfer learning. Moreover,\nwe show that the usefulness of a transfer learning setting is fragile and\ndepends on a delicate interplay among the set of transferred parameters, the\nrelation between the tasks, and the true solution.",
          "link": "http://arxiv.org/abs/2006.07002",
          "publishedOn": "2021-06-16T01:21:10.971Z",
          "wordCount": 727,
          "title": "Double Double Descent: On Generalization Errors in Transfer Learning between Linear Regression Tasks. (arXiv:2006.07002v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04975",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Immer_A/0/1/0/all/0/1\">Alexander Immer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bauer_M/0/1/0/all/0/1\">Matthias Bauer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1\">Gunnar R&#xe4;tsch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Emtiyaz Khan</a>",
          "description": "Marginal-likelihood based model-selection, even though promising, is rarely\nused in deep learning due to estimation difficulties. Instead, most approaches\nrely on validation data, which may not be readily available. In this work, we\npresent a scalable marginal-likelihood estimation method to select both\nhyperparameters and network architectures, based on the training data alone.\nSome hyperparameters can be estimated online during training, simplifying the\nprocedure. Our marginal-likelihood estimate is based on Laplace's method and\nGauss-Newton approximations to the Hessian, and it outperforms cross-validation\nand manual-tuning on standard regression and image classification datasets,\nespecially in terms of calibration and out-of-distribution detection. Our work\nshows that marginal likelihoods can improve generalization and be useful when\nvalidation data is unavailable (e.g., in nonstationary settings).",
          "link": "http://arxiv.org/abs/2104.04975",
          "publishedOn": "2021-06-16T01:21:10.956Z",
          "wordCount": 583,
          "title": "Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning. (arXiv:2104.04975v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1\">Abhishek Bhandwaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shari Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1\">Dan Gutfreund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spelke_E/0/1/0/all/0/1\">Elizabeth Spelke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1\">Tomer D. Ullman</a>",
          "description": "For machine agents to successfully interact with humans in real-world\nsettings, they will need to develop an understanding of human mental life.\nIntuitive psychology, the ability to reason about hidden mental variables that\ndrive observable actions, comes naturally to people: even pre-verbal infants\ncan tell agents from objects, expecting agents to act efficiently to achieve\ngoals given constraints. Despite recent interest in machine agents that reason\nabout other agents, it is not clear if such agents learn or hold the core\npsychology principles that drive human reasoning. Inspired by cognitive\ndevelopment studies on intuitive psychology, we present a benchmark consisting\nof a large dataset of procedurally generated 3D animations, AGENT (Action,\nGoal, Efficiency, coNstraint, uTility), structured around four scenarios (goal\npreferences, action efficiency, unobserved constraints, and cost-reward\ntrade-offs) that probe key concepts of core intuitive psychology. We validate\nAGENT with human-ratings, propose an evaluation protocol emphasizing\ngeneralization, and compare two strong baselines built on Bayesian inverse\nplanning and a Theory of Mind neural network. Our results suggest that to pass\nthe designed tests of core intuitive psychology at human levels, a model must\nacquire or have built-in representations of how agents plan, combining utility\ncomputations and core knowledge of objects and physics.",
          "link": "http://arxiv.org/abs/2102.12321",
          "publishedOn": "2021-06-16T01:21:10.940Z",
          "wordCount": 697,
          "title": "AGENT: A Benchmark for Core Psychological Reasoning. (arXiv:2102.12321v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thornton_C/0/1/0/all/0/1\">Charles E. Thornton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buehrer_R/0/1/0/all/0/1\">R. Michael Buehrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martone_A/0/1/0/all/0/1\">Anthony F. Martone</a>",
          "description": "A sequential decision process in which an adaptive radar system repeatedly\ninteracts with a finite-state target channel is studied. The radar is capable\nof passively sensing the spectrum at regular intervals, which provides side\ninformation for the waveform selection process. The radar transmitter uses the\nsequence of spectrum observations as well as feedback from a collocated\nreceiver to select waveforms which accurately estimate target parameters. It is\nshown that the waveform selection problem can be effectively addressed using a\nlinear contextual bandit formulation in a manner that is both computationally\nfeasible and sample efficient. Stochastic and adversarial linear contextual\nbandit models are introduced, allowing the radar to achieve effective\nperformance in broad classes of physical environments. Simulations in a\nradar-communication coexistence scenario, as well as in an adversarial\nradar-jammer scenario, demonstrate that the proposed formulation provides a\nsubstantial improvement in target detection performance when Thompson Sampling\nand EXP3 algorithms are used to drive the waveform selection process. Further,\nit is shown that the harmful impacts of pulse-agile behavior on coherently\nprocessed radar data can be mitigated by adopting a time-varying constraint on\nthe radar's waveform catalog.",
          "link": "http://arxiv.org/abs/2103.05541",
          "publishedOn": "2021-06-16T01:21:10.922Z",
          "wordCount": 665,
          "title": "Constrained Contextual Bandit Learning for Adaptive Radar Waveform Selection. (arXiv:2103.05541v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00774",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Alvarez_Melis_D/0/1/0/all/0/1\">David Alvarez-Melis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schiff_Y/0/1/0/all/0/1\">Yair Schiff</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1\">Youssef Mroueh</a>",
          "description": "Gradient flows are a powerful tool for optimizing functionals in general\nmetric spaces, including the space of probabilities endowed with the\nWasserstein metric. A typical approach to solving this optimization problem\nrelies on its connection to the dynamic formulation of optimal transport and\nthe celebrated Jordan-Kinderlehrer-Otto (JKO) scheme. However, this formulation\ninvolves optimization over convex functions, which is challenging, especially\nin high dimensions. In this work, we propose an approach that relies on the\nrecently introduced input-convex neural networks (ICNN) to parameterize the\nspace of convex functions in order to approximate the JKO scheme, as well as in\ndesigning functionals over measures that enjoy convergence guarantees. We\nderive a computationally efficient implementation of this JKO-ICNN framework\nand use various experiments to demonstrate its feasibility and validity in\napproximating solutions of low-dimensional partial differential equations with\nknown solutions. We also explore the use of our JKO-ICNN approach in high\ndimensions with an experiment in controlled generation for molecular discovery.",
          "link": "http://arxiv.org/abs/2106.00774",
          "publishedOn": "2021-06-16T01:21:10.876Z",
          "wordCount": 615,
          "title": "Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks. (arXiv:2106.00774v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yangkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1\">Quan Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhewei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zengfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>",
          "description": "Despite the recent success of graph neural networks (GNN), common\narchitectures often exhibit significant limitations, including sensitivity to\noversmoothing, long-range dependencies, and spurious edges, e.g., as can occur\nas a result of graph heterophily or adversarial attacks. To at least partially\naddress these issues within a simple transparent framework, we consider a new\nfamily of GNN layers designed to mimic and integrate the update rules of two\nclassical iterative algorithms, namely, proximal gradient descent and iterative\nreweighted least squares (IRLS). The former defines an extensible base GNN\narchitecture that is immune to oversmoothing while nonetheless capturing\nlong-range dependencies by allowing arbitrary propagation steps. In contrast,\nthe latter produces a novel attention mechanism that is explicitly anchored to\nan underlying end-to-end energy function, contributing stability with respect\nto edge uncertainty. When combined we obtain an extremely simple yet robust\nmodel that we evaluate across disparate scenarios including standardized\nbenchmarks, adversarially-perturbated graphs, graphs with heterophily, and\ngraphs involving long-range dependencies. In doing so, we compare against SOTA\nGNN approaches that have been explicitly designed for the respective task,\nachieving competitive or superior node classification accuracy. Our code is\navailable at https://github.com/FFTYYY/TWIRLS.",
          "link": "http://arxiv.org/abs/2103.06064",
          "publishedOn": "2021-06-16T01:21:10.870Z",
          "wordCount": 664,
          "title": "Graph Neural Networks Inspired by Classical Iterative Algorithms. (arXiv:2103.06064v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.06723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yichen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "In the area of natural language processing, deep learning models are recently\nknown to be vulnerable to various types of adversarial perturbations, but\nrelatively few works are done on the defense side. Especially, there exists few\neffective defense method against the successful synonym substitution based\nattacks that preserve the syntactic structure and semantic information of the\noriginal text while fooling the deep learning models. We contribute in this\ndirection and propose a novel adversarial defense method called Synonym\nEncoding Method (SEM). Specifically, SEM inserts an encoder before the input\nlayer of the target model to map each cluster of synonyms to a unique encoding\nand trains the model to eliminate possible adversarial perturbations without\nmodifying the network architecture or adding extra data. Extensive experiments\ndemonstrate that SEM can effectively defend the current synonym substitution\nbased attacks and block the transferability of adversarial examples. SEM is\nalso easy and efficient to scale to large models and big datasets.",
          "link": "http://arxiv.org/abs/1909.06723",
          "publishedOn": "2021-06-16T01:21:10.863Z",
          "wordCount": 643,
          "title": "Natural Language Adversarial Defense through Synonym Encoding. (arXiv:1909.06723v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07367",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Khanduri_P/0/1/0/all/0/1\">Prashant Khanduri</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zeng_S/0/1/0/all/0/1\">Siliang Zeng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hong_M/0/1/0/all/0/1\">Mingyi Hong</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wai_H/0/1/0/all/0/1\">Hoi-To Wai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>",
          "description": "This paper proposes a new algorithm -- the \\underline{S}ingle-timescale\nDo\\underline{u}ble-momentum \\underline{St}ochastic\n\\underline{A}pprox\\underline{i}matio\\underline{n} (SUSTAIN) -- for tackling\nstochastic unconstrained bilevel optimization problems. We focus on bilevel\nproblems where the lower level subproblem is strongly-convex and the upper\nlevel objective function is smooth. Unlike prior works which rely on\n\\emph{two-timescale} or \\emph{double loop} techniques, we design a stochastic\nmomentum-assisted gradient estimator for both the upper and lower level\nupdates. The latter allows us to control the error in the stochastic gradient\nupdates due to inaccurate solution to both subproblems. If the upper objective\nfunction is smooth but possibly non-convex, we show that {\\aname}~requires\n$\\mathcal{O}(\\epsilon^{-3/2})$ iterations (each using ${\\cal O}(1)$ samples) to\nfind an $\\epsilon$-stationary solution. The $\\epsilon$-stationary solution is\ndefined as the point whose squared norm of the gradient of the outer function\nis less than or equal to $\\epsilon$. The total number of stochastic gradient\nsamples required for the upper and lower level objective functions matches the\nbest-known complexity for single-level stochastic gradient algorithms. We also\nanalyze the case when the upper level objective function is strongly-convex.",
          "link": "http://arxiv.org/abs/2102.07367",
          "publishedOn": "2021-06-16T01:21:10.857Z",
          "wordCount": 648,
          "title": "A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum. (arXiv:2102.07367v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1\">Dimitrios Vytiniotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1\">Grzegorz Swirszcz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1\">Viorica Patraucean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Joao Carreira</a>",
          "description": "How can neural networks be trained on large-volume temporal data efficiently?\nTo compute the gradients required to update parameters, backpropagation blocks\ncomputations until the forward and backward passes are completed. For temporal\nsignals, this introduces high latency and hinders real-time learning. It also\ncreates a coupling between consecutive layers, which limits model parallelism\nand increases memory consumption. In this paper, we build upon Sideways, which\navoids blocking by propagating approximate gradients forward in time, and we\npropose mechanisms for temporal integration of information based on different\nvariants of skip connections. We also show how to decouple computation and\ndelegate individual neural modules to different devices, allowing distributed\nand parallel training. The proposed Skip-Sideways achieves low latency\ntraining, model parallelism, and, importantly, is capable of extracting\ntemporal features, leading to more stable training and improved performance on\nreal-world action recognition video datasets such as HMDB51, UCF101, and the\nlarge-scale Kinetics-600. Finally, we also show that models trained with\nSkip-Sideways generate better future frames than Sideways models, and hence\nthey can better utilize motion cues.",
          "link": "http://arxiv.org/abs/2106.08318",
          "publishedOn": "2021-06-16T01:21:10.797Z",
          "wordCount": 632,
          "title": "Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duboudin_T/0/1/0/all/0/1\">Thomas Duboudin</a> (imagine), <a href=\"http://arxiv.org/find/cs/1/au:+Dellandrea_E/0/1/0/all/0/1\">Emmanuel Dellandr&#xe9;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abgrall_C/0/1/0/all/0/1\">Corentin Abgrall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_G/0/1/0/all/0/1\">Gilles H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "Traditional deep learning algorithms often fail to generalize when they are\ntested outside of the domain of training data. Because data distributions can\nchange dynamically in real-life applications once a learned model is deployed,\nin this paper we are interested in single-source domain generalization (SDG)\nwhich aims to develop deep learning algorithms able to generalize from a single\ntraining domain where no information about the test domain is available at\ntraining time. Firstly, we design two simple MNISTbased SDG benchmarks, namely\nMNIST Color SDG-MP and MNIST Color SDG-UP, which highlight the two different\nfundamental SDG issues of increasing difficulties: 1) a class-correlated\npattern in the training domain is missing (SDG-MP), or 2) uncorrelated with the\nclass (SDG-UP), in the testing data domain. This is in sharp contrast with the\ncurrent domain generalization (DG) benchmarks which mix up different\ncorrelation and variation factors and thereby make hard to disentangle success\nor failure factors when benchmarking DG algorithms. We further evaluate several\nstate-of-the-art SDG algorithms through our simple benchmark, namely MNIST\nColor SDG-MP, and show that the issue SDG-MP is largely unsolved despite of a\ndecade of efforts in developing DG algorithms. Finally, we also propose a\npartially reversed contrastive loss to encourage intra-class diversity and find\nless strongly correlated patterns, to deal with SDG-MP and show that the\nproposed approach is very effective on our MNIST Color SDG-MP benchmark.",
          "link": "http://arxiv.org/abs/2106.07916",
          "publishedOn": "2021-06-16T01:21:10.753Z",
          "wordCount": 678,
          "title": "Encouraging Intra-Class Diversity Through a Reverse Contrastive Loss for Better Single-Source Domain Generalization. (arXiv:2106.07916v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02397",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gerazov_B/0/1/0/all/0/1\">Branislav Gerazov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_M/0/1/0/all/0/1\">Michael Wagner</a>",
          "description": "The labelling of speech corpora is a laborious and time-consuming process.\nThe ProsoBeast Annotation Tool seeks to ease and accelerate this process by\nproviding an interactive 2D representation of the prosodic landscape of the\ndata, in which contours are distributed based on their similarity. This\ninteractive map allows the user to inspect and label the utterances. The tool\nintegrates several state-of-the-art methods for dimensionality reduction and\nfeature embedding, including variational autoencoders. The user can use these\nto find a good representation for their data. In addition, as most of these\nmethods are stochastic, each can be used to generate an unlimited number of\ndifferent prosodic maps. The web app then allows the user to seamlessly switch\nbetween these alternative representations in the annotation process.\nExperiments with a sample prosodically rich dataset have shown that the tool\nmanages to find good representations of varied data and is helpful both for\nannotation and label correction. The tool is released as free software for use\nby the community.",
          "link": "http://arxiv.org/abs/2104.02397",
          "publishedOn": "2021-06-16T01:21:10.731Z",
          "wordCount": 610,
          "title": "ProsoBeast Prosody Annotation Tool. (arXiv:2104.02397v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02469",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Karra_K/0/1/0/all/0/1\">Kiran Karra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McCree_A/0/1/0/all/0/1\">Alan McCree</a>",
          "description": "Many modern systems for speaker diarization, such as the recently-developed\nVBx approach, rely on clustering of DNN speaker embeddings followed by\nresegmentation. Two problems with this approach are that the DNN is not\ndirectly optimized for this task, and the parameters need significant retuning\nfor different applications. We have recently presented progress in this\ndirection with a Leave-One-Out Gaussian PLDA (LGP) clustering algorithm and an\napproach to training the DNN such that embeddings directly optimize performance\nof this scoring method. This paper presents a new two-pass version of this\nsystem, where the second pass uses finer time resolution to significantly\nimprove overall performance. For the Callhome corpus, we achieve the first\npublished error rate below 4% without any task-dependent parameter tuning. We\nalso show significant progress towards a robust single solution for multiple\ndiarization tasks.",
          "link": "http://arxiv.org/abs/2104.02469",
          "publishedOn": "2021-06-16T01:21:10.709Z",
          "wordCount": 612,
          "title": "Speaker Diarization using Two-pass Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings. (arXiv:2104.02469v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1\">Muhammad Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopinath_D/0/1/0/all/0/1\">Divya Gopinath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Youcheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noller_Y/0/1/0/all/0/1\">Yannic Noller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasareanu_C/0/1/0/all/0/1\">Corina Pasareanu</a>",
          "description": "We present NNrepair, a constraint-based technique for repairing neural\nnetwork classifiers. The technique aims to fix the logic of the network at an\nintermediate layer or at the last layer. NNrepair first uses fault localization\nto find potentially faulty network parameters (such as the weights) and then\nperforms repair using constraint solving to apply small modifications to the\nparameters to remedy the defects. We present novel strategies to enable precise\nyet efficient repair such as inferring correctness specifications to act as\noracles for intermediate layer repair, and generation of experts for each\nclass. We demonstrate the technique in the context of three different\nscenarios: (1) Improving the overall accuracy of a model, (2) Fixing security\nvulnerabilities caused by poisoning of training data and (3) Improving the\nrobustness of the network against adversarial attacks. Our evaluation on MNIST\nand CIFAR-10 models shows that NNrepair can improve the accuracy by 45.56\npercentage points on poisoned data and 10.40 percentage points on adversarial\ndata. NNrepair also provides small improvement in the overall accuracy of\nmodels, without requiring new data or re-training.",
          "link": "http://arxiv.org/abs/2103.12535",
          "publishedOn": "2021-06-16T01:21:10.700Z",
          "wordCount": 637,
          "title": "NNrepair: Constraint-based Repair of Neural Network Classifiers. (arXiv:2103.12535v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Sheng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Ju Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jiang Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junshan Zhang</a>",
          "description": "In order to meet the requirements for performance, safety, and latency in\nmany IoT applications, intelligent decisions must be made right here right now\nat the network edge. However, the constrained resources and limited local data\namount pose significant challenges to the development of edge AI. To overcome\nthese challenges, we explore continual edge learning capable of leveraging the\nknowledge transfer from previous tasks. Aiming to achieve fast and continual\nedge learning, we propose a platform-aided federated meta-learning architecture\nwhere edge nodes collaboratively learn a meta-model, aided by the knowledge\ntransfer from prior tasks. The edge learning problem is cast as a regularized\noptimization problem, where the valuable knowledge learned from previous tasks\nis extracted as regularization. Then, we devise an ADMM based federated\nmeta-learning algorithm, namely ADMM-FedMeta, where ADMM offers a natural\nmechanism to decompose the original problem into many subproblems which can be\nsolved in parallel across edge nodes and the platform. Further, a variant of\ninexact-ADMM method is employed where the subproblems are `solved' via linear\napproximation as well as Hessian estimation to reduce the computational cost\nper round to $\\mathcal{O}(n)$. We provide a comprehensive analysis of\nADMM-FedMeta, in terms of the convergence properties, the rapid adaptation\nperformance, and the forgetting effect of prior knowledge transfer, for the\ngeneral non-convex case. Extensive experimental studies demonstrate the\neffectiveness and efficiency of ADMM-FedMeta, and showcase that it\nsubstantially outperforms the existing baselines.",
          "link": "http://arxiv.org/abs/2012.08677",
          "publishedOn": "2021-06-16T01:21:10.617Z",
          "wordCount": null,
          "title": "Inexact-ADMM Based Federated Meta-Learning for Fast and Continual Edge Learning. (arXiv:2012.08677v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1\">Roberta Raileanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1\">Rob Fergus</a>",
          "description": "Standard deep reinforcement learning algorithms use a shared representation\nfor the policy and value function, especially when training directly from\nimages. However, we argue that more information is needed to accurately\nestimate the value function than to learn the optimal policy. Consequently, the\nuse of a shared representation for the policy and value function can lead to\noverfitting. To alleviate this problem, we propose two approaches which are\ncombined to create IDAAC: Invariant Decoupled Advantage Actor-Critic. First,\nIDAAC decouples the optimization of the policy and value function, using\nseparate networks to model them. Second, it introduces an auxiliary loss which\nencourages the representation to be invariant to task-irrelevant properties of\nthe environment. IDAAC shows good generalization to unseen environments,\nachieving a new state-of-the-art on the Procgen benchmark and outperforming\npopular methods on DeepMind Control tasks with distractors. Our implementation\nis available at https://github.com/rraileanu/idaac.",
          "link": "http://arxiv.org/abs/2102.10330",
          "publishedOn": "2021-06-16T01:21:10.616Z",
          "wordCount": 598,
          "title": "Decoupling Value and Policy for Generalization in Reinforcement Learning. (arXiv:2102.10330v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blum_A/0/1/0/all/0/1\">Avrim Blum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1\">Steve Hanneke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jian Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Han Shao</a>",
          "description": "We study the problem of robust learning under clean-label data-poisoning\nattacks, where the attacker injects (an arbitrary set of) correctly-labeled\nexamples to the training set to fool the algorithm into making mistakes on\nspecific test instances at test time. The learning goal is to minimize the\nattackable rate (the probability mass of attackable test instances), which is\nmore difficult than optimal PAC learning. As we show, any robust algorithm with\ndiminishing attackable rate can achieve the optimal dependence on $\\epsilon$ in\nits PAC sample complexity, i.e., $O(1/\\epsilon)$. On the other hand, the\nattackable rate might be large even for some optimal PAC learners, e.g., SVM\nfor linear classifiers. Furthermore, we show that the class of linear\nhypotheses is not robustly learnable when the data distribution has zero margin\nand is robustly learnable in the case of positive margin but requires sample\ncomplexity exponential in the dimension. For a general hypothesis class with\nbounded VC dimension, if the attacker is limited to add at most $t>0$ poison\nexamples, the optimal robust learning sample complexity grows almost linearly\nwith $t$.",
          "link": "http://arxiv.org/abs/2103.00671",
          "publishedOn": "2021-06-16T01:21:10.608Z",
          "wordCount": null,
          "title": "Robust learning under clean-label attack. (arXiv:2103.00671v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00769",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Berg_A/0/1/0/all/0/1\">Axel Berg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OConnor_M/0/1/0/all/0/1\">Mark O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cruz_M/0/1/0/all/0/1\">Miguel Tairum Cruz</a>",
          "description": "The Transformer architecture has been successful across many domains,\nincluding natural language processing, computer vision and speech recognition.\nIn keyword spotting, self-attention has primarily been used on top of\nconvolutional or recurrent encoders. We investigate a range of ways to adapt\nthe Transformer architecture to keyword spotting and introduce the Keyword\nTransformer (KWT), a fully self-attentional architecture that exceeds\nstate-of-the-art performance across multiple tasks without any pre-training or\nadditional data. Surprisingly, this simple architecture outperforms more\ncomplex models that mix convolutional, recurrent and attentive layers. KWT can\nbe used as a drop-in replacement for these models, setting two new benchmark\nrecords on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on\nthe 12 and 35-command tasks respectively.",
          "link": "http://arxiv.org/abs/2104.00769",
          "publishedOn": "2021-06-16T01:21:10.607Z",
          "wordCount": null,
          "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting. (arXiv:2104.00769v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1901.10002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_H/0/1/0/all/0/1\">Harini Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1\">John V. Guttag</a>",
          "description": "As machine learning (ML) increasingly affects people and society, awareness\nof its potential unwanted consequences has also grown. To anticipate, prevent,\nand mitigate undesirable downstream consequences, it is critical that we\nunderstand when and how harm might be introduced throughout the ML life cycle.\nIn this paper, we provide a framework that identifies seven distinct potential\nsources of downstream harm in machine learning, spanning data collection,\ndevelopment, and deployment. In doing so, we aim to facilitate more productive\nand precise communication around these issues, as well as more direct,\napplication-grounded ways to mitigate them.",
          "link": "http://arxiv.org/abs/1901.10002",
          "publishedOn": "2021-06-16T01:21:10.491Z",
          "wordCount": 597,
          "title": "A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. (arXiv:1901.10002v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1\">A. Feder Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1\">Karen Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>",
          "description": "Trade-offs between accuracy and efficiency are found in multiple\nnon-computing domains, such as law and public health, which have developed\nrules and heuristics to guide how to balance the two in conditions of\nuncertainty. While accuracy-efficiency trade-offs are also commonly\nacknowledged in some areas of computer science, their policy implications\nremain poorly examined. Drawing on risk assessment practices in the US, we\nargue that, since examining accuracy-efficiency trade-offs has been useful for\nguiding governance in other domains, explicitly framing such trade-offs in\ncomputing is similarly useful for the governance of computer systems. Our\ndiscussion focuses on real-time distributed ML systems; understanding the\npolicy implications in this area is particularly urgent because such systems,\nwhich include autonomous vehicles, tend to be high-stakes and safety-critical.\nWe describe how the trade-off takes shape for these systems, highlight gaps\nbetween existing US risk assessment standards and what these systems require in\norder to be properly assessed, and make specific calls to action to facilitate\naccountability when hypothetical risks become realized as accidents in the real\nworld. We close by discussing how such accountability mechanisms encourage more\njust, transparent governance aligned with public values.",
          "link": "http://arxiv.org/abs/2007.02203",
          "publishedOn": "2021-06-16T01:21:10.464Z",
          "wordCount": 682,
          "title": "Understanding Accuracy-Efficiency Trade-Offs as a Means for Holding Distributed ML Systems Accountable. (arXiv:2007.02203v5 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_V/0/1/0/all/0/1\">Venkatesh Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>",
          "description": "Speaker identification in the household scenario (e.g., for smart speakers)\nis typically based on only a few enrollment utterances but a much larger set of\nunlabeled data, suggesting semisupervised learning to improve speaker profiles.\nWe propose a graph-based semi-supervised learning approach for speaker\nidentification in the household scenario, to leverage the unlabeled speech\nsamples. In contrast to most of the works in speaker recognition that focus on\nspeaker-discriminative embeddings, this work focuses on speaker label inference\n(scoring). Given a pre-trained embedding extractor, graph-based learning allows\nus to integrate information about both labeled and unlabeled utterances.\nConsidering each utterance as a graph node, we represent pairwise utterance\nsimilarity scores as edge weights. Graphs are constructed per household, and\nspeaker identities are propagated to unlabeled nodes to optimize a global\nconsistency criterion. We show in experiments on the VoxCeleb dataset that this\napproach makes effective use of unlabeled data and improves speaker\nidentification accuracy compared to two state-of-the-art scoring methods as\nwell as their semi-supervised variants based on pseudo-labels.",
          "link": "http://arxiv.org/abs/2106.08207",
          "publishedOn": "2021-06-16T01:21:10.431Z",
          "wordCount": 599,
          "title": "Graph-based Label Propagation for Semi-Supervised Speaker Identification. (arXiv:2106.08207v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1\">Vanessa D&#x27;Amario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>",
          "description": "Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via\ncomposition of modules that tackle a sub-task. NMNs are a promising strategy to\nachieve systematic generalization, i.e. overcoming biasing factors in the\ntraining distribution. However, the aspects of NMNs that facilitate systematic\ngeneralization are not fully understood. In this paper, we demonstrate that the\nstage and the degree at which modularity is defined has large influence on\nsystematic generalization. In a series of experiments on three VQA datasets\n(MNIST with multiple attributes, SQOOP, and CLEVR-CoGenT), our results reveal\nthat tuning the degree of modularity in the network, especially at the image\nencoder stage, reaches substantially higher systematic generalization. These\nfindings lead to new NMN architectures that outperform previous ones in terms\nof systematic generalization.",
          "link": "http://arxiv.org/abs/2106.08170",
          "publishedOn": "2021-06-16T01:21:10.424Z",
          "wordCount": 557,
          "title": "How Modular Should Neural Module Networks Be for Systematic Generalization?. (arXiv:2106.08170v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08185",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Simpson_F/0/1/0/all/0/1\">Fergus Simpson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Davies_I/0/1/0/all/0/1\">Ian Davies</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lalchand_V/0/1/0/all/0/1\">Vidhi Lalchand</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vullo_A/0/1/0/all/0/1\">Alessandro Vullo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durrande_N/0/1/0/all/0/1\">Nicolas Durrande</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rasmussen_C/0/1/0/all/0/1\">Carl Rasmussen</a>",
          "description": "Kernel selection plays a central role in determining the performance of\nGaussian Process (GP) models, as the chosen kernel determines both the\ninductive biases and prior support of functions under the GP prior. This work\naddresses the challenge of constructing custom kernel functions for\nhigh-dimensional GP regression models. Drawing inspiration from recent progress\nin deep learning, we introduce a novel approach named KITT: Kernel\nIdentification Through Transformers. KITT exploits a transformer-based\narchitecture to generate kernel recommendations in under 0.1 seconds, which is\nseveral orders of magnitude faster than conventional kernel search algorithms.\nWe train our model using synthetic data generated from priors over a vocabulary\nof known kernels. By exploiting the nature of the self-attention mechanism,\nKITT is able to process datasets with inputs of arbitrary dimension. We\ndemonstrate that kernels chosen by KITT yield strong performance over a diverse\ncollection of regression benchmarks.",
          "link": "http://arxiv.org/abs/2106.08185",
          "publishedOn": "2021-06-16T01:21:10.417Z",
          "wordCount": 573,
          "title": "Kernel Identification Through Transformers. (arXiv:2106.08185v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08320",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yazhe Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pogodin_R/0/1/0/all/0/1\">Roman Pogodin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1\">Danica J. Sutherland</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1\">Arthur Gretton</a>",
          "description": "We approach self-supervised learning of image representations from a\nstatistical dependence perspective, proposing Self-Supervised Learning with the\nHilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes\ndependence between representations of transformed versions of an image and the\nimage identity, while minimizing the kernelized variance of those features.\nThis self-supervised learning framework yields a new understanding of InfoNCE,\na variational lower bound on the mutual information (MI) between different\ntransformations. While the MI itself is known to have pathologies which can\nresult in meaningless representations being learned, its bound is much better\nbehaved: we show that it implicitly approximates SSL-HSIC (with a slightly\ndifferent regularizer). Our approach also gives us insight into BYOL, since\nSSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to\ndirectly optimize statistical dependence in time linear in the batch size,\nwithout restrictive data assumptions or indirect mutual information estimators.\nTrained with or without a target network, SSL-HSIC matches the current\nstate-of-the-art for standard linear evaluation on ImageNet, semi-supervised\nlearning and transfer to other classification and vision tasks such as semantic\nsegmentation, depth estimation and object recognition.",
          "link": "http://arxiv.org/abs/2106.08320",
          "publishedOn": "2021-06-16T01:21:10.380Z",
          "wordCount": 615,
          "title": "Self-Supervised Learning with Kernel Dependence Maximization. (arXiv:2106.08320v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boutsikas_J/0/1/0/all/0/1\">John Boutsikas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eren_M/0/1/0/all/0/1\">Maksim E. Eren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varga_C/0/1/0/all/0/1\">Charles Varga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matuszek_C/0/1/0/all/0/1\">Cynthia Matuszek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1\">Charles Nicholas</a>",
          "description": "The use of Machine Learning has become a significant part of malware\ndetection efforts due to the influx of new malware, an ever changing threat\nlandscape, and the ability of Machine Learning methods to discover meaningful\ndistinctions between malicious and benign software. Antivirus vendors have also\nbegun to widely utilize malware classifiers based on dynamic and static malware\nanalysis features. Therefore, a malware author might make evasive binary\nmodifications against Machine Learning models as part of the malware\ndevelopment life cycle to execute an attack successfully. This makes the\nstudying of possible classifier evasion strategies an essential part of cyber\ndefense against malice. To this extent, we stage a grey box setup to analyze a\nscenario where the malware author does not know the target classifier\nalgorithm, and does not have access to decisions made by the classifier, but\nknows the features used in training. In this experiment, a malicious actor\ntrains a surrogate model using the EMBER-2018 dataset to discover binary\nmutations that cause an instance to be misclassified via a Monte Carlo tree\nsearch. Then, mutated malware is sent to the victim model that takes the place\nof an antivirus API to test whether it can evade detection.",
          "link": "http://arxiv.org/abs/2106.07860",
          "publishedOn": "2021-06-16T01:21:10.347Z",
          "wordCount": 651,
          "title": "Evading Malware Classifiers via Monte Carlo Mutant Feature Discovery. (arXiv:2106.07860v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2006.01017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kahale_N/0/1/0/all/0/1\">Nabil Kahale</a>",
          "description": "We analyse an iterative algorithm to minimize quadratic functions whose\nHessian matrix $H$ is the expectation of a random symmetric $d\\times d$ matrix.\nThe algorithm is a variant of the stochastic variance reduced gradient (SVRG).\nIn several applications, including least-squares regressions, ridge\nregressions, linear discriminant analysis and regularized linear discriminant\nanalysis, the running time of each iteration is proportional to $d$. Under\nsmoothness and convexity conditions, the algorithm has linear convergence. When\napplied to quadratic functions, our analysis improves the state-of-the-art\nperformance of SVRG up to a logarithmic factor. Furthermore, for\nwell-conditioned quadratic problems, our analysis improves the state-of-the-art\nrunning times of accelerated SVRG, and is better than the known matching lower\nbound, by a logarithmic factor. Our theoretical results are backed with\nnumerical experiments.",
          "link": "http://arxiv.org/abs/2006.01017",
          "publishedOn": "2021-06-16T01:21:10.341Z",
          "wordCount": 580,
          "title": "Improved SVRG for quadratic functions. (arXiv:2006.01017v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdolmaleki_A/0/1/0/all/0/1\">Abbas Abdolmaleki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sandy H. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vezzani_G/0/1/0/all/0/1\">Giulia Vezzani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriari_B/0/1/0/all/0/1\">Bobak Shahriari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Springenberg_J/0/1/0/all/0/1\">Jost Tobias Springenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shruti Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+TB_D/0/1/0/all/0/1\">Dhruva TB</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byravan_A/0/1/0/all/0/1\">Arunkumar Byravan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bousmalis_K/0/1/0/all/0/1\">Konstantinos Bousmalis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1\">Andras Gyorgy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesvari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1\">Raia Hadsell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1\">Nicolas Heess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1\">Martin Riedmiller</a>",
          "description": "Many advances that have improved the robustness and efficiency of deep\nreinforcement learning (RL) algorithms can, in one way or another, be\nunderstood as introducing additional objectives, or constraints, in the policy\noptimization step. This includes ideas as far ranging as exploration bonuses,\nentropy regularization, and regularization toward teachers or data priors when\nlearning from experts or in offline RL. Often, task reward and auxiliary\nobjectives are in conflict with each other and it is therefore natural to treat\nthese examples as instances of multi-objective (MO) optimization problems. We\nstudy the principles underlying MORL and introduce a new algorithm,\nDistillation of a Mixture of Experts (DiME), that is intuitive and\nscale-invariant under some conditions. We highlight its strengths on standard\nMO benchmark problems and consider case studies in which we recast offline RL\nand learning from experts as MO problems. This leads to a natural algorithmic\nformulation that sheds light on the connection between existing approaches. For\noffline RL, we use the MO perspective to derive a simple algorithm, that\noptimizes for the standard RL objective plus a behavioral cloning term. This\noutperforms state-of-the-art on two established offline RL benchmarks.",
          "link": "http://arxiv.org/abs/2106.08199",
          "publishedOn": "2021-06-16T01:21:10.334Z",
          "wordCount": 643,
          "title": "On Multi-objective Policy Optimization as a Tool for Reinforcement Learning. (arXiv:2106.08199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blakeney_C/0/1/0/all/0/1\">Cody Blakeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huish_N/0/1/0/all/0/1\">Nathaniel Huish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Ziliang Zong</a>",
          "description": "In recent years the ubiquitous deployment of AI has posed great concerns in\nregards to algorithmic bias, discrimination, and fairness. Compared to\ntraditional forms of bias or discrimination caused by humans, algorithmic bias\ngenerated by AI is more abstract and unintuitive therefore more difficult to\nexplain and mitigate. A clear gap exists in the current literature on\nevaluating and mitigating bias in pruned neural networks. In this work, we\nstrive to tackle the challenging issues of evaluating, mitigating, and\nexplaining induced bias in pruned neural networks. Our paper makes three\ncontributions. First, we propose two simple yet effective metrics, Combined\nError Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively\nevaluate the induced bias prevention quality of pruned models. Second, we\ndemonstrate that knowledge distillation can mitigate induced bias in pruned\nneural networks, even with unbalanced datasets. Third, we reveal that model\nsimilarity has strong correlations with pruning induced bias, which provides a\npowerful method to explain why bias occurs in pruned neural networks. Our code\nis available at https://github.com/codestar12/pruning-distilation-bias",
          "link": "http://arxiv.org/abs/2106.07849",
          "publishedOn": "2021-06-16T01:21:10.326Z",
          "wordCount": 616,
          "title": "Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks with Knowledge Distillation. (arXiv:2106.07849v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08317",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoqiao Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1\">Sisi Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Thomson_M/0/1/0/all/0/1\">Matt Thomson</a>",
          "description": "Sequencing costs currently prohibit the application of single cell mRNA-seq\nfor many biological and clinical tasks of interest. Here, we introduce an\nactive learning framework that constructs compressed gene sets that enable high\naccuracy classification of cell-types and physiological states while analyzing\na minimal number of gene transcripts. Our active feature selection procedure\nconstructs gene sets through an iterative cell-type classification task where\nmisclassified cells are examined at each round to identify maximally\ninformative genes through an `active' support vector machine (SVM) classifier.\nOur active SVM procedure automatically identifies gene sets that enables\n$>90\\%$ cell-type classification accuracy in the Tabula Muris mouse tissue\nsurvey as well as a $\\sim 40$ gene set that enables classification of multiple\nmyeloma patient samples with $>95\\%$ accuracy. Broadly, the discovery of\ncompact but highly informative gene sets might enable drastic reductions in\nsequencing requirements for applications of single-cell mRNA-seq.",
          "link": "http://arxiv.org/abs/2106.08317",
          "publishedOn": "2021-06-16T01:21:10.312Z",
          "wordCount": 591,
          "title": "Active feature selection discovers minimal gene-sets for classifying cell-types and disease states in single-cell mRNA-seq data. (arXiv:2106.08317v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Biwei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seljak_U/0/1/0/all/0/1\">Uros Seljak</a>",
          "description": "We develop an iterative (greedy) deep learning (DL) algorithm which is able\nto transform an arbitrary probability distribution function (PDF) into the\ntarget PDF. The model is based on iterative Optimal Transport of a series of 1D\nslices, matching on each slice the marginal PDF to the target. The axes of the\northogonal slices are chosen to maximize the PDF difference using Wasserstein\ndistance at each iteration, which enables the algorithm to scale well to high\ndimensions. As special cases of this algorithm, we introduce two sliced\niterative Normalizing Flow (SINF) models, which map from the data to the latent\nspace (GIS) and vice versa (SIG). We show that SIG is able to generate high\nquality samples of image datasets, which match the GAN benchmarks, while GIS\nobtains competitive results on density estimation tasks compared to the density\ntrained NFs, and is more stable, faster, and achieves higher $p(x)$ when\ntrained on small training sets. SINF approach deviates significantly from the\ncurrent DL paradigm, as it is greedy and does not use concepts such as\nmini-batching, stochastic gradient descent and gradient back-propagation\nthrough deep layers.",
          "link": "http://arxiv.org/abs/2007.00674",
          "publishedOn": "2021-06-16T01:21:10.292Z",
          "wordCount": null,
          "title": "Sliced Iterative Normalizing Flows. (arXiv:2007.00674v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Arun S. Maiya</a>",
          "description": "The vast majority of existing methods and systems for causal inference assume\nthat all variables under consideration are categorical or numerical (e.g.,\ngender, price, blood pressure, enrollment). In this paper, we present\nCausalNLP, a toolkit for inferring causality from observational data that\nincludes text in addition to traditional numerical and categorical variables.\nCausalNLP employs the use of meta-learners for treatment effect estimation and\nsupports using raw text and its linguistic properties as both a treatment and a\n\"controlled-for\" variable (e.g., confounder). The library is open-source and\navailable at: https://github.com/amaiya/causalnlp.",
          "link": "http://arxiv.org/abs/2106.08043",
          "publishedOn": "2021-06-16T01:21:10.274Z",
          "wordCount": 521,
          "title": "CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1911.01529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blumenkamp_J/0/1/0/all/0/1\">Jan Blumenkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baude_A/0/1/0/all/0/1\">Andreas Baude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laue_T/0/1/0/all/0/1\">Tim Laue</a>",
          "description": "Deep learning approaches have become the standard solution to many problems\nin computer vision and robotics, but obtaining sufficient training data in high\nenough quality is challenging, as human labor is error prone, time consuming,\nand expensive. Solutions based on simulation have become more popular in recent\nyears, but the gap between simulation and reality is still a major issue. In\nthis paper, we introduce a novel method for augmenting synthetic image data\nthrough unsupervised image-to-image translation by applying the style of real\nworld images to simulated images with open source frameworks. The generated\ndataset is combined with conventional augmentation methods and is then applied\nto a neural network model running in real-time on autonomous soccer robots. Our\nevaluation shows a significant improvement compared to models trained on images\ngenerated entirely in simulation.",
          "link": "http://arxiv.org/abs/1911.01529",
          "publishedOn": "2021-06-16T01:21:10.268Z",
          "wordCount": 599,
          "title": "Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation. (arXiv:1911.01529v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhaozhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Minghao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>",
          "description": "Transformer models have demonstrated superior performance in natural language\nprocessing. The dot product self-attention in Transformer allows us to model\ninteractions between words. However, this modeling comes with significant\ncomputational overhead. In this work, we revisit the memory-compute trade-off\nassociated with Transformer, particularly multi-head attention, and show a\nmemory-heavy but significantly more compute-efficient alternative to\nTransformer. Our proposal, denoted as PairConnect, a multilayer perceptron\n(MLP), models the pairwise interaction between words by explicit pairwise word\nembeddings. As a result, PairConnect substitutes self dot product with a simple\nembedding lookup. We show mathematically that despite being an MLP, our\ncompute-efficient PairConnect is strictly more expressive than Transformer. Our\nexperiment on language modeling tasks suggests that PairConnect could achieve\ncomparable results with Transformer while reducing the computational cost\nassociated with inference significantly.",
          "link": "http://arxiv.org/abs/2106.08235",
          "publishedOn": "2021-06-16T01:21:10.262Z",
          "wordCount": 557,
          "title": "PairConnect: A Compute-Efficient MLP Alternative to Attention. (arXiv:2106.08235v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08217",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Alakus_C/0/1/0/all/0/1\">Cansu Alakus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Larocque_D/0/1/0/all/0/1\">Denis Larocque</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Labbe_A/0/1/0/all/0/1\">Aurelie Labbe</a>",
          "description": "Like many predictive models, random forests provide a point prediction for a\nnew observation. Besides the point prediction, it is important to quantify the\nuncertainty in the prediction. Prediction intervals provide information about\nthe reliability of the point predictions. We have developed a comprehensive R\npackage, RFpredInterval, that integrates 16 methods to build prediction\nintervals with random forests and boosted forests. The methods implemented in\nthe package are a new method to build prediction intervals with boosted forests\n(PIBF) and 15 different variants to produce prediction intervals with random\nforests proposed by Roy and Larocque (2020). We perform an extensive simulation\nstudy and apply real data analyses to compare the performance of the proposed\nmethod to ten existing methods to build prediction intervals with random\nforests. The results show that the proposed method is very competitive and,\nglobally, it outperforms the competing methods.",
          "link": "http://arxiv.org/abs/2106.08217",
          "publishedOn": "2021-06-16T01:21:10.255Z",
          "wordCount": 586,
          "title": "RFpredInterval: An R Package for Prediction Intervals with Random Forests and Boosted Forests. (arXiv:2106.08217v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chulin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Federated Learning (FL) as a distributed learning paradigm that aggregates\ninformation from diverse clients to train a shared global model, has\ndemonstrated great success. However, malicious clients can perform poisoning\nattacks and model replacement to introduce backdoors into the trained global\nmodel. Although there have been intensive studies designing robust aggregation\nmethods and empirical robust federated training protocols against backdoors,\nexisting approaches lack robustness certification. This paper provides the\nfirst general framework, Certifiably Robust Federated Learning (CRFL), to train\ncertifiably robust FL models against backdoors. Our method exploits clipping\nand smoothing on model parameters to control the global model smoothness, which\nyields a sample-wise robustness certification on backdoors with limited\nmagnitude. Our certification also specifies the relation to federated learning\nparameters, such as poisoning ratio on instance level, number of attackers, and\ntraining iterations. Practically, we conduct comprehensive experiments across a\nrange of federated datasets, and provide the first benchmark for certified\nrobustness against backdoor attacks in federated learning. Our code is\navailable at https://github.com/AI-secure/CRFL.",
          "link": "http://arxiv.org/abs/2106.08283",
          "publishedOn": "2021-06-16T01:21:10.205Z",
          "wordCount": 594,
          "title": "CRFL: Certifiably Robust Federated Learning against Backdoor Attacks. (arXiv:2106.08283v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07801",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Bi_H/0/1/0/all/0/1\">Hangrui Bi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1\">Hengyi Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shi_C/0/1/0/all/0/1\">Chence Shi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Coley_C/0/1/0/all/0/1\">Connor Coley</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>",
          "description": "Reliably predicting the products of chemical reactions presents a fundamental\nchallenge in synthetic chemistry. Existing machine learning approaches\ntypically produce a reaction product by sequentially forming its subparts or\nintermediate molecules. Such autoregressive methods, however, not only require\na pre-defined order for the incremental construction but preclude the use of\nparallel decoding for efficient computation. To address these issues, we devise\na non-autoregressive learning paradigm that predicts reaction in one shot.\nLeveraging the fact that chemical reactions can be described as a\nredistribution of electrons in molecules, we formulate a reaction as an\narbitrary electron flow and predict it with a novel multi-pointer decoding\nnetwork. Experiments on the USPTO-MIT dataset show that our approach has\nestablished a new state-of-the-art top-1 accuracy and achieves at least 27\ntimes inference speedup over the state-of-the-art methods. Also, our\npredictions are easier for chemists to interpret owing to predicting the\nelectron flows.",
          "link": "http://arxiv.org/abs/2106.07801",
          "publishedOn": "2021-06-16T01:21:10.185Z",
          "wordCount": 584,
          "title": "Non-Autoregressive Electron Redistribution Modeling for Reaction Prediction. (arXiv:2106.07801v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bowen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>",
          "description": "Maximum likelihood estimation (MLE) is the predominant algorithm for training\ntext generation models. This paradigm relies on direct supervision examples,\nwhich is not applicable to many applications, such as generating adversarial\nattacks or generating prompts to control language models. Reinforcement\nlearning (RL) on the other hand offers a more flexible solution by allowing\nusers to plug in arbitrary task metrics as reward. Yet previous RL algorithms\nfor text generation, such as policy gradient (on-policy RL) and Q-learning\n(off-policy RL), are often notoriously inefficient or unstable to train due to\nthe large sequence space and the sparse reward received only at the end of\nsequences. In this paper, we introduce a new RL formulation for text generation\nfrom the soft Q-learning perspective. It further enables us to draw from the\nlatest RL advances, such as path consistency learning, to combine the best of\non-/off-policy updates, and learn effectively from sparse reward. We apply the\napproach to a wide range of tasks, including learning from noisy/negative\nexamples, adversarial attacks, and prompt generation. Experiments show our\napproach consistently outperforms both task-specialized algorithms and the\nprevious RL methods. On standard supervised tasks where MLE prevails, our\napproach also achieves competitive performance and stability by training text\ngeneration from scratch.",
          "link": "http://arxiv.org/abs/2106.07704",
          "publishedOn": "2021-06-16T01:21:10.177Z",
          "wordCount": 641,
          "title": "Text Generation with Efficient (Soft) Q-Learning. (arXiv:2106.07704v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1912.08421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Liyao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Congcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Powered by machine learning services in the cloud, numerous learning-driven\nmobile applications are gaining popularity in the market. As deep learning\ntasks are mostly computation-intensive, it has become a trend to process raw\ndata on devices and send the deep neural network (DNN) features to the cloud,\nwhere the features are further processed to return final results. However,\nthere is always unexpected leakage with the release of features, with which an\nadversary could infer a significant amount of information about the original\ndata. We propose a privacy-preserving reinforcement learning framework on top\nof the mobile cloud infrastructure from the perspective of DNN structures. The\nframework aims to learn a policy to modify the base DNNs to prevent information\nleakage while maintaining high inference accuracy. The policy can also be\nreadily transferred to large-size DNNs to speed up learning. Extensive\nevaluations on a variety of DNNs have shown that our framework can successfully\nfind privacy-preserving DNN structures to defend different privacy attacks.",
          "link": "http://arxiv.org/abs/1912.08421",
          "publishedOn": "2021-06-16T01:21:10.170Z",
          "wordCount": 636,
          "title": "Learning to Prevent Leakage: Privacy-Preserving Inference in the Mobile Cloud. (arXiv:1912.08421v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1\">Zhiqi Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qi Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "In deep learning with differential privacy (DP), the neural network achieves\nthe privacy usually at the cost of slower convergence (and thus lower\nperformance) than its non-private counterpart. This work gives the first\nconvergence analysis of the DP deep learning, through the lens of training\ndynamics and the neural tangent kernel (NTK). Our convergence theory\nsuccessfully characterizes the effects of two key components in the DP\ntraining: the per-sample clipping (flat or layerwise) and the noise addition.\nOur analysis not only initiates a general principled framework to understand\nthe DP deep learning with any network architecture and loss function, but also\nmotivates a new clipping method -- the global clipping, that significantly\nimproves the convergence while preserving the same privacy guarantee as the\nexisting local clipping.\n\nIn terms of theoretical results, we establish the precise connection between\nthe per-sample clipping and NTK matrix. We show that in the gradient flow,\ni.e., with infinitesimal learning rate, the noise level of DP optimizers does\nnot affect the convergence. We prove that DP gradient descent (GD) with global\nclipping guarantees the monotone convergence to zero loss, which can be\nviolated by the existing DP-GD with local clipping. Notably, our analysis\nframework easily extends to other optimizers, e.g., DP-Adam. Empirically\nspeaking, DP optimizers equipped with global clipping perform strongly on a\nwide range of classification and regression tasks. In particular, our global\nclipping is surprisingly effective at learning calibrated classifiers, in\ncontrast to the existing DP classifiers which are oftentimes over-confident and\nunreliable. Implementation-wise, the new clipping can be realized by adding one\nline of code into the Opacus library.",
          "link": "http://arxiv.org/abs/2106.07830",
          "publishedOn": "2021-06-16T01:21:10.161Z",
          "wordCount": 698,
          "title": "On the Convergence of Deep Learning with Differential Privacy. (arXiv:2106.07830v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Levy_E/0/1/0/all/0/1\">Efrat Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1\">Asaf Shabtai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groza_B/0/1/0/all/0/1\">Bogdan Groza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murvay_P/0/1/0/all/0/1\">Pal-Stefan Murvay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>",
          "description": "The Controller Area Network (CAN) is used for communication between\nin-vehicle devices. The CAN bus has been shown to be vulnerable to remote\nattacks. To harden vehicles against such attacks, vehicle manufacturers have\ndivided in-vehicle networks into sub-networks, logically isolating critical\ndevices. However, attackers may still have physical access to various\nsub-networks where they can connect a malicious device. This threat has not\nbeen adequately addressed, as methods proposed to determine physical intrusion\npoints have shown weak results, emphasizing the need to develop more advanced\ntechniques. To address this type of threat, we propose a security hardening\nsystem for in-vehicle networks. The proposed system includes two mechanisms\nthat process deep features extracted from voltage signals measured on the CAN\nbus. The first mechanism uses data augmentation and deep learning to detect and\nlocate physical intrusions when the vehicle starts; this mechanism can detect\nand locate intrusions, even when the connected malicious devices are silent.\nThis mechanism's effectiveness (100% accuracy) is demonstrated in a wide\nvariety of insertion scenarios on a CAN bus prototype. The second mechanism is\na continuous device authentication mechanism, which is also based on deep\nlearning; this mechanism's robustness (99.8% accuracy) is demonstrated on a\nreal moving vehicle.",
          "link": "http://arxiv.org/abs/2106.07895",
          "publishedOn": "2021-06-16T01:21:10.152Z",
          "wordCount": 655,
          "title": "CAN-LOC: Spoofing Detection and Physical Intrusion Localization on an In-Vehicle CAN Bus Based on Deep Features of Voltage Signals. (arXiv:2106.07895v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Junchen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1\">Michael T. Schaub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>",
          "description": "Recent studies have exposed that many graph neural networks (GNNs) are\nsensitive to adversarial attacks, and can suffer from performance loss if the\ngraph structure is intentionally perturbed. A different line of research has\nshown that many GNN architectures implicitly assume that the underlying graph\ndisplays homophily, i.e., connected nodes are more likely to have similar\nfeatures and class labels, and perform poorly if this assumption is not\nfulfilled. In this work, we formalize the relation between these two seemingly\ndifferent issues. We theoretically show that in the standard scenario in which\nnode features exhibit homophily, impactful structural attacks always lead to\nincreased levels of heterophily. Then, inspired by GNN architectures that\ntarget heterophily, we present two designs -- (i) separate aggregators for ego-\nand neighbor-embeddings, and (ii) a reduced scope of aggregation -- that can\nsignificantly improve the robustness of GNNs. Our extensive empirical\nevaluations show that GNNs featuring merely these two designs can achieve\nsignificantly improved robustness compared to the best-performing unvaccinated\nmodel with 24.99% gain in average performance under targeted attacks, while\nhaving smaller computational overhead than existing defense mechanisms.\nFurthermore, these designs can be readily combined with explicit defense\nmechanisms to yield state-of-the-art robustness with up to 18.33% increase in\nperformance under attacks compared to the best-performing vaccinated model.",
          "link": "http://arxiv.org/abs/2106.07767",
          "publishedOn": "2021-06-16T01:21:10.112Z",
          "wordCount": 653,
          "title": "Improving Robustness of Graph Neural Networks with Heterophily-Inspired Designs. (arXiv:2106.07767v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akundi_P/0/1/0/all/0/1\">Prathyusha Akundi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gun_S/0/1/0/all/0/1\">Soumyasis Gun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivaswamy_J/0/1/0/all/0/1\">Jayanthi Sivaswamy</a>",
          "description": "Melanoma is a leading cause of deaths due to skin cancer deaths and hence,\nearly and effective diagnosis of melanoma is of interest. Current approaches\nfor automated diagnosis of melanoma either use pattern recognition or\nanalytical recognition like ABCDE (asymmetry, border, color, diameter and\nevolving) criterion. In practice however, a differential approach wherein\noutliers (ugly duckling) are detected and used to evaluate nevi/lesions.\nIncorporation of differential recognition in Computer Aided Diagnosis (CAD)\nsystems has not been explored but can be beneficial as it can provide a\nclinical justification for the derived decision. We present a method for\nidentifying and quantifying ugly ducklings by performing Intra-Patient\nComparative Analysis (IPCA) of neighboring nevi. This is then incorporated in a\nCAD system design for melanoma detection. This design ensures flexibility to\nhandle cases where IPCA is not possible. Our experiments on a public dataset\nshow that the outlier information helps boost the sensitivity of detection by\nat least 4.1 % and specificity by 4.0 % to 8.9 %, depending on the use of a\nstrong (EfficientNet) or moderately strong (VGG or ResNet) classifier.",
          "link": "http://arxiv.org/abs/2106.08021",
          "publishedOn": "2021-06-16T01:21:10.098Z",
          "wordCount": 620,
          "title": "A Clinically Inspired Approach for Melanoma classification. (arXiv:2106.08021v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">A. Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Toan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1\">At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</a>",
          "description": "Domain adaptation is an important problem and often needed for real-world\napplications. In this problem, instead of i.i.d. datapoints, we assume that the\nsource (training) data and the target (testing) data have different\ndistributions. With that setting, the empirical risk minimization training\nprocedure often does not perform well, since it does not account for the change\nin the distribution. A common approach in the domain adaptation literature is\nto learn a representation of the input that has the same distributions over the\nsource and the target domain. However, these approaches often require\nadditional networks and/or optimizing an adversarial (minimax) objective, which\ncan be very expensive or unstable in practice. To tackle this problem, we first\nderive a generalization bound for the target loss based on the training loss\nand the reverse Kullback-Leibler (KL) divergence between the source and the\ntarget representation distributions. Based on this bound, we derive an\nalgorithm that minimizes the KL term to obtain a better generalization to the\ntarget domain. We show that with a probabilistic representation network, the KL\nterm can be estimated efficiently via minibatch samples without any additional\nnetwork or a minimax objective. This leads to a theoretically sound alignment\nmethod which is also very efficient and stable in practice. Experimental\nresults also suggest that our method outperforms other representation-alignment\napproaches.",
          "link": "http://arxiv.org/abs/2106.07780",
          "publishedOn": "2021-06-16T01:21:10.092Z",
          "wordCount": 641,
          "title": "KL Guided Domain Adaptation. (arXiv:2106.07780v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08208",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Adaptive gradient methods have shown excellent performance for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using specific adaptive learning rates. It\nis desired to design a universal framework for practical algorithms of adaptive\ngradients with theoretical guarantee to solve general problems. To fill this\ngap, we propose a faster and universal framework of adaptive gradients (i.e.,\nSUPER-ADAM) by introducing a universal adaptive matrix that includes most\nexisting adaptive gradient forms. Moreover, our framework can flexibly\nintegrates the momentum and variance reduced techniques. In particular, our\nnovel framework provides the convergence analysis support for adaptive gradient\nmethods under the nonconvex setting. In theoretical analysis, we prove that our\nnew algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms.",
          "link": "http://arxiv.org/abs/2106.08208",
          "publishedOn": "2021-06-16T01:21:10.083Z",
          "wordCount": 616,
          "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1\">Zachary Charles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1\">Zachary Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmulyian_S/0/1/0/all/0/1\">Sergei Shmulyian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1\">Virginia Smith</a>",
          "description": "Federated learning methods typically learn a model by iteratively sampling\nupdates from a population of clients. In this work, we explore how the number\nof clients sampled at each round (the cohort size) impacts the quality of the\nlearned model and the training dynamics of federated learning algorithms. Our\nwork poses three fundamental questions. First, what challenges arise when\ntrying to scale federated learning to larger cohorts? Second, what parallels\nexist between cohort sizes in federated learning and batch sizes in centralized\nlearning? Last, how can we design federated learning methods that effectively\nutilize larger cohort sizes? We give partial answers to these questions based\non extensive empirical evaluation. Our work highlights a number of challenges\nstemming from the use of larger cohorts. While some of these (such as\ngeneralization issues and diminishing returns) are analogs of large-batch\ntraining challenges, others (including training failures and fairness concerns)\nare unique to federated learning.",
          "link": "http://arxiv.org/abs/2106.07820",
          "publishedOn": "2021-06-16T01:21:10.074Z",
          "wordCount": 581,
          "title": "On Large-Cohort Training for Federated Learning. (arXiv:2106.07820v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaemoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1\">Changyeon Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1\">Jeongwoo Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungjoo Kang</a>",
          "description": "Out-of-distribution (OOD) detection is an important task in machine learning\nsystems for ensuring their reliability and safety. Deep probabilistic\ngenerative models facilitate OOD detection by estimating the likelihood of a\ndata sample. However, such models frequently assign a suspiciously high\nlikelihood to a specific outlier. Several recent works have addressed this\nissue by training a neural network with auxiliary outliers, which are generated\nby perturbing the input data. In this paper, we discover that these approaches\nfail for certain OOD datasets. Thus, we suggest a new detection metric that\noperates without outlier exposure. We observe that our metric is robust to\ndiverse variations of an image compared to the previous outlier-exposing\nmethods. Furthermore, our proposed score requires neither auxiliary models nor\nadditional training. Instead, this paper utilizes the likelihood ratio\nstatistic in a new perspective to extract genuine properties from the given\nsingle deep probabilistic generative model. We also apply a novel numerical\napproximation to enable fast implementation. Finally, we demonstrate\ncomprehensive experiments on various probabilistic generative models and show\nthat our method achieves state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.07903",
          "publishedOn": "2021-06-16T01:21:10.049Z",
          "wordCount": 612,
          "title": "Robust Out-of-Distribution Detection on Deep Probabilistic Generative Models. (arXiv:2106.07903v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songnan Li</a>",
          "description": "Compressing Deep Neural Network (DNN) models to alleviate the storage and\ncomputation requirements is essential for practical applications, especially\nfor resource limited devices. Although capable of reducing a reasonable amount\nof model parameters, previous unstructured or structured weight pruning methods\ncan hardly truly accelerate inference, either due to the poor hardware\ncompatibility of the unstructured sparsity or due to the low sparse rate of the\nstructurally pruned network. Aiming at reducing both storage and computation,\nas well as preserving the original task performance, we propose a generalized\nweight unification framework at a hardware compatible micro-structured level to\nachieve high amount of compression and acceleration. Weight coefficients of a\nselected micro-structured block are unified to reduce the storage and\ncomputation of the block without changing the neuron connections, which turns\nto a micro-structured pruning special case when all unified coefficients are\nset to zero, where neuron connections (hence storage and computation) are\ncompletely removed. In addition, we developed an effective training framework\nbased on the alternating direction method of multipliers (ADMM), which converts\nour complex constrained optimization into separately solvable subproblems.\nThrough iteratively optimizing the subproblems, the desired micro-structure can\nbe ensured with high compression ratio and low performance degradation. We\nextensively evaluated our method using a variety of benchmark models and\ndatasets for different applications. Experimental results demonstrate\nstate-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.08301",
          "publishedOn": "2021-06-16T01:21:10.042Z",
          "wordCount": 669,
          "title": "Efficient Micro-Structured Weight Unification for Neural Network Compression. (arXiv:2106.08301v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1\">Arsenii Ashukha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1\">Andrei Atanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Averaging predictions over a set of models -- an ensemble -- is widely used\nto improve predictive performance and uncertainty estimation of deep learning\nmodels. At the same time, many machine learning systems, such as search,\nmatching, and recommendation systems, heavily rely on embeddings.\nUnfortunately, due to misalignment of features of independently trained models,\nembeddings, cannot be improved with a naive deep ensemble like approach. In\nthis work, we look at the ensembling of representations and propose mean\nembeddings with test-time augmentation (MeTTA) simple yet well-performing\nrecipe for ensembling representations. Empirically we demonstrate that MeTTA\nsignificantly boosts the quality of linear evaluation on ImageNet for both\nsupervised and self-supervised models. Even more exciting, we draw connections\nbetween MeTTA, image retrieval, and transformation invariant models. We believe\nthat spreading the success of ensembles to inference higher-quality\nrepresentations is the important step that will open many new applications of\nensembling.",
          "link": "http://arxiv.org/abs/2106.08038",
          "publishedOn": "2021-06-16T01:21:10.030Z",
          "wordCount": 580,
          "title": "Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1\">Daniel J. Gauthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollt_E/0/1/0/all/0/1\">Erik Bollt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffith_A/0/1/0/all/0/1\">Aaron Griffith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_W/0/1/0/all/0/1\">Wendson A.S. Barbosa</a>",
          "description": "Reservoir computing is a best-in-class machine learning algorithm for\nprocessing information generated by dynamical systems using observed\ntime-series data. Importantly, it requires very small training data sets, uses\nlinear optimization, and thus requires minimal computing resources. However,\nthe algorithm uses randomly sampled matrices to define the underlying recurrent\nneural network and has a multitude of metaparameters that must be optimized.\nRecent results demonstrate the equivalence of reservoir computing to nonlinear\nvector autoregression, which requires no random matrices, fewer metaparameters,\nand provides interpretable results. Here, we demonstrate that nonlinear vector\nautoregression excels at reservoir computing benchmark tasks and requires even\nshorter training data sets and training time, heralding the next generation of\nreservoir computing.",
          "link": "http://arxiv.org/abs/2106.07688",
          "publishedOn": "2021-06-16T01:21:10.023Z",
          "wordCount": 537,
          "title": "Next Generation Reservoir Computing. (arXiv:2106.07688v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1\">Ziheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>",
          "description": "Deep neural network (DNN) generally takes thousands of iterations to optimize\nvia gradient descent and thus has a slow convergence. In addition, softmax, as\na decision layer, may ignore the distribution information of the data during\nclassification. Aiming to tackle the referred problems, we propose a novel\nmanifold neural network based on non-gradient optimization, i.e., the\nclosed-form solutions. Considering that the activation function is generally\ninvertible, we reconstruct the network via forward ridge regression and low\nrank backward approximation, which achieve the rapid convergence. Moreover, by\nunifying the flexible Stiefel manifold and adaptive support vector machine, we\ndevise the novel decision layer which efficiently fits the manifold structure\nof the data and label information. Consequently, a jointly non-gradient\noptimization method is designed to generate the network with closed-form\nresults. Eventually, extensive experiments validate the superior performance of\nthe model.",
          "link": "http://arxiv.org/abs/2106.07905",
          "publishedOn": "2021-06-16T01:21:10.014Z",
          "wordCount": 566,
          "title": "Non-Gradient Manifold Neural Network. (arXiv:2106.07905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "Automatic speaker verification (ASV) is a well developed technology for\nbiometric identification, and has been ubiquitous implemented in\nsecurity-critic applications, such as banking and access control. However,\nprevious works have shown that ASV is under the radar of adversarial attacks,\nwhich are very similar to their original counterparts from human's perception,\nyet will manipulate the ASV render wrong prediction. Due to the very late\nemergence of adversarial attacks for ASV, effective countermeasures against\nthem are limited. Given that the security of ASV is of high priority, in this\nwork, we propose the idea of \"voting for the right answer\" to prevent risky\ndecisions of ASV in blind spot areas, by employing random sampling and voting.\nExperimental results show that our proposed method improves the robustness\nagainst both the limited-knowledge attackers by pulling the adversarial samples\nout of the blind spots, and the perfect-knowledge attackers by introducing\nrandomness and increasing the attackers' budgets. The code for reproducing main\nresults is available at https://github.com/thuhcsi/adsv_voting.",
          "link": "http://arxiv.org/abs/2106.07868",
          "publishedOn": "2021-06-16T01:21:10.001Z",
          "wordCount": 620,
          "title": "Voting for the right answer: Adversarial defense for speaker verification. (arXiv:2106.07868v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Sungyong Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1\">Sercan O. Arik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jinsung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>",
          "description": "We propose a novel training method to integrate rules into deep learning, in\na way their strengths are controllable at inference. Deep Neural Networks with\nControllable Rule Representations (DeepCTRL) incorporates a rule encoder into\nthe model coupled with a rule-based objective, enabling a shared representation\nfor decision making. DeepCTRL is agnostic to data type and model architecture.\nIt can be applied to any kind of rule defined for inputs and outputs. The key\naspect of DeepCTRL is that it does not require retraining to adapt the rule\nstrength -- at inference, the user can adjust it based on the desired operation\npoint on accuracy vs. rule verification ratio. In real-world domains where\nincorporating rules is critical -- such as Physics, Retail and Healthcare -- we\nshow the effectiveness of DeepCTRL in teaching rules for deep learning.\nDeepCTRL improves the trust and reliability of the trained models by\nsignificantly increasing their rule verification ratio, while also providing\naccuracy gains at downstream tasks. Additionally, DeepCTRL enables novel use\ncases such as hypothesis testing of the rules on data samples, and unsupervised\nadaptation based on shared rules between datasets.",
          "link": "http://arxiv.org/abs/2106.07804",
          "publishedOn": "2021-06-16T01:21:09.977Z",
          "wordCount": 615,
          "title": "Controlling Neural Networks with Rule Representations. (arXiv:2106.07804v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajput_S/0/1/0/all/0/1\">Shashank Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasan_K/0/1/0/all/0/1\">Kartik Sreenivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1\">Amin Karbasi</a>",
          "description": "It is well known that modern deep neural networks are powerful enough to\nmemorize datasets even when the labels have been randomized. Recently,\nVershynin (2020) settled a long standing question by Baum (1988), proving that\n\\emph{deep threshold} networks can memorize $n$ points in $d$ dimensions using\n$\\widetilde{\\mathcal{O}}(e^{1/\\delta^2}+\\sqrt{n})$ neurons and\n$\\widetilde{\\mathcal{O}}(e^{1/\\delta^2}(d+\\sqrt{n})+n)$ weights, where $\\delta$\nis the minimum distance between the points. In this work, we improve the\ndependence on $\\delta$ from exponential to almost linear, proving that\n$\\widetilde{\\mathcal{O}}(\\frac{1}{\\delta}+\\sqrt{n})$ neurons and\n$\\widetilde{\\mathcal{O}}(\\frac{d}{\\delta}+n)$ weights are sufficient. Our\nconstruction uses Gaussian random weights only in the first layer, while all\nthe subsequent layers use binary or integer weights. We also prove new lower\nbounds by connecting memorization in neural networks to the purely geometric\nproblem of separating $n$ points on a sphere using hyperplanes.",
          "link": "http://arxiv.org/abs/2106.07724",
          "publishedOn": "2021-06-16T01:21:09.963Z",
          "wordCount": 571,
          "title": "An Exponential Improvement on the Memorization Capacity of Deep Threshold Networks. (arXiv:2106.07724v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "We introduce a self-supervised vision representation model BEiT, which stands\nfor Bidirectional Encoder representation from Image Transformers. Following\nBERT developed in the natural language processing area, we propose a masked\nimage modeling task to pretrain vision Transformers. Specifically, each image\nhas two views in our pre-training, i.e, image patches (such as 16x16 pixels),\nand visual tokens (i.e., discrete tokens). We first \"tokenize\" the original\nimage into visual tokens. Then we randomly mask some image patches and fed them\ninto the backbone Transformer. The pre-training objective is to recover the\noriginal visual tokens based on the corrupted image patches. After pre-training\nBEiT, we directly fine-tune the model parameters on downstream tasks by\nappending task layers upon the pretrained encoder. Experimental results on\nimage classification and semantic segmentation show that our model achieves\ncompetitive results with previous pre-training methods. For example, base-size\nBEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming\nfrom-scratch DeiT training (81.8%) with the same setup. Moreover, large-size\nBEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with\nsupervised pre-training on ImageNet-22K (85.2%). The code and pretrained models\nare available at https://aka.ms/beit.",
          "link": "http://arxiv.org/abs/2106.08254",
          "publishedOn": "2021-06-16T01:21:09.955Z",
          "wordCount": 625,
          "title": "BEiT: BERT Pre-Training of Image Transformers. (arXiv:2106.08254v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08247",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1\">Sikai Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1\">Tingna Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Worden_K/0/1/0/all/0/1\">Keith Worden</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cross_E/0/1/0/all/0/1\">Elizabeth J. Cross</a>",
          "description": "This paper proposes a canonical-correlation-based filter method for feature\nselection. The sum of squared canonical correlation coefficients is adopted as\nthe feature ranking criterion. The proposed method boosts the computational\nspeed of the ranking criterion in greedy search. The supporting theorems\ndeveloped for the feature selection method are fundamental to the understanding\nof the canonical correlation analysis. In empirical studies, a synthetic\ndataset is used to demonstrate the speed advantage of the proposed method, and\neight real datasets are applied to show the effectiveness of the proposed\nfeature ranking criterion in both classification and regression. The results\nshow that the proposed method is considerably faster than the definition-based\nmethod, and the proposed ranking criterion is competitive compared with the\nseven mutual-information-based criteria.",
          "link": "http://arxiv.org/abs/2106.08247",
          "publishedOn": "2021-06-16T01:21:09.944Z",
          "wordCount": 543,
          "title": "Canonical-Correlation-Based Fast Feature Selection. (arXiv:2106.08247v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malik_D/0/1/0/all/0/1\">Dhruv Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1\">Aldo Pacchiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_V/0/1/0/all/0/1\">Vishwak Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>",
          "description": "Reinforcement learning (RL) is empirically successful in complex nonlinear\nMarkov decision processes (MDPs) with continuous state spaces. By contrast, the\nmajority of theoretical RL literature requires the MDP to satisfy some form of\nlinear structure, in order to guarantee sample efficient RL. Such efforts\ntypically assume the transition dynamics or value function of the MDP are\ndescribed by linear functions of the state features. To resolve this\ndiscrepancy between theory and practice, we introduce the Effective Planning\nWindow (EPW) condition, a structural condition on MDPs that makes no linearity\nassumptions. We demonstrate that the EPW condition permits sample efficient RL,\nby providing an algorithm which provably solves MDPs satisfying this condition.\nOur algorithm requires minimal assumptions on the policy class, which can\ninclude multi-layer neural networks with nonlinear activation functions.\nNotably, the EPW condition is directly motivated by popular gaming benchmarks,\nand we show that many classic Atari games satisfy this condition. We\nadditionally show the necessity of conditions like EPW, by demonstrating that\nsimple MDPs with slight nonlinearities cannot be solved sample efficiently.",
          "link": "http://arxiv.org/abs/2106.07814",
          "publishedOn": "2021-06-16T01:21:09.938Z",
          "wordCount": 618,
          "title": "Sample Efficient Reinforcement Learning In Continuous State Spaces: A Perspective Beyond Linearity. (arXiv:2106.07814v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07875",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengze Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hooker_G/0/1/0/all/0/1\">Giles Hooker</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>",
          "description": "An increasing number of machine learning models have been deployed in domains\nwith high stakes such as finance and healthcare. Despite their superior\nperformances, many models are black boxes in nature which are hard to explain.\nThere are growing efforts for researchers to develop methods to interpret these\nblack-box models. Post hoc explanations based on perturbations, such as LIME,\nare widely used approaches to interpret a machine learning model after it has\nbeen built. This class of methods has been shown to exhibit large instability,\nposing serious challenges to the effectiveness of the method itself and harming\nuser trust. In this paper, we propose S-LIME, which utilizes a hypothesis\ntesting framework based on central limit theorem for determining the number of\nperturbation points needed to guarantee stability of the resulting explanation.\nExperiments on both simulated and real world data sets are provided to\ndemonstrate the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2106.07875",
          "publishedOn": "2021-06-16T01:21:09.919Z",
          "wordCount": 593,
          "title": "S-LIME: Stabilized-LIME for Model Explanation. (arXiv:2106.07875v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shreyan Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praher_V/0/1/0/all/0/1\">Verena Praher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1\">Gerhard Widmer</a>",
          "description": "Music emotion recognition is an important task in MIR (Music Information\nRetrieval) research. Owing to factors like the subjective nature of the task\nand the variation of emotional cues between musical genres, there are still\nsignificant challenges in developing reliable and generalizable models. One\nimportant step towards better models would be to understand what a model is\nactually learning from the data and how the prediction for a particular input\nis made. In previous work, we have shown how to derive explanations of model\npredictions in terms of spectrogram image segments that connect to the\nhigh-level emotion prediction via a layer of easily interpretable perceptual\nfeatures. However, that scheme lacks intuitive musical comprehensibility at the\nspectrogram level. In the present work, we bridge this gap by merging audioLIME\n-- a source-separation based explainer -- with mid-level perceptual features,\nthus forming an intuitive connection chain between the input audio and the\noutput emotion predictions. We demonstrate the usefulness of this method by\napplying it to debug a biased emotion prediction model.",
          "link": "http://arxiv.org/abs/2106.07787",
          "publishedOn": "2021-06-16T01:21:09.912Z",
          "wordCount": 615,
          "title": "Tracing Back Music Emotion Predictions to Sound Sources and Intuitive Perceptual Qualities. (arXiv:2106.07787v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07644",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Even_M/0/1/0/all/0/1\">Mathieu Even</a>, <a href=\"http://arxiv.org/find/math/1/au:+Berthier_R/0/1/0/all/0/1\">Rapha&#xeb;l Berthier</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/math/1/au:+Flammarion_N/0/1/0/all/0/1\">Nicolas Flammarion</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gaillard_P/0/1/0/all/0/1\">Pierre Gaillard</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hendrikx_H/0/1/0/all/0/1\">Hadrien Hendrikx</a>, <a href=\"http://arxiv.org/find/math/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>, <a href=\"http://arxiv.org/find/math/1/au:+Taylor_A/0/1/0/all/0/1\">Adrien Taylor</a>",
          "description": "We introduce the continuized Nesterov acceleration, a close variant of\nNesterov acceleration whose variables are indexed by a continuous time\nparameter. The two variables continuously mix following a linear ordinary\ndifferential equation and take gradient steps at random times. This continuized\nvariant benefits from the best of the continuous and the discrete frameworks:\nas a continuous process, one can use differential calculus to analyze\nconvergence and obtain analytical expressions for the parameters; and a\ndiscretization of the continuized process can be computed exactly with\nconvergence rates similar to those of Nesterov original acceleration. We show\nthat the discretization has the same structure as Nesterov acceleration, but\nwith random parameters. We provide continuized Nesterov acceleration under\ndeterministic as well as stochastic gradients, with either additive or\nmultiplicative noise. Finally, using our continuized framework and expressing\nthe gossip averaging problem as the stochastic minimization of a certain energy\nfunction, we provide the first rigorous acceleration of asynchronous gossip\nalgorithms.",
          "link": "http://arxiv.org/abs/2106.07644",
          "publishedOn": "2021-06-16T01:21:09.905Z",
          "wordCount": 625,
          "title": "A Continuized View on Nesterov Acceleration for Stochastic Gradient Descent and Randomized Gossip. (arXiv:2106.07644v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07900",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1\">Chaoqi Yang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1\">Cheng Qian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1\">Navjot Singh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1\">Cao Xiao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Westover_M/0/1/0/all/0/1\">M Brandon Westover</a>, <a href=\"http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1\">Edgar Solomonik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>",
          "description": "Tensor decompositions are powerful tools for dimensionality reduction and\nfeature interpretation of multidimensional data such as signals. Existing\ntensor decomposition objectives (e.g., Frobenius norm) are designed for fitting\nraw data under statistical assumptions, which may not align with downstream\nclassification tasks. Also, real-world tensor data are usually high-ordered and\nhave large dimensions with millions or billions of entries. Thus, it is\nexpensive to decompose the whole tensor with traditional algorithms. In\npractice, raw tensor data also contains redundant information while data\naugmentation techniques may be used to smooth out noise in samples. This paper\naddresses the above challenges by proposing augmented tensor decomposition\n(ATD), which effectively incorporates data augmentations to boost downstream\nclassification. To reduce the memory footprint of the decomposition, we propose\na stochastic algorithm that updates the factor matrices in a batch fashion. We\nevaluate ATD on multiple signal datasets. It shows comparable or better\nperformance (e.g., up to 15% in accuracy) over self-supervised and autoencoder\nbaselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy\ngain over other tensor-based baselines, and reduces the memory footprint by 9X\nwhen compared to standard tensor decomposition algorithms.",
          "link": "http://arxiv.org/abs/2106.07900",
          "publishedOn": "2021-06-16T01:21:09.888Z",
          "wordCount": 635,
          "title": "Augmented Tensor Decomposition with Stochastic Optimization. (arXiv:2106.07900v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asnani_V/0/1/0/all/0/1\">Vishal Asnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1\">Tal Hassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>",
          "description": "State-of-the-art (SOTA) Generative Models (GMs) can synthesize\nphoto-realistic images that are hard for humans to distinguish from genuine\nphotos. We propose to perform reverse engineering of GMs to infer the model\nhyperparameters from the images generated by these models. We define a novel\nproblem, \"model parsing\", as estimating GM network architectures and training\nloss functions by examining their generated images -- a task seemingly\nimpossible for human beings. To tackle this problem, we propose a framework\nwith two components: a Fingerprint Estimation Network (FEN), which estimates a\nGM fingerprint from a generated image by training with four constraints to\nencourage the fingerprint to have desired properties, and a Parsing Network\n(PN), which predicts network architecture and loss functions from the estimated\nfingerprints. To evaluate our approach, we collect a fake image dataset with\n$100$K images generated by $100$ GMs. Extensive experiments show encouraging\nresults in parsing the hyperparameters of the unseen models. Finally, our\nfingerprint estimation can be leveraged for deepfake detection and image\nattribution, as we show by reporting SOTA results on both the recent Celeb-DF\nand image attribution benchmarks.",
          "link": "http://arxiv.org/abs/2106.07873",
          "publishedOn": "2021-06-16T01:21:09.877Z",
          "wordCount": 624,
          "title": "Reverse Engineering of Generative Models: Inferring Model Hyperparameters from Generated Images. (arXiv:2106.07873v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_R/0/1/0/all/0/1\">Rupak Vignesh Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_B/0/1/0/all/0/1\">Brian King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Grant P. Strimel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>",
          "description": "We propose a simple yet effective method to compress an RNN-Transducer\n(RNN-T) through the well-known knowledge distillation paradigm. We show that\nthe transducer's encoder outputs naturally have a high entropy and contain rich\ninformation about acoustically similar word-piece confusions. This rich\ninformation is suppressed when combined with the lower entropy decoder outputs\nto produce the joint network logits. Consequently, we introduce an auxiliary\nloss to distill the encoder logits from a teacher transducer's encoder, and\nexplore training strategies where this encoder distillation works effectively.\nWe find that tandem training of teacher and student encoders with an inplace\nencoder distillation outperforms the use of a pre-trained and static teacher\ntransducer. We also report an interesting phenomenon we refer to as implicit\ndistillation, that occurs when the teacher and student encoders share the same\ndecoder. Our experiments show 5.37-8.4% relative word error rate reductions\n(WERR) on in-house test sets, and 5.05-6.18% relative WERRs on LibriSpeech test\nsets.",
          "link": "http://arxiv.org/abs/2106.07734",
          "publishedOn": "2021-06-16T01:21:09.858Z",
          "wordCount": 606,
          "title": "CoDERT: Distilling Encoder Representations with Co-learning for Transducer-based Speech Recognition. (arXiv:2106.07734v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curry_M/0/1/0/all/0/1\">Michael J. Curry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyi_U/0/1/0/all/0/1\">Uro Lyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John Dickerson</a>",
          "description": "We propose a new architecture to approximately learn incentive compatible,\nrevenue-maximizing auctions from sampled valuations. Our architecture uses the\nSinkhorn algorithm to perform a differentiable bipartite matching which allows\nthe network to learn strategyproof revenue-maximizing mechanisms in settings\nnot learnable by the previous RegretNet architecture. In particular, our\narchitecture is able to learn mechanisms in settings without free disposal\nwhere each bidder must be allocated exactly some number of items. In\nexperiments, we show our approach successfully recovers multiple known optimal\nmechanisms and high-revenue, low-regret mechanisms in larger settings where the\noptimal mechanism is unknown.",
          "link": "http://arxiv.org/abs/2106.07877",
          "publishedOn": "2021-06-16T01:21:09.842Z",
          "wordCount": 526,
          "title": "Learning Revenue-Maximizing Auctions With Differentiable Matching. (arXiv:2106.07877v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Avram_R/0/1/0/all/0/1\">Robert Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olgin_J/0/1/0/all/0/1\">Jeffrey E. Olgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_A/0/1/0/all/0/1\">Alvin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zeeshan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verreault_Julien_L/0/1/0/all/0/1\">Louis Verreault-Julien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abreau_S/0/1/0/all/0/1\">Sean Abreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1\">Derek Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">Derek Y. So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_K/0/1/0/all/0/1\">Krishan Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tison_G/0/1/0/all/0/1\">Geoffrey H. Tison</a>",
          "description": "Coronary heart disease (CHD) is the leading cause of adult death in the\nUnited States and worldwide, and for which the coronary angiography procedure\nis the primary gateway for diagnosis and clinical management decisions. The\nstandard-of-care for interpretation of coronary angiograms depends upon ad-hoc\nvisual assessment by the physician operator. However, ad-hoc visual\ninterpretation of angiograms is poorly reproducible, highly variable and bias\nprone. Here we show for the first time that fully-automated angiogram\ninterpretation to estimate coronary artery stenosis is possible using a\nsequence of deep neural network algorithms. The algorithmic pipeline we\ndeveloped--called CathAI--achieves state-of-the art performance across the\nsequence of tasks required to accomplish automated interpretation of\nunselected, real-world angiograms. CathAI (Algorithms 1-2) demonstrated\npositive predictive value, sensitivity and F1 score of >=90% to identify the\nprojection angle overall and >=93% for left or right coronary artery angiogram\ndetection, the primary anatomic structures of interest. To predict obstructive\ncoronary artery stenosis (>=70% stenosis), CathAI (Algorithm 4) exhibited an\narea under the receiver operating characteristic curve (AUC) of 0.862 (95% CI:\n0.843-0.880). When externally validated in a healthcare system in another\ncountry, CathAI AUC was 0.869 (95% CI: 0.830-0.907) to predict obstructive\ncoronary artery stenosis. Our results demonstrate that multiple purpose-built\nneural networks can function in sequence to accomplish the complex series of\ntasks required for automated analysis of real-world angiograms. Deployment of\nCathAI may serve to increase standardization and reproducibility in coronary\nstenosis assessment, while providing a robust foundation to accomplish future\ntasks for algorithmic angiographic interpretation.",
          "link": "http://arxiv.org/abs/2106.07708",
          "publishedOn": "2021-06-16T01:21:09.834Z",
          "wordCount": 727,
          "title": "CathAI: Fully Automated Interpretation of Coronary Angiograms Using Neural Networks. (arXiv:2106.07708v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07802",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ganea_O/0/1/0/all/0/1\">Octavian-Eugen Ganea</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pattanaik_L/0/1/0/all/0/1\">Lagnajit Pattanaik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Coley_C/0/1/0/all/0/1\">Connor W. Coley</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jensen_K/0/1/0/all/0/1\">Klavs F. Jensen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Green_W/0/1/0/all/0/1\">William H. Green</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi S. Jaakkola</a>",
          "description": "Prediction of a molecule's 3D conformer ensemble from the molecular graph\nholds a key role in areas of cheminformatics and drug discovery. Existing\ngenerative models have several drawbacks including lack of modeling important\nmolecular geometry elements (e.g. torsion angles), separate optimization stages\nprone to error accumulation, and the need for structure fine-tuning based on\napproximate classical force-fields or computationally expensive methods such as\nmetadynamics with approximate quantum mechanics calculations at each geometry.\nWe propose GeoMol--an end-to-end, non-autoregressive and SE(3)-invariant\nmachine learning approach to generate distributions of low-energy molecular 3D\nconformers. Leveraging the power of message passing neural networks (MPNNs) to\ncapture local and global graph information, we predict local atomic 3D\nstructures and torsion angles, avoiding unnecessary over-parameterization of\nthe geometric degrees of freedom (e.g. one angle per non-terminal bond). Such\nlocal predictions suffice both for the training loss computation, as well as\nfor the full deterministic conformer assembly (at test time). We devise a\nnon-adversarial optimal transport based loss function to promote diverse\nconformer generation. GeoMol predominantly outperforms popular open-source,\ncommercial, or state-of-the-art machine learning (ML) models, while achieving\nsignificant speed-ups. We expect such differentiable 3D structure generators to\nsignificantly impact molecular modeling and related applications.",
          "link": "http://arxiv.org/abs/2106.07802",
          "publishedOn": "2021-06-16T01:21:09.826Z",
          "wordCount": 637,
          "title": "GeoMol: Torsional Geometric Generation of Molecular 3D Conformer Ensembles. (arXiv:2106.07802v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meulemans_A/0/1/0/all/0/1\">Alexander Meulemans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinha_M/0/1/0/all/0/1\">Matilde Tristany Farinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_J/0/1/0/all/0/1\">Javier Garc&#xed;a Ord&#xf3;&#xf1;ez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aceituno_P/0/1/0/all/0/1\">Pau Vilimelis Aceituno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1\">Jo&#xe3;o Sacramento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1\">Benjamin F. Grewe</a>",
          "description": "The success of deep learning sparked interest in whether the brain learns by\nusing similar techniques for assigning credit to each synaptic weight for its\ncontribution to the network output. However, the majority of current attempts\nat biologically-plausible learning methods are either non-local in time,\nrequire highly specific connectivity motives, or have no clear link to any\nknown mathematical optimization method. Here, we introduce Deep Feedback\nControl (DFC), a new learning method that uses a feedback controller to drive a\ndeep neural network to match a desired output target and whose control signal\ncan be used for credit assignment. The resulting learning rule is fully local\nin space and time and approximates Gauss-Newton optimization for a wide range\nof feedback connectivity patterns. To further underline its biological\nplausibility, we relate DFC to a multi-compartment model of cortical pyramidal\nneurons with a local voltage-dependent synaptic plasticity rule, consistent\nwith recent theories of dendritic processing. By combining dynamical system\ntheory with mathematical optimization theory, we provide a strong theoretical\nfoundation for DFC that we corroborate with detailed results on toy experiments\nand standard computer-vision benchmarks.",
          "link": "http://arxiv.org/abs/2106.07887",
          "publishedOn": "2021-06-16T01:21:09.818Z",
          "wordCount": 642,
          "title": "Credit Assignment in Neural Networks through Deep Feedback Control. (arXiv:2106.07887v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashcraft_C/0/1/0/all/0/1\">Chace Ashcraft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karra_K/0/1/0/all/0/1\">Kiran Karra</a>",
          "description": "In this paper, we propose a new data poisoning attack and apply it to deep\nreinforcement learning agents. Our attack centers on what we call\nin-distribution triggers, which are triggers native to the data distributions\nthe model will be trained on and deployed in. We outline a simple procedure for\nembedding these, and other, triggers in deep reinforcement learning agents\nfollowing a multi-task learning paradigm, and demonstrate in three common\nreinforcement learning environments. We believe that this work has important\nimplications for the security of deep learning models.",
          "link": "http://arxiv.org/abs/2106.07798",
          "publishedOn": "2021-06-16T01:21:09.796Z",
          "wordCount": 531,
          "title": "Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers. (arXiv:2106.07798v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishfaq_H/0/1/0/all/0/1\">Haque Ishfaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Qiwen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Viet Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayoub_A/0/1/0/all/0/1\">Alex Ayoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin F. Yang</a>",
          "description": "We propose a model-free reinforcement learning algorithm inspired by the\npopular randomized least squares value iteration (RLSVI) algorithm as well as\nthe optimism principle. Unlike existing upper-confidence-bound (UCB) based\napproaches, which are often computationally intractable, our algorithm drives\nexploration by simply perturbing the training data with judiciously chosen\ni.i.d. scalar noises. To attain optimistic value function estimation without\nresorting to a UCB-style bonus, we introduce an optimistic reward sampling\nprocedure. When the value functions can be represented by a function class\n$\\mathcal{F}$, our algorithm achieves a worst-case regret bound of\n$\\widetilde{O}(\\mathrm{poly}(d_EH)\\sqrt{T})$ where $T$ is the time elapsed, $H$\nis the planning horizon and $d_E$ is the $\\textit{eluder dimension}$ of\n$\\mathcal{F}$. In the linear setting, our algorithm reduces to LSVI-PHE, a\nvariant of RLSVI, that enjoys an $\\widetilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$\nregret. We complement the theory with an empirical evaluation across known\ndifficult exploration tasks.",
          "link": "http://arxiv.org/abs/2106.07841",
          "publishedOn": "2021-06-16T01:21:09.777Z",
          "wordCount": 600,
          "title": "Randomized Exploration for Reinforcement Learning with General Value Function Approximation. (arXiv:2106.07841v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1\">Krishnateja Killamsetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xujiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "Semi-supervised learning (SSL) algorithms have had great success in recent\nyears in limited labeled data regimes. However, the current state-of-the-art\nSSL algorithms are computationally expensive and entail significant compute\ntime and energy requirements. This can prove to be a huge limitation for many\nsmaller companies and academic groups. Our main insight is that training on a\nsubset of unlabeled data instead of entire unlabeled data enables the current\nSSL algorithms to converge faster, thereby reducing the computational costs\nsignificantly. In this work, we propose RETRIEVE, a coreset selection framework\nfor efficient and robust semi-supervised learning. RETRIEVE selects the coreset\nby solving a mixed discrete-continuous bi-level optimization problem such that\nthe selected coreset minimizes the labeled set loss. We use a one-step gradient\napproximation and show that the discrete optimization problem is approximately\nsubmodular, thereby enabling simple greedy algorithms to obtain the coreset. We\nempirically demonstrate on several real-world datasets that existing SSL\nalgorithms like VAT, Mean-Teacher, FixMatch, when used with RETRIEVE, achieve\na) faster training times, b) better performance when unlabeled data consists of\nOut-of-Distribution(OOD) data and imbalance. More specifically, we show that\nwith minimal accuracy degradation, RETRIEVE achieves a speedup of around 3X in\nthe traditional SSL setting and achieves a speedup of 5X compared to\nstate-of-the-art (SOTA) robust SSL algorithms in the case of imbalance and OOD\ndata.",
          "link": "http://arxiv.org/abs/2106.07760",
          "publishedOn": "2021-06-16T01:21:09.767Z",
          "wordCount": 649,
          "title": "RETRIEVE: Coreset Selection for Efficient and Robust Semi-Supervised Learning. (arXiv:2106.07760v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1903.04556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mesquita_D/0/1/0/all/0/1\">Diego Mesquita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blomstedt_P/0/1/0/all/0/1\">Paul Blomstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1\">Samuel Kaski</a>",
          "description": "While MCMC methods have become a main work-horse for Bayesian inference,\nscaling them to large distributed datasets is still a challenge. Embarrassingly\nparallel MCMC strategies take a divide-and-conquer stance to achieve this by\nwriting the target posterior as a product of subposteriors, running MCMC for\neach of them in parallel and subsequently combining the results. The challenge\nthen lies in devising efficient aggregation strategies. Current strategies\ntrade-off between approximation quality, and costs of communication and\ncomputation. In this work, we introduce a novel method that addresses these\nissues simultaneously. Our key insight is to introduce a deep invertible\ntransformation to approximate each of the subposteriors. These approximations\ncan be made accurate even for complex distributions and serve as intermediate\nrepresentations, keeping the total communication cost limited. Moreover, they\nenable us to sample from the product of the subposteriors using an efficient\nand stable importance sampling scheme. We demonstrate the approach outperforms\navailable state-of-the-art methods in a range of challenging scenarios,\nincluding high-dimensional and heterogeneous subposteriors.",
          "link": "http://arxiv.org/abs/1903.04556",
          "publishedOn": "2021-06-16T01:21:09.684Z",
          "wordCount": 632,
          "title": "Embarrassingly parallel MCMC using deep invertible transformations. (arXiv:1903.04556v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+LeJeune_D/0/1/0/all/0/1\">Daniel LeJeune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javadi_H/0/1/0/all/0/1\">Hamid Javadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>",
          "description": "Among the most successful methods for sparsifying deep (neural) networks are\nthose that adaptively mask the network weights throughout training. By\nexamining this masking, or dropout, in the linear case, we uncover a duality\nbetween such adaptive methods and regularization through the so-called\n\"$\\eta$-trick\" that casts both as iteratively reweighted optimizations. We show\nthat any dropout strategy that adapts to the weights in a monotonic way\ncorresponds to an effective subquadratic regularization penalty, and therefore\nleads to sparse solutions. We obtain the effective penalties for several\npopular sparsification strategies, which are remarkably similar to classical\npenalties commonly used in sparse optimization. Considering variational dropout\nas a case study, we demonstrate similar empirical behavior between the adaptive\ndropout method and classical methods on the task of deep network\nsparsification, validating our theory.",
          "link": "http://arxiv.org/abs/2106.07769",
          "publishedOn": "2021-06-16T01:21:09.666Z",
          "wordCount": 577,
          "title": "The Flip Side of the Reweighted Coin: Duality of Adaptive Dropout and Regularization. (arXiv:2106.07769v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.12016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1\">Hao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Canyi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "Recently, the \\textit{Tensor Nuclear Norm~(TNN)} regularization based on\nt-SVD has been widely used in various low tubal-rank tensor recovery tasks.\nHowever, these models usually require smooth change of data along the third\ndimension to ensure their low rank structures. In this paper, we propose a new\ndefinition of data dependent tensor rank named \\textit{tensor Q-rank} by a\nlearnable orthogonal matrix $\\mathbf{Q}$, and further introduce a unified data\ndependent low rank tensor recovery model. According to the low rank hypothesis,\nwe introduce two explainable selection method of $\\mathbf{Q}$, under which the\ndata tensor may have a more significant low tensor Q-rank structure than that\nof low tubal-rank structure. Specifically, maximizing the variance of singular\nvalue distribution leads to Variance Maximization Tensor Q-Nuclear\nnorm~(VMTQN), while minimizing the value of nuclear norm through manifold\noptimization leads to Manifold Optimization Tensor Q-Nuclear norm~(MOTQN).\nMoreover, we apply these two models to the low rank tensor completion problem,\nand then give an effective algorithm and briefly analyze why our method works\nbetter than TNN based methods in the case of complex data with low sampling\nrate. Finally, experimental results on real-world datasets demonstrate the\nsuperiority of our proposed model in the tensor completion problem with respect\nto other tensor rank regularization models.",
          "link": "http://arxiv.org/abs/1910.12016",
          "publishedOn": "2021-06-16T01:21:09.654Z",
          "wordCount": 679,
          "title": "Tensor Q-Rank: New Data Dependent Definition of Tensor Rank. (arXiv:1910.12016v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07898",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Lang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pillutla_K/0/1/0/all/0/1\">Krishna Pillutla</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oh_S/0/1/0/all/0/1\">Sewoong Oh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "The spectacular success of deep generative models calls for quantitative\ntools to measure their statistical performance. Divergence frontiers have\nrecently been proposed as an evaluation framework for generative models, due to\ntheir ability to measure the quality-diversity trade-off inherent to deep\ngenerative modeling. However, the statistical behavior of divergence frontiers\nestimated from data remains unknown to this day. In this paper, we establish\nnon-asymptotic bounds on the sample complexity of the plug-in estimator of\ndivergence frontiers. Along the way, we introduce a novel integral summary of\ndivergence frontiers. We derive the corresponding non-asymptotic bounds and\ndiscuss the choice of the quantization level by balancing the two types of\napproximation errors arisen from its computation. We also augment the\ndivergence frontier framework by investigating the statistical performance of\nsmoothed distribution estimators such as the Good-Turing estimator. We\nillustrate the theoretical results with numerical examples from natural\nlanguage processing and computer vision.",
          "link": "http://arxiv.org/abs/2106.07898",
          "publishedOn": "2021-06-16T01:21:09.647Z",
          "wordCount": 590,
          "title": "Divergence Frontiers for Generative Models: Sample Complexity, Quantization Level, and Frontier Integral. (arXiv:2106.07898v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1910.10897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quillen_D/0/1/0/all/0/1\">Deirdre Quillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhanpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1\">Ryan Julian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1\">Avnish Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shively_H/0/1/0/all/0/1\">Hayden Shively</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellathur_A/0/1/0/all/0/1\">Adithya Bellathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Meta-reinforcement learning algorithms can enable robots to acquire new\nskills much more quickly, by leveraging prior experience to learn how to learn.\nHowever, much of the current research on meta-reinforcement learning focuses on\ntask distributions that are very narrow. For example, a commonly used\nmeta-reinforcement learning benchmark uses different running velocities for a\nsimulated robot as different tasks. When policies are meta-trained on such\nnarrow task distributions, they cannot possibly generalize to more quickly\nacquire entirely new tasks. Therefore, if the aim of these methods is to enable\nfaster acquisition of entirely new behaviors, we must evaluate them on task\ndistributions that are sufficiently broad to enable generalization to new\nbehaviors. In this paper, we propose an open-source simulated benchmark for\nmeta-reinforcement learning and multi-task learning consisting of 50 distinct\nrobotic manipulation tasks. Our aim is to make it possible to develop\nalgorithms that generalize to accelerate the acquisition of entirely new,\nheld-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and\nmulti-task learning algorithms on these tasks. Surprisingly, while each task\nand its variations (e.g., with different object positions) can be learned with\nreasonable success, these algorithms struggle to learn with multiple tasks at\nthe same time, even with as few as ten distinct training tasks. Our analysis\nand open-source environments pave the way for future research in multi-task\nlearning and meta-learning that can enable meaningful generalization, thereby\nunlocking the full potential of these methods.",
          "link": "http://arxiv.org/abs/1910.10897",
          "publishedOn": "2021-06-16T01:21:09.640Z",
          "wordCount": 759,
          "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning. (arXiv:1910.10897v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yufei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkhir_N/0/1/0/all/0/1\">Nacim Belkhir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angulo_J/0/1/0/all/0/1\">Jesus Angulo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>",
          "description": "Deep Neural Networks (DNNs) are generated by sequentially performing linear\nand non-linear processes. Using a combination of linear and non-linear\nprocedures is critical for generating a sufficiently deep feature space. The\nmajority of non-linear operators are derivations of activation functions or\npooling functions. Mathematical morphology is a branch of mathematics that\nprovides non-linear operators for a variety of image processing problems. We\ninvestigate the utility of integrating these operations in an end-to-end deep\nlearning framework in this paper. DNNs are designed to acquire a realistic\nrepresentation for a particular job. Morphological operators give topological\ndescriptors that convey salient information about the shapes of objects\ndepicted in images. We propose a method based on meta-learning to incorporate\nmorphological operators into DNNs. The learned architecture demonstrates how\nour novel morphological operations significantly increase DNN performance on\nvarious tasks, including picture classification and edge detection.",
          "link": "http://arxiv.org/abs/2106.07714",
          "publishedOn": "2021-06-16T01:21:09.633Z",
          "wordCount": 584,
          "title": "Learning Deep Morphological Networks with Neural Architecture Search. (arXiv:2106.07714v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaini_P/0/1/0/all/0/1\">Priyank Jaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holdijk_L/0/1/0/all/0/1\">Lars Holdijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>",
          "description": "We focus on the problem of efficient sampling and learning of probability\ndensities by incorporating symmetries in probabilistic models. We first\nintroduce Equivariant Stein Variational Gradient Descent algorithm -- an\nequivariant sampling method based on Stein's identity for sampling from\ndensities with symmetries. Equivariant SVGD explicitly incorporates symmetry\ninformation in a density through equivariant kernels which makes the resultant\nsampler efficient both in terms of sample complexity and the quality of\ngenerated samples. Subsequently, we define equivariant energy based models to\nmodel invariant densities that are learned using contrastive divergence. By\nutilizing our equivariant SVGD for training equivariant EBMs, we propose new\nways of improving and scaling up training of energy based models. We apply\nthese equivariant energy models for modelling joint densities in regression and\nclassification tasks for image datasets, many-body particle systems and\nmolecular structure generation.",
          "link": "http://arxiv.org/abs/2106.07832",
          "publishedOn": "2021-06-16T01:21:09.623Z",
          "wordCount": 574,
          "title": "Learning Equivariant Energy Based Models with Equivariant Stein Variational Gradient Descent. (arXiv:2106.07832v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1\">Markus Nagel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fournarakis_M/0/1/0/all/0/1\">Marios Fournarakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1\">Rana Ali Amjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondarenko_Y/0/1/0/all/0/1\">Yelysei Bondarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baalen_M/0/1/0/all/0/1\">Mart van Baalen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1\">Tijmen Blankevoort</a>",
          "description": "While neural networks have advanced the frontiers in many applications, they\noften come at a high computational cost. Reducing the power and latency of\nneural network inference is key if we want to integrate modern networks into\nedge devices with strict power and compute requirements. Neural network\nquantization is one of the most effective ways of achieving these savings but\nthe additional noise it induces can lead to accuracy degradation. In this white\npaper, we introduce state-of-the-art algorithms for mitigating the impact of\nquantization noise on the network's performance while maintaining low-bit\nweights and activations. We start with a hardware motivated introduction to\nquantization and then consider two main classes of algorithms: Post-Training\nQuantization (PTQ) and Quantization-Aware-Training (QAT). PTQ requires no\nre-training or labelled data and is thus a lightweight push-button approach to\nquantization. In most cases, PTQ is sufficient for achieving 8-bit quantization\nwith close to floating-point accuracy. QAT requires fine-tuning and access to\nlabeled training data but enables lower bit quantization with competitive\nresults. For both solutions, we provide tested pipelines based on existing\nliterature and extensive experimentation that lead to state-of-the-art\nperformance for common deep learning models and tasks.",
          "link": "http://arxiv.org/abs/2106.08295",
          "publishedOn": "2021-06-16T01:21:09.583Z",
          "wordCount": 630,
          "title": "A White Paper on Neural Network Quantization. (arXiv:2106.08295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07806",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bridge_C/0/1/0/all/0/1\">Christopher P. Bridge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorman_C/0/1/0/all/0/1\">Chris Gorman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pieper_S/0/1/0/all/0/1\">Steven Pieper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doyle_S/0/1/0/all/0/1\">Sean W. Doyle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lennerz_J/0/1/0/all/0/1\">Jochen K. Lennerz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clunie_D/0/1/0/all/0/1\">David A. Clunie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fedorov_A/0/1/0/all/0/1\">Andriy Y. Fedorov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herrmann_M/0/1/0/all/0/1\">Markus D. Herrmann</a>",
          "description": "Machine learning is revolutionizing image-based diagnostics in pathology and\nradiology. ML models have shown promising results in research settings, but\ntheir lack of interoperability has been a major barrier for clinical\nintegration and evaluation. The DICOM a standard specifies Information Object\nDefinitions and Services for the representation and communication of digital\nimages and related information, including image-derived annotations and\nanalysis results. However, the complexity of the standard represents an\nobstacle for its adoption in the ML community and creates a need for software\nlibraries and tools that simplify working with data sets in DICOM format. Here\nwe present the highdicom library, which provides a high-level application\nprogramming interface for the Python programming language that abstracts\nlow-level details of the standard and enables encoding and decoding of\nimage-derived information in DICOM format in a few lines of Python code. The\nhighdicom library ties into the extensive Python ecosystem for image processing\nand machine learning. Simultaneously, by simplifying creation and parsing of\nDICOM-compliant files, highdicom achieves interoperability with the medical\nimaging systems that hold the data used to train and run ML models, and\nultimately communicate and store model outputs for clinical use. We demonstrate\nthrough experiments with slide microscopy and computed tomography imaging,\nthat, by bridging these two ecosystems, highdicom enables developers to train\nand evaluate state-of-the-art ML models in pathology and radiology while\nremaining compliant with the DICOM standard and interoperable with clinical\nsystems at all stages. To promote standardization of ML research and streamline\nthe ML model development and deployment process, we made the library available\nfree and open-source.",
          "link": "http://arxiv.org/abs/2106.07806",
          "publishedOn": "2021-06-16T01:21:09.475Z",
          "wordCount": 740,
          "title": "Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mashayekhi_M/0/1/0/all/0/1\">Maryam Mashayekhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapia_I/0/1/0/all/0/1\">Itzel Ramirez Tapia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balagopal_A/0/1/0/all/0/1\">Anjali Balagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xinran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barkousaraie_A/0/1/0/all/0/1\">Azar Sadeghnejad Barkousaraie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McBeth_R/0/1/0/all/0/1\">Rafe McBeth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mu-Han Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Steve Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dan Nguyen</a>",
          "description": "Typically, the current dose prediction models are limited to small amounts of\ndata and require re-training for a specific site, often leading to suboptimal\nperformance. We propose a site-agnostic, 3D dose distribution prediction model\nusing deep learning that can leverage data from any treatment site, thus\nincreasing the total data available to train the model. Applying our proposed\nmodel to a new target treatment site requires only a brief fine-tuning of the\nmodel to the new data and involves no modifications to the model input channels\nor its parameters. Thus, it can be efficiently adapted to a different treatment\nsite, even with a small training dataset.",
          "link": "http://arxiv.org/abs/2106.07825",
          "publishedOn": "2021-06-16T01:21:09.468Z",
          "wordCount": 554,
          "title": "Site-Agnostic 3D Dose Distribution Prediction with Deep Learning Neural Networks. (arXiv:2106.07825v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+E%2E_W/0/1/0/all/0/1\">Wilson E. Marc&#xed;lio-Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eler_D/0/1/0/all/0/1\">Danilo M. Eler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulovich_F/0/1/0/all/0/1\">Fernando V. Paulovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_R/0/1/0/all/0/1\">Rafael M. Martins</a>",
          "description": "Dimensionality reduction (DR) techniques help analysts to understand patterns\nin high-dimensional spaces. These techniques, often represented by scatter\nplots, are employed in diverse science domains and facilitate similarity\nanalysis among clusters and data samples. For datasets containing many\ngranularities or when analysis follows the information visualization mantra,\nhierarchical DR techniques are the most suitable approach since they present\nmajor structures beforehand and details on demand. However, current\nhierarchical DR techniques are not fully capable of addressing literature\nproblems because they do not preserve the projection mental map across\nhierarchical levels or are not suitable for most data types. This work presents\nHUMAP, a novel hierarchical dimensionality reduction technique designed to be\nflexible on preserving local and global structures and preserve the mental map\nthroughout hierarchical exploration. We provide empirical evidence of our\ntechnique's superiority compared with current hierarchical approaches and show\ntwo case studies to demonstrate its strengths.",
          "link": "http://arxiv.org/abs/2106.07718",
          "publishedOn": "2021-06-16T01:21:09.401Z",
          "wordCount": 579,
          "title": "HUMAP: Hierarchical Uniform Manifold Approximation and Projection. (arXiv:2106.07718v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.11468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benko_Z/0/1/0/all/0/1\">Zsigmond Benk&#x151;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babel_T/0/1/0/all/0/1\">Tam&#xe1;s B&#xe1;bel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somogyvari_Z/0/1/0/all/0/1\">Zolt&#xe1;n Somogyv&#xe1;ri</a>",
          "description": "Recognition of anomalous events is a challenging but critical task in many\nscientific and industrial fields, especially when the properties of anomalies\nare unknown. In this paper, we introduce a new anomaly concept called \"unicorn\"\nor unique event and present a new, model-free, unsupervised detection algorithm\nto detect unicorns. The key component of the new algorithm is the Temporal\nOutlier Factor (TOF) to measure the uniqueness of events in continuous data\nsets from dynamic systems. The concept of unique events differs significantly\nfrom traditional outliers in many aspects: while repetitive outliers are no\nlonger unique events, a unique event is not necessarily an outlier; it does not\nnecessarily fall out from the distribution of normal activity. The performance\nof our algorithm was examined in recognizing unique events on different types\nof simulated data sets with anomalies and it was compared with the Local\nOutlier Factor (LOF) and discord discovery algorithms. TOF had superior\nperformance compared to LOF and discord algorithms even in recognizing\ntraditional outliers and it also recognized unique events that those did not.\nThe benefits of the unicorn concept and the new detection method were\nillustrated by example data sets from very different scientific fields. Our\nalgorithm successfully recognized unique events in those cases where they were\nalready known such as the gravitational waves of a binary black hole merger on\nLIGO detector data and the signs of respiratory failure on ECG data series.\nFurthermore, unique events were found on the LIBOR data set of the last 30\nyears.",
          "link": "http://arxiv.org/abs/2004.11468",
          "publishedOn": "2021-06-16T01:21:09.222Z",
          "wordCount": 742,
          "title": "How to find a unicorn: a novel model-free, unsupervised anomaly detection method for time series. (arXiv:2004.11468v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Czolbe_S/0/1/0/all/0/1\">Steffen Czolbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>",
          "description": "Geometric alignment appears in a variety of applications, ranging from domain\nadaptation, optimal transport, and normalizing flows in machine learning;\noptical flow and learned augmentation in computer vision and deformable\nregistration within biomedical imaging. A recurring challenge is the alignment\nof domains whose topology is not the same; a problem that is routinely ignored,\npotentially introducing bias in downstream analysis. As a first step towards\nsolving such alignment problems, we propose an unsupervised topological\ndifference detection algorithm. The model is based on a conditional variational\nauto-encoder and detects topological anomalies with regards to a reference\nalongside the registration step. We consider both a) topological changes in the\nimage under spatial variation and b) unexpected transformations. Our approach\nis validated on a proxy task of unsupervised anomaly detection in images.",
          "link": "http://arxiv.org/abs/2106.08233",
          "publishedOn": "2021-06-16T01:21:09.202Z",
          "wordCount": 573,
          "title": "Spot the Difference: Topological Anomaly Detection via Geometric Alignment. (arXiv:2106.08233v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1\">Tim Prangemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1\">Christoph Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1\">Christian Wildner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1\">Heinz Koeppl</a>",
          "description": "Time-lapse fluorescent microscopy (TLFM) combined with predictive\nmathematical modelling is a powerful tool to study the inherently dynamic\nprocesses of life on the single-cell level. Such experiments are costly,\ncomplex and labour intensive. A complimentary approach and a step towards\ncompletely in silico experiments, is to synthesise the imagery itself. Here, we\npropose Multi-StyleGAN as a descriptive approach to simulate time-lapse\nfluorescence microscopy imagery of living cells, based on a past experiment.\nThis novel generative adversarial network synthesises a multi-domain sequence\nof consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple\nlive yeast cells in microstructured environments and train on a dataset\nrecorded in our laboratory. The simulation captures underlying biophysical\nfactors and time dependencies, such as cell morphology, growth, physical\ninteractions, as well as the intensity of a fluorescent reporter protein. An\nimmediate application is to generate additional training and validation data\nfor feature extraction algorithms or to aid and expedite development of\nadvanced experimental techniques such as online monitoring or control of cells.\n\nCode and dataset is available at\nhttps://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.",
          "link": "http://arxiv.org/abs/2106.08285",
          "publishedOn": "2021-06-16T01:21:09.193Z",
          "wordCount": 640,
          "title": "Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1\">Luis Felipe Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1\">S&#xe9;rgio Colcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1\">Ruy Luiz Milidi&#xfa;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulcao_A/0/1/0/all/0/1\">Andr&#xe9; Bulc&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barros_P/0/1/0/all/0/1\">Pablo Barros</a>",
          "description": "Nowadays, subsurface salt body localization and delineation, also called\nsemantic segmentation of salt bodies, are among the most challenging\ngeophysicist tasks. Thus, identifying large salt bodies is notoriously tricky\nand is crucial for identifying hydrocarbon reservoirs and drill path planning.\nThis work proposes a Data Augmentation method based on training two generative\nmodels to augment the number of samples in a seismic image dataset for the\nsemantic segmentation of salt bodies. Our method uses deep learning models to\ngenerate pairs of seismic image patches and their respective salt masks for the\nData Augmentation. The first model is a Variational Autoencoder and is\nresponsible for generating patches of salt body masks. The second is a\nConditional Normalizing Flow model, which receives the generated masks as\ninputs and generates the associated seismic image patches. We evaluate the\nproposed method by comparing the performance of ten distinct state-of-the-art\nmodels for semantic segmentation, trained with and without the generated\naugmentations, in a dataset from two synthetic seismic images. The proposed\nmethodology yields an average improvement of 8.57% in the IoU metric across all\ncompared models. The best result is achieved by a DeeplabV3+ model variant,\nwhich presents an IoU score of 95.17% when trained with our augmentations.\nAdditionally, our proposal outperformed six selected data augmentation methods,\nand the most significant improvement in the comparison, of 9.77%, is achieved\nby composing our DA with augmentations from an elastic transformation. At last,\nwe show that the proposed method is adaptable for a larger context size by\nachieving results comparable to the obtained on the smaller context size.",
          "link": "http://arxiv.org/abs/2106.08269",
          "publishedOn": "2021-06-16T01:21:09.187Z",
          "wordCount": 719,
          "title": "Generating Data Augmentation samples for Semantic Segmentation of Salt Bodies in a Synthetic Seismic Image Dataset. (arXiv:2106.08269v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1\">Peter Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahann_J/0/1/0/all/0/1\">Johannes Lahann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fettke_P/0/1/0/all/0/1\">Peter Fettke</a>",
          "description": "Learning meaningful representations of data is an important aspect of machine\nlearning and has recently been successfully applied to many domains like\nlanguage understanding or computer vision. Instead of training a model for one\nspecific task, representation learning is about training a model to capture all\nuseful information in the underlying data and make it accessible for a\npredictor. For predictive process analytics, it is essential to have all\nexplanatory characteristics of a process instance available when making\npredictions about the future, as well as for clustering and anomaly detection.\nDue to the large variety of perspectives and types within business process\ndata, generating a good representation is a challenging task. In this paper, we\npropose a novel approach for representation learning of business process\ninstances which can process and combine most perspectives in an event log. In\nconjunction with a self-supervised pre-training method, we show the\ncapabilities of the approach through a visualization of the representation\nspace and case retrieval. Furthermore, the pre-trained model is fine-tuned to\nmultiple process prediction tasks and demonstrates its effectiveness in\ncomparison with existing approaches.",
          "link": "http://arxiv.org/abs/2106.08027",
          "publishedOn": "2021-06-16T01:21:09.180Z",
          "wordCount": 630,
          "title": "Multivariate Business Process Representation Learning utilizing Gramian Angular Fields and Convolutional Neural Networks. (arXiv:2106.08027v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Surana_A/0/1/0/all/0/1\">Amit Surana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Can Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajapakse_I/0/1/0/all/0/1\">Indika Rajapakse</a>",
          "description": "In this paper, we propose two novel approaches for hypergraph comparison. The\nfirst approach transforms the hypergraph into a graph representation for use of\nstandard graph dissimilarity measures. The second approach exploits the\nmathematics of tensors to intrinsically capture multi-way relations. For each\napproach, we present measures that assess hypergraph dissimilarity at a\nspecific scale or provide a more holistic multi-scale comparison. We test these\nmeasures on synthetic hypergraphs and apply them to biological datasets.",
          "link": "http://arxiv.org/abs/2106.08206",
          "publishedOn": "2021-06-16T01:21:09.173Z",
          "wordCount": 487,
          "title": "Hypergraph Dissimilarity Measures. (arXiv:2106.08206v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08151",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schneider_M/0/1/0/all/0/1\">Maja Schneider</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Broszeit_A/0/1/0/all/0/1\">Amelie Broszeit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korner_M/0/1/0/all/0/1\">Marco K&#xf6;rner</a>",
          "description": "We present EuroCrops, a dataset based on self-declared field annotations for\ntraining and evaluating methods for crop type classification and mapping,\ntogether with its process of acquisition and harmonisation. By this, we aim to\nenrich the research efforts and discussion for data-driven land cover\nclassification via Earth observation and remote sensing. Additionally, through\ninclusion of self-declarations gathered in the scope of subsidy control from\nall countries of the European Union (EU), this dataset highlights the\ndifficulties and pitfalls one comes across when operating on a transnational\nlevel. We, therefore, also introduce a new taxonomy scheme, HCAT-ID, that\naspires to capture all the aspects of reference data originating from\nadministrative and agency databases. To address researchers from both the\nremote sensing and the computer vision and machine learning communities, we\npublish the dataset in different formats and processing levels.",
          "link": "http://arxiv.org/abs/2106.08151",
          "publishedOn": "2021-06-16T01:21:09.149Z",
          "wordCount": 607,
          "title": "EuroCrops: A Pan-European Dataset for Time Series Crop Type Classification. (arXiv:2106.08151v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peiyuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yu-Hang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Muqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Amity Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murugesan_V/0/1/0/all/0/1\">Vijayakumar Murugesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollas_A/0/1/0/all/0/1\">Aaron Hollas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "The solvation free energy of organic molecules is a critical parameter in\ndetermining emergent properties such as solubility, liquid-phase equilibrium\nconstants, and pKa and redox potentials in an organic redox flow battery. In\nthis work, we present a machine learning (ML) model that can learn and predict\nthe aqueous solvation free energy of an organic molecule using Gaussian process\nregression method based on a new molecular graph kernel. To investigate the\nperformance of the ML model on electrostatic interaction, the nonpolar\ninteraction contribution of solvent and the conformational entropy of solute in\nsolvation free energy, three data sets with implicit or explicit water solvent\nmodels, and contribution of conformational entropy of solute are tested. We\ndemonstrate that our ML model can predict the solvation free energy of\nmolecules at chemical accuracy with a mean absolute error of less than 1\nkcal/mol for subsets of the QM9 dataset and the Freesolv database. To solve the\ngeneral data scarcity problem for a graph-based ML model, we propose a\ndimension reduction algorithm based on the distance between molecular graphs,\nwhich can be used to examine the diversity of the molecular data set. It\nprovides a promising way to build a minimum training set to improve prediction\nfor certain test sets where the space of molecular structures is predetermined.",
          "link": "http://arxiv.org/abs/2106.08146",
          "publishedOn": "2021-06-16T01:21:09.140Z",
          "wordCount": 683,
          "title": "Graphical Gaussian Process Regression Model for Aqueous Solvation Free Energy Prediction of Organic Molecules in Redox Flow Battery. (arXiv:2106.08146v1 [cs.CE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08153",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Foote_A/0/1/0/all/0/1\">Alex Foote</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asif_A/0/1/0/all/0/1\">Amina Asif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azam_A/0/1/0/all/0/1\">Ayesha Azam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>",
          "description": "Deep learning models are routinely employed in computational pathology\n(CPath) for solving problems of diagnostic and prognostic significance.\nTypically, the generalization performance of CPath models is analyzed using\nevaluation protocols such as cross-validation and testing on multi-centric\ncohorts. However, to ensure that such CPath solutions are robust and safe for\nuse in a clinical setting, a critical analysis of their predictive performance\nand vulnerability to adversarial attacks is required, which is the focus of\nthis paper. Specifically, we show that a highly accurate model for\nclassification of tumour patches in pathology images (AUC > 0.95) can easily be\nattacked with minimal perturbations which are imperceptible to lay humans and\ntrained pathologists alike. Our analytical results show that it is possible to\ngenerate single-instance white-box attacks on specific input images with high\nsuccess rate and low perturbation energy. Furthermore, we have also generated a\nsingle universal perturbation matrix using the training dataset only which,\nwhen added to unseen test images, results in forcing the trained neural network\nto flip its prediction labels with high confidence at a success rate of > 84%.\nWe systematically analyze the relationship between perturbation energy of an\nadversarial attack, its impact on morphological constructs of clinical\nsignificance, their perceptibility by a trained pathologist and saliency maps\nobtained using deep learning models. Based on our analysis, we strongly\nrecommend that computational pathology models be critically analyzed using the\nproposed adversarial validation strategy prior to clinical adoption.",
          "link": "http://arxiv.org/abs/2106.08153",
          "publishedOn": "2021-06-16T01:21:09.112Z",
          "wordCount": 686,
          "title": "Now You See It, Now You Dont: Adversarial Vulnerabilities in Computational Pathology. (arXiv:2106.08153v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08008",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ingolfsson_T/0/1/0/all/0/1\">Thorir Mar Ingolfsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cossettini_A/0/1/0/all/0/1\">Andrea Cossettini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaying Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tabanelli_E/0/1/0/all/0/1\">Enrico Tabanelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tagliavini_G/0/1/0/all/0/1\">Guiseppe Tagliavini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ryvlin_P/0/1/0/all/0/1\">Philippe Ryvlin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benini_L/0/1/0/all/0/1\">Luca Benini</a>",
          "description": "We present the implementation of seizure detection algorithms based on a\nminimal number of EEG channels on a parallel ultra-low-power embedded platform.\nThe analyses are based on the CHB-MIT dataset, and include explorations of\ndifferent classification approaches (Support Vector Machines, Random Forest,\nExtra Trees, AdaBoost) and different pre/post-processing techniques to maximize\nsensitivity while guaranteeing no false alarms. We analyze global and\nsubject-specific approaches, considering all 23-electrodes or only 4 temporal\nchannels. For 8s window size and subject-specific approach, we report zero\nfalse positives and 100% sensitivity. These algorithms are parallelized and\noptimized for a parallel ultra-low power (PULP) platform, enabling 300h of\ncontinuous monitoring on a 300 mAh battery, in a wearable form factor and power\nbudget. These results pave the way for the implementation of affordable,\nwearable, long-term epilepsy monitoring solutions with low false-positive rates\nand high sensitivity, meeting both patient and caregiver requirements.",
          "link": "http://arxiv.org/abs/2106.08008",
          "publishedOn": "2021-06-16T01:21:09.103Z",
          "wordCount": 601,
          "title": "Towards Long-term Non-invasive Monitoring for Epilepsy via Wearable EEG Devices. (arXiv:2106.08008v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pablo Samuel Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kastner_T/0/1/0/all/0/1\">Tyler Kastner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panangaden_P/0/1/0/all/0/1\">Prakash Panangaden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1\">Mark Rowland</a>",
          "description": "We present a new behavioural distance over the state space of a Markov\ndecision process, and demonstrate the use of this distance as an effective\nmeans of shaping the learnt representations of deep reinforcement learning\nagents. While existing notions of state similarity are typically difficult to\nlearn at scale due to high computational cost and lack of sample-based\nalgorithms, our newly-proposed distance addresses both of these issues. In\naddition to providing detailed theoretical analysis, we provide empirical\nevidence that learning this distance alongside the value function yields\nstructured and informative representations, including strong results on the\nArcade Learning Environment benchmark.",
          "link": "http://arxiv.org/abs/2106.08229",
          "publishedOn": "2021-06-16T01:21:09.083Z",
          "wordCount": 538,
          "title": "MICo: Learning improved representations via sampling-based state similarity for Markov decision processes. (arXiv:2106.08229v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08138",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Corzo_H/0/1/0/all/0/1\">Hector H. Corzo</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sehanobish_A/0/1/0/all/0/1\">Arijit Sehanobish</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Kara_O/0/1/0/all/0/1\">Onur Kara</a>",
          "description": "In this report, the application of the Quantum Potential Neural Network\n(QPNN) framework to many electron atomic systems is presented. For this study,\nfull configuration interaction (FCI) one--electron density functions within\npredefined limits of accuracy were used to train the QPNN. The obtained results\nsuggest that this new neural network is capable of learning the effective\npotential functions of many electron atoms in a completely unsupervised manner,\nand using only limited information from the probability density. Using the\neffective potential functions learned for each of the studied systems the QPNN\nwas able to estimate the total energies of each of the systems (with a maximum\nof 10 trials) with a remarkable accuracy when compared to the FCI energies.",
          "link": "http://arxiv.org/abs/2106.08138",
          "publishedOn": "2021-06-16T01:21:09.067Z",
          "wordCount": 559,
          "title": "Application of the Quantum Potential Neural Network to multi-electronic atoms. (arXiv:2106.08138v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiu-Shen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "One single instance could possess multiple portraits and reveal diverse\nrelationships with others according to different contexts. Those ambiguities\nincrease the difficulty of learning a generalizable model when there exists one\nconcept or mixed concepts in a task. We propose a general approach Learning to\nDecompose Network (LeadNet) for both two cases, which contextualizes a model\nthrough meta-learning multiple maps for concepts discovery -- the\nrepresentations of instances are decomposed and adapted conditioned on the\ncontexts. Through taking a holistic view over multiple latent components over\ninstances in a sampled pseudo task, LeadNet learns to automatically select the\nright concept via incorporating those rich semantics inside and between\nobjects. LeadNet demonstrates its superiority in various applications,\nincluding exploring multiple views of confusing tasks, out-of-distribution\nrecognition, and few-shot image classification.",
          "link": "http://arxiv.org/abs/2106.08112",
          "publishedOn": "2021-06-16T01:21:09.050Z",
          "wordCount": 562,
          "title": "Contextualizing Multiple Tasks via Learning to Decompose. (arXiv:2106.08112v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1\">Chengxuan Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1\">Guolin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>",
          "description": "In this technical report, we present our solution of KDD Cup 2021 OGB\nLarge-Scale Challenge - PCQM4M-LSC Track. We adopt Graphormer and ExpC as our\nbasic models. We train each model by 8-fold cross-validation, and additionally\ntrain two Graphormer models on the union of training and validation sets with\ndifferent random seeds. For final submission, we use a naive ensemble for these\n18 models by taking average of their outputs. Using our method, our team\nMachineLearning achieved 0.1200 MAE on test set.",
          "link": "http://arxiv.org/abs/2106.08279",
          "publishedOn": "2021-06-16T01:21:09.031Z",
          "wordCount": 524,
          "title": "Awardee Solution of KDD Cup 2021 OGB Large-Scale Challenge Graph-Level Track. (arXiv:2106.08279v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Min Ye</a>",
          "description": "The cyclically equivariant neural decoder was recently proposed in [Chen-Ye,\nInternational Conference on Machine Learning, 2021] to decode cyclic codes. In\nthe same paper, a list decoding procedure was also introduced for two widely\nused classes of cyclic codes -- BCH codes and punctured Reed-Muller (RM) codes.\nWhile the list decoding procedure significantly improves the Frame Error Rate\n(FER) of the cyclically equivariant neural decoder, the Bit Error Rate (BER) of\nthe list decoding procedure is even worse than the unique decoding algorithm\nwhen the list size is small. In this paper, we propose an improved version of\nthe list decoding algorithm for BCH codes and punctured RM codes. Our new\nproposal significantly reduces the BER while maintaining the same (in some\ncases even smaller) FER. More specifically, our new decoder provides up to\n$2$dB gain over the previous list decoder when measured by BER, and the running\ntime of our new decoder is $15\\%$ smaller. Code available at\nhttps://github.com/improvedlistdecoder/code",
          "link": "http://arxiv.org/abs/2106.07964",
          "publishedOn": "2021-06-16T01:21:09.025Z",
          "wordCount": 601,
          "title": "Improving the List Decoding Version of the Cyclically Equivariant Neural Decoder. (arXiv:2106.07964v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lapeyrolerie_M/0/1/0/all/0/1\">Marcus Lapeyrolerie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapman_M/0/1/0/all/0/1\">Melissa S. Chapman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norman_K/0/1/0/all/0/1\">Kari E. A. Norman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boettiger_C/0/1/0/all/0/1\">Carl Boettiger</a>",
          "description": "Can machine learning help us make better decisions about a changing planet?\nIn this paper, we illustrate and discuss the potential of a promising corner of\nmachine learning known as _reinforcement learning_ (RL) to help tackle the most\nchallenging conservation decision problems. RL is uniquely well suited to\nconservation and global change challenges for three reasons: (1) RL explicitly\nfocuses on designing an agent who _interacts_ with an environment which is\ndynamic and uncertain, (2) RL approaches do not require massive amounts of\ndata, (3) RL approaches would utilize rather than replace existing models,\nsimulations, and the knowledge they contain. We provide a conceptual and\ntechnical introduction to RL and its relevance to ecological and conservation\nchallenges, including examples of a problem in setting fisheries quotas and in\nmanaging ecological tipping points. Four appendices with annotated code provide\na tangible introduction to researchers looking to adopt, evaluate, or extend\nthese approaches.",
          "link": "http://arxiv.org/abs/2106.08272",
          "publishedOn": "2021-06-16T01:21:09.019Z",
          "wordCount": 581,
          "title": "Deep Reinforcement Learning for Conservation Decisions. (arXiv:2106.08272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongsheng Wang</a>",
          "description": "Semantic representation and inference is essential for Natural Language\nProcessing (NLP). The state of the art for semantic representation and\ninference is deep learning, and particularly Recurrent Neural Networks (RNNs),\nConvolutional Neural Networks (CNNs), and transformer Self-Attention models.\nThis thesis investigates the use of deep learning for novel semantic\nrepresentation and inference, and makes contributions in the following three\nareas: creating training data, improving semantic representations and extending\ninference learning. In terms of creating training data, we contribute the\nlargest publicly available dataset of real-life factual claims for the purpose\nof automatic claim verification (MultiFC), and we present a novel inference\nmodel composed of multi-scale CNNs with different kernel sizes that learn from\nexternal sources to infer fact checking labels. In terms of improving semantic\nrepresentations, we contribute a novel model that captures non-compositional\nsemantic indicators. By definition, the meaning of a non-compositional phrase\ncannot be inferred from the individual meanings of its composing words (e.g.,\nhot dog). Motivated by this, we operationalize the compositionality of a phrase\ncontextually by enriching the phrase representation with external word\nembeddings and knowledge graphs. Finally, in terms of inference learning, we\npropose a series of novel deep learning architectures that improve inference by\nusing syntactic dependencies, by ensembling role guided attention heads,\nincorporating gating layers, and concatenating multiple heads in novel and\neffective ways. This thesis consists of seven publications (five published and\ntwo under review).",
          "link": "http://arxiv.org/abs/2106.08117",
          "publishedOn": "2021-06-16T01:21:09.012Z",
          "wordCount": 663,
          "title": "Semantic Representation and Inference for NLP. (arXiv:2106.08117v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Falaah Arif Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manis_E/0/1/0/all/0/1\">Eleni Manis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanovich_J/0/1/0/all/0/1\">Julia Stoyanovich</a>",
          "description": "Recent interest in codifying fairness in Automated Decision Systems (ADS) has\nresulted in a wide range of formulations of what it means for an algorithmic\nsystem to be fair. Most of these propositions are inspired by, but inadequately\ngrounded in, political philosophy scholarship. This paper aims to correct that\ndeficit. We introduce a taxonomy of fairness ideals using doctrines of Equality\nof Opportunity (EOP) from political philosophy, clarifying their conceptions in\nphilosophy and the proposed codification in fair machine learning. We arrange\nthese fairness ideals onto an EOP spectrum, which serves as a useful frame to\nguide the design of a fair ADS in a given context.\n\nWe use our fairness-as-EOP framework to re-interpret the impossibility\nresults from a philosophical perspective, as the in-compatibility between\ndifferent value systems, and demonstrate the utility of the framework with\nseveral real-world and hypothetical examples. Through our EOP-framework we hope\nto answer what it means for an ADS to be fair from a moral and political\nphilosophy standpoint, and to pave the way for similar scholarship from ethics\nand legal experts.",
          "link": "http://arxiv.org/abs/2106.08259",
          "publishedOn": "2021-06-16T01:21:09.006Z",
          "wordCount": 614,
          "title": "Fairness as Equality of Opportunity: Normative Guidance from Political Philosophy. (arXiv:2106.08259v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08126",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arabskyy_Y/0/1/0/all/0/1\">Yuriy Arabskyy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_A/0/1/0/all/0/1\">Aashish Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_S/0/1/0/all/0/1\">Subhadeep Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koller_O/0/1/0/all/0/1\">Oscar Koller</a>",
          "description": "This paper describes the winning approach in the public SwissText 2021\ncompetition on dialect recognition and translation of Swiss German speech to\nstandard German text. Swiss German refers to the multitude of Alemannic\ndialects spoken in the German-speaking parts of Switzerland. Swiss German\ndiffers significantly from standard German in pronunciation, word inventory and\ngrammar. It is mostly incomprehensible to native German speakers. Moreover, it\nlacks a standardized written script. To solve the challenging task, we propose\na hybrid automatic speech recognition system with a lexicon that incorporates\ntranslations, a 1st pass language model that deals with Swiss German\nparticularities, a transfer-learned acoustic model and a strong neural language\nmodel for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a blind\nconversational test set and outperforms the second best competitor by a 12%\nrelative margin.",
          "link": "http://arxiv.org/abs/2106.08126",
          "publishedOn": "2021-06-16T01:21:08.999Z",
          "wordCount": 598,
          "title": "Dialectal Speech Recognition and Translation of Swiss German Speech to Standard German Text: Microsoft's Submission to SwissText 2021. (arXiv:2106.08126v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vazirizade_S/0/1/0/all/0/1\">Sayyed Mohsen Vazirizade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Ayan Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pettet_G/0/1/0/all/0/1\">Geoffrey Pettet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Said_S/0/1/0/all/0/1\">Said El Said</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baroud_H/0/1/0/all/0/1\">Hiba Baroud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Abhishek Dubey</a>",
          "description": "Principled decision making in emergency response management necessitates the\nuse of statistical models that predict the spatial-temporal likelihood of\nincident occurrence. These statistical models are then used for proactive\nstationing which allocates first responders across the spatial area in order to\nreduce overall response time. Traditional methods that simply aggregate past\nincidents over space and time fail to make useful short-term predictions when\nthe spatial region is large and focused on fine-grained spatial entities like\ninterstate highway networks. This is partially due to the sparsity of incidents\nwith respect to the area in consideration. Further, accidents are affected by\nseveral covariates, and collecting, cleaning, and managing multiple streams of\ndata from various sources is challenging for large spatial areas. In this\npaper, we highlight how this problem is being solved for the state of\nTennessee, a state in the USA with a total area of over 100,000 sq. km. Our\npipeline, based on a combination of synthetic resampling, non-spatial\nclustering, and learning from data can efficiently forecast the spatial and\ntemporal dynamics of accident occurrence, even under sparse conditions. In the\npaper, we describe our pipeline that uses data related to roadway geometry,\nweather, historical accidents, and real-time traffic congestion to aid accident\nforecasting. To understand how our forecasting model can affect allocation and\ndispatch, we improve upon a classical resource allocation approach.\nExperimental results show that our approach can significantly reduce response\ntimes in the field in comparison with current approaches followed by first\nresponders.",
          "link": "http://arxiv.org/abs/2106.08307",
          "publishedOn": "2021-06-16T01:21:08.980Z",
          "wordCount": 684,
          "title": "Learning Incident Prediction Models Over Large Geographical Areas for Emergency Response Systems. (arXiv:2106.08307v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mingfu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shichang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yushu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqiang Liu</a>",
          "description": "Deep neural networks (DNN) have achieved remarkable performance in various\nfields. However, training a DNN model from scratch requires a lot of computing\nresources and training data. It is difficult for most individual users to\nobtain such computing resources and training data. Model copyright infringement\nis an emerging problem in recent years. For instance, pre-trained models may be\nstolen or abuse by illegal users without the authorization of the model owner.\nRecently, many works on protecting the intellectual property of DNN models have\nbeen proposed. In these works, embedding watermarks into DNN based on backdoor\nis one of the widely used methods. However, when the DNN model is stolen, the\nbackdoor-based watermark may face the risk of being detected and removed by an\nadversary. In this paper, we propose a scheme to detect and remove watermark in\ndeep neural networks via generative adversarial networks (GAN). We demonstrate\nthat the backdoor-based DNN watermarks are vulnerable to the proposed GAN-based\nwatermark removal attack. The proposed attack method includes two phases. In\nthe first phase, we use the GAN and few clean images to detect and reverse the\nwatermark in the DNN model. In the second phase, we fine-tune the watermarked\nDNN based on the reversed backdoor images. Experimental evaluations on the\nMNIST and CIFAR10 datasets demonstrate that, the proposed method can\neffectively remove about 98% of the watermark in DNN models, as the watermark\nretention rate reduces from 100% to less than 2% after applying the proposed\nattack. In the meantime, the proposed attack hardly affects the model's\nperformance. The test accuracy of the watermarked DNN on the MNIST and the\nCIFAR10 datasets drops by less than 1% and 3%, respectively.",
          "link": "http://arxiv.org/abs/2106.08104",
          "publishedOn": "2021-06-16T01:21:08.973Z",
          "wordCount": 724,
          "title": "Detect and remove watermark in deep neural networks via generative adversarial networks. (arXiv:2106.08104v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08105",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bommert_A/0/1/0/all/0/1\">Andrea Bommert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rahnenfuhrer_J/0/1/0/all/0/1\">J&#xf6;rg Rahnenf&#xfc;hrer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lang_M/0/1/0/all/0/1\">Michel Lang</a>",
          "description": "Fitting models with high predictive accuracy that include all relevant but no\nirrelevant or redundant features is a challenging task on data sets with\nsimilar (e.g. highly correlated) features. We propose the approach of tuning\nthe hyperparameters of a predictive model in a multi-criteria fashion with\nrespect to predictive accuracy and feature selection stability. We evaluate\nthis approach based on both simulated and real data sets and we compare it to\nthe standard approach of single-criteria tuning of the hyperparameters as well\nas to the state-of-the-art technique \"stability selection\". We conclude that\nour approach achieves the same or better predictive performance compared to the\ntwo established approaches. Considering the stability during tuning does not\ndecrease the predictive accuracy of the resulting models. Our approach succeeds\nat selecting the relevant features while avoiding irrelevant or redundant\nfeatures. The single-criteria approach fails at avoiding irrelevant or\nredundant features and the stability selection approach fails at selecting\nenough relevant features for achieving acceptable predictive accuracy. For our\napproach, for data sets with many similar features, the feature selection\nstability must be evaluated with an adjusted stability measure, that is, a\nmeasure that considers similarities between features. For data sets with only\nfew similar features, an unadjusted stability measure suffices and is faster to\ncompute.",
          "link": "http://arxiv.org/abs/2106.08105",
          "publishedOn": "2021-06-16T01:21:08.964Z",
          "wordCount": 651,
          "title": "Employing an Adjusted Stability Measure for Multi-Criteria Model Fitting on Data Sets with Similar Features. (arXiv:2106.08105v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08299",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tommy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merkel_C/0/1/0/all/0/1\">Cory Merkel</a>",
          "description": "Artificial neural networks (ANNs) have gained significant popularity in the\nlast decade for solving narrow AI problems in domains such as healthcare,\ntransportation, and defense. As ANNs become more ubiquitous, it is imperative\nto understand their associated safety, security, and privacy vulnerabilities.\nRecently, it has been shown that ANNs are susceptible to a number of\nadversarial evasion attacks--inputs that cause the ANN to make high-confidence\nmisclassifications despite being almost indistinguishable from the data used to\ntrain and test the network. This work explores to what degree finding these\nexamples maybe aided by using side-channel information, specifically switching\npower consumption, of hardware implementations of ANNs. A black-box threat\nscenario is assumed, where an attacker has access to the ANN hardware's input,\noutputs, and topology, but the trained model parameters are unknown. Then, a\nsurrogate model is trained to have similar functional (i.e. input-output\nmapping) and switching power characteristics as the oracle (black-box) model.\nOur results indicate that the inclusion of power consumption data increases the\nfidelity of the model extraction by up to 30 percent based on a mean square\nerror comparison of the oracle and surrogate weights. However, transferability\nof adversarial examples from the surrogate to the oracle model was not\nsignificantly affected.",
          "link": "http://arxiv.org/abs/2106.08299",
          "publishedOn": "2021-06-16T01:21:08.957Z",
          "wordCount": 631,
          "title": "Model Extraction and Adversarial Attacks on Neural Networks using Switching Power Information. (arXiv:2106.08299v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gronlund_A/0/1/0/all/0/1\">Allan Gr&#xf8;nlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogsgaard_M/0/1/0/all/0/1\">Mikael H&#xf8;gsgaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamma_L/0/1/0/all/0/1\">Lior Kamma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1\">Kasper Green Larsen</a>",
          "description": "Explaining the surprising generalization performance of deep neural networks\nis an active and important line of research in theoretical machine learning.\nInfluential work by Arora et al. (ICML'18) showed that, noise stability\nproperties of deep nets occurring in practice can be used to provably compress\nmodel representations. They then argued that the small representations of\ncompressed networks imply good generalization performance albeit only of the\ncompressed nets. Extending their compression framework to yield generalization\nbounds for the original uncompressed networks remains elusive.\n\nOur main contribution is the establishment of a compression-based framework\nfor proving generalization bounds. The framework is simple and powerful enough\nto extend the generalization bounds by Arora et al. to also hold for the\noriginal network. To demonstrate the flexibility of the framework, we also show\nthat it allows us to give simple proofs of the strongest known generalization\nbounds for other popular machine learning models, namely Support Vector\nMachines and Boosting.",
          "link": "http://arxiv.org/abs/2106.07989",
          "publishedOn": "2021-06-16T01:21:08.950Z",
          "wordCount": 571,
          "title": "Compression Implies Generalization. (arXiv:2106.07989v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Soyoung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>",
          "description": "Data augmentation with mixup has shown to be effective on various computer\nvision tasks. Despite its great success, there has been a hurdle to apply mixup\nto NLP tasks since text consists of discrete tokens with variable length. In\nthis work, we propose SSMix, a novel mixup method where the operation is\nperformed on input text rather than on hidden vectors like previous approaches.\nSSMix synthesizes a sentence while preserving the locality of two original\ntexts by span-based mixing and keeping more tokens related to the prediction\nrelying on saliency information. With extensive experiments, we empirically\nvalidate that our method outperforms hidden-level mixup methods on a wide range\nof text classification benchmarks, including textual entailment, sentiment\nclassification, and question-type classification. Our code is available at\nhttps://github.com/clovaai/ssmix.",
          "link": "http://arxiv.org/abs/2106.08062",
          "publishedOn": "2021-06-16T01:21:08.928Z",
          "wordCount": 563,
          "title": "SSMix: Saliency-Based Span Mixup for Text Classification. (arXiv:2106.08062v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Godwin_J/0/1/0/all/0/1\">Jonathan Godwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaarschmidt_M/0/1/0/all/0/1\">Michael Schaarschmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaunt_A/0/1/0/all/0/1\">Alexander Gaunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1\">Alvaro Sanchez-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubanova_Y/0/1/0/all/0/1\">Yulia Rubanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirkpatrick_J/0/1/0/all/0/1\">James Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1\">Peter Battaglia</a>",
          "description": "Graph Neural Networks (GNNs) perform learned message passing over an input\ngraph, but conventional wisdom says performing more than handful of steps makes\ntraining difficult and does not yield improved performance. Here we show the\ncontrary. We train a deep GNN with up to 100 message passing steps and achieve\nseveral state-of-the-art results on two challenging molecular property\nprediction benchmarks, Open Catalyst 2020 IS2RE and QM9. Our approach depends\ncrucially on a novel but simple regularisation method, which we call ``Noisy\nNodes'', in which we corrupt the input graph with noise and add an auxiliary\nnode autoencoder loss if the task is graph property prediction. Our results\nshow this regularisation method allows the model to monotonically improve in\nperformance with increased message passing steps. Our work opens new\nopportunities for reaping the benefits of deep neural networks in the space of\ngraph and other structured prediction problems.",
          "link": "http://arxiv.org/abs/2106.07971",
          "publishedOn": "2021-06-16T01:21:08.921Z",
          "wordCount": 578,
          "title": "Very Deep Graph Neural Networks Via Noise Regularisation. (arXiv:2106.07971v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kao_T/0/1/0/all/0/1\">Ta-Chu Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensen_K/0/1/0/all/0/1\">Kristopher T. Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1\">Alberto Bernacchia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_G/0/1/0/all/0/1\">Guillaume Hennequin</a>",
          "description": "Biological agents are known to learn many different tasks over the course of\ntheir lives, and to be able to revisit previous tasks and behaviors with little\nto no loss in performance. In contrast, artificial agents are prone to\n'catastrophic forgetting' whereby performance on previous tasks deteriorates\nrapidly as new ones are acquired. This shortcoming has recently been addressed\nusing methods that encourage parameters to stay close to those used for\nprevious tasks. This can be done by (i) using specific parameter regularizers\nthat map out suitable destinations in parameter space, or (ii) guiding the\noptimization journey by projecting gradients into subspaces that do not\ninterfere with previous tasks. However, parameter regularization has been shown\nto be relatively ineffective in recurrent neural networks (RNNs), a setting\nrelevant to the study of neural dynamics supporting biological continual\nlearning. Similarly, projection based methods can reach capacity and fail to\nlearn any further as the number of tasks increases. To address these\nlimitations, we propose Natural Continual Learning (NCL), a new method that\nunifies weight regularization and projected gradient descent. NCL uses Bayesian\nweight regularization to encourage good performance on all tasks at convergence\nand combines this with gradient projections designed to prevent catastrophic\nforgetting during optimization. NCL formalizes gradient projection as a trust\nregion algorithm based on the Fisher information metric, and achieves\nscalability via a novel Kronecker-factored approximation strategy. Our method\noutperforms both standard weight regularization techniques and projection based\napproaches when applied to continual learning problems in RNNs. The trained\nnetworks evolve task-specific dynamics that are strongly preserved as new tasks\nare learned, similar to experimental findings in biological circuits.",
          "link": "http://arxiv.org/abs/2106.08085",
          "publishedOn": "2021-06-16T01:21:08.915Z",
          "wordCount": 706,
          "title": "Natural continual learning: success is a journey, not (just) a destination. (arXiv:2106.08085v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianlei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1\">Wenzhi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xingzhou Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xucheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Pengcheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weisheng Zhao</a>",
          "description": "Convolutional neural networks (CNNs) have achieved great success in\nperforming cognitive tasks. However, execution of CNNs requires a large amount\nof computing resources and generates heavy memory traffic, which imposes a\nsevere challenge on computing system design. Through optimizing parallel\nexecutions and data reuse in convolution, systolic architecture demonstrates\ngreat advantages in accelerating CNN computations. However, regular internal\ndata transmission path in traditional systolic architecture prevents the\nsystolic architecture from completely leveraging the benefits introduced by\nneural network sparsity. Deployment of fine-grained sparsity on the existing\nsystolic architectures is greatly hindered by the incurred computational\noverheads. In this work, we propose S2Engine $-$ a novel systolic architecture\nthat can fully exploit the sparsity in CNNs with maximized data reuse. S2Engine\ntransmits compressed data internally and allows each processing element to\ndynamically select an aligned data from the compressed dataflow in convolution.\nCompared to the naive systolic array, S2Engine achieves about $3.2\\times$ and\nabout $3.0\\times$ improvements on speed and energy efficiency, respectively.",
          "link": "http://arxiv.org/abs/2106.07894",
          "publishedOn": "2021-06-16T01:21:08.908Z",
          "wordCount": 618,
          "title": "S2Engine: A Novel Systolic Architecture for Sparse Convolutional Neural Networks. (arXiv:2106.07894v1 [cs.AR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tseytlin_B/0/1/0/all/0/1\">Boris Tseytlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1\">Ilya Makarov</a>",
          "description": "We approach the problem of hotel recognition with deep metric learning. We\noverview the existing approaches and propose a modification to Contrastive loss\ncalled Contrastive-Triplet loss. We construct a robust pipeline for\nbenchmarking metric learning models and perform experiments on Hotels-50K and\nCUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval\non Hotels-50k. We open-source our code.",
          "link": "http://arxiv.org/abs/2106.08042",
          "publishedOn": "2021-06-16T01:21:08.901Z",
          "wordCount": 491,
          "title": "Hotel Recognition via Latent Image Embedding. (arXiv:2106.08042v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boney_R/0/1/0/all/0/1\">Rinu Boney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1\">Alexander Ilin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1\">Juho Kannala</a>",
          "description": "In many control problems that include vision, optimal controls can be\ninferred from the location of the objects in the scene. This information can be\nrepresented using keypoints, which is a list of spatial locations in the input\nimage. Previous works show that keypoint representations learned during\nunsupervised pre-training using encoder-decoder architectures can provide good\nfeatures for control tasks. In this paper, we show that it is possible to learn\nefficient keypoint representations end-to-end, without the need for\nunsupervised pre-training, decoders, or additional losses. Our proposed\narchitecture consists of a differentiable keypoint extractor that feeds the\ncoordinates of the estimated keypoints directly to a soft actor-critic agent.\nThe proposed algorithm yields performance competitive to the state-of-the art\non DeepMind Control Suite tasks.",
          "link": "http://arxiv.org/abs/2106.07995",
          "publishedOn": "2021-06-16T01:21:08.881Z",
          "wordCount": 558,
          "title": "End-to-End Learning of Keypoint Representations for Continuous Control from Images. (arXiv:2106.07995v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rabold_J/0/1/0/all/0/1\">Johannes Rabold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebers_M/0/1/0/all/0/1\">Michael Siebers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1\">Ute Schmid</a>",
          "description": "In recent research, human-understandable explanations of machine learning\nmodels have received a lot of attention. Often explanations are given in form\nof model simplifications or visualizations. However, as shown in cognitive\nscience as well as in early AI research, concept understanding can also be\nimproved by the alignment of a given instance for a concept with a similar\ncounterexample. Contrasting a given instance with a structurally similar\nexample which does not belong to the concept highlights what characteristics\nare necessary for concept membership. Such near misses have been proposed by\nWinston (1970) as efficient guidance for learning in relational domains. We\nintroduce an explanation generation algorithm for relational concepts learned\nwith Inductive Logic Programming (\\textsc{GeNME}). The algorithm identifies\nnear miss examples from a given set of instances and ranks these examples by\ntheir degree of closeness to a specific positive instance. A modified rule\nwhich covers the near miss but not the original instance is given as an\nexplanation. We illustrate \\textsc{GeNME} with the well known family domain\nconsisting of kinship relations, the visual relational Winston arches domain\nand a real-world domain dealing with file management. We also present a\npsychological experiment comparing human preferences of rule-based,\nexample-based, and near miss explanations in the family and the arches domains.",
          "link": "http://arxiv.org/abs/2106.08064",
          "publishedOn": "2021-06-16T01:21:08.874Z",
          "wordCount": 645,
          "title": "Generating Contrastive Explanations for Inductive Logic Programming Based on a Near Miss Approach. (arXiv:2106.08064v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joe_B/0/1/0/all/0/1\">Byunggill Joe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1\">Akshay Mehra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1\">Insik Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamm_J/0/1/0/all/0/1\">Jihun Hamm</a>",
          "description": "Electronic Health Records (EHRs) provide a wealth of information for machine\nlearning algorithms to predict the patient outcome from the data including\ndiagnostic information, vital signals, lab tests, drug administration, and\ndemographic information. Machine learning models can be built, for example, to\nevaluate patients based on their predicted mortality or morbidity and to\npredict required resources for efficient resource management in hospitals. In\nthis paper, we demonstrate that an attacker can manipulate the machine learning\npredictions with EHRs easily and selectively at test time by backdoor attacks\nwith the poisoned training data. Furthermore, the poison we create has\nstatistically similar features to the original data making it hard to detect,\nand can also attack multiple machine learning models without any knowledge of\nthe models. With less than 5% of the raw EHR data poisoned, we achieve average\nattack success rates of 97% on mortality prediction tasks with MIMIC-III\ndatabase against Logistic Regression, Multilayer Perceptron, and Long\nShort-term Memory models simultaneously.",
          "link": "http://arxiv.org/abs/2106.07925",
          "publishedOn": "2021-06-16T01:21:08.868Z",
          "wordCount": 606,
          "title": "Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks. (arXiv:2106.07925v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07953",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1\">Po-Yu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Min Tsai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_H/0/1/0/all/0/1\">Hsien-Kai Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Hantao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hsin-Hung Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_S/0/1/0/all/0/1\">Sheng-Hong Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ou_W/0/1/0/all/0/1\">Wei-Lun Ou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Chia-Ming Cheng</a>",
          "description": "Owing to the complicated characteristics of 5G communication system,\ndesigning RF components through mathematical modeling becomes a challenging\nobstacle. Moreover, such mathematical models need numerous manual adjustments\nfor various specification requirements. In this paper, we present a\nlearning-based framework to model and compensate Power Amplifiers (PAs) in 5G\ncommunication. In the proposed framework, Deep Neural Networks (DNNs) are used\nto learn the characteristics of the PAs, while, correspondent Digital\nPre-Distortions (DPDs) are also learned to compensate for the nonlinear and\nmemory effects of PAs. On top of the framework, we further propose two\nfrequency domain losses to guide the learning process to better optimize the\ntarget, compared to naive time domain Mean Square Error (MSE). The proposed\nframework serves as a drop-in replacement for the conventional approach. The\nproposed approach achieves an average of 56.7% reduction of nonlinear and\nmemory effects, which converts to an average of 16.3% improvement over a\ncarefully-designed mathematical model, and even reaches 34% enhancement in\nsevere distortion scenarios.",
          "link": "http://arxiv.org/abs/2106.07953",
          "publishedOn": "2021-06-16T01:21:08.861Z",
          "wordCount": 622,
          "title": "Learning to Compensate: A Deep Neural Network Framework for 5G Power Amplifier Compensation. (arXiv:2106.07953v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tianhao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mark Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>",
          "description": "Federated learning can be a promising solution for enabling IoT cybersecurity\n(i.e., anomaly detection in the IoT environment) while preserving data privacy\nand mitigating the high communication/storage overhead (e.g., high-frequency\ndata from time-series sensors) of centralized over-the-cloud approaches. In\nthis paper, to further push forward this direction with a comprehensive study\nin both algorithm and system design, we build FedIoT platform that contains a\nsynthesized dataset using N-BaIoT, FedDetect algorithm, and a system design for\nIoT devices. Furthermore, the proposed FedDetect learning framework improves\nthe performance by utilizing an adaptive optimizer (e.g., Adam) and a\ncross-round learning rate scheduler. In a network of realistic IoT devices\n(Raspberry PI), we evaluate FedIoT platform and FedDetect algorithm in both\nmodel and system performance. Our results demonstrate the efficacy of federated\nlearning in detecting a large range of attack types. The system efficiency\nanalysis indicates that both end-to-end training time and memory cost are\naffordable and promising for resource-constrained IoT devices. The source code\nis publicly available.",
          "link": "http://arxiv.org/abs/2106.07976",
          "publishedOn": "2021-06-16T01:21:08.854Z",
          "wordCount": 610,
          "title": "Federated Learning for Internet of Things: A Federated Learning Framework for On-device Anomaly Data Detection. (arXiv:2106.07976v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1\">Chenze Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In recent years, Neural Machine Translation (NMT) has achieved notable\nresults in various translation tasks. However, the word-by-word generation\nmanner determined by the autoregressive mechanism leads to high translation\nlatency of the NMT and restricts its low-latency applications.\nNon-Autoregressive Neural Machine Translation (NAT) removes the autoregressive\nmechanism and achieves significant decoding speedup through generating target\nwords independently and simultaneously. Nevertheless, NAT still takes the\nword-level cross-entropy loss as the training objective, which is not optimal\nbecause the output of NAT cannot be properly evaluated due to the multimodality\nproblem. In this paper, we propose using sequence-level training objectives to\ntrain NAT models, which evaluate the NAT outputs as a whole and correlates well\nwith the real translation quality. Firstly, we propose training NAT models to\noptimize sequence-level evaluation metrics (e.g., BLEU) based on several novel\nreinforcement algorithms customized for NAT, which outperforms the conventional\nmethod by reducing the variance of gradient estimation. Secondly, we introduce\na novel training objective for NAT models, which aims to minimize the\nBag-of-Ngrams (BoN) difference between the model output and the reference\nsentence. The BoN training objective is differentiable and can be calculated\nefficiently without doing any approximations. Finally, we apply a three-stage\ntraining strategy to combine these two methods to train the NAT model. We\nvalidate our approach on four translation tasks (WMT14 En$\\leftrightarrow$De,\nWMT16 En$\\leftrightarrow$Ro), which shows that our approach largely outperforms\nNAT baselines and achieves remarkable performance on all translation tasks.",
          "link": "http://arxiv.org/abs/2106.08122",
          "publishedOn": "2021-06-16T01:21:08.846Z",
          "wordCount": 669,
          "title": "Sequence-Level Training for Non-Autoregressive Neural Machine Translation. (arXiv:2106.08122v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08147",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ma_D/0/1/0/all/0/1\">Di Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Afonso_M/0/1/0/all/0/1\">Mariana Afonso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1\">David R. Bull</a>",
          "description": "Spatial resolution adaptation is a technique which has often been employed in\nvideo compression to enhance coding efficiency. This approach encodes a lower\nresolution version of the input video and reconstructs the original resolution\nduring decoding. Instead of using conventional up-sampling filters, recent work\nhas employed advanced super-resolution methods based on convolutional neural\nnetworks (CNNs) to further improve reconstruction quality. These approaches are\nusually trained to minimise pixel-based losses such as Mean-Squared Error\n(MSE), despite the fact that this type of loss metric does not correlate well\nwith subjective opinions. In this paper, a perceptually-inspired\nsuper-resolution approach (M-SRGAN) is proposed for spatial up-sampling of\ncompressed video using a modified CNN model, which has been trained using a\ngenerative adversarial network (GAN) on compressed content with perceptual loss\nfunctions. The proposed method was integrated with HEVC HM 16.20, and has been\nevaluated on the JVET Common Test Conditions (UHD test sequences) using the\nRandom Access configuration. The results show evident perceptual quality\nimprovement over the original HM 16.20, with an average bitrate saving of 35.6%\n(Bj{\\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF.",
          "link": "http://arxiv.org/abs/2106.08147",
          "publishedOn": "2021-06-16T01:21:08.826Z",
          "wordCount": 624,
          "title": "Perceptually-inspired super-resolution of compressed videos. (arXiv:2106.08147v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Isonuma_M/0/1/0/all/0/1\">Masaru Isonuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_J/0/1/0/all/0/1\">Junichiro Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakata_I/0/1/0/all/0/1\">Ichiro Sakata</a>",
          "description": "This paper presents a novel unsupervised abstractive summarization method for\nopinionated texts. While the basic variational autoencoder-based models assume\na unimodal Gaussian prior for the latent code of sentences, we alternate it\nwith a recursive Gaussian mixture, where each mixture component corresponds to\nthe latent code of a topic sentence and is mixed by a tree-structured topic\ndistribution. By decoding each Gaussian component, we generate sentences with\ntree-structured topic guidance, where the root sentence conveys generic\ncontent, and the leaf sentences describe specific topics. Experimental results\ndemonstrate that the generated topic sentences are appropriate as a summary of\nopinionated texts, which are more informative and cover more input contents\nthan those generated by the recent unsupervised summarization model\n(Bra\\v{z}inskas et al., 2020). Furthermore, we demonstrate that the variance of\nlatent Gaussians represents the granularity of sentences, analogous to Gaussian\nword embedding (Vilnis and McCallum, 2015).",
          "link": "http://arxiv.org/abs/2106.08007",
          "publishedOn": "2021-06-16T01:21:08.819Z",
          "wordCount": 592,
          "title": "Unsupervised Abstractive Opinion Summarization by Generating Sentences with Tree-Structured Topic Guidance. (arXiv:2106.08007v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mnih_A/0/1/0/all/0/1\">Andriy Mnih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_G/0/1/0/all/0/1\">George Tucker</a>",
          "description": "Training models with discrete latent variables is challenging due to the high\nvariance of unbiased gradient estimators. While low-variance reparameterization\ngradients of a continuous relaxation can provide an effective solution, a\ncontinuous relaxation is not always available or tractable. Dong et al. (2020)\nand Yin et al. (2020) introduced a performant estimator that does not rely on\ncontinuous relaxations; however, it is limited to binary random variables. We\nintroduce a novel derivation of their estimator based on importance sampling\nand statistical couplings, which we extend to the categorical setting.\nMotivated by the construction of a stick-breaking coupling, we introduce\ngradient estimators based on reparameterizing categorical variables as\nsequences of binary variables and Rao-Blackwellization. In systematic\nexperiments, we show that our proposed categorical gradient estimators provide\nstate-of-the-art performance, whereas even with additional\nRao-Blackwellization, previous estimators (Yin et al., 2019) underperform a\nsimpler REINFORCE with a leave-one-out-baseline estimator (Kool et al., 2019).",
          "link": "http://arxiv.org/abs/2106.08056",
          "publishedOn": "2021-06-16T01:21:08.813Z",
          "wordCount": 577,
          "title": "Coupled Gradient Estimators for Discrete Latent Variables. (arXiv:2106.08056v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07991",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">Xuan Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoming Yuan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zeng_S/0/1/0/all/0/1\">Shangzhi Zeng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>",
          "description": "Bi-level optimization model is able to capture a wide range of complex\nlearning tasks with practical interest. Due to the witnessed efficiency in\nsolving bi-level programs, gradient-based methods have gained popularity in the\nmachine learning community. In this work, we propose a new gradient-based\nsolution scheme, namely, the Bi-level Value-Function-based Interior-point\nMethod (BVFIM). Following the main idea of the log-barrier interior-point\nscheme, we penalize the regularized value function of the lower level problem\ninto the upper level objective. By further solving a sequence of differentiable\nunconstrained approximation problems, we consequently derive a sequential\nprogramming scheme. The numerical advantage of our scheme relies on the fact\nthat, when gradient methods are applied to solve the approximation problem, we\nsuccessfully avoid computing any expensive Hessian-vector or Jacobian-vector\nproduct. We prove the convergence without requiring any convexity assumption on\neither the upper level or the lower level objective. Experiments demonstrate\nthe efficiency of the proposed BVFIM on non-convex bi-level problems.",
          "link": "http://arxiv.org/abs/2106.07991",
          "publishedOn": "2021-06-16T01:21:08.806Z",
          "wordCount": 600,
          "title": "A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization. (arXiv:2106.07991v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Corsini_A/0/1/0/all/0/1\">Andrea Corsini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shanchieh Jay Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apruzzese_G/0/1/0/all/0/1\">Giovanni Apruzzese</a>",
          "description": "Recent advances in deep learning renewed the research interests in machine\nlearning for Network Intrusion Detection Systems (NIDS). Specifically,\nattention has been given to sequential learning models, due to their ability to\nextract the temporal characteristics of Network traffic Flows (NetFlows), and\nuse them for NIDS tasks. However, the applications of these sequential models\noften consist of transferring and adapting methodologies directly from other\nfields, without an in-depth investigation on how to leverage the specific\ncircumstances of cybersecurity scenarios; moreover, there is a lack of\ncomprehensive studies on sequential models that rely on NetFlow data, which\npresents significant advantages over traditional full packet captures. We\ntackle this problem in this paper. We propose a detailed methodology to extract\ntemporal sequences of NetFlows that denote patterns of malicious activities.\nThen, we apply this methodology to compare the efficacy of sequential learning\nmodels against traditional static learning models. In particular, we perform a\nfair comparison of a `sequential' Long Short-Term Memory (LSTM) against a\n`static' Feedforward Neural Networks (FNN) in distinct environments represented\nby two well-known datasets for NIDS: the CICIDS2017 and the CTU13. Our results\nhighlight that LSTM achieves comparable performance to FNN in the CICIDS2017\nwith over 99.5\\% F1-score; while obtaining superior performance in the CTU13,\nwith 95.7\\% F1-score against 91.5\\%. This paper thus paves the way to future\napplications of sequential learning models for NIDS.",
          "link": "http://arxiv.org/abs/2106.07961",
          "publishedOn": "2021-06-16T01:21:08.800Z",
          "wordCount": 662,
          "title": "On the Evaluation of Sequential Machine Learning for Network Intrusion Detection. (arXiv:2106.07961v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1\">Matthias Minderer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1\">Josip Djolonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1\">Rob Romijnders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubis_F/0/1/0/all/0/1\">Frances Hubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dustin Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lucic</a>",
          "description": "Accurate estimation of predictive uncertainty (model calibration) is\nessential for the safe application of neural networks. Many instances of\nmiscalibration in modern neural networks have been reported, suggesting a trend\nthat newer, more accurate models produce poorly calibrated predictions. Here,\nwe revisit this question for recent state-of-the-art image classification\nmodels. We systematically relate model calibration and accuracy, and find that\nthe most recent models, notably those not using convolutions, are among the\nbest calibrated. Trends observed in prior model generations, such as decay of\ncalibration with distribution shift or model size, are less pronounced in\nrecent architectures. We also show that model size and amount of pretraining do\nnot fully explain these differences, suggesting that architecture is a major\ndeterminant of calibration properties.",
          "link": "http://arxiv.org/abs/2106.07998",
          "publishedOn": "2021-06-16T01:21:08.781Z",
          "wordCount": 560,
          "title": "Revisiting the Calibration of Modern Neural Networks. (arXiv:2106.07998v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alivanistos_D/0/1/0/all/0/1\">Dimitrios Alivanistos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1\">Max Berrendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1\">Michael Cochez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1\">Mikhail Galkin</a>",
          "description": "Multi-hop logical reasoning is an established problem in the field of\nrepresentation learning on knowledge graphs (KGs). It subsumes both one-hop\nlink prediction as well as other more complex types of logical queries.\nExisting algorithms operate only on classical, triple-based graphs, whereas\nmodern KGs often employ a hyper-relational modeling paradigm. In this paradigm,\ntyped edges may have several key-value pairs known as qualifiers that provide\nfine-grained context for facts. In queries, this context modifies the meaning\nof relations, and usually reduces the answer set. Hyper-relational queries are\noften observed in real-world KG applications, and existing approaches for\napproximate query answering cannot make use of qualifier pairs. In this work,\nwe bridge this gap and extend the multi-hop reasoning problem to\nhyper-relational KGs allowing to tackle this new type of complex queries.\nBuilding upon recent advancements in Graph Neural Networks and query embedding\ntechniques, we study how to embed and answer hyper-relational conjunctive\nqueries. Besides that, we propose a method to answer such queries and\ndemonstrate in our experiments that qualifiers improve query answering on a\ndiverse set of query patterns.",
          "link": "http://arxiv.org/abs/2106.08166",
          "publishedOn": "2021-06-16T01:21:08.775Z",
          "wordCount": 611,
          "title": "Query Embedding on Hyper-relational Knowledge Graphs. (arXiv:2106.08166v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Long Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zehong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1\">Shasha Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1\">Gang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongyang Chen</a>",
          "description": "In this paper, we propose a Thompson Sampling algorithm for \\emph{unimodal}\nbandits, where the expected reward is unimodal over the partially ordered arms.\nTo exploit the unimodal structure better, at each step, instead of exploration\nfrom the entire decision space, our algorithm makes decision according to\nposterior distribution only in the neighborhood of the arm that has the highest\nempirical mean estimate. We theoretically prove that, for Bernoulli rewards,\nthe regret of our algorithm reaches the lower bound of unimodal bandits, thus\nit is asymptotically optimal. For Gaussian rewards, the regret of our algorithm\nis $\\mathcal{O}(\\log T)$, which is far better than standard Thompson Sampling\nalgorithms. Extensive experiments demonstrate the effectiveness of the proposed\nalgorithm on both synthetic data sets and the real-world applications.",
          "link": "http://arxiv.org/abs/2106.08187",
          "publishedOn": "2021-06-16T01:21:08.768Z",
          "wordCount": 553,
          "title": "Thompson Sampling for Unimodal Bandits. (arXiv:2106.08187v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1\">Tony Q. S. Quek</a>",
          "description": "This paper presents a machine learning strategy that tackles a distributed\noptimization task in a wireless network with an arbitrary number of randomly\ninterconnected nodes. Individual nodes decide their optimal states with\ndistributed coordination among other nodes through randomly varying backhaul\nlinks. This poses a technical challenge in distributed universal optimization\npolicy robust to a random topology of the wireless network, which has not been\nproperly addressed by conventional deep neural networks (DNNs) with rigid\nstructural configurations. We develop a flexible DNN formalism termed\ndistributed message-passing neural network (DMPNN) with forward and backward\ncomputations independent of the network topology. A key enabler of this\napproach is an iterative message-sharing strategy through arbitrarily connected\nbackhaul links. The DMPNN provides a convergent solution for iterative\ncoordination by learning numerous random backhaul interactions. The DMPNN is\ninvestigated for various configurations of the power control in wireless\nnetworks, and intensive numerical results prove its universality and viability\nover conventional optimization and DNN approaches.",
          "link": "http://arxiv.org/abs/2106.07984",
          "publishedOn": "2021-06-16T01:21:08.762Z",
          "wordCount": 597,
          "title": "Learning Autonomy in Management of Wireless Random Networks. (arXiv:2106.07984v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08086",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1\">Gunnar K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Freiesleben_T/0/1/0/all/0/1\">Timo Freiesleben</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Casalicchio_G/0/1/0/all/0/1\">Giuseppe Casalicchio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grosse_Wentrup_M/0/1/0/all/0/1\">Moritz Grosse-Wentrup</a>",
          "description": "Global model-agnostic feature importance measures either quantify whether\nfeatures are directly used for a model's predictions (direct importance) or\nwhether they contain prediction-relevant information (associative importance).\nDirect importance provides causal insight into the model's mechanism, yet it\nfails to expose the leakage of information from associated but not directly\nused variables. In contrast, associative importance exposes information leakage\nbut does not provide causal insight into the model's mechanism. We introduce\nDEDACT - a framework to decompose well-established direct and associative\nimportance measures into their respective associative and direct components.\nDEDACT provides insight into both the sources of prediction-relevant\ninformation in the data and the direct and indirect feature pathways by which\nthe information enters the model. We demonstrate the method's usefulness on\nsimulated examples.",
          "link": "http://arxiv.org/abs/2106.08086",
          "publishedOn": "2021-06-16T01:21:08.731Z",
          "wordCount": 559,
          "title": "Decomposition of Global Feature Importance into Direct and Associative Components (DEDACT). (arXiv:2106.08086v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07963",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Yokoo_K/0/1/0/all/0/1\">Kazuki Yokoo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ishida_K/0/1/0/all/0/1\">Kei Ishida</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ercan_A/0/1/0/all/0/1\">Ali Ercan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tu_T/0/1/0/all/0/1\">Tongbi Tu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nagasato_T/0/1/0/all/0/1\">Takeyoshi Nagasato</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kiyama_M/0/1/0/all/0/1\">Masato Kiyama</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Amagasaki_M/0/1/0/all/0/1\">Motoki Amagasaki</a>",
          "description": "This study investigates the relationships which deep learning methods can\nidentify between the input and output data. As a case study, rainfall-runoff\nmodeling in a snow-dominated watershed by means of a long- and short-term\nmemory (LSTM) network is selected. Daily precipitation and mean air temperature\nwere used as model input to estimate daily flow discharge. After model training\nand verification, two experimental simulations were conducted with hypothetical\ninputs instead of observed meteorological data to clarify the response of the\ntrained model to the inputs. The first numerical experiment showed that even\nwithout input precipitation, the trained model generated flow discharge,\nparticularly winter low flow and high flow during the snow-melting period. The\neffects of warmer and colder conditions on the flow discharge were also\nreplicated by the trained model without precipitation. Additionally, the model\nreflected only 17-39% of the total precipitation mass during the snow\naccumulation period in the total annual flow discharge, revealing a strong lack\nof water mass conservation. The results of this study indicated that a deep\nlearning method may not properly learn the explicit physical relationships\nbetween input and target variables, although they are still capable of\nmaintaining strong goodness-of-fit results.",
          "link": "http://arxiv.org/abs/2106.07963",
          "publishedOn": "2021-06-16T01:21:08.651Z",
          "wordCount": 651,
          "title": "Capabilities of Deep Learning Models on Learning Physical Relationships: Case of Rainfall-Runoff Modeling with LSTM. (arXiv:2106.07963v1 [physics.ao-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braverman_M/0/1/0/all/0/1\">Mark Braverman</a>",
          "description": "The goal of this paper is to develop a generic framework for converting\nmodern optimization algorithms into mechanisms where inputs come from\nself-interested agents. We focus on aggregating preferences from $n$ players in\na context without money. Special cases of this setting include voting,\nallocation of items by lottery, and matching. Our key technical contribution is\na new meta-algorithm we call \\apex (Adaptive Pricing Equalizing Externalities).\nThe framework is sufficiently general to be combined with any optimization\nalgorithm that is based on local search. We outline an agenda for studying the\nalgorithm's properties and its applications. As a special case of applying the\nframework to the problem of one-sided assignment with lotteries, we obtain a\nstrengthening of the 1979 result by Hylland and Zeckhauser on allocation via a\ncompetitive equilibrium from equal incomes (CEEI). The [HZ79] result posits\nthat there is a (fractional) allocation and a set of item prices such that the\nallocation is a competitive equilibrium given prices. We further show that\nthere is always a reweighing of the players' utility values such that running\nunit-demand VCG with reweighed utilities leads to a HZ-equilibrium prices.\nInterestingly, not all HZ competitive equilibria come from VCG prices. As part\nof our proof, we re-prove the [HZ79] result using only Brouwer's fixed point\ntheorem (and not the more general Kakutani's theorem). This may be of\nindependent interest.",
          "link": "http://arxiv.org/abs/2106.07752",
          "publishedOn": "2021-06-16T01:21:08.578Z",
          "wordCount": 667,
          "title": "Optimization-friendly generic mechanisms without money. (arXiv:2106.07752v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Cheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1\">Pengwei Tian</a>",
          "description": "Recent advances in AIoT technologies have led to an increasing popularity of\nutilizing machine learning algorithms to detect operational failures for\ncyber-physical systems (CPS). In its basic form, an anomaly detection module\nmonitors the sensor measurements and actuator states from the physical plant,\nand detects anomalies in these measurements to identify abnormal operation\nstatus. Nevertheless, building effective anomaly detection models for CPS is\nrather challenging as the model has to accurately detect anomalies in presence\nof highly complicated system dynamics and unknown amount of sensor noise. In\nthis work, we propose a novel time series anomaly detection method called\nNeural System Identification and Bayesian Filtering (NSIBF) in which a\nspecially crafted neural network architecture is posed for system\nidentification, i.e., capturing the dynamics of CPS in a dynamical state-space\nmodel; then a Bayesian filtering algorithm is naturally applied on top of the\n\"identified\" state-space model for robust anomaly detection by tracking the\nuncertainty of the hidden state of the system recursively over time. We provide\nqualitative as well as quantitative experiments with the proposed method on a\nsynthetic and three real-world CPS datasets, showing that NSIBF compares\nfavorably to the state-of-the-art methods with considerable improvements on\nanomaly detection in CPS.",
          "link": "http://arxiv.org/abs/2106.07992",
          "publishedOn": "2021-06-16T01:21:08.560Z",
          "wordCount": 650,
          "title": "Time Series Anomaly Detection for Cyber-physical Systems via Neural System Identification and Bayesian Filtering. (arXiv:2106.07992v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07823",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vivek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>",
          "description": "Multilingualism refers to the high degree of proficiency in two or more\nlanguages in the written and oral communication modes. It often results in\nlanguage mixing, a.k.a. code-mixing, when a multilingual speaker switches\nbetween multiple languages in a single utterance of a text or speech. This\npaper discusses the current state of the NLP research, limitations, and\nforeseeable pitfalls in addressing five real-world applications for social good\ncrisis management, healthcare, political campaigning, fake news, and hate\nspeech for multilingual societies. We also propose futuristic datasets, models,\nand tools that can significantly advance the current research in multilingual\nNLP applications for the societal good. As a representative example, we\nconsider English-Hindi code-mixing but draw similar inferences for other\nlanguage pairs",
          "link": "http://arxiv.org/abs/2106.07823",
          "publishedOn": "2021-06-16T01:21:08.554Z",
          "wordCount": 547,
          "title": "Challenges and Considerations with Code-Mixed NLP for Multilingual Societies. (arXiv:2106.07823v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valente_F/0/1/0/all/0/1\">Francisco Valente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paredes_S/0/1/0/all/0/1\">Simao Paredes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1\">Jorge Henriques</a>",
          "description": "One of the key challenges when developing a predictive model is the\ncapability to describe the domain knowledge and the cause-effect relationships\nin a simple way. Decision rules are a useful and important methodology in this\ncontext, justifying their application in several areas, in particular in\nclinical practice. Several machine-learning classifiers have exploited the\nadvantageous properties of decision rules to build intelligent prediction\nmodels, namely decision trees and ensembles of trees (ETs). However, such\nmethodologies usually suffer from a trade-off between interpretability and\npredictive performance. Some procedures consider a simplification of ETs, using\nheuristic approaches to select an optimal reduced set of decision rules. In\nthis paper, we introduce a novel step to those methodologies. We create a new\ncomponent to predict if a given rule will be correct or not for a particular\npatient, which introduces personalization into the procedure. Furthermore, the\nvalidation results using three public clinical datasets show that it also\nallows to increase the predictive performance of the selected set of rules,\nimproving the mentioned trade-off.",
          "link": "http://arxiv.org/abs/2106.07827",
          "publishedOn": "2021-06-16T01:21:08.548Z",
          "wordCount": 606,
          "title": "Improving the compromise between accuracy, interpretability and personalization of rule-based machine learning in medical problems. (arXiv:2106.07827v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poskitt_C/0/1/0/all/0/1\">Christopher M. Poskitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>",
          "description": "Cyber-physical systems (CPSs) are widespread in critical domains, and\nsignificant damage can be caused if an attacker is able to modify the code of\ntheir programmable logic controllers (PLCs). Unfortunately, traditional\ntechniques for attesting code integrity (i.e. verifying that it has not been\nmodified) rely on firmware access or roots-of-trust, neither of which\nproprietary or legacy PLCs are likely to provide. In this paper, we propose a\npractical code integrity checking solution based on privacy-preserving black\nbox models that instead attest the input/output behaviour of PLC programs.\nUsing faithful offline copies of the PLC programs, we identify their most\nimportant inputs through an information flow analysis, execute them on multiple\ncombinations to collect data, then train neural networks able to predict PLC\noutputs (i.e. actuator commands) from their inputs. By exploiting the black box\nnature of the model, our solution maintains the privacy of the original PLC\ncode and does not assume that attackers are unaware of its presence. The trust\ninstead comes from the fact that it is extremely hard to attack the PLC code\nand neural networks at the same time and with consistent outcomes. We evaluated\nour approach on a modern six-stage water treatment plant testbed, finding that\nit could predict actuator states from PLC inputs with near-100% accuracy, and\nthus could detect all 120 effective code mutations that we subjected the PLCs\nto. Finally, we found that it is not practically possible to simultaneously\nmodify the PLC code and apply discreet adversarial noise to our attesters in a\nway that leads to consistent (mis-)predictions.",
          "link": "http://arxiv.org/abs/2106.07851",
          "publishedOn": "2021-06-16T01:21:08.498Z",
          "wordCount": 715,
          "title": "Code Integrity Attestation for PLCs using Black Box Neural Network Predictions. (arXiv:2106.07851v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kidambi_R/0/1/0/all/0/1\">Rahul Kidambi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jonathan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>",
          "description": "This paper studies Imitation Learning from Observations alone (ILFO) where\nthe learner is presented with expert demonstrations that consist only of states\nvisited by an expert (without access to actions taken by the expert). We\npresent a provably efficient model-based framework MobILE to solve the ILFO\nproblem. MobILE involves carefully trading off strategic exploration against\nimitation - this is achieved by integrating the idea of optimism in the face of\nuncertainty into the distribution matching imitation learning (IL) framework.\nWe provide a unified analysis for MobILE, and demonstrate that MobILE enjoys\nstrong performance guarantees for classes of MDP dynamics that satisfy certain\nwell studied notions of structural complexity. We also show that the ILFO\nproblem is strictly harder than the standard IL problem by presenting an\nexponential sample complexity separation between IL and ILFO. We complement\nthese theoretical results with experimental simulations on benchmark OpenAI Gym\ntasks that indicate the efficacy of MobILE.",
          "link": "http://arxiv.org/abs/2102.10769",
          "publishedOn": "2021-06-16T01:21:08.466Z",
          "wordCount": 614,
          "title": "MobILE: Model-Based Imitation Learning From Observation Alone. (arXiv:2102.10769v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liexing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoxue Wang</a>",
          "description": "Loan risk for small business has long been a complex problem worthy of\nexploring. Predicting the loan risk approximately can benefit entrepreneurship\nby developing more jobs for the society. CatBoost (Categorical Boosting) is a\npowerful machine learning algorithm that is suitable for dataset with many\ncategorical variables like the dataset for forecasting loan risk. In this\npaper, we identify the important risk factors that contribute to loan status\nclassification problem. Then we compare the the performance between\nboosting-type algorithms(especially CatBoost) with other traditional yet\npopular ones. The dataset we adopt in the research comes from the U.S. Small\nBusiness Administration (SBA) and holds a very large sample size (899,164\nobservations and 27 features). We obtain a high accuracy of 95.74% and\nwell-performed AUC of 98.59% compared with the existent literature of related\nresearch. In order to make best use of the important features in the dataset,\nwe propose a technique named \"synthetic generation\" to develop more combined\nfeatures based on arithmetic operation, which ends up improving the accuracy\nand AUC of original CatBoost model.",
          "link": "http://arxiv.org/abs/2106.07954",
          "publishedOn": "2021-06-16T01:21:08.426Z",
          "wordCount": 616,
          "title": "CatBoost model with synthetic features in application to loan risk assessment of small businesses. (arXiv:2106.07954v1 [cs.CE])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1\">Ramon Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>",
          "description": "Speech-based image retrieval has been studied as a proxy for joint\nrepresentation learning, usually without emphasis on retrieval itself. As such,\nit is unclear how well speech-based retrieval can work in practice -- both in\nan absolute sense and versus alternative strategies that combine automatic\nspeech recognition (ASR) with strong text encoders. In this work, we\nextensively study and expand choices of encoder architectures, training\nmethodology (including unimodal and multimodal pretraining), and other factors.\nOur experiments cover different types of speech in three datasets: Flickr\nAudio, Places Audio, and Localized Narratives. Our best model configuration\nachieves large gains over state of the art, e.g., pushing recall-at-one from\n21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also\nshow our best speech-based models can match or exceed cascaded ASR-to-text\nencoding when speech is spontaneous, accented, or otherwise hard to\nautomatically transcribe.",
          "link": "http://arxiv.org/abs/2104.01894",
          "publishedOn": "2021-06-16T01:21:08.419Z",
          "wordCount": 631,
          "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval. (arXiv:2104.01894v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We study transfer learning in the presence of spurious correlations. We\nexperimentally demonstrate that directly transferring the stable feature\nextractor learned on the source task may not eliminate these biases for the\ntarget task. However, we hypothesize that the unstable features in the source\ntask and those in the target task are directly related. By explicitly informing\nthe target classifier of the source task's unstable features, we can regularize\nthe biases in the target task. Specifically, we derive a representation that\nencodes the unstable features by contrasting different data environments in the\nsource task. On the target task, we cluster data from this representation, and\nachieve robustness by minimizing the worst-case risk across all clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask, outperforming the best baseline by 22.9% in absolute accuracy across 12\ntransfer settings. Our code is available at https://github.com/YujiaBao/Tofu.",
          "link": "http://arxiv.org/abs/2106.07847",
          "publishedOn": "2021-06-16T01:21:08.412Z",
          "wordCount": 601,
          "title": "Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12690",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heijden_B/0/1/0/all/0/1\">Bas van der Heijden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferranti_L/0/1/0/all/0/1\">Laura Ferranti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1\">Jens Kober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1\">Robert Babuska</a>",
          "description": "This paper presents DeepKoCo, a novel model-based agent that learns a latent\nKoopman representation from images. This representation allows DeepKoCo to plan\nefficiently using linear control methods, such as linear model predictive\ncontrol. Compared to traditional agents, DeepKoCo is robust to task-irrelevant\ndynamics, thanks to the use of a tailored lossy autoencoder network that allows\nDeepKoCo to learn latent dynamics that reconstruct and predict only observed\ncosts, rather than all observed dynamics. As our results show, DeepKoCo\nachieves a similar final performance as traditional model-free methods on\ncomplex control tasks, while being considerably more robust to distractor\ndynamics, making the proposed agent more amenable for real-life applications.",
          "link": "http://arxiv.org/abs/2011.12690",
          "publishedOn": "2021-06-16T01:21:08.391Z",
          "wordCount": 581,
          "title": "DeepKoCo: Efficient latent planning with a robust Koopman representation. (arXiv:2011.12690v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Hao-Zhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhaoyang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>",
          "description": "Conventional unsupervised multi-source domain adaptation (UMDA) methods\nassume all source domains can be accessed directly. This neglects the\nprivacy-preserving policy, that is, all the data and computations must be kept\ndecentralized. There exists three problems in this scenario: (1) Minimizing the\ndomain distance requires the pairwise calculation of the data from source and\ntarget domains, which is not accessible. (2) The communication cost and privacy\nsecurity limit the application of UMDA methods (e.g., the domain adversarial\ntraining). (3) Since users have no authority to check the data quality, the\nirrelevant or malicious source domains are more likely to appear, which causes\nnegative transfer. In this study, we propose a privacy-preserving UMDA paradigm\nnamed Knowledge Distillation based Decentralized Domain Adaptation (KD3A),\nwhich performs domain adaptation through the knowledge distillation on models\nfrom different source domains. KD3A solves the above problems with three\ncomponents: (1) A multi-source knowledge distillation method named Knowledge\nVote to learn high-quality domain consensus knowledge. (2) A dynamic weighting\nstrategy named Consensus Focus to identify both the malicious and irrelevant\ndomains. (3) A decentralized optimization strategy for domain distance named\nBatchNorm MMD. The extensive experiments on DomainNet demonstrate that KD3A is\nrobust to the negative transfer and brings a 100x reduction of communication\ncost compared with other decentralized UMDA methods. Moreover, our KD3A\nsignificantly outperforms state-of-the-art UMDA approaches.",
          "link": "http://arxiv.org/abs/2011.09757",
          "publishedOn": "2021-06-16T01:21:08.385Z",
          "wordCount": 740,
          "title": "KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation. (arXiv:2011.09757v7 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1\">Nikos Vlassis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrashekar_A/0/1/0/all/0/1\">Ashok Chandrashekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gil_F/0/1/0/all/0/1\">Fernando Amat Gil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kallus_N/0/1/0/all/0/1\">Nathan Kallus</a>",
          "description": "We study the problem of off-policy evaluation from batched contextual bandit\ndata with multidimensional actions, often termed slates. The problem is common\nto recommender systems and user-interface optimization, and it is particularly\nchallenging because of the combinatorially-sized action space. Swaminathan et\nal. (2017) have proposed the pseudoinverse (PI) estimator under the assumption\nthat the conditional mean rewards are additive in actions. Using control\nvariates, we consider a large class of unbiased estimators that includes as\nspecific cases the PI estimator and (asymptotically) its self-normalized\nvariant. By optimizing over this class, we obtain new estimators with risk\nimprovement guarantees over both the PI and self-normalized PI estimators.\nExperiments with real-world recommender data as well as synthetic data validate\nthese improvements in practice.",
          "link": "http://arxiv.org/abs/2106.07914",
          "publishedOn": "2021-06-16T01:21:08.377Z",
          "wordCount": 546,
          "title": "Control Variates for Slate Off-Policy Evaluation. (arXiv:2106.07914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.05905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhichuang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruimin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Amrita Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Long Lu</a>",
          "description": "With the increased usage of AI accelerators on mobile and edge devices,\non-device machine learning (ML) is gaining popularity. Consequently, thousands\nof proprietary ML models are being deployed on billions of untrusted devices.\nThis raises serious security concerns about model privacy. However, protecting\nthe model privacy without losing access to the AI accelerators is a challenging\nproblem. In this paper, we present a novel on-device model inference system,\nShadowNet. ShadowNet protects the model privacy with Trusted Execution\nEnvironment (TEE) while securely outsourcing the heavy linear layers of the\nmodel to the untrusted hardware accelerators. ShadowNet achieves this by\ntransforming the weights of the linear layers before outsourcing them and\nrestoring the results inside the TEE. The nonlinear layers are also kept secure\ninside the TEE. The transformation of the weights and the restoration of the\nresults are designed in a way that can be implemented efficiently. We have\nbuilt a ShadowNet prototype based on TensorFlow Lite and applied it on four\npopular CNNs, namely, MobileNets, ResNet-44, AlexNet and MiniVGG. Our\nevaluation shows that ShadowNet achieves strong security guarantees with\nreasonable performance, offering a practical solution for secure on-device\nmodel inference.",
          "link": "http://arxiv.org/abs/2011.05905",
          "publishedOn": "2021-06-16T01:21:08.371Z",
          "wordCount": 669,
          "title": "ShadowNet: A Secure and Efficient System for On-device Model Inference. (arXiv:2011.05905v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Crupi_R/0/1/0/all/0/1\">Riccardo Crupi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castelnovo_A/0/1/0/all/0/1\">Alessandro Castelnovo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1\">Daniele Regoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_B/0/1/0/all/0/1\">Beatriz San Miguel Gonzalez</a>",
          "description": "Explainable Artificial Intelligence (XAI) is a set of techniques that allows\nthe understanding of both technical and non-technical aspects of Artificial\nIntelligence (AI) systems. XAI is crucial to help satisfying the increasingly\nimportant demand of \\emph{trustworthy} Artificial Intelligence, characterized\nby fundamental characteristics such as respect of human autonomy, prevention of\nharm, transparency, accountability, etc. Within XAI techniques, counterfactual\nexplanations aim to provide to end users a set of features (and their\ncorresponding values) that need to be changed in order to achieve a desired\noutcome. Current approaches rarely take into account the feasibility of actions\nneeded to achieve the proposed explanations, and in particular they fall short\nof considering the causal impact of such actions. In this paper, we present\nCounterfactual Explanations as Interventions in Latent Space (CEILS), a\nmethodology to generate counterfactual explanations capturing by design the\nunderlying causal relations from the data, and at the same time to provide\nfeasible recommendations to reach the proposed profile. Moreover, our\nmethodology has the advantage that it can be set on top of existing\ncounterfactuals generator algorithms, thus minimising the complexity of\nimposing additional causal constrains. We demonstrate the effectiveness of our\napproach with a set of different experiments using synthetic and real datasets\n(including a proprietary dataset of the financial domain).",
          "link": "http://arxiv.org/abs/2106.07754",
          "publishedOn": "2021-06-16T01:21:08.364Z",
          "wordCount": 655,
          "title": "Counterfactual Explanations as Interventions in Latent Space. (arXiv:2106.07754v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1\">Mahdi Hajiaghayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1\">Monir Hajiaghayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolin_M/0/1/0/all/0/1\">Mark Bolin</a>",
          "description": "In this paper, we present a multi-lingual sentence encoder that can be used\nin search engines as a query and document encoder. This embedding enables a\nsemantic similarity score between queries and documents that can be an\nimportant feature in document ranking and relevancy. To train such a customized\nsentence encoder, it is beneficial to leverage users search data in the form of\nquery-document clicked pairs however, we must avoid relying too much on search\nclick data as it is biased and does not cover many unseen cases. The search\ndata is heavily skewed towards short queries and for long queries is small and\noften noisy. The goal is to design a universal multi-lingual encoder that works\nfor all cases and covers both short and long queries. We select a number of\npublic NLI datasets in different languages and translation data and together\nwith user search data we train a language model using a multi-task approach. A\nchallenge is that these datasets are not homogeneous in terms of content, size\nand the balance ratio. While the public NLI datasets are usually two-sentence\nbased with the same portion of positive and negative pairs, the user search\ndata can contain multi-sentence documents and only positive pairs. We show how\nmulti-task training enables us to leverage all these datasets and exploit\nknowledge sharing across these tasks.",
          "link": "http://arxiv.org/abs/2106.07719",
          "publishedOn": "2021-06-16T01:21:08.343Z",
          "wordCount": 651,
          "title": "Unbiased Sentence Encoder For Large-Scale Multi-lingual Search Engines. (arXiv:2106.07719v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08315",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Beznosikov_A/0/1/0/all/0/1\">Aleksandr Beznosikov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dvurechensky_P/0/1/0/all/0/1\">Pavel Dvurechensky</a>, <a href=\"http://arxiv.org/find/math/1/au:+Koloskova_A/0/1/0/all/0/1\">Anastasia Koloskova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Samokhin_V/0/1/0/all/0/1\">Valentin Samokhin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Stich_S/0/1/0/all/0/1\">Sebastian U Stich</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1\">Alexander Gasnikov</a>",
          "description": "We consider decentralized stochastic variational inequalities where the\nproblem data is distributed across many participating devices (heterogeneous,\nor non-IID data setting). We propose a novel method - based on stochastic\nextra-gradient - where participating devices can communicate over arbitrary,\npossibly time-varying network topologies. This covers both the fully\ndecentralized optimization setting and the centralized topologies commonly used\nin Federated Learning. Our method further supports multiple local updates on\nthe workers for reducing the communication frequency between workers. We\ntheoretically analyze the proposed scheme in the strongly monotone, monotone\nand non-monotone setting. As a special case, our method and analysis apply in\nparticular to decentralized stochastic min-max problems which are being studied\nwith increased interest in Deep Learning. For example, the training objective\nof Generative Adversarial Networks (GANs) are typically saddle point problems\nand the decentralized training of GANs has been reported to be extremely\nchallenging. While SOTA techniques rely on either repeated gossip rounds or\nproximal updates, we alleviate both of these requirements. Experimental results\nfor decentralized GAN demonstrate the effectiveness of our proposed algorithm.",
          "link": "http://arxiv.org/abs/2106.08315",
          "publishedOn": "2021-06-16T01:21:08.337Z",
          "wordCount": 615,
          "title": "Decentralized Local Stochastic Extra-Gradient for Variational Inequalities. (arXiv:2106.08315v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1909.00453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wintner_S/0/1/0/all/0/1\">Shuly Wintner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "Despite impressive performance on many text classification tasks, deep neural\nnetworks tend to learn frequent superficial patterns that are specific to the\ntraining data and do not always generalize well. In this work, we observe this\nlimitation with respect to the task of native language identification. We find\nthat standard text classifiers which perform well on the test set end up\nlearning topical features which are confounds of the prediction task (e.g., if\nthe input text mentions Sweden, the classifier predicts that the author's\nnative language is Swedish). We propose a method that represents the latent\ntopical confounds and a model which \"unlearns\" confounding features by\npredicting both the label of the input text and the confound; but we train the\ntwo predictors adversarially in an alternating fashion to learn a text\nrepresentation that predicts the correct label but is less prone to using\ninformation about the confound. We show that this model generalizes better and\nlearns features that are indicative of the writing style rather than the\ncontent.",
          "link": "http://arxiv.org/abs/1909.00453",
          "publishedOn": "2021-06-16T01:21:08.330Z",
          "wordCount": 648,
          "title": "Topics to Avoid: Demoting Latent Confounds in Text Classification. (arXiv:1909.00453v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shui_C/0/1/0/all/0/1\">Changjian Shui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zijian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1\">Christian Gagn&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Charles Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boyu Wang</a>",
          "description": "Multi-source domain adaptation aims at leveraging the knowledge from multiple\ntasks for predicting a related target domain. Hence, a crucial aspect is to\nproperly combine different sources based on their relations. In this paper, we\nanalyzed the problem for aggregating source domains with different label\ndistributions, where most recent source selection approaches fail. Our proposed\nalgorithm differs from previous approaches in two key ways: the model\naggregates multiple sources mainly through the similarity of semantic\nconditional distribution rather than marginal distribution; the model proposes\na \\emph{unified} framework to select relevant sources for three popular\nscenarios, i.e., domain adaptation with limited label on target domain,\nunsupervised domain adaptation and label partial unsupervised domain adaption.\nWe evaluate the proposed method through extensive experiments. The empirical\nresults significantly outperform the baselines.",
          "link": "http://arxiv.org/abs/2105.04051",
          "publishedOn": "2021-06-16T01:21:08.323Z",
          "wordCount": 587,
          "title": "Aggregating From Multiple Target-Shifted Sources. (arXiv:2105.04051v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1\">Aditya Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1\">Suvrit Sra</a>",
          "description": "The label shift problem refers to the supervised learning setting where the\ntrain and test label distributions do not match. Existing work addressing label\nshift usually assumes access to an \\emph{unlabelled} test sample. This sample\nmay be used to estimate the test label distribution, and to then train a\nsuitably re-weighted classifier. While approaches using this idea have proven\neffective, their scope is limited as it is not always feasible to access the\ntarget domain; further, they require repeated retraining if the model is to be\ndeployed in \\emph{multiple} test environments. Can one instead learn a\n\\emph{single} classifier that is robust to arbitrary label shifts from a broad\nfamily? In this paper, we answer this question by proposing a model that\nminimises an objective based on distributionally robust optimisation (DRO). We\nthen design and analyse a gradient descent-proximal mirror ascent algorithm\ntailored for large-scale problems to optimise the proposed objective. %, and\nestablish its convergence. Finally, through experiments on CIFAR-100 and\nImageNet, we show that our technique can significantly improve performance over\na number of baselines in settings where label shift is present.",
          "link": "http://arxiv.org/abs/2010.12230",
          "publishedOn": "2021-06-16T01:21:08.316Z",
          "wordCount": 656,
          "title": "Coping with Label Shift via Distributionally Robust Optimisation. (arXiv:2010.12230v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shivin Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Siddharth Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lingxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_L/0/1/0/all/0/1\">Lim Jun Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_V/0/1/0/all/0/1\">Vaibhav Rajan</a>",
          "description": "In data containing heterogeneous subpopulations, classification performance\nbenefits from incorporating the knowledge of cluster structure in the\nclassifier. Previous methods for such combined clustering and classification\nare either 1) classifier-specific and not generic, or 2) independently perform\nclustering and classifier training, which may not form clusters that can\npotentially benefit classifier performance. The question of how to perform\nclustering to improve the performance of classifiers trained on the clusters\nhas received scant attention in previous literature, despite its importance in\nseveral real-world applications. In this paper, we design a simple and\nefficient classification algorithm called Clustering Aware Classification\n(CAC), to find clusters that are well suited for being used as training\ndatasets by classifiers for each underlying subpopulation. Our experiments on\nsynthetic and real benchmark datasets demonstrate the efficacy of CAC over\nprevious methods for combined clustering and classification.",
          "link": "http://arxiv.org/abs/2102.11872",
          "publishedOn": "2021-06-16T01:21:08.297Z",
          "wordCount": 600,
          "title": "Dont Just Divide; Polarize and Conquer!. (arXiv:2102.11872v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1\">Ilias Diakonikolas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Impagliazzo_R/0/1/0/all/0/1\">Russell Impagliazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1\">Daniel Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_R/0/1/0/all/0/1\">Rex Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorrell_J/0/1/0/all/0/1\">Jessica Sorrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1\">Christos Tzamos</a>",
          "description": "We study the problem of boosting the accuracy of a weak learner in the\n(distribution-independent) PAC model with Massart noise. In the Massart noise\nmodel, the label of each example $x$ is independently misclassified with\nprobability $\\eta(x) \\leq \\eta$, where $\\eta<1/2$. The Massart model lies\nbetween the random classification noise model and the agnostic model. Our main\npositive result is the first computationally efficient boosting algorithm in\nthe presence of Massart noise that achieves misclassification error arbitrarily\nclose to $\\eta$. Prior to our work, no non-trivial booster was known in this\nsetting. Moreover, we show that this error upper bound is best possible for\npolynomial-time black-box boosters, under standard cryptographic assumptions.\nOur upper and lower bounds characterize the complexity of boosting in the\ndistribution-independent PAC model with Massart noise. As a simple application\nof our positive result, we give the first efficient Massart learner for unions\nof high-dimensional rectangles.",
          "link": "http://arxiv.org/abs/2106.07779",
          "publishedOn": "2021-06-16T01:21:08.291Z",
          "wordCount": 581,
          "title": "Boosting in the Presence of Massart Noise. (arXiv:2106.07779v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02409",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1\">Niladri S. Chatterji</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Long_P/0/1/0/all/0/1\">Philip M. Long</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter L. Bartlett</a>",
          "description": "We study the training of finite-width two-layer smoothed ReLU networks for\nbinary classification using the logistic loss. We show that gradient descent\ndrives the training loss to zero if the initial loss is small enough. When the\ndata satisfies certain cluster and separation conditions and the network is\nwide enough, we show that one step of gradient descent reduces the loss\nsufficiently that the first result applies.",
          "link": "http://arxiv.org/abs/2012.02409",
          "publishedOn": "2021-06-16T01:21:08.285Z",
          "wordCount": 539,
          "title": "When does gradient descent with logistic loss find interpolating two-layer networks?. (arXiv:2012.02409v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03639",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lang_H/0/1/0/all/0/1\">Hunter Lang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vijayaraghavan_A/0/1/0/all/0/1\">Aravindan Vijayaraghavan</a>",
          "description": "We prove that the $\\alpha$-expansion algorithm for MAP inference always\nreturns a globally optimal assignment for Markov Random Fields with Potts\npairwise potentials, with a catch: the returned assignment is only guaranteed\nto be optimal for an instance within a small perturbation of the original\nproblem instance. In other words, all local minima with respect to expansion\nmoves are global minima to slightly perturbed versions of the problem. On\n\"real-world\" instances, MAP assignments of small perturbations of the problem\nshould be very similar to the MAP assignment(s) of the original problem\ninstance. We design an algorithm that can certify whether this is the case in\npractice. On several MAP inference problem instances from computer vision, this\nalgorithm certifies that MAP solutions to all of these perturbations are very\nclose to solutions of the original instance. These results taken together give\na cohesive explanation for the good performance of \"graph cuts\" algorithms in\npractice. Every local expansion minimum is a global minimum in a small\nperturbation of the problem, and all of these global minima are close to the\noriginal solution.",
          "link": "http://arxiv.org/abs/2011.03639",
          "publishedOn": "2021-06-16T01:21:08.278Z",
          "wordCount": 652,
          "title": "Graph cuts always find a global optimum for Potts models (with a catch). (arXiv:2011.03639v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.12019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Huanhou Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jinglun Shi</a>",
          "description": "Automatically describing video content with text description is challenging\nbut important task, which has been attracting a lot of attention in computer\nvision community. Previous works mainly strive for the accuracy of the\ngenerated sentences, while ignoring the sentences diversity, which is\ninconsistent with human behavior. In this paper, we aim to caption each video\nwith multiple descriptions and propose a novel framework. Concretely, for a\ngiven video, the intermediate latent variables of conventional encode-decode\nprocess are utilized as input to the conditional generative adversarial network\n(CGAN) with the purpose of generating diverse sentences. We adopt different\nConvolutional Neural Networks (CNNs) as our generator that produces\ndescriptions conditioned on latent variables and discriminator that assesses\nthe quality of generated sentences. Simultaneously, a novel DCE metric is\ndesigned to assess the diverse captions. We evaluate our method on the\nbenchmark datasets, where it demonstrates its ability to generate diverse\ndescriptions and achieves superior results against other state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/1910.12019",
          "publishedOn": "2021-06-16T01:21:08.269Z",
          "wordCount": 645,
          "title": "Diverse Video Captioning Through Latent Variable Expansion. (arXiv:1910.12019v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07908",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Truong-Vinh Hoang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Krumscheid_S/0/1/0/all/0/1\">Sebastian Krumscheid</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Matthies_H/0/1/0/all/0/1\">Hermann G. Matthies</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Tempone_R/0/1/0/all/0/1\">Ra&#xfa;l Tempone</a> (1 and 3) ((1) Chair of Mathematics for Uncertainty Quantification, RWTH Aachen University, (2) Technische Universit&#xe4;t Braunschweig (3) Computer, Electrical and Mathematical Sciences and Engineering, KAUST, and Alexander von Humboldt professor in Mathematics of Uncertainty Quantification, RWTH Aachen University)",
          "description": "Filtering is a data assimilation technique that performs the sequential\ninference of dynamical systems states from noisy observations. Herein, we\npropose a machine learning-based ensemble conditional mean filter (ML-EnCMF)\nfor tracking possibly high-dimensional non-Gaussian state models with nonlinear\ndynamics based on sparse observations. The proposed filtering method is\ndeveloped based on the conditional expectation and numerically implemented\nusing machine learning (ML) techniques combined with the ensemble method. The\ncontribution of this work is twofold. First, we demonstrate that the ensembles\nassimilated using the ensemble conditional mean filter (EnCMF) provide an\nunbiased estimator of the Bayesian posterior mean, and their variance matches\nthe expected conditional variance. Second, we implement the EnCMF using\nartificial neural networks, which have a significant advantage in representing\nnonlinear functions over high-dimensional domains such as the conditional mean.\nFinally, we demonstrate the effectiveness of the ML-EnCMF for tracking the\nstates of Lorenz-63 and Lorenz-96 systems under the chaotic regime. Numerical\nresults show that the ML-EnCMF outperforms the ensemble Kalman filter.",
          "link": "http://arxiv.org/abs/2106.07908",
          "publishedOn": "2021-06-16T01:21:08.252Z",
          "wordCount": 660,
          "title": "Machine learning-based conditional mean filter: a generalization of the ensemble Kalman filter for nonlinear data assimilation. (arXiv:2106.07908v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1\">Amir Zandieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_I/0/1/0/all/0/1\">Insu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avron_H/0/1/0/all/0/1\">Haim Avron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoham_N/0/1/0/all/0/1\">Neta Shoham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chaewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide\nneural networks trained under least squares loss by gradient descent. Recent\nworks also report that NTK regression can outperform finitely-wide neural\nnetworks trained on small-scale datasets. However, the computational complexity\nof kernel methods has limited its use in large-scale learning tasks. To\naccelerate learning with NTK, we design a near input-sparsity time\napproximation algorithm for NTK, by sketching the polynomial expansions of\narc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK)\ncan transform any image using a linear runtime in the number of pixels.\nFurthermore, we prove a spectral approximation guarantee for the NTK matrix, by\ncombining random features (based on leverage score sampling) of the arc-cosine\nkernels with a sketching algorithm. We benchmark our methods on various\nlarge-scale regression and classification tasks and show that a linear\nregressor trained on our CNTK features matches the accuracy of exact CNTK on\nCIFAR-10 dataset while achieving 150x speedup.",
          "link": "http://arxiv.org/abs/2106.07880",
          "publishedOn": "2021-06-16T01:21:08.245Z",
          "wordCount": 609,
          "title": "Scaling Neural Tangent Kernels via Sketching and Random Features. (arXiv:2106.07880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peito_J/0/1/0/all/0/1\">Joel Peito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qiwei Han</a>",
          "description": "In contrast to many other domains, recommender systems in health services may\nbenefit particularly from the incorporation of health domain knowledge, as it\nhelps to provide meaningful and personalised recommendations catering to the\nindividual's health needs. With recent advances in representation learning\nenabling the hierarchical embedding of health knowledge into the hyperbolic\nPoincare space, this work proposes a content-based recommender system for\npatient-doctor matchmaking in primary care based on patients' health profiles,\nenriched by pre-trained Poincare embeddings of the ICD-9 codes through transfer\nlearning. The proposed model outperforms its conventional counterpart in terms\nof recommendation accuracy and has several important business implications for\nimproving the patient-doctor relationship.",
          "link": "http://arxiv.org/abs/2106.07720",
          "publishedOn": "2021-06-16T01:21:08.228Z",
          "wordCount": 552,
          "title": "Incorporating Domain Knowledge into Health Recommender Systems using Hyperbolic Embeddings. (arXiv:2106.07720v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saglietti_L/0/1/0/all/0/1\">Luca Saglietti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannelli_S/0/1/0/all/0/1\">Stefano Sarao Mannelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1\">Andrew Saxe</a>",
          "description": "In humans and animals, curriculum learning -- presenting data in a curated\norder - is critical to rapid learning and effective pedagogy. Yet in machine\nlearning, curricula are not widely used and empirically often yield only\nmoderate benefits. This stark difference in the importance of curriculum raises\na fundamental theoretical question: when and why does curriculum learning help?\n\nIn this work, we analyse a prototypical neural network model of curriculum\nlearning in the high-dimensional limit, employing statistical physics methods.\nCurricula could in principle change both the learning speed and asymptotic\nperformance of a model. To study the former, we provide an exact description of\nthe online learning setting, confirming the long-standing experimental\nobservation that curricula can modestly speed up learning. To study the latter,\nwe derive performance in a batch learning setting, in which a network trains to\nconvergence in successive phases of learning on dataset slices of varying\ndifficulty. With standard training losses, curriculum does not provide\ngeneralisation benefit, in line with empirical observations. However, we show\nthat by connecting different learning phases through simple Gaussian priors,\ncurriculum can yield a large improvement in test performance. Taken together,\nour reduced analytical descriptions help reconcile apparently conflicting\nempirical results and trace regimes where curriculum learning yields the\nlargest gains. More broadly, our results suggest that fully exploiting a\ncurriculum may require explicit changes to the loss function at curriculum\nboundaries.",
          "link": "http://arxiv.org/abs/2106.08068",
          "publishedOn": "2021-06-16T01:21:08.207Z",
          "wordCount": 674,
          "title": "An Analytical Theory of Curriculum Learning in Teacher-Student Networks. (arXiv:2106.08068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07736",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jin_D/0/1/0/all/0/1\">Dian Jin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bing_X/0/1/0/all/0/1\">Xin Bing</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqian Zhang</a>",
          "description": "The problem of finding the unique low dimensional decomposition of a given\nmatrix has been a fundamental and recurrent problem in many areas. In this\npaper, we study the problem of seeking a unique decomposition of a low rank\nmatrix $Y\\in \\mathbb{R}^{p\\times n}$ that admits a sparse representation.\nSpecifically, we consider $Y = A X\\in \\mathbb{R}^{p\\times n}$ where the matrix\n$A\\in \\mathbb{R}^{p\\times r}$ has full column rank, with $r < \\min\\{n,p\\}$, and\nthe matrix $X\\in \\mathbb{R}^{r\\times n}$ is element-wise sparse. We prove that\nthis sparse decomposition of $Y$ can be uniquely identified, up to some\nintrinsic signed permutation. Our approach relies on solving a nonconvex\noptimization problem constrained over the unit sphere. Our geometric analysis\nfor the nonconvex optimization landscape shows that any {\\em strict} local\nsolution is close to the ground truth solution, and can be recovered by a\nsimple data-driven initialization followed with any second order descent\nalgorithm. At last, we corroborate these theoretical results with numerical\nexperiments.",
          "link": "http://arxiv.org/abs/2106.07736",
          "publishedOn": "2021-06-16T01:21:08.196Z",
          "wordCount": 597,
          "title": "Unique sparse decomposition of low rank matrices. (arXiv:2106.07736v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>",
          "description": "Reweighting adversarial data during training has been recently shown to\nimprove adversarial robustness, where data closer to the current decision\nboundaries are regarded as more critical and given larger weights. However,\nexisting methods measuring the closeness are not very reliable: they are\ndiscrete and can take only a few values, and they are path-dependent, i.e.,\nthey may change given the same start and end points with different attack\npaths. In this paper, we propose three types of probabilistic margin (PM),\nwhich are continuous and path-independent, for measuring the aforementioned\ncloseness and reweighting adversarial data. Specifically, a PM is defined as\nthe difference between two estimated class-posterior probabilities, e.g., such\nthe probability of the true label minus the probability of the most confusing\nlabel given some natural data. Though different PMs capture different geometric\nproperties, all three PMs share a negative correlation with the vulnerability\nof data: data with larger/smaller PMs are safer/riskier and should have\nsmaller/larger weights. Experiments demonstrate that PMs are reliable\nmeasurements and PM-based reweighting methods outperform state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2106.07904",
          "publishedOn": "2021-06-16T01:21:08.190Z",
          "wordCount": 608,
          "title": "Probabilistic Margins for Instance Reweighting in Adversarial Training. (arXiv:2106.07904v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Sahil Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1\">Aditya Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John P. Dickerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Su-In Lee</a>",
          "description": "As machine learning (ML) systems take a more prominent and central role in\ncontributing to life-impacting decisions, ensuring their trustworthiness and\naccountability is of utmost importance. Explanations sit at the core of these\ndesirable attributes of a ML system. The emerging field is frequently called\n``Explainable AI (XAI)'' or ``Explainable ML.'' The goal of explainable ML is\nto intuitively explain the predictions of a ML system, while adhering to the\nneeds to various stakeholders. Many explanation techniques were developed with\ncontributions from both academia and industry. However, there are several\nexisting challenges that have not garnered enough interest and serve as\nroadblocks to widespread adoption of explainable ML. In this short paper, we\nenumerate challenges in explainable ML from an industry perspective. We hope\nthese challenges will serve as promising future research directions, and would\ncontribute to democratizing explainable ML.",
          "link": "http://arxiv.org/abs/2106.07758",
          "publishedOn": "2021-06-16T01:21:08.116Z",
          "wordCount": 579,
          "title": "Pitfalls of Explainable ML: An Industry Perspective. (arXiv:2106.07758v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07677",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Herlihy_C/0/1/0/all/0/1\">Christine Herlihy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prins_A/0/1/0/all/0/1\">Aviva Prins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Aravind Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John Dickerson</a>",
          "description": "Restless and collapsing bandits are commonly used to model constrained\nresource allocation in settings featuring arms with action-dependent transition\nprobabilities, such as allocating health interventions among patients [Whittle,\n1988; Mate et al., 2020]. However, state-of-the-art Whittle-index-based\napproaches to this planning problem either do not consider fairness among arms,\nor incentivize fairness without guaranteeing it [Mate et al., 2021].\nAdditionally, their optimality guarantees only apply when arms are indexable\nand threshold-optimal. We demonstrate that the incorporation of hard fairness\nconstraints necessitates the coupling of arms, which undermines the\ntractability, and by extension, indexability of the problem. We then introduce\nProbFair, a probabilistically fair stationary policy that maximizes total\nexpected reward and satisfies the budget constraint, while ensuring a strictly\npositive lower bound on the probability of being pulled at each timestep. We\nevaluate our algorithm on a real-world application, where interventions support\ncontinuous positive airway pressure (CPAP) therapy adherence among obstructive\nsleep apnea (OSA) patients, as well as simulations on a broader class of\nsynthetic transition matrices.",
          "link": "http://arxiv.org/abs/2106.07677",
          "publishedOn": "2021-06-16T01:21:08.108Z",
          "wordCount": 610,
          "title": "Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting. (arXiv:2106.07677v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2002.03206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Ziheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1\">Kunal Talwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>",
          "description": "Humans are accustomed to environments that contain both regularities and\nexceptions. For example, at most gas stations, one pays prior to pumping, but\nthe occasional rural station does not accept payment in advance. Likewise, deep\nneural networks can generalize across instances that share common patterns or\nstructures, yet have the capacity to memorize rare or irregular forms. We\nanalyze how individual instances are treated by a model via a consistency\nscore. The score characterizes the expected accuracy for a held-out instance\ngiven training sets of varying size sampled from the data distribution. We\nobtain empirical estimates of this score for individual instances in multiple\ndata sets, and we show that the score identifies out-of-distribution and\nmislabeled examples at one end of the continuum and strongly regular examples\nat the other end. We identify computationally inexpensive proxies to the\nconsistency score using statistics collected during training. We show examples\nof potential applications to the analysis of deep-learning systems.",
          "link": "http://arxiv.org/abs/2002.03206",
          "publishedOn": "2021-06-16T01:21:08.101Z",
          "wordCount": 635,
          "title": "Characterizing Structural Regularities of Labeled Data in Overparameterized Models. (arXiv:2002.03206v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07761",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kramer_N/0/1/0/all/0/1\">Nicholas Kr&#xe4;mer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hennig_P/0/1/0/all/0/1\">Philipp Hennig</a>",
          "description": "We propose a fast algorithm for the probabilistic solution of boundary value\nproblems (BVPs), which are ordinary differential equations subject to boundary\nconditions. In contrast to previous work, we introduce a Gauss--Markov prior\nand tailor it specifically to BVPs, which allows computing a posterior\ndistribution over the solution in linear time, at a quality and cost comparable\nto that of well-established, non-probabilistic methods. Our model further\ndelivers uncertainty quantification, mesh refinement, and hyperparameter\nadaptation. We demonstrate how these practical considerations positively impact\nthe efficiency of the scheme. Altogether, this results in a practically usable\nprobabilistic BVP solver that is (in contrast to non-probabilistic algorithms)\nnatively compatible with other parts of the statistical modelling tool-chain.",
          "link": "http://arxiv.org/abs/2106.07761",
          "publishedOn": "2021-06-16T01:21:08.028Z",
          "wordCount": 541,
          "title": "Linear-Time Probabilistic Solutions of Boundary Value Problems. (arXiv:2106.07761v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fazel_A/0/1/0/all/0/1\">Amin Fazel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yulan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barra_Chicote_R/0/1/0/all/0/1\">Roberto Barra-Chicote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yixiong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maas_R/0/1/0/all/0/1\">Roland Maas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>",
          "description": "End-to-end (E2E) automatic speech recognition (ASR) models have recently\ndemonstrated superior performance over the traditional hybrid ASR models.\nTraining an E2E ASR model requires a large amount of data which is not only\nexpensive but may also raise dependency on production data. At the same time,\nsynthetic speech generated by the state-of-the-art text-to-speech (TTS) engines\nhas advanced to near-human naturalness. In this work, we propose to utilize\nsynthetic speech for ASR training (SynthASR) in applications where data is\nsparse or hard to get for ASR model training. In addition, we apply continual\nlearning with a novel multi-stage training strategy to address catastrophic\nforgetting, achieved by a mix of weighted multi-style training, data\naugmentation, encoder freezing, and parameter regularization. In our\nexperiments conducted on in-house datasets for a new application of recognizing\nmedication names, training ASR RNN-T models with synthetic audio via the\nproposed multi-stage training improved the recognition performance on new\napplication by more than 65% relative, without degradation on existing general\napplications. Our observations show that SynthASR holds great promise in\ntraining the state-of-the-art large-scale E2E ASR models for new applications\nwhile reducing the costs and dependency on production data.",
          "link": "http://arxiv.org/abs/2106.07803",
          "publishedOn": "2021-06-16T01:21:08.001Z",
          "wordCount": 634,
          "title": "SynthASR: Unlocking Synthetic Data for Speech Recognition. (arXiv:2106.07803v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Sahil Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John Dickerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hines_K/0/1/0/all/0/1\">Keegan Hines</a>",
          "description": "Counterfactual explanations (CFEs) are an emerging technique under the\numbrella of interpretability of machine learning (ML) models. They provide\n``what if'' feedback of the form ``if an input datapoint were $x'$ instead of\n$x$, then an ML model's output would be $y'$ instead of $y$.'' Counterfactual\nexplainability for ML models has yet to see widespread adoption in industry. In\nthis short paper, we posit reasons for this slow uptake. Leveraging recent work\noutlining desirable properties of CFEs and our experience running the ML wing\nof a model monitoring startup, we identify outstanding obstacles hindering CFE\ndeployment in industry.",
          "link": "http://arxiv.org/abs/2106.07756",
          "publishedOn": "2021-06-16T01:21:07.904Z",
          "wordCount": 529,
          "title": "Counterfactual Explanations for Machine Learning: Challenges Revisited. (arXiv:2106.07756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brima_Y/0/1/0/all/0/1\">Yusuf Brima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tushar_M/0/1/0/all/0/1\">Mossadek Hossain Kamal Tushar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_U/0/1/0/all/0/1\">Upama Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tariqul Islam</a>",
          "description": "Magnetic Resonance Imaging (MRI) is a principal diagnostic approach used in\nthe field of radiology to create images of the anatomical and physiological\nstructure of patients. MRI is the prevalent medical imaging practice to find\nabnormalities in soft tissues. Traditionally they are analyzed by a radiologist\nto detect abnormalities in soft tissues, especially the brain. The process of\ninterpreting a massive volume of patient's MRI is laborious. Hence, the use of\nMachine Learning methodologies can aid in detecting abnormalities in soft\ntissues with considerable accuracy. In this research, we have curated a novel\ndataset and developed a framework that uses Deep Transfer Learning to perform a\nmulti-classification of tumors in the brain MRI images. In this paper, we\nadopted the Deep Residual Convolutional Neural Network (ResNet50) architecture\nfor the experiments along with discriminative learning techniques to train the\nmodel. Using the novel dataset and two publicly available MRI brain datasets,\nthis proposed approach attained a classification accuracy of 86.40% on the\ncurated dataset, 93.80% on the Harvard Whole Brain Atlas dataset, and 97.05%\naccuracy on the School of Biomedical Engineering dataset. Results of our\nexperiments significantly demonstrate our proposed framework for transfer\nlearning is a potential and effective method for brain tumor\nmulti-classification tasks.",
          "link": "http://arxiv.org/abs/2106.07333",
          "publishedOn": "2021-06-16T01:21:07.891Z",
          "wordCount": 701,
          "title": "Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class Classification. (arXiv:2106.07333v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1\">Guoming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qianyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jiadong Lou</a>",
          "description": "Non-intrusive load monitoring (NILM) helps disaggregate the household's main\nelectricity consumption to energy usages of individual appliances, thus greatly\ncutting down the cost in fine-grained household load monitoring. To address the\narisen privacy concern in NILM applications, federated learning (FL) could be\nleveraged for NILM model training and sharing. When applying the FL paradigm in\nreal-world NILM applications, however, we are faced with the challenges of edge\nresource restriction, edge model personalization and edge training data\nscarcity.\n\nIn this paper we present FedNILM, a practical FL paradigm for NILM\napplications at the edge client. Specifically, FedNILM is designed to deliver\nprivacy-preserving and personalized NILM services to large-scale edge clients,\nby leveraging i) secure data aggregation through federated learning, ii)\nefficient cloud model compression via filter pruning and multi-task learning,\nand iii) personalized edge model building with unsupervised transfer learning.\nOur experiments on real-world energy data show that, FedNILM is able to achieve\npersonalized energy disaggregation with the state-of-the-art accuracy, while\nensuring privacy preserving at the edge client.",
          "link": "http://arxiv.org/abs/2106.07751",
          "publishedOn": "2021-06-16T01:21:07.847Z",
          "wordCount": 605,
          "title": "FedNILM: Applying Federated Learning to NILM Applications at the Edge. (arXiv:2106.07751v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>",
          "description": "Adversarial examples contain carefully crafted perturbations that can fool\ndeep neural networks (DNNs) into making wrong predictions. Enhancing the\nadversarial robustness of DNNs has gained considerable interest in recent\nyears. Although image transformation-based defenses were widely considered at\nan earlier time, most of them have been defeated by adaptive attacks. In this\npaper, we propose a new image transformation defense based on error diffusion\nhalftoning, and combine it with adversarial training to defend against\nadversarial examples. Error diffusion halftoning projects an image into a 1-bit\nspace and diffuses quantization error to neighboring pixels. This process can\nremove adversarial perturbations from a given image while maintaining\nacceptable image quality in the meantime in favor of recognition. Experimental\nresults demonstrate that the proposed method is able to improve adversarial\nrobustness even under advanced adaptive attacks, while most of the other image\ntransformation-based defenses do not. We show that a proper image\ntransformation can still be an effective defense approach. Code:\nhttps://github.com/shaoyuanlo/Halftoning-Defense",
          "link": "http://arxiv.org/abs/2101.09451",
          "publishedOn": "2021-06-16T01:21:07.729Z",
          "wordCount": 639,
          "title": "Error Diffusion Halftoning Against Adversarial Examples. (arXiv:2101.09451v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1\">Ryan A. Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anup Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1\">Tung Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1\">Nesreen K. Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>",
          "description": "Graph Neural Networks (GNNs) have proven to be useful for many different\npractical applications. However, many existing GNN models have implicitly\nassumed homophily among the nodes connected in the graph, and therefore have\nlargely overlooked the important setting of heterophily, where most connected\nnodes are from different classes. In this work, we propose a novel framework\ncalled CPGNN that generalizes GNNs for graphs with either homophily or\nheterophily. The proposed framework incorporates an interpretable compatibility\nmatrix for modeling the heterophily or homophily level in the graph, which can\nbe learned in an end-to-end fashion, enabling it to go beyond the assumption of\nstrong homophily. Theoretically, we show that replacing the compatibility\nmatrix in our framework with the identity (which represents pure homophily)\nreduces to GCN. Our extensive experiments demonstrate the effectiveness of our\napproach in more realistic and challenging experimental settings with\nsignificantly less training data compared to previous works: CPGNN variants\nachieve state-of-the-art results in heterophily settings with or without\ncontextual node features, while maintaining comparable performance in homophily\nsettings.",
          "link": "http://arxiv.org/abs/2009.13566",
          "publishedOn": "2021-06-16T01:21:07.710Z",
          "wordCount": 682,
          "title": "Graph Neural Networks with Heterophily. (arXiv:2009.13566v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kidger_P/0/1/0/all/0/1\">Patrick Kidger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">James Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1\">Terry Lyons</a>",
          "description": "Neural SDEs combine many of the best qualities of both RNNs and SDEs: memory\nefficient training, high-capacity function approximation, and strong priors on\nmodel space. This makes them a natural choice for modelling many types of\ntemporal dynamics. Training a Neural SDE (either as a VAE or as a GAN) requires\nbackpropagating through an SDE solve. This may be done by solving a\nbackwards-in-time SDE whose solution is the desired parameter gradients.\nHowever, this has previously suffered from severe speed and accuracy issues,\ndue to high computational cost and numerical truncation errors. Here, we\novercome these issues through several technical innovations. First, we\nintroduce the \\textit{reversible Heun method}. This is a new SDE solver that is\n\\textit{algebraically reversible}: eliminating numerical gradient errors, and\nthe first such solver of which we are aware. Moreover it requires half as many\nfunction evaluations as comparable solvers, giving up to a $1.98\\times$\nspeedup. Second, we introduce the \\textit{Brownian Interval}: a new, fast,\nmemory efficient, and exact way of sampling \\textit{and reconstructing}\nBrownian motion. With this we obtain up to a $10.6\\times$ speed improvement\nover previous techniques, which in contrast are both approximate and relatively\nslow. Third, when specifically training Neural SDEs as GANs (Kidger et al.\n2021), we demonstrate how SDE-GANs may be trained through careful weight\nclipping and choice of activation function. This reduces computational cost\n(giving up to a $1.87\\times$ speedup) and removes the numerical truncation\nerrors associated with gradient penalty. Altogether, we outperform the\nstate-of-the-art by substantial margins, with respect to training speed, and\nwith respect to classification, prediction, and MMD test metrics. We have\ncontributed implementations of all of our techniques to the torchsde library to\nhelp facilitate their adoption.",
          "link": "http://arxiv.org/abs/2105.13493",
          "publishedOn": "2021-06-16T01:21:07.703Z",
          "wordCount": 748,
          "title": "Efficient and Accurate Gradients for Neural SDEs. (arXiv:2105.13493v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1\">Yulong Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>",
          "description": "Sparse neural networks have been widely applied to reduce the computational\ndemands of training and deploying over-parameterized deep neural networks. For\ninference acceleration, methods that discover a sparse network from a\npre-trained dense network (dense-to-sparse training) work effectively.\nRecently, dynamic sparse training (DST) has been proposed to train sparse\nneural networks without pre-training a dense model (sparse-to-sparse training),\nso that the training process can also be accelerated. However, previous\nsparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs)\nand Convolutional Neural Networks (CNNs), failing to match the performance of\ndense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In\nthis paper, we propose an approach to train intrinsically sparse RNNs with a\nfixed parameter count in one single run, without compromising performance.\nDuring training, we allow RNN layers to have a non-uniform redistribution\nacross cell gates for better regularization. Further, we propose SNT-ASGD, a\nnovel variant of the averaged stochastic gradient optimizer, which\nsignificantly improves the performance of all sparse training methods for RNNs.\nUsing these strategies, we achieve state-of-the-art sparse training results,\nbetter than the dense-to-sparse methods, with various types of RNNs on Penn\nTreeBank and Wikitext-2 datasets. Our codes are available at\nhttps://github.com/Shiweiliuiiiiiii/Selfish-RNN.",
          "link": "http://arxiv.org/abs/2101.09048",
          "publishedOn": "2021-06-16T01:21:07.696Z",
          "wordCount": 689,
          "title": "Selfish Sparse RNN Training. (arXiv:2101.09048v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fujita_K/0/1/0/all/0/1\">Kazuhisa Fujita</a>",
          "description": "Spectral clustering (SC) is one of the most popular clustering methods and\noften outperforms traditional clustering methods. SC uses the eigenvectors of a\nLaplacian matrix calculated from a similarity matrix of a dataset. SC has the\nserious drawbacks that are the significant increases in the time complexity\nderived from the computation of eigenvectors and the memory space complexity to\nstore the similarity matrix. To address the issues, I develop a new approximate\nspectral clustering using the network generated by growing neural gas (GNG),\ncalled ASC with GNG in this study. ASC with GNG uses not only reference vectors\nfor vector quantization but also the topology of the network for extraction of\nthe topological relationship between data points in a dataset. ASC with GNG\nmakes the similarity matrix from both the reference vectors and the topology of\nthe network generated by GNG. Using the network generated from a dataset by\nGNG, ASC with GNG achieves to reduce the computational and space complexities\nand improve clustering quality. In this study, I demonstrate that ASC with GNG\neffectively reduces computational time. Moreover, this study shows that ASC\nwith GNG displays equal to or better clustering performance than SC.",
          "link": "http://arxiv.org/abs/2009.07101",
          "publishedOn": "2021-06-16T01:21:07.690Z",
          "wordCount": 670,
          "title": "Approximate spectral clustering using both reference vectors and topology of the network generated by growing neural gas. (arXiv:2009.07101v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aldaghri_N/0/1/0/all/0/1\">Nasser Aldaghri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavifar_H/0/1/0/all/0/1\">Hessam Mahdavifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>",
          "description": "There are applications that may require removing the trace of a sample from\nthe system, e.g., a user requests their data to be deleted, or corrupted data\nis discovered. Simply removing a sample from storage units does not necessarily\nremove its entire trace since downstream machine learning models may store some\ninformation about the samples used to train them. A sample can be perfectly\nunlearned if we retrain all models that used it from scratch with that sample\nremoved from their training dataset. When multiple such unlearning requests are\nexpected to be served, unlearning by retraining becomes prohibitively\nexpensive. Ensemble learning enables the training data to be split into smaller\ndisjoint shards that are assigned to non-communicating weak learners. Each\nshard is used to produce a weak model. These models are then aggregated to\nproduce the final central model. This setup introduces an inherent trade-off\nbetween performance and unlearning cost, as reducing the shard size reduces the\nunlearning cost but may cause degradation in performance. In this paper, we\npropose a coded learning protocol where we utilize linear encoders to encode\nthe training data into shards prior to the learning phase. We also present the\ncorresponding unlearning protocol and show that it satisfies the perfect\nunlearning criterion. Our experimental results show that the proposed coded\nmachine unlearning provides a better performance versus unlearning cost\ntrade-off compared to the uncoded baseline.",
          "link": "http://arxiv.org/abs/2012.15721",
          "publishedOn": "2021-06-16T01:21:07.683Z",
          "wordCount": 677,
          "title": "Coded Machine Unlearning. (arXiv:2012.15721v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yufeng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>",
          "description": "The recent emergence of contrastive learning approaches facilitates the\nresearch on graph representation learning (GRL), introducing graph contrastive\nlearning (GCL) into the literature. These methods contrast semantically similar\nand dissimilar sample pairs to encode the semantics into node or graph\nembeddings. However, most existing works only performed model-level evaluation,\nand did not explore the combination space of modules for more comprehensive and\nsystematic studies. For effective module-level evaluation, we propose a\nframework that decomposes GCL models into four modules: (1) a sampler to\ngenerate anchor, positive and negative data samples (nodes or graphs); (2) an\nencoder and a readout function to get sample embeddings; (3) a discriminator to\nscore each sample pair (anchor-positive and anchor-negative); and (4) an\nestimator to define the loss function. Based on this framework, we conduct\ncontrolled experiments over a wide range of architectural designs and\nhyperparameter settings on node and graph classification tasks. Specifically,\nwe manage to quantify the impact of a single module, investigate the\ninteraction between modules, and compare the overall performance with current\nmodel architectures. Our key findings include a set of module-level guidelines\nfor GCL, e.g., simple samplers from LINE and DeepWalk are strong and robust; an\nMLP encoder associated with Sum readout could achieve competitive performance\non graph classification. Finally, we release our implementations and results as\nOpenGCL, a modularized toolkit that allows convenient reproduction, standard\nmodel and module evaluation, and easy extension.",
          "link": "http://arxiv.org/abs/2106.08171",
          "publishedOn": "2021-06-16T01:21:07.652Z",
          "wordCount": 663,
          "title": "Evaluating Modules in Graph Contrastive Learning. (arXiv:2106.08171v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>",
          "description": "In this paper, we introduce a new perspective on training deep neural\nnetworks capable of state-of-the-art performance without the need for the\nexpensive over-parameterization by proposing the concept of In-Time\nOver-Parameterization (ITOP) in sparse training. By starting from a random\nsparse network and continuously exploring sparse connectivities during\ntraining, we can perform an Over-Parameterization in the space-time manifold,\nclosing the gap in the expressibility between sparse training and dense\ntraining. We further use ITOP to understand the underlying mechanism of Dynamic\nSparse Training (DST) and indicate that the benefits of DST come from its\nability to consider across time all possible parameters when searching for the\noptimal sparse connectivity. As long as there are sufficient parameters that\nhave been reliably explored during training, DST can outperform the dense\nneural network by a large margin. We present a series of experiments to support\nour conjecture and achieve the state-of-the-art sparse training performance\nwith ResNet-50 on ImageNet. More impressively, our method achieves dominant\nperformance over the overparameterization-based sparse methods at extreme\nsparsity levels. When trained on CIFAR-100, our method can match the\nperformance of the dense model even at an extreme sparsity (98%). Code can be\nfound https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization.",
          "link": "http://arxiv.org/abs/2102.02887",
          "publishedOn": "2021-06-16T01:21:07.644Z",
          "wordCount": 713,
          "title": "Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training. (arXiv:2102.02887v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gondere_M/0/1/0/all/0/1\">Mesay Samuel Gondere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1\">Lars Schmidt-Thieme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Durga Prasad Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholz_R/0/1/0/all/0/1\">Randolf Scholz</a>",
          "description": "Handwritten digit recognition is one of the extensively studied area in\nmachine learning. Apart from the wider research on handwritten digit\nrecognition on MNIST dataset, there are many other research works on various\nscript recognition. However, it is not very common for multi-script digit\nrecognition which encourage the development of robust and multipurpose systems.\nAdditionally working on multi-script digit recognition enables multi-task\nlearning, considering the script classification as a related task for instance.\nIt is evident that multi-task learning improves model performance through\ninductive transfer using the information contained in related tasks. Therefore,\nin this study multi-script handwritten digit recognition using multi-task\nlearning will be investigated. As a specific case of demonstrating the solution\nto the problem, Amharic handwritten character recognition will also be\nexperimented. The handwritten digits of three scripts including Latin, Arabic\nand Kannada are studied to show that multi-task models with reformulation of\nthe individual tasks have shown promising results. In this study a novel way of\nusing the individual tasks predictions was proposed to help classification\nperformance and regularize the different loss for the purpose of the main task.\nThis finding has outperformed the baseline and the conventional multi-task\nlearning models. More importantly, it avoided the need for weighting the\ndifferent losses of the tasks, which is one of the challenges in multi-task\nlearning.",
          "link": "http://arxiv.org/abs/2106.08267",
          "publishedOn": "2021-06-16T01:21:07.638Z",
          "wordCount": 651,
          "title": "Multi-script Handwritten Digit Recognition Using Multi-task Learning. (arXiv:2106.08267v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Pre-trained text-to-text transformers such as BART have achieved impressive\nperformance across a range of NLP tasks. Recent study further shows that they\ncan learn to generalize to novel tasks, by including task descriptions as part\nof the source sequence and training the model with (source, target) examples.\nAt test time, these fine-tuned models can make inferences on new tasks using\nthe new task descriptions as part of the input. However, this approach has\npotential limitations, as the model learns to solve individual (source, target)\nexamples (i.e., at the instance level), instead of learning to solve tasks by\ntaking all examples within a task as a whole (i.e., at the task level). To this\nend, we introduce Hypter, a framework that improves text-to-text transformer's\ngeneralization ability to unseen tasks by training a hypernetwork to generate\ntask-specific, light-weight adapters from task descriptions. Experiments on\nZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves\nupon fine-tuning baselines. Notably, when using BART-Large as the main network,\nHypter brings 11.3% comparative improvement on ZEST dataset.",
          "link": "http://arxiv.org/abs/2101.00420",
          "publishedOn": "2021-06-16T01:21:07.630Z",
          "wordCount": 636,
          "title": "Learning to Generate Task-Specific Adapters from Task Description. (arXiv:2101.00420v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caucheteux_C/0/1/0/all/0/1\">Charlotte Caucheteux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gramfort_A/0/1/0/all/0/1\">Alexandre Gramfort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_J/0/1/0/all/0/1\">Jean-Remi King</a>",
          "description": "The activations of language transformers like GPT-2 have been shown to\nlinearly map onto brain activity during speech comprehension. However, the\nnature of these activations remains largely unknown and presumably conflate\ndistinct linguistic classes. Here, we propose a taxonomy to factorize the\nhigh-dimensional activations of language models into four combinatorial\nclasses: lexical, compositional, syntactic, and semantic representations. We\nthen introduce a statistical method to decompose, through the lens of GPT-2's\nactivations, the brain activity of 345 subjects recorded with functional\nmagnetic resonance imaging (fMRI) during the listening of ~4.6 hours of\nnarrated text. The results highlight two findings. First, compositional\nrepresentations recruit a more widespread cortical network than lexical ones,\nand encompass the bilateral temporal, parietal and prefrontal cortices. Second,\ncontrary to previous claims, syntax and semantics are not associated with\nseparated modules, but, instead, appear to share a common and distributed\nneural substrate. Overall, this study introduces a versatile framework to\nisolate, in the brain activity, the distributed representations of linguistic\nconstructs.",
          "link": "http://arxiv.org/abs/2103.01620",
          "publishedOn": "2021-06-16T01:21:07.603Z",
          "wordCount": 631,
          "title": "Disentangling Syntax and Semantics in the Brain with Deep Networks. (arXiv:2103.01620v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Rui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "While multitask representation learning has become a popular approach in\nreinforcement learning (RL), theoretical understanding of why and when it works\nremains limited. This paper presents analyses for the statistical benefit of\nmultitask representation learning in linear Markov Decision Process (MDP) under\na generative model. In this paper, we consider an agent to learn a\nrepresentation function $\\phi$ out of a function class $\\Phi$ from $T$ source\ntasks with $N$ data per task, and then use the learned $\\hat{\\phi}$ to reduce\nthe required number of sample for a new task. We first discover a\n\\emph{Least-Activated-Feature-Abundance} (LAFA) criterion, denoted as $\\kappa$,\nwith which we prove that a straightforward least-square algorithm learns a\npolicy which is $\\tilde{O}(H^2\\sqrt{\\frac{\\mathcal{C}(\\Phi)^2 \\kappa\nd}{NT}+\\frac{\\kappa d}{n}})$ sub-optimal. Here $H$ is the planning horizon,\n$\\mathcal{C}(\\Phi)$ is $\\Phi$'s complexity measure, $d$ is the dimension of the\nrepresentation (usually $d\\ll \\mathcal{C}(\\Phi)$) and $n$ is the number of\nsamples for the new task. Thus the required $n$ is $O(\\kappa d H^4)$ for the\nsub-optimality to be close to zero, which is much smaller than\n$O(\\mathcal{C}(\\Phi)^2\\kappa d H^4)$ in the setting without multitask\nrepresentation learning, whose sub-optimality gap is\n$\\tilde{O}(H^2\\sqrt{\\frac{\\kappa \\mathcal{C}(\\Phi)^2d}{n}})$. This\ntheoretically explains the power of multitask representation learning in\nreducing sample complexity. Further, we note that to ensure high sample\nefficiency, the LAFA criterion $\\kappa$ should be small. In fact, $\\kappa$\nvaries widely in magnitude depending on the different sampling distribution for\nnew task. This indicates adaptive sampling technique is important to make\n$\\kappa$ solely depend on $d$. Finally, we provide empirical results of a noisy\ngrid-world environment to corroborate our theoretical findings.",
          "link": "http://arxiv.org/abs/2106.08053",
          "publishedOn": "2021-06-16T01:21:07.596Z",
          "wordCount": 693,
          "title": "On the Power of Multitask Representation Learning in Linear MDP. (arXiv:2106.08053v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1\">Matthew Jagielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1\">Ariel Herbert-Voss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erlingsson_U/0/1/0/all/0/1\">Ulfar Erlingsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oprea_A/0/1/0/all/0/1\">Alina Oprea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>",
          "description": "It has become common to publish large (billion parameter) language models\nthat have been trained on private datasets. This paper demonstrates that in\nsuch settings, an adversary can perform a training data extraction attack to\nrecover individual training examples by querying the language model.\n\nWe demonstrate our attack on GPT-2, a language model trained on scrapes of\nthe public Internet, and are able to extract hundreds of verbatim text\nsequences from the model's training data. These extracted examples include\n(public) personally identifiable information (names, phone numbers, and email\naddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible\neven though each of the above sequences are included in just one document in\nthe training data.\n\nWe comprehensively evaluate our extraction attack to understand the factors\nthat contribute to its success. Worryingly, we find that larger models are more\nvulnerable than smaller models. We conclude by drawing lessons and discussing\npossible safeguards for training large language models.",
          "link": "http://arxiv.org/abs/2012.07805",
          "publishedOn": "2021-06-16T01:21:07.588Z",
          "wordCount": 643,
          "title": "Extracting Training Data from Large Language Models. (arXiv:2012.07805v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07682",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_Y/0/1/0/all/0/1\">Yamini Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1\">Preetum Nakkiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1\">Boaz Barak</a>",
          "description": "We revisit and extend model stitching (Lenc & Vedaldi 2015) as a methodology\nto study the internal representations of neural networks. Given two trained and\nfrozen models $A$ and $B$, we consider a \"stitched model'' formed by connecting\nthe bottom-layers of $A$ to the top-layers of $B$, with a simple trainable\nlayer between them. We argue that model stitching is a powerful and perhaps\nunder-appreciated tool, which reveals aspects of representations that measures\nsuch as centered kernel alignment (CKA) cannot. Through extensive experiments,\nwe use model stitching to obtain quantitative verifications for intuitive\nstatements such as \"good networks learn similar representations'', by\ndemonstrating that good networks of the same architecture, but trained in very\ndifferent ways (e.g.: supervised vs. self-supervised learning), can be stitched\nto each other without drop in performance. We also give evidence for the\nintuition that \"more is better'' by showing that representations learnt with\n(1) more data, (2) bigger width, or (3) more training time can be \"plugged in''\nto weaker models to improve performance. Finally, our experiments reveal a new\nstructural property of SGD which we call \"stitching connectivity'', akin to\nmode-connectivity: typical minima reached by SGD can all be stitched to each\nother with minimal change in accuracy.",
          "link": "http://arxiv.org/abs/2106.07682",
          "publishedOn": "2021-06-16T01:21:07.579Z",
          "wordCount": 629,
          "title": "Revisiting Model Stitching to Compare Neural Representations. (arXiv:2106.07682v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.07024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1\">Dipendra Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schapire_R/0/1/0/all/0/1\">Robert Schapire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1\">Miro Dud&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafto_P/0/1/0/all/0/1\">Patrick Shafto</a>",
          "description": "We present a novel interactive learning protocol that enables training\nrequest-fulfilling agents by verbally describing their activities. Unlike\nimitation learning (IL), our protocol allows the teaching agent to provide\nfeedback in a language that is most appropriate for them. Compared with reward\nin reinforcement learning (RL), the description feedback is richer and allows\nfor improved sample complexity. We develop a probabilistic framework and an\nalgorithm that practically implements our protocol. Empirical results in two\nchallenging request-fulfilling problems demonstrate the strengths of our\napproach: compared with RL baselines, it is more sample-efficient; compared\nwith IL baselines, it achieves competitive success rates without requiring the\nteaching agent to be able to demonstrate the desired behavior using the\nlearning agent's actions. Apart from empirical evaluation, we also provide\ntheoretical guarantees for our algorithm under certain assumptions about the\nteacher and the environment.",
          "link": "http://arxiv.org/abs/2102.07024",
          "publishedOn": "2021-06-16T01:21:07.559Z",
          "wordCount": 606,
          "title": "Interactive Learning from Activity Description. (arXiv:2102.07024v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07683",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Eslami_M/0/1/0/all/0/1\">Mohammed Eslami</a>, <a href=\"http://arxiv.org/find/math/1/au:+Eramian_H/0/1/0/all/0/1\">Hamed Eramian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gameiro_M/0/1/0/all/0/1\">Marcio Gameiro</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kalies_W/0/1/0/all/0/1\">William Kalies</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mischaikow_K/0/1/0/all/0/1\">Konstantin Mischaikow</a>",
          "description": "Deep learning models evolve through training to learn the manifold in which\nthe data exists to satisfy an objective. It is well known that evolution leads\nto different final states which produce inconsistent predictions of the same\ntest data points. This calls for techniques to be able to empirically quantify\nthe difference in the trajectories and highlight problematic regions. While\nmuch focus is placed on discovering what models learn, the question of how a\nmodel learns is less studied beyond theoretical landscape characterizations and\nlocal geometric approximations near optimal conditions. Here, we present a\ntoolkit for the Dynamical Organization Of Deep Learning Loss Landscapes, or\nDOODL3. DOODL3 formulates the training of neural networks as a dynamical\nsystem, analyzes the learning process, and presents an interpretable global\nview of trajectories in the loss landscape. Our approach uses the coarseness of\ntopology to capture the granularity of geometry to mitigate against states of\ninstability or elongated training. Overall, our analysis presents an empirical\nframework to extract the global dynamics of a model and to use that information\nto guide the training of neural networks.",
          "link": "http://arxiv.org/abs/2106.07683",
          "publishedOn": "2021-06-16T01:21:07.551Z",
          "wordCount": 621,
          "title": "Extracting Global Dynamics of Loss Landscape in Deep Learning Models. (arXiv:2106.07683v1 [math.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2007.05426",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Caterini_A/0/1/0/all/0/1\">Anthony Caterini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cornish_R/0/1/0/all/0/1\">Rob Cornish</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sejdinovic_D/0/1/0/all/0/1\">Dino Sejdinovic</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>",
          "description": "Continuously-indexed flows (CIFs) have recently achieved improvements over\nbaseline normalizing flows on a variety of density estimation tasks. CIFs do\nnot possess a closed-form marginal density, and so, unlike standard flows,\ncannot be plugged in directly to a variational inference (VI) scheme in order\nto produce a more expressive family of approximate posteriors. However, we show\nhere how CIFs can be used as part of an auxiliary VI scheme to formulate and\ntrain expressive posterior approximations in a natural way. We exploit the\nconditional independence structure of multi-layer CIFs to build the required\nauxiliary inference models, which we show empirically yield low-variance\nestimators of the model evidence. We then demonstrate the advantages of CIFs\nover baseline flows in VI problems when the posterior distribution of interest\npossesses a complicated topology, obtaining improved results in both the\nBayesian inference and surrogate maximum likelihood settings.",
          "link": "http://arxiv.org/abs/2007.05426",
          "publishedOn": "2021-06-16T01:21:07.543Z",
          "wordCount": 591,
          "title": "Variational Inference with Continuously-Indexed Normalizing Flows. (arXiv:2007.05426v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1\">Michael Laskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metz_L/0/1/0/all/0/1\">Luke Metz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabarro_S/0/1/0/all/0/1\">Seth Nabarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saroufim_M/0/1/0/all/0/1\">Mark Saroufim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noune_B/0/1/0/all/0/1\">Badreddine Noune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1\">Carlo Luschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1\">Jascha Sohl-Dickstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>",
          "description": "Deep learning models trained on large data sets have been widely successful\nin both vision and language domains. As state-of-the-art deep learning\narchitectures have continued to grow in parameter count so have the compute\nbudgets and times required to train them, increasing the need for\ncompute-efficient methods that parallelize training. Two common approaches to\nparallelize the training of deep networks have been data and model parallelism.\nWhile useful, data and model parallelism suffer from diminishing returns in\nterms of compute efficiency for large batch sizes. In this paper, we\ninvestigate how to continue scaling compute efficiently beyond the point of\ndiminishing returns for large batches through local parallelism, a framework\nwhich parallelizes training of individual layers in deep networks by replacing\nglobal backpropagation with truncated layer-wise backpropagation. Local\nparallelism enables fully asynchronous layer-wise parallelism with a low memory\nfootprint, and requires little communication overhead compared with model\nparallelism. We show results in both vision and language domains across a\ndiverse set of architectures, and find that local parallelism is particularly\neffective in the high-compute regime.",
          "link": "http://arxiv.org/abs/2012.03837",
          "publishedOn": "2021-06-16T01:21:07.536Z",
          "wordCount": 666,
          "title": "Parallel Training of Deep Networks with Local Updates. (arXiv:2012.03837v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08372",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Karkar_S/0/1/0/all/0/1\">Skander Karkar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ayed_I/0/1/0/all/0/1\">Ibrahim Ayed</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bezenac_E/0/1/0/all/0/1\">Emmanuel de B&#xe9;zenac</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "Neural networks have been achieving high generalization performance on many\ntasks despite being highly over-parameterized. Since classical statistical\nlearning theory struggles to explain this behavior, much effort has recently\nbeen focused on uncovering the mechanisms behind it, in the hope of developing\na more adequate theoretical framework and having a better control over the\ntrained models. In this work, we adopt an alternate perspective, viewing the\nneural network as a dynamical system displacing input particles over time. We\nconduct a series of experiments and, by analyzing the network's behavior\nthrough its displacements, we show the presence of a low kinetic energy\ndisplacement bias in the transport map of the network, and link this bias with\ngeneralization performance. From this observation, we reformulate the learning\nproblem as follows: finding neural networks which solve the task while\ntransporting the data as efficiently as possible. This offers a novel\nformulation of the learning problem which allows us to provide regularity\nresults for the solution network, based on Optimal Transport theory. From a\npractical viewpoint, this allows us to propose a new learning algorithm, which\nautomatically adapts to the complexity of the given task, and leads to networks\nwith a high generalization ability even in low data regimes.",
          "link": "http://arxiv.org/abs/2009.08372",
          "publishedOn": "2021-06-16T01:21:07.521Z",
          "wordCount": 679,
          "title": "A Principle of Least Action for the Training of Neural Networks. (arXiv:2009.08372v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1\">Yaron Lipman</a>",
          "description": "Representing surfaces as zero level sets of neural networks recently emerged\nas a powerful modeling paradigm, named Implicit Neural Representations (INRs),\nserving numerous downstream applications in geometric deep learning and 3D\nvision. Training INRs previously required choosing between occupancy and\ndistance function representation and different losses with unknown limit\nbehavior and/or bias. In this paper we draw inspiration from the theory of\nphase transitions of fluids and suggest a loss for training INRs that learns a\ndensity function that converges to a proper occupancy function, while its log\ntransform converges to a distance function. Furthermore, we analyze the limit\nminimizer of this loss showing it satisfies the reconstruction constraints and\nhas minimal surface perimeter, a desirable inductive bias for surface\nreconstruction. Training INRs with this new loss leads to state-of-the-art\nreconstructions on a standard benchmark.",
          "link": "http://arxiv.org/abs/2106.07689",
          "publishedOn": "2021-06-16T01:21:07.502Z",
          "wordCount": 557,
          "title": "Phase Transitions, Distance Functions, and Implicit Neural Representations. (arXiv:2106.07689v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.07428",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Xin_R/0/1/0/all/0/1\">Ran Xin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Khan_U/0/1/0/all/0/1\">Usman A. Khan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kar_S/0/1/0/all/0/1\">Soummya Kar</a>",
          "description": "This paper considers decentralized minimization of $N:=nm$ smooth non-convex\ncost functions equally divided over a directed network of $n$ nodes.\nSpecifically, we describe a stochastic first-order gradient method, called\nGT-SARAH, that employs a SARAH-type variance reduction technique and gradient\ntracking (GT) to address the stochastic and decentralized nature of the\nproblem. We show that GT-SARAH, with appropriate algorithmic parameters, finds\nan $\\epsilon$-accurate first-order stationary point with\n$O\\big(\\max\\big\\{N^{\\frac{1}{2}},n(1-\\lambda)^{-2},n^{\\frac{2}{3}}m^{\\frac{1}{3}}(1-\\lambda)^{-1}\\big\\}L\\epsilon^{-2}\\big)$\ngradient complexity, where ${(1-\\lambda)\\in(0,1]}$ is the spectral gap of the\nnetwork weight matrix and $L$ is the smoothness parameter of the cost\nfunctions. This gradient complexity outperforms that of the existing\ndecentralized stochastic gradient methods. In particular, in a big-data regime\nsuch that ${n = O(N^{\\frac{1}{2}}(1-\\lambda)^{3})}$, this gradient complexity\nfurthers reduces to ${O(N^{\\frac{1}{2}}L\\epsilon^{-2})}$, independent of the\nnetwork topology, and matches that of the centralized near-optimal\nvariance-reduced methods. Moreover, in this regime GT-SARAH achieves a\nnon-asymptotic linear speedup, in that, the total number of gradient\ncomputations at each node is reduced by a factor of $1/n$ compared to the\ncentralized near-optimal algorithms that perform all gradient computations at a\nsingle node. To the best of our knowledge, GT-SARAH is the first algorithm that\nachieves this property. In addition, we show that appropriate choices of local\nminibatch size balance the trade-offs between the gradient and communication\ncomplexity of GT-SARAH. Over infinite time horizon, we establish that all nodes\nin GT-SARAH asymptotically achieve consensus and converge to a first-order\nstationary point in the almost sure and mean-squared sense.",
          "link": "http://arxiv.org/abs/2008.07428",
          "publishedOn": "2021-06-16T01:21:07.488Z",
          "wordCount": 733,
          "title": "Fast decentralized non-convex finite-sum optimization with recursive variance reduction. (arXiv:2008.07428v5 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Butte_S/0/1/0/all/0/1\">Sujata Butte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakanski_A/0/1/0/all/0/1\">Aleksandar Vakanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duellman_K/0/1/0/all/0/1\">Kasia Duellman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirkouei_A/0/1/0/all/0/1\">Amin Mirkouei</a>",
          "description": "Recent research on the application of remote sensing and deep learning-based\nanalysis in precision agriculture demonstrated a potential for improved crop\nmanagement and reduced environmental impacts of agricultural production.\nDespite the promising results, the practical relevance of these technologies\nfor actual field deployment requires novel algorithms that are customized for\nanalysis of agricultural images and robust to implementation on natural field\nimagery. The paper presents an approach for analyzing aerial images of a potato\ncrop using deep neural networks. The main objective is to demonstrate automated\nspatial recognition of a healthy versus stressed crop at a plant level.\nSpecifically, we examine premature plant senescence resulting in drought stress\non Russet Burbank potato plants. The proposed deep learning model, named\nRetina-UNet-Ag, is a variant of Retina-UNet (Jaeger et al., 2018) and includes\nconnections from low-level semantic dense representation maps to the feature\npyramid network. The paper also introduces a dataset of field images acquired\nwith a Parrot Sequoia camera carried by a Solo unmanned aerial vehicle.\nExperimental validation demonstrated the ability for distinguishing healthy and\nstressed plants in field images, achieving an average Dice score coefficient of\n0.74. A comparison to related state-of-the-art deep learning models for object\ndetection revealed that the presented approach is effective for the task at\nhand. The method applied here is conducive toward the assessment and\nrecognition of potato crop stress (early plant senescence resulting from\ndrought stress in this case) in natural aerial field images collected under\nreal conditions.",
          "link": "http://arxiv.org/abs/2106.07770",
          "publishedOn": "2021-06-16T01:21:07.474Z",
          "wordCount": 692,
          "title": "Potato Crop Stress Identification in Aerial Images using Deep Learning-based Object Detection. (arXiv:2106.07770v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.13511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niaki_S/0/1/0/all/0/1\">Sina Amini Niaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghighat_E/0/1/0/all/0/1\">Ehsan Haghighat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_T/0/1/0/all/0/1\">Trevor Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poursartip_A/0/1/0/all/0/1\">Anoush Poursartip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaziri_R/0/1/0/all/0/1\">Reza Vaziri</a>",
          "description": "We present a Physics-Informed Neural Network (PINN) to simulate the\nthermochemical evolution of a composite material on a tool undergoing cure in\nan autoclave. In particular, we solve the governing coupled system of\ndifferential equations -- including conductive heat transfer and resin cure\nkinetics -- by optimizing the parameters of a deep neural network (DNN) using a\nphysics-based loss function. To account for the vastly different behaviour of\nthermal conduction and resin cure, we design a PINN consisting of two\ndisconnected subnetworks, and develop a sequential training algorithm that\nmitigates instability present in traditional training methods. Further, we\nincorporate explicit discontinuities into the DNN at the composite-tool\ninterface and enforce known physical behaviour directly in the loss function to\nimprove the solution near the interface. We train the PINN with a technique\nthat automatically adapts the weights on the loss terms corresponding to PDE,\nboundary, interface, and initial conditions. Finally, we demonstrate that one\ncan include problem parameters as an input to the model -- resulting in a\nsurrogate that provides real-time simulation for a range of problem settings --\nand that one can use transfer learning to significantly reduce the training\ntime for problem settings similar to that of an initial trained model. The\nperformance of the proposed PINN is demonstrated in multiple scenarios with\ndifferent material thicknesses and thermal boundary conditions.",
          "link": "http://arxiv.org/abs/2011.13511",
          "publishedOn": "2021-06-16T01:21:07.467Z",
          "wordCount": 707,
          "title": "Physics-Informed Neural Network for Modelling the Thermochemical Curing Process of Composite-Tool Systems During Manufacture. (arXiv:2011.13511v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>",
          "description": "Reverberation from audio reflecting off surfaces and objects in the\nenvironment not only degrades the quality of speech for human perception, but\nalso severely impacts the accuracy of automatic speech recognition. Prior work\nattempts to remove reverberation based on the audio modality only. Our idea is\nto learn to dereverberate speech from audio-visual observations. The visual\nenvironment surrounding a human speaker reveals important cues about the room\ngeometry, materials, and speaker location, all of which influence the precise\nreverberation effects in the audio stream. We introduce Visually-Informed\nDereverberation of Audio (VIDA), an end-to-end approach that learns to remove\nreverberation based on both the observed sounds and visual scene. In support of\nthis new task, we develop a large-scale dataset that uses realistic acoustic\nrenderings of speech in real-world 3D scans of homes offering a variety of room\nacoustics. Demonstrating our approach on both simulated and real imagery for\nspeech enhancement, speech recognition, and speaker identification, we show it\nachieves state-of-the-art performance and substantially improves over\ntraditional audio-only methods. Project page:\nthis http URL",
          "link": "http://arxiv.org/abs/2106.07732",
          "publishedOn": "2021-06-16T01:21:07.443Z",
          "wordCount": 602,
          "title": "Learning Audio-Visual Dereverberation. (arXiv:2106.07732v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2012.07919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Giray_G/0/1/0/all/0/1\">G&#xf6;rkem Giray</a>",
          "description": "Context: Advancements in machine learning (ML) lead to a shift from the\ntraditional view of software development, where algorithms are hard-coded by\nhumans, to ML systems materialized through learning from data. Therefore, we\nneed to revisit our ways of developing software systems and consider the\nparticularities required by these new types of systems. Objective: The purpose\nof this study is to systematically identify, analyze, summarize, and synthesize\nthe current state of software engineering (SE) research for engineering ML\nsystems. Method: I performed a systematic literature review (SLR). I\nsystematically selected a pool of 141 studies from SE venues and then conducted\na quantitative and qualitative analysis using the data extracted from these\nstudies. Results: The non-deterministic nature of ML systems complicates all SE\naspects of engineering ML systems. Despite increasing interest from 2018\nonwards, the results reveal that none of the SE aspects have a mature set of\ntools and techniques. Testing is by far the most popular area among\nresearchers. Even for testing ML systems, engineers have only some tool\nprototypes and solution proposals with weak experimental proof. Many of the\nchallenges of ML systems engineering were identified through surveys and\ninterviews. Researchers should conduct experiments and case studies, ideally in\nindustrial environments, to further understand these challenges and propose\nsolutions. Conclusion: The results may benefit (1) practitioners in foreseeing\nthe challenges of ML systems engineering; (2) researchers and academicians in\nidentifying potential research questions; and (3) educators in designing or\nupdating SE courses to cover ML systems engineering.",
          "link": "http://arxiv.org/abs/2012.07919",
          "publishedOn": "2021-06-16T01:21:07.427Z",
          "wordCount": 735,
          "title": "A Software Engineering Perspective on Engineering Machine Learning Systems: State of the Art and Challenges. (arXiv:2012.07919v3 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1\">Geand Trindade Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1\">Moises Rocha dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1\">Andre Carlos Ponce de Leon Ferreira de Carvalho</a>",
          "description": "With the popularity of Machine Learning (ML) solutions, algorithms and data\nhave been released faster than the capacity of processing them. In this\ncontext, the problem of Algorithm Recommendation (AR) is receiving a\nsignificant deal of attention recently. This problem has been addressed in the\nliterature as a learning task, often as a Meta-Learning problem where the aim\nis to recommend the best alternative for a specific dataset. For such, datasets\nencoded by meta-features are explored by ML algorithms that try to learn the\nmapping between meta-representations and the best technique to be used. One of\nthe challenges for the successful use of ML is to define which features are the\nmost valuable for a specific dataset since several meta-features can be used,\nwhich increases the meta-feature dimension. This paper presents an empirical\nanalysis of Feature Selection and Feature Extraction in the meta-level for the\nAR problem. The present study was focused on three criteria: predictive\nperformance, dimensionality reduction, and pipeline runtime. As we verified,\napplying Dimensionality Reduction (DR) methods did not improve predictive\nperformances in general. However, DR solutions reduced about 80% of the\nmeta-features, obtaining pretty much the same performance as the original setup\nbut with lower runtimes. The only exception was PCA, which presented about the\nsame runtime as the original meta-features. Experimental results also showed\nthat various datasets have many non-informative meta-features and that it is\npossible to obtain high predictive performance using around 20% of the original\nmeta-features. Therefore, due to their natural trend for high dimensionality,\nDR methods should be used for Meta-Feature Selection and Meta-Feature\nExtraction.",
          "link": "http://arxiv.org/abs/2106.03954",
          "publishedOn": "2021-06-15T22:41:25.615Z",
          "wordCount": 718,
          "title": "Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1\">Ameya D. Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuttle_M/0/1/0/all/0/1\">Michael Tuttle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanbhag_N/0/1/0/all/0/1\">Naresh R. Shanbhag</a>",
          "description": "Classical adversarial training (AT) frameworks are designed to achieve high\nadversarial accuracy against a single attack type, typically $\\ell_\\infty$\nnorm-bounded perturbations. Recent extensions in AT have focused on defending\nagainst the union of multiple perturbations but this benefit is obtained at the\nexpense of a significant (up to $10\\times$) increase in training complexity\nover single-attack $\\ell_\\infty$ AT. In this work, we expand the capabilities\nof widely popular single-attack $\\ell_\\infty$ AT frameworks to provide\nrobustness to the union of ($\\ell_\\infty, \\ell_2, \\ell_1$) perturbations while\npreserving their training efficiency. Our technique, referred to as Shaped\nNoise Augmented Processing (SNAP), exploits a well-established byproduct of\nsingle-attack AT frameworks -- the reduction in the curvature of the decision\nboundary of networks. SNAP prepends a given deep net with a shaped noise\naugmentation layer whose distribution is learned along with network parameters\nusing any standard single-attack AT. As a result, SNAP enhances adversarial\naccuracy of ResNet-18 on CIFAR-10 against the union of ($\\ell_\\infty, \\ell_2,\n\\ell_1$) perturbations by 14%-to-20% for four state-of-the-art (SOTA)\nsingle-attack $\\ell_\\infty$ AT frameworks, and, for the first time, establishes\na benchmark for ResNet-50 and ResNet-101 on ImageNet.",
          "link": "http://arxiv.org/abs/2105.14710",
          "publishedOn": "2021-06-15T22:41:25.605Z",
          "wordCount": 646,
          "title": "Robustifying $\\ell_\\infty$ Adversarial Training to the Union of Perturbation Models. (arXiv:2105.14710v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yihong Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Ying Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Songtao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qingjiang Shi</a>",
          "description": "Deep neural networks have been shown as a class of useful tools for\naddressing signal recognition issues in recent years, especially for\nidentifying the nonlinear feature structures of signals. However, this power of\nmost deep learning techniques heavily relies on an abundant amount of training\ndata, so the performance of classic neural nets decreases sharply when the\nnumber of training data samples is small or unseen data are presented in the\ntesting phase. This calls for an advanced strategy, i.e., model-agnostic\nmeta-learning (MAML), which is able to capture the invariant representation of\nthe data samples or signals. In this paper, inspired by the special structure\nof the signal, i.e., real and imaginary parts consisted in practical\ntime-series signals, we propose a Complex-valued Attentional MEta Learner\n(CAMEL) for the problem of few-shot signal recognition by leveraging attention\nand meta-learning in the complex domain. To the best of our knowledge, this is\nalso the first complex-valued MAML that can find the first-order stationary\npoints of general nonconvex problems with theoretical convergence guarantees.\nExtensive experiments results showcase the superiority of the proposed CAMEL\ncompared with the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.04392",
          "publishedOn": "2021-06-15T22:41:25.595Z",
          "wordCount": 639,
          "title": "Signal Transformer: Complex-valued Attention and Meta-Learning for Signal Recognition. (arXiv:2106.04392v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sellars_P/0/1/0/all/0/1\">Philip Sellars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I. Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>",
          "description": "Semi-supervised learning has received a lot of recent attention as it\nalleviates the need for large amounts of labelled data which can often be\nexpensive, requires expert knowledge and be time consuming to collect. Recent\ndevelopments in deep semi-supervised classification have reached unprecedented\nperformance and the gap between supervised and semi-supervised learning is\never-decreasing. This improvement in performance has been based on the\ninclusion of numerous technical tricks, strong augmentation techniques and\ncostly optimisation schemes with multi-term loss functions. We propose a new\nframework, LaplaceNet, for deep semi-supervised classification that has a\ngreatly reduced model complexity. We utilise a hybrid energy-neural network\nwhere graph based pseudo-labels, generated by minimising the graphical\nLaplacian, are used to iteratively improve a neural-network backbone. Our model\noutperforms state-of-the-art methods for deep semi-supervised classification,\nover several benchmark datasets. Furthermore, we consider the application of\nstrong-augmentations to neural networks theoretically and justify the use of a\nmulti-sampling approach for semi-supervised learning. We demonstrate, through\nrigorous experimentation, that a multi-sampling augmentation approach improves\ngeneralisation and reduces the sensitivity of the network to augmentation.",
          "link": "http://arxiv.org/abs/2106.04527",
          "publishedOn": "2021-06-15T22:41:25.576Z",
          "wordCount": 617,
          "title": "LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised Classification. (arXiv:2106.04527v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "While deep and large pre-trained models are the state-of-the-art for various\nnatural language processing tasks, their huge size poses significant challenges\nfor practical uses in resource constrained settings. Recent works in knowledge\ndistillation propose task-agnostic as well as task-specific methods to compress\nthese models, with task-specific ones often yielding higher compression rate.\nIn this work, we develop a new task-agnostic distillation framework\nXtremeDistilTransformers that leverages the advantage of task-specific methods\nfor learning a small universal model that can be applied to arbitrary tasks and\nlanguages. To this end, we study the transferability of several source tasks,\naugmentation resources and model architecture for distillation. We evaluate our\nmodel performance on multiple tasks, including the General Language\nUnderstanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and\na massive multi-lingual NER dataset with 41 languages. We release three\ndistilled task-agnostic checkpoints with 13MM, 22MM and 33MM parameters\nobtaining SOTA performance in several tasks.",
          "link": "http://arxiv.org/abs/2106.04563",
          "publishedOn": "2021-06-15T22:41:25.566Z",
          "wordCount": 604,
          "title": "XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04496",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Haotian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chuanlong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>",
          "description": "Generalization to out-of-distribution (OOD) data, or domain generalization,\nis one of the central problems in modern machine learning. Recently, there is a\nsurge of attempts to propose algorithms for OOD that mainly build upon the idea\nof extracting invariant features. Although intuitively reasonable, theoretical\nunderstanding of what kind of invariance can guarantee OOD generalization is\nstill limited, and generalization to arbitrary out-of-distribution is clearly\nimpossible. In this work, we take the first step towards rigorous and\nquantitative definitions of 1) what is OOD; and 2) what does it mean by saying\nan OOD problem is learnable. We also introduce a new concept of expansion\nfunction, which characterizes to what extent the variance is amplified in the\ntest domains over the training domains, and therefore give a quantitative\nmeaning of invariant features. Based on these, we prove OOD generalization\nerror bounds. It turns out that OOD generalization largely depends on the\nexpansion function. As recently pointed out by Gulrajani and Lopez-Paz (2020),\nany OOD learning algorithm without a model selection module is incomplete. Our\ntheory naturally induces a model selection criterion. Extensive experiments on\nbenchmark OOD datasets demonstrate that our model selection criterion has a\nsignificant advantage over baselines.",
          "link": "http://arxiv.org/abs/2106.04496",
          "publishedOn": "2021-06-15T22:41:25.555Z",
          "wordCount": 641,
          "title": "Towards a Theoretical Framework of Out-of-Distribution Generalization. (arXiv:2106.04496v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Changlin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Muhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1\">Wei Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Sha Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>",
          "description": "Hypergraph offers a framework to depict the multilateral relationships in\nreal-world complex data. Predicting higher-order relationships, i.e hyperedge,\nbecomes a fundamental problem for the full understanding of complicated\ninteractions. The development of graph neural network (GNN) has greatly\nadvanced the analysis of ordinary graphs with pair-wise relations. However,\nthese methods could not be easily extended to the case of hypergraph. In this\npaper, we generalize the challenges of GNN in representing higher-order data in\nprinciple, which are edge- and node-level ambiguities. To overcome the\nchallenges, we present SNALS that utilizes bipartite graph neural network with\nstructural features to collectively tackle the two ambiguity issues. SNALS\ncaptures the joint interactions of a hyperedge by its local environment, which\nis retrieved by collecting the spectrum information of their connections. As a\nresult, SNALS achieves nearly 30% performance increase compared with most\nrecent GNN-based models. In addition, we applied SNALS to predict genetic\nhigher-order interactions on 3D genome organization data. SNALS showed\nconsistently high prediction accuracy across different chromosomes, and\ngenerated novel findings on 4-way gene interaction, which is further validated\nby existing literature.",
          "link": "http://arxiv.org/abs/2106.04292",
          "publishedOn": "2021-06-15T22:41:25.543Z",
          "wordCount": 656,
          "title": "Principled Hyperedge Prediction with Structural Spectral Features and Neural Networks. (arXiv:2106.04292v4 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04740",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1\">Mert Gurbuzbalaban</a>, <a href=\"http://arxiv.org/find/math/1/au:+Simsekli_U/0/1/0/all/0/1\">Umut &#x15e;im&#x15f;ekli</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhu_L/0/1/0/all/0/1\">Lingjiong Zhu</a>",
          "description": "In recent years, various notions of capacity and complexity have been\nproposed for characterizing the generalization properties of stochastic\ngradient descent (SGD) in deep learning. Some of the popular notions that\ncorrelate well with the performance on unseen data are (i) the `flatness' of\nthe local minimum found by SGD, which is related to the eigenvalues of the\nHessian, (ii) the ratio of the stepsize $\\eta$ to the batch-size $b$, which\nessentially controls the magnitude of the stochastic gradient noise, and (iii)\nthe `tail-index', which measures the heaviness of the tails of the network\nweights at convergence. In this paper, we argue that these three seemingly\nunrelated perspectives for generalization are deeply linked to each other. We\nclaim that depending on the structure of the Hessian of the loss at the\nminimum, and the choices of the algorithm parameters $\\eta$ and $b$, the SGD\niterates will converge to a \\emph{heavy-tailed} stationary distribution. We\nrigorously prove this claim in the setting of quadratic optimization: we show\nthat even in a simple linear regression problem with independent and\nidentically distributed data whose distribution has finite moments of all\norder, the iterates can be heavy-tailed with infinite variance. We further\ncharacterize the behavior of the tails with respect to algorithm parameters,\nthe dimension, and the curvature. We then translate our results into insights\nabout the behavior of SGD in deep learning. We support our theory with\nexperiments conducted on synthetic data, fully connected, and convolutional\nneural networks.",
          "link": "http://arxiv.org/abs/2006.04740",
          "publishedOn": "2021-06-15T22:41:25.530Z",
          "wordCount": 731,
          "title": "The Heavy-Tail Phenomenon in SGD. (arXiv:2006.04740v5 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Liyi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Junqi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhenzhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhizhuang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Lvyin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoqiang Zhu</a>",
          "description": "Advertising expenditures have become the major source of revenue for\ne-commerce platforms. Providing good advertising experiences for advertisers by\nreducing their costs of trial and error in discovering the optimal advertising\nstrategies is crucial for the long-term prosperity of online advertising. To\nachieve this goal, the advertising platform needs to identify the advertiser's\noptimization objectives, and then recommend the corresponding strategies to\nfulfill the objectives. In this work, we first deploy a prototype of strategy\nrecommender system on Taobao display advertising platform, which indeed\nincreases the advertisers' performance and the platform's revenue, indicating\nthe effectiveness of strategy recommendation for online advertising. We further\naugment this prototype system by explicitly learning the advertisers'\npreferences over various advertising performance indicators and then\noptimization objectives through their adoptions of different recommending\nadvertising strategies. We use contextual bandit algorithms to efficiently\nlearn the advertisers' preferences and maximize the recommendation adoption,\nsimultaneously. Simulation experiments based on Taobao online bidding data show\nthat the designed algorithms can effectively optimize the strategy adoption\nrate of advertisers.",
          "link": "http://arxiv.org/abs/2105.14188",
          "publishedOn": "2021-06-15T22:41:25.502Z",
          "wordCount": 675,
          "title": "We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruiqi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "As Artificial Intelligence as a Service gains popularity, protecting\nwell-trained models as intellectual property is becoming increasingly\nimportant. Generally speaking, there are two common protection methods:\nownership verification and usage authorization. In this paper, we propose\nNon-Transferable Learning (NTL), a novel approach that captures the exclusive\ndata representation in the learned model and restricts the model generalization\nability to certain domains. This approach provides effective solutions to both\nmodel verification and authorization. For ownership verification, watermarking\ntechniques are commonly used but are often vulnerable to sophisticated\nwatermark removal methods. Our NTL-based model verification approach instead\nprovides robust resistance to state-of-the-art watermark removal methods, as\nshown in extensive experiments for four of such methods over the digits,\nCIFAR10 & STL10, and VisDA datasets. For usage authorization, prior solutions\nfocus on authorizing specific users to use the model, but authorized users can\nstill apply the model to any data without restriction. Our NTL-based\nauthorization approach instead provides data-centric usage protection by\nsignificantly degrading the performance of usage on unauthorized data. Its\neffectiveness is also shown through experiments on a variety of datasets.",
          "link": "http://arxiv.org/abs/2106.06916",
          "publishedOn": "2021-06-15T22:07:49.359Z",
          "wordCount": 617,
          "title": "Non-Transferable Learning: A New Approach for Model Verification and Authorization. (arXiv:2106.06916v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06733",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanfei Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingguang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_G/0/1/0/all/0/1\">Guocai Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Radiation therapy treatment planning is a complex process, as the target dose\nprescription and normal tissue sparing are conflicting objectives. Automated\nand accurate dose prediction for radiation therapy planning is in high demand.\nIn this study, we propose a novel learning-based ensemble approach, named\nLE-NAS, which integrates neural architecture search (NAS) with knowledge\ndistillation for 3D radiotherapy dose prediction. Specifically, the prediction\nnetwork first exhaustively searches each block from enormous architecture\nspace. Then, multiple architectures are selected with promising performance and\ndiversity. To reduce the inference time, we adopt the teacher-student paradigm\nby treating the combination of diverse outputs from multiple searched networks\nas supervisions to guide the student network training. In addition, we apply\nadversarial learning to optimize the student network to recover the knowledge\nin teacher networks. To the best of our knowledge, we are the first to\ninvestigate the combination of NAS and knowledge distillation. The proposed\nmethod has been evaluated on the public OpenKBP dataset, and experimental\nresults demonstrate the effectiveness of our method and its superior\nperformance to the state-of-the-art method.",
          "link": "http://arxiv.org/abs/2106.06733",
          "publishedOn": "2021-06-15T22:07:49.342Z",
          "wordCount": 622,
          "title": "LE-NAS: Learning-based Ensenble with NAS for Dose Prediction. (arXiv:2106.06733v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junfu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bi Wang</a>",
          "description": "Working memory (WM) is a basic part of human cognition, which plays an\nimportant role in the study of human cognitive load. Among various brain\nimaging techniques, electroencephalography has shown its advantage on easy\naccess and reliability. However, one of the critical challenges is that\nindividual difference may cause the ineffective results, especially when the\nestablished model meets an unfamiliar subject. In this work, we propose a\ncross-subject deep adaptation model with spatial attention (CS-DASA) to\ngeneralize the workload classifications across subjects. First, we transform\ntime-series EEG data into multi-frame EEG images incorporating more\nspatio-temporal information. First, the subject-shared module in CS-DASA\nreceives multi-frame EEG image data from both source and target subjects and\nlearns the common feature representations. Then, in subject-specific module,\nthe maximum mean discrepancy is implemented to measure the domain distribution\ndivergence in a reproducing kernel Hilbert space, which can add an effective\npenalty loss for domain adaptation. Additionally, the subject-to-subject\nspatial attention mechanism is employed to focus on the most discriminative\nspatial feature in EEG image data. Experiments conducted on a public WM EEG\ndataset containing 13 subjects show that the proposed model is capable of\nachieve better performance than existing state-of-the art methods.",
          "link": "http://arxiv.org/abs/2106.06769",
          "publishedOn": "2021-06-15T22:07:49.270Z",
          "wordCount": 632,
          "title": "Cross-Subject Domain Adaptation for Multi-Frame EEG Images. (arXiv:2106.06769v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.10.2"
}