 
<!DOCTYPE html>
<html lang="en">

<head>
    <title>ArxivDaily</title>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="noindex, nofollow" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
    <link rel="alternate" type="application/rss+xml" title="ArxivDaily" href="feed.atom" />
    <link href="index.css" rel="stylesheet" />
    <!-- %before-head-end.html% -->
</head>

<body>
    <!-- %after-body-begin.html% -->
    <a href="https://github.com/LooperXX/ArxivDaily" style="margin: 0 auto;padding: 0.5em 1em;">LooperXX/ArxivDaily</a>
    <section class="daily-content">
        <h2 class="daily-heading"><time datatime="2021-06-15">2021-06-15</time></h2>
        <ul class="sources card">
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.CL"">cs.CL updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1">Subhabrata Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1">Ahmed Hassan Awadallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04563">
                                    <div class="article-summary-box-inner">
                                        <span>While deep and large pre-trained models are the state-of-the-art for various
natural language processing tasks, their huge size poses significant challenges
for practical uses in resource constrained settings. Recent works in knowledge
distillation propose task-agnostic as well as task-specific methods to compress
these models, with task-specific ones often yielding higher compression rate.
In this work, we develop a new task-agnostic distillation framework
XtremeDistilTransformers that leverages the advantage of task-specific methods
for learning a small universal model that can be applied to arbitrary tasks and
languages. To this end, we study the transferability of several source tasks,
augmentation resources and model architecture for distillation. We evaluate our
model performance on multiple tasks, including the General Language
Understanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and
a massive multi-lingual NER dataset with 41 languages. We release three
distilled task-agnostic checkpoints with 13MM, 22MM and 33MM parameters
obtaining SOTA performance in several tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality. (arXiv:2106.04102v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Mina Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1">Chris Donahue</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyabor_A/0/1/0/all/0/1">Alexander Iyabor</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Percy Liang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04102">
                                    <div class="article-summary-box-inner">
                                        <span>We release a new benchmark for lexical substitution, the task of finding
appropriate substitutes for a target word in a context. To assist humans with
writing, lexical substitution systems can suggest words that humans cannot
easily think of. However, existing benchmarks depend on human recall as the
only source of data, and therefore lack coverage of the substitutes that would
be most helpful to humans. Furthermore, annotators often provide substitutes of
low quality, which are not actually appropriate in the given context. We
collect higher-coverage and higher-quality data by framing lexical substitution
as a classification problem, guided by the intuition that it is easier for
humans to judge the appropriateness of candidate substitutes than conjure them
from memory. To this end, we use a context-free thesaurus to produce candidates
and rely on human judgement to determine contextual appropriateness. Compared
to the previous largest benchmark, our Swords benchmark has 4.1x more
substitutes per target word for the same level of quality, and its substitutes
are 1.5x more appropriate (based on human judgement) for the same number of
substitutes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets. (arXiv:2106.04476v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Damonte_M/0/1/0/all/0/1">Marco Damonte</a>, <a href="http://arxiv.org/find/cs/1/au:+Monti_E/0/1/0/all/0/1">Emilio Monti</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04476">
                                    <div class="article-summary-box-inner">
                                        <span>Semantic parsers map natural language utterances to meaning representations.
The lack of a single standard for meaning representations led to the creation
of a plethora of semantic parsing datasets. To unify different datasets and
train a single model for them, we investigate the use of Multi-Task Learning
(MTL) architectures. We experiment with five datasets (Geoquery, NLMaps, TOP,
Overnight, AMR). We find that an MTL architecture that shares the entire
network across datasets yields competitive or better parsing accuracies than
the single-task baselines, while reducing the total number of parameters by
68%. We further provide evidence that MTL has also better compositional
generalization than single-task models. We also present a comparison of task
sampling methods and propose a competitive alternative to widespread
proportional sampling strategies.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability. (arXiv:2104.01408v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1">Berrak Sisman</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haizhou Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01408">
                                    <div class="article-summary-box-inner">
                                        <span>Emotional text-to-speech synthesis (ETTS) has seen much progress in recent
years. However, the generated voice is often not perceptually identifiable by
its intended emotion category. To address this problem, we propose a new
interactive training paradigm for ETTS, denoted as i-ETTS, which seeks to
directly improve the emotion discriminability by interacting with a speech
emotion recognition (SER) model. Moreover, we formulate an iterative training
strategy with reinforcement learning to ensure the quality of i-ETTS
optimization. Experimental results demonstrate that the proposed i-ETTS
outperforms the state-of-the-art baselines by rendering speech with more
accurate emotion style. To our best knowledge, this is the first study of
reinforcement learning in emotional text-to-speech synthesis.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">EL-Attention: Memory Efficient Lossless Attention for Generation. (arXiv:2105.04779v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiusheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1">Weizhen Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1">Nikhil Bhendawade</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yeyun Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1">Nan Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruofei Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04779">
                                    <div class="article-summary-box-inner">
                                        <span>Transformer model with multi-head attention requires caching intermediate
results for efficient inference in generation tasks. However, cache brings new
memory-related costs and prevents leveraging larger batch size for faster
speed. We propose memory-efficient lossless attention (called EL-attention) to
address this issue. It avoids heavy operations for building multi-head keys and
values, cache for them is not needed. EL-attention constructs an ensemble of
attention results by expanding query while keeping key and value shared. It
produces the same result as multi-head attention with less GPU memory and
faster inference speed. We conduct extensive experiments on Transformer, BART,
and GPT-2 for summarization and question generation tasks. The results show
EL-attention speeds up existing models by 1.6x to 5.3x without accuracy loss.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data. (arXiv:2106.01797v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Qin_P/0/1/0/all/0/1">Pengda Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1">Kefeng Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qiang Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01797">
                                    <div class="article-summary-box-inner">
                                        <span>Among ubiquitous multimodal data in the real world, text is the modality
generated by human, while image reflects the physical world honestly. In a
visual understanding application, machines are expected to understand images
like human. Inspired by this, we propose a novel self-supervised learning
method, named Text-enhanced Visual Deep InfoMax (TVDIM), to learn better visual
representations by fully utilizing the naturally-existing multimodal data. Our
core idea of self-supervised learning is to maximize the mutual information
between features extracted from multiple views of a shared context to a
rational degree. Different from previous methods which only consider multiple
views from a single modality, our work produces multiple views from different
modalities, and jointly optimizes the mutual information for features pairs of
intra-modality and inter-modality. Considering the information gap between
inter-modality features pairs from data noise, we adopt a \emph{ranking-based}
contrastive learning to optimize the mutual information. During evaluation, we
directly use the pre-trained visual representations to complete various image
classification tasks. Experimental results show that, TVDIM significantly
outperforms previous visual self-supervised methods when processing the same
set of images.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Certified Robustness to Text Adversarial Attacks by Randomized [MASK]. (arXiv:2105.03743v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1">Jiehang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaoqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jianhan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Liping Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03743">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, few certified defense methods have been developed to provably
guarantee the robustness of a text classifier to adversarial synonym
substitutions. However, all existing certified defense methods assume that the
defenders are informed of how the adversaries generate synonyms, which is not a
realistic scenario. In this paper, we propose a certifiably robust defense
method by randomly masking a certain proportion of the words in an input text,
in which the above unrealistic assumption is no longer necessary. The proposed
method can defend against not only word substitution-based attacks, but also
character-level perturbations. We can certify the classifications of over 50%
texts to be robust to any perturbation of 5 words on AGNEWS, and 2 words on
SST2 dataset. The experimental results show that our randomized smoothing
method significantly outperforms recently proposed defense methods across
multiple datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Kwame: A Bilingual AI Teaching Assistant for Online SuaCode Courses. (arXiv:2010.11387v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1">George Boateng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11387">
                                    <div class="article-summary-box-inner">
                                        <span>Introductory hands-on courses such as our smartphone-based coding course,
SuaCode require a lot of support for students to accomplish learning goals.
Online environments make it even more difficult to get assistance especially
more recently because of COVID-19. Given the multilingual context of SuaCode
students - learners across 42 African countries that are mostly Anglophone or
Francophone - in this work, we developed a bilingual Artificial Intelligence
(AI) Teaching Assistant (TA) - Kwame - that provides answers to students&#x27;
coding questions from SuaCode courses in English and French. Kwame is a
Sentence-BERT (SBERT)-based question-answering (QA) system that we trained and
evaluated offline using question-answer pairs created from the course&#x27;s
quizzes, lesson notes and students&#x27; questions in past cohorts. Kwame finds the
paragraph most semantically similar to the question via cosine similarity. We
compared the system with TF-IDF and Universal Sentence Encoder. Our results
showed that fine-tuning on the course data and returning the top 3 and 5
answers improved the accuracy results. Kwame will make it easy for students to
get quick and accurate answers to questions in SuaCode courses.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Lightweight Cross-Lingual Sentence Representation Learning. (arXiv:2105.13856v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1">Zhuoyuan Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1">Prakhar Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1">Chenhui Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1">Martin Jaggi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1">Sadao Kurohashi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13856">
                                    <div class="article-summary-box-inner">
                                        <span>Large-scale models for learning fixed-dimensional cross-lingual sentence
representations like LASER (Artetxe and Schwenk, 2019b) lead to significant
improvement in performance on downstream tasks. However, further increases and
modifications based on such large-scale models are usually impractical due to
memory limitations. In this work, we introduce a lightweight dual-transformer
architecture with just 2 layers for generating memory-efficient cross-lingual
sentence representations. We explore different training tasks and observe that
current cross-lingual training tasks leave a lot to be desired for this shallow
architecture. To ameliorate this, we propose a novel cross-lingual language
model, which combines the existing single-word masked language model with the
newly proposed cross-lingual token-level reconstruction task. We further
augment the training task by the introduction of two computationally-lite
sentence-level contrastive learning tasks to enhance the alignment of
cross-lingual sentence representation space, which compensates for the learning
bottleneck of the lightweight transformer for generative tasks. Our comparisons
with competing models on cross-lingual sentence retrieval and multilingual
document classification confirm the effectiveness of the newly proposed
training tasks for a shallow model.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust Optimization for Multilingual Translation with Imbalanced Data. (arXiv:2104.07639v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1">Hongyu Gong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07639">
                                    <div class="article-summary-box-inner">
                                        <span>Multilingual models are parameter-efficient with the prospect improving
low-resource languages by leveraging crosslingual transfer. Despite recent
advance in massive multilingual translation with ever-growing model and data,
how to effectively train multilingual models has not been well understood. In
this paper, we show that a common situation in multilingual training, data
imbalance among languages, poses optimization tension between high resource and
low resource languages where the found multilingual solution is often
sub-optimal for low resources. We show that common training method which
upsamples low resources can not robustly optimize population loss with risks of
either underfitting high resource languages or overfitting low resource ones.
Drawing on recent findings on the geometry of loss landscape and its effect on
generalization, we propose a principled optimization algorithm, Curvature Aware
Task Scaling (CATS), which adaptively rescales gradients from different tasks
with a meta objective of guiding multilingual training to low-curvature
neighborhoods with uniformly low loss for all languages. We ran experiments on
common benchmarks (TED, WMT and OPUS-100) with varying degrees of data
imbalance. CATS effectively improved multilingual optimization and as a result
demonstrated consistent gains on low resources ( to BLEU) without hurting high
resources. In addition, CATS is robust to overparameterization and large batch
size training, making it a promising training method for massive multilingual
models that truly improve low resource languages.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment. (arXiv:2101.00148v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1">Haoyue Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sida I. Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00148">
                                    <div class="article-summary-box-inner">
                                        <span>Bilingual lexicons map words in one language to their translations in
another, and are typically induced by learning linear projections to align
monolingual word embedding spaces. In this paper, we show it is possible to
produce much higher quality lexicons with methods that combine (1) unsupervised
bitext mining and (2) unsupervised word alignment. Directly applying a pipeline
that uses recent algorithms for both subproblems significantly improves induced
lexicon quality and further gains are possible by learning to filter the
resulting lexical entries, with both unsupervised and semi-supervised schemes.
Our final model outperforms the state of the art on the BUCC 2020 shared task
by 14 $F_1$ points averaged over 12 language pairs, while also providing a more
interpretable approach that allows for rich reasoning of word meaning in
context. Further analysis of our output and the standard reference lexicons
suggests they are of comparable quality, and new benchmarks may be needed to
measure further progress on this task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1">An Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Junyang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1">Rui Men</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Le Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xianyan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Ang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiamang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Di Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Wei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Lin Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongxia Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15082">
                                    <div class="article-summary-box-inner">
                                        <span>Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search. (arXiv:2010.07003v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Gyuwan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1">Kyunghyun Cho</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07003">
                                    <div class="article-summary-box-inner">
                                        <span>Despite transformers&#x27; impressive accuracy, their computational cost is often
prohibitive to use with limited computational resources. Most previous
approaches to improve inference efficiency require a separate model for each
possible computational budget. In this paper, we extend PoWER-BERT (Goyal et
al., 2020) and propose Length-Adaptive Transformer that can be used for various
inference scenarios after one-shot training. We train a transformer with
LengthDrop, a structural variant of dropout, which stochastically determines a
sequence length at each layer. We then conduct a multi-objective evolutionary
search to find a length configuration that maximizes the accuracy and minimizes
the efficiency metric under any given computational budget. Additionally, we
significantly extend the applicability of PoWER-BERT beyond sequence-level
classification into token-level classification with Drop-and-Restore process
that drops word-vectors temporarily in intermediate layers and restores at the
last layer if necessary. We empirically verify the utility of the proposed
approach by demonstrating the superior accuracy-efficiency trade-off under
various setups, including span-based question answering and text
classification. Code is available at
https://github.com/clovaai/length-adaptive-transformer.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion. (arXiv:2104.02194v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1">Duc Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1">Mahaveer Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Keren_G/0/1/0/all/0/1">Gil Keren</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Suyoun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yangyang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1">Jay Mahadeokar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1">Julian Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1">Yuan Shangguan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuegen_C/0/1/0/all/0/1">Christian Fuegen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1">Ozlem Kalinli</a>, <a href="http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1">Yatharth Saraf</a>, <a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1">Michael L. Seltzer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02194">
                                    <div class="article-summary-box-inner">
                                        <span>How to leverage dynamic contextual information in end-to-end speech
recognition has remained an active research area. Previous solutions to this
problem were either designed for specialized use cases that did not generalize
well to open-domain scenarios, did not scale to large biasing lists, or
underperformed on rare long-tail words. We address these limitations by
proposing a novel solution that combines shallow fusion, trie-based deep
biasing, and neural network language model contextualization. These techniques
result in significant 19.5% relative Word Error Rate improvement over existing
contextual biasing approaches and 5.4%-9.3% improvement compared to a strong
hybrid baseline on both open-domain and constrained contextualization tasks,
where the targets consist of mostly rare long-tail words. Our final system
remains lightweight and modular, allowing for quick modification without model
re-training.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Accelerating BERT Inference for Sequence Labeling via Early-Exit. (arXiv:2105.13878v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaonan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1">Yunfan Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tianxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1">Hang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13878">
                                    <div class="article-summary-box-inner">
                                        <span>Both performance and efficiency are crucial factors for sequence labeling
tasks in many real-world scenarios. Although the pre-trained models (PTMs) have
significantly improved the performance of various sequence labeling tasks,
their computational cost is expensive. To alleviate this problem, we extend the
recent successful early-exit mechanism to accelerate the inference of PTMs for
sequence labeling tasks. However, existing early-exit mechanisms are
specifically designed for sequence-level tasks, rather than sequence labeling.
In this paper, we first propose a simple extension of sentence-level early-exit
for sequence labeling tasks. To further reduce the computational cost, we also
propose a token-level early-exit mechanism that allows partial tokens to exit
early at different layers. Considering the local dependency inherent in
sequence labeling, we employed a window-based criterion to decide for a token
whether or not to exit. The token-level early-exit brings the gap between
training and inference, so we introduce an extra self-sampling fine-tuning
stage to alleviate it. The extensive experiments on three popular sequence
labeling tasks show that our approach can save up to 66%-75% inference cost
with minimal performance degradation. Compared with competitive compressed
models such as DistilBERT, our approach can achieve better performance under
the same speed-up ratios of 2X, 3X, and 4X.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-Lingual Abstractive Summarization with Limited Parallel Resources. (arXiv:2105.13648v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yu Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heyan Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13648">
                                    <div class="article-summary-box-inner">
                                        <span>Parallel cross-lingual summarization data is scarce, requiring models to
better use the limited available cross-lingual resources. Existing methods to
do so often adopt sequence-to-sequence networks with multi-task frameworks.
Such approaches apply multiple decoders, each of which is utilized for a
specific task. However, these independent decoders share no parameters, hence
fail to capture the relationships between the discrete phrases of summaries in
different languages, breaking the connections in order to transfer the
knowledge of the high-resource languages to low-resource languages. To bridge
these connections, we propose a novel Multi-Task framework for Cross-Lingual
Abstractive Summarization (MCLAS) in a low-resource setting. Employing one
unified decoder to generate the sequential concatenation of monolingual and
cross-lingual summaries, MCLAS makes the monolingual summarization task a
prerequisite of the cross-lingual summarization (CLS) task. In this way, the
shared decoder learns interactions involving alignments and summary patterns
across languages, which encourages attaining knowledge transfer. Experiments on
two CLS datasets demonstrate that our model significantly outperforms three
baseline models in both low-resource and full-dataset scenarios. Moreover,
in-depth analysis on the generated summaries and attention heads verifies that
interactions are learned well using MCLAS, which benefits the CLS task under
limited parallel resources.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">InFillmore: Frame-Guided Language Generation with Bidirectional Context. (arXiv:2103.04941v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1">Jiefu Ou</a>, <a href="http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1">Nathaniel Weir</a>, <a href="http://arxiv.org/find/cs/1/au:+Belyy_A/0/1/0/all/0/1">Anton Belyy</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Felix Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1">Benjamin Van Durme</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04941">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a structured extension to bidirectional-context conditional
language generation, or &quot;infilling,&quot; inspired by Frame Semantic theory
(Fillmore, 1976). Guidance is provided through two approaches: (1) model
fine-tuning, conditioning directly on observed symbolic frames, and (2) a novel
extension to disjunctive lexically constrained decoding that leverages frame
semantic lexical units. Automatic and human evaluations confirm that
frame-guided generation allows for explicit manipulation of intended infill
semantics, with minimal loss in distinguishability from human-generated text.
Our methods flexibly apply to a variety of use scenarios, and we provide a
codebase and interactive demo available from
https://nlp.jhu.edu/demos/infillmore.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fenglin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1">Changchang Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1">Shen Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Ping Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xu Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06965">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, chest X-ray report generation, which aims to automatically generate
descriptions of given chest X-ray images, has received growing research
interests. The key challenge of chest X-ray report generation is to accurately
capture and describe the abnormal regions. In most cases, the normal regions
dominate the entire chest X-ray image, and the corresponding descriptions of
these normal regions dominate the final report. Due to such data bias,
learning-based models may fail to attend to abnormal regions. In this work, to
effectively capture and describe abnormal regions, we propose the Contrastive
Attention (CA) model. Instead of solely focusing on the current input image,
the CA model compares the current input image with normal images to distill the
contrastive information. The acquired contrastive information can better
represent the visual features of abnormal regions. According to the experiments
on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into
several existing models can boost their performance across most metrics. In
addition, according to the analysis, the CA model can help existing models
better attend to the abnormal regions and provide more accurate descriptions
which are crucial for an interpretable diagnosis. Specifically, we achieve the
state-of-the-art results on the two public datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-Label Annotation of Chest Abdomen Pelvis Computed Tomography Text Reports Using Deep Learning. (arXiv:2102.02959v4 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+DAnniballe_V/0/1/0/all/0/1">Vincent M. D&#x27;Anniballe</a>, <a href="http://arxiv.org/find/cs/1/au:+Tushar_F/0/1/0/all/0/1">Fakrul I. Tushar</a>, <a href="http://arxiv.org/find/cs/1/au:+Faryna_K/0/1/0/all/0/1">Khrystyna Faryna</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Songyue Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1">Maciej A. Mazurowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubin_G/0/1/0/all/0/1">Geoffrey D. Rubin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_J/0/1/0/all/0/1">Joseph Y. Lo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02959">
                                    <div class="article-summary-box-inner">
                                        <span>Purpose: To develop high throughput multi-label annotators for body (chest,
abdomen, and pelvis) Computed Tomography (CT) reports that can be applied
across a variety of abnormalities, organs, and disease states.

Approach: We used a dictionary approach to develop rule-based algorithms
(RBA) for extraction of disease labels from radiology text reports. We targeted
three organ systems (lungs/pleura, liver/gallbladder, kidneys/ureters) with
four diseases per system based on their prevalence in our dataset. To expand
the algorithms beyond pre-defined keywords, attention-guided recurrent neural
networks (RNN) were trained using the RBA-extracted labels to classify reports
as being positive for one or more diseases or normal for each organ system.
Confounding effects on model performance were evaluated using random
initialization or pre-trained embedding as well as different sizes of training
datasets. Performance was evaluated using the receiver operating characteristic
(ROC) area under the curve (AUC) against 2,158 manually obtained labels.

Results: Our models extracted disease labels from 261,229 radiology reports
of 112,501 unique subjects. Pre-trained models outperformed random
initialization across all diseases. As the training dataset size was reduced,
performance was robust except for a few diseases with relatively small number
of cases. Pre-trained classification AUCs achieved &gt; 0.95 for all five disease
outcomes across all three organ systems.

Conclusions: Our label-extracting pipeline was able to encompass a variety of
cases and diseases by generalizing beyond strict rules with exceptional
accuracy. This method can be easily adapted to enable automated labeling of
hospital-scale medical data sets for training image-based disease classifiers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Case Study on Detecting COVID-19 Health-Related Misinformation in Social Media. (arXiv:2106.06811v1 [cs.SI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pritom_M/0/1/0/all/0/1">Mir Mehedi A. Pritom</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_R/0/1/0/all/0/1">Rosana Montanez Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Asad Ali Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nugroho_S/0/1/0/all/0/1">Sebastian A. Nugroho</a>, <a href="http://arxiv.org/find/cs/1/au:+Alrashydah_E/0/1/0/all/0/1">Esra&#x27;a Alrashydah</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruiz_B/0/1/0/all/0/1">Beatrice N. Ruiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1">Anthony Rios</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06811">
                                    <div class="article-summary-box-inner">
                                        <span>COVID-19 pandemic has generated what public health officials called an
infodemic of misinformation. As social distancing and stay-at-home orders came
into effect, many turned to social media for socializing. This increase in
social media usage has made it a prime vehicle for the spreading of
misinformation. This paper presents a mechanism to detect COVID-19
health-related misinformation in social media following an interdisciplinary
approach. Leveraging social psychology as a foundation and existing
misinformation frameworks, we defined misinformation themes and associated
keywords incorporated into the misinformation detection mechanism using applied
machine learning techniques. Next, using the Twitter dataset, we explored the
performance of the proposed methodology using multiple state-of-the-art machine
learning classifiers. Our method shows promising results with at most 78%
accuracy in classifying health-related misinformation versus true information
using uni-gram-based NLP feature generations from tweets and the Decision Tree
classifier. We also provide suggestions on alternatives for countering
misinformation and ethical consideration for the study.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Prompting Contrastive Explanations for Commonsense Reasoning Tasks. (arXiv:2106.06823v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Paranjape_B/0/1/0/all/0/1">Bhargavi Paranjape</a>, <a href="http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1">Julian Michael</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1">Marjan Ghazvininejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06823">
                                    <div class="article-summary-box-inner">
                                        <span>Many commonsense reasoning NLP tasks involve choosing between one or more
possible answers to a question or prompt based on knowledge that is often
implicit. Large pretrained language models (PLMs) can achieve near-human
performance on such tasks, while providing little human-interpretable evidence
of the underlying reasoning they use. In this work, we show how to use these
same models to generate such evidence: inspired by the contrastive nature of
human explanations, we use PLMs to complete explanation prompts which contrast
alternatives according to the key attribute(s) required to justify the correct
answer (for example, peanuts are usually salty while raisins are sweet).
Conditioning model decisions on these explanations improves performance on two
commonsense reasoning benchmarks, as compared to previous non-contrastive
alternatives. These explanations are also judged by humans to be more relevant
for solving the task, and facilitate a novel method to evaluate explanation
faithfulfness.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation. (arXiv:2101.06561v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1">Daniel Khashabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1">Gabriel Stanovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1">Jonathan Bragg</a>, <a href="http://arxiv.org/find/cs/1/au:+Lourie_N/0/1/0/all/0/1">Nicholas Lourie</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1">Jungo Kasai</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1">Yejin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1">Noah A. Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1">Daniel S. Weld</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06561">
                                    <div class="article-summary-box-inner">
                                        <span>Leaderboards have eased model development for many NLP datasets by
standardizing their evaluation and delegating it to an independent external
repository. Their adoption, however, is so far limited to tasks that can be
reliably evaluated in an automatic manner. This work introduces GENIE, an
extensible human evaluation leaderboard, which brings the ease of leaderboards
to text generation tasks. GENIE automatically posts leaderboard submissions to
crowdsourcing platforms asking human annotators to evaluate them on various
axes (e.g., correctness, conciseness, fluency) and compares their answers to
various automatic metrics. We introduce several datasets in English to GENIE,
representing four core challenges in text generation: machine translation,
summarization, commonsense reasoning, and machine comprehension. We provide
formal granular evaluation metrics and identify areas for future research. We
make GENIE publicly available and hope that it will spur progress in language
generation models as well as their automatic and manual evaluation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation. (arXiv:2106.06751v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1">Shuhao Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Dengji Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhengxin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1">Chenze Shao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06751">
                                    <div class="article-summary-box-inner">
                                        <span>Although teacher forcing has become the main training paradigm for neural
machine translation, it usually makes predictions only conditioned on past
information, and hence lacks global planning for the future. To address this
problem, we introduce another decoder, called seer decoder, into the
encoder-decoder framework during training, which involves future information in
target predictions. Meanwhile, we force the conventional decoder to simulate
the behaviors of the seer decoder via knowledge distillation. In this way, at
test the conventional decoder can perform like the seer decoder without the
attendance of it. Experiment results on the Chinese-English, English-German and
English-Romanian translation tasks show our method can outperform competitive
baselines significantly and achieves greater improvements on the bigger data
sets. Besides, the experiments also prove knowledge distillation the best way
to transfer knowledge from the seer decoder to the conventional decoder
compared to adversarial learning and L2 regularization.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning. (arXiv:2101.07393v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1">Austin W. Hanjie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1">Victor Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1">Karthik Narasimhan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07393">
                                    <div class="article-summary-box-inner">
                                        <span>We investigate the use of natural language to drive the generalization of
control policies and introduce the new multi-task environment Messenger with
free-form text manuals describing the environment dynamics. Unlike previous
work, Messenger does not assume prior knowledge connecting text and state
observations $-$ the control policy must simultaneously ground the game manual
to entity symbols and dynamics in the environment. We develop a new model, EMMA
(Entity Mapper with Multi-modal Attention) which uses an entity-conditioned
attention module that allows for selective focus over relevant descriptions in
the manual for each entity in the environment. EMMA is end-to-end
differentiable and learns a latent grounding of entities and dynamics from text
to observations using only environment rewards. EMMA achieves successful
zero-shot generalization to unseen games with new dynamics, obtaining a 40%
higher win rate compared to multiple baselines. However, win rate on the
hardest stage of Messenger remains low (10%), demonstrating the need for
additional work in this direction.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks. (arXiv:2101.06969v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhengyan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1">Guangxuan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1">Tian Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1">Fanchao Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yasheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06969">
                                    <div class="article-summary-box-inner">
                                        <span>Pre-trained models (PTMs) have been widely used in various downstream tasks.
The parameters of PTMs are distributed on the Internet and may suffer backdoor
attacks. In this work, we demonstrate the universal vulnerability of PTMs,
where fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary
downstream tasks. Specifically, attackers can add a simple pre-training task,
which restricts the output representations of trigger instances to pre-defined
vectors, namely neuron-level backdoor attack (NeuBA). If the backdoor
functionality is not eliminated during fine-tuning, the triggers can make the
fine-tuned model predict fixed labels by pre-defined vectors. In the
experiments of both natural language processing (NLP) and computer vision (CV),
we show that NeuBA absolutely controls the predictions for trigger instances
without any knowledge of downstream tasks. Finally, we apply several defense
methods to NeuBA and find that model pruning is a promising direction to resist
NeuBA by excluding backdoored neurons. Our findings sound a red alarm for the
wide use of PTMs. Our source code and models are available at
\url{https://github.com/thunlp/NeuBA}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Machine Translation into Low-resource Language Varieties. (arXiv:2106.06797v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sachin Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1">Antonios Anastasopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Wintner_S/0/1/0/all/0/1">Shuly Wintner</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1">Yulia Tsvetkov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06797">
                                    <div class="article-summary-box-inner">
                                        <span>State-of-the-art machine translation (MT) systems are typically trained to
generate the &quot;standard&quot; target language; however, many languages have multiple
varieties (regional varieties, dialects, sociolects, non-native varieties) that
are different from the standard language. Such varieties are often
low-resource, and hence do not benefit from contemporary NLP solutions, MT
included. We propose a general framework to rapidly adapt MT systems to
generate language varieties that are close to, but different from, the standard
target language, using no parallel (source--variety) data. This also includes
adaptation of MT systems to low-resource typologically-related target
languages. We experiment with adapting an English--Russian MT system to
generate Ukrainian and Belarusian, an English--Norwegian Bokm{\aa}l system to
generate Nynorsk, and an English--Arabic system to generate four Arabic
dialects, obtaining significant improvements over competitive baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Study of sampling methods in sentiment analysis of imbalanced data. (arXiv:2106.06673v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sayyed_Z/0/1/0/all/0/1">Zeeshan Ali Sayyed</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06673">
                                    <div class="article-summary-box-inner">
                                        <span>This work investigates the application of sampling methods for sentiment
analysis on two different highly imbalanced datasets. One dataset contains
online user reviews from the cooking platform Epicurious and the other contains
comments given to the Planned Parenthood organization. In both these datasets,
the classes of interest are rare. Word n-grams were used as features from these
datasets. A feature selection technique based on information gain is first
applied to reduce the number of features to a manageable space. A number of
different sampling methods were then applied to mitigate the class imbalance
problem which are then analyzed.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploiting Parallel Corpora to Improve Multilingual Embedding based Document and Sentence Alignment. (arXiv:2106.06766v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sachintha_D/0/1/0/all/0/1">Dilan Sachintha</a>, <a href="http://arxiv.org/find/cs/1/au:+Piyarathna_L/0/1/0/all/0/1">Lakmali Piyarathna</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajitha_C/0/1/0/all/0/1">Charith Rajitha</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1">Surangika Ranathunga</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06766">
                                    <div class="article-summary-box-inner">
                                        <span>Multilingual sentence representations pose a great advantage for low-resource
languages that do not have enough data to build monolingual models on their
own. These multilingual sentence representations have been separately exploited
by few research for document and sentence alignment. However, most of the
low-resource languages are under-represented in these pre-trained models. Thus,
in the context of low-resource languages, these models have to be fine-tuned
for the task at hand, using additional data sources. This paper presents a
weighting mechanism that makes use of available small-scale parallel corpora to
improve the performance of multilingual sentence representations on document
and sentence alignment. Experiments are conducted with respect to two
low-resource languages, Sinhala and Tamil. Results on a newly created dataset
of Sinhala-English, Tamil-English, and Sinhala-Tamil show that this new
weighting mechanism significantly improves both document and sentence
alignment. This dataset, as well as the source-code, is publicly released.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Modeling Language Usage and Listener Engagement in Podcasts. (arXiv:2106.06605v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Sravana Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazarova_M/0/1/0/all/0/1">Marina Lazarova</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yongze Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1">Rosie Jones</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06605">
                                    <div class="article-summary-box-inner">
                                        <span>While there is an abundance of popular writing targeted to podcast creators
on how to speak in ways that engage their listeners, there has been little
data-driven analysis of podcasts that relates linguistic style with listener
engagement. In this paper, we investigate how various factors -- vocabulary
diversity, distinctiveness, emotion, and syntax, among others -- correlate with
engagement, based on analysis of the creators&#x27; written descriptions and
transcripts of the audio. We build models with different textual
representations, and show that the identified features are highly predictive of
engagement. Our analysis tests popular wisdom about stylistic elements in
high-engagement podcasts, corroborating some aspects, and adding new
perspectives on others.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fenglin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1">Shen Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1">Wei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yuexian Zou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06963">
                                    <div class="article-summary-box-inner">
                                        <span>Automatically generating radiology reports can improve current clinical
practice in diagnostic radiology. On one hand, it can relieve radiologists from
the heavy burden of report writing; On the other hand, it can remind
radiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.
Yet, this task remains a challenging job for data-driven neural networks, due
to the serious visual and textual data biases. To this end, we propose a
Posterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to
imitate the working patterns of radiologists, who will first examine the
abnormal regions and assign the disease topic tags to the abnormal regions, and
then rely on the years of prior medical knowledge and prior working experience
accumulations to write reports. Thus, the PPKED includes three modules:
Posterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and
Multi-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior
knowledge, which provides explicit abnormal visual regions to alleviate visual
data bias; PrKE explores the prior knowledge from the prior medical knowledge
graph (medical knowledge) and prior radiology reports (working experience) to
alleviate textual data bias. The explored knowledge is distilled by the MKD to
generate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our
method is able to outperform previous state-of-the-art models on these two
datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multitask Training with Text Data for End-to-End Speech Recognition. (arXiv:2010.14318v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peidong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1">Tara N. Sainath</a>, <a href="http://arxiv.org/find/cs/1/au:+Weiss_R/0/1/0/all/0/1">Ron J. Weiss</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.14318">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a multitask training method for attention-based end-to-end speech
recognition models. We regularize the decoder in a listen, attend, and spell
model by multitask training it on both audio-text and text-only data. Trained
on the 100-hour subset of LibriSpeech, the proposed method, without requiring
an additional language model, leads to an 11% relative performance improvement
over the baseline and approaches the performance of language model shallow
fusion on the test-clean evaluation set. We observe a similar trend on the
whole 960-hour LibriSpeech training set. Analyses of different types of errors
and sample output sentences demonstrate that the proposed method can
incorporate language level information, suggesting its effectiveness in
real-world applications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Span-based Semantic Parsing for Compositional Generalization. (arXiv:2009.06040v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1">Jonathan Herzig</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06040">
                                    <div class="article-summary-box-inner">
                                        <span>Despite the success of sequence-to-sequence (seq2seq) models in semantic
parsing, recent work has shown that they fail in compositional generalization,
i.e., the ability to generalize to new structures built of components observed
during training. In this work, we posit that a span-based parser should lead to
better compositional generalization. we propose SpanBasedSP, a parser that
predicts a span tree over an input utterance, explicitly encoding how partial
programs compose over spans in the input. SpanBasedSP extends Pasupat et al.
(2019) to be comparable to seq2seq models by (i) training from programs,
without access to gold trees, treating trees as latent variables, (ii) parsing
a class of non-projective trees through an extension to standard CKY. On
GeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong
seq2seq baselines on random splits, but dramatically improves performance
compared to baselines on splits that require compositional generalization: from
$61.0 \rightarrow 88.9$ average accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning. (arXiv:2106.06937v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Bill Yuchen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seyeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_X/0/1/0/all/0/1">Xiaoyang Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiang Ren</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06937">
                                    <div class="article-summary-box-inner">
                                        <span>Commonsense reasoning research has so far been limited to English. We aim to
evaluate and improve popular multilingual language models (ML-LMs) to help
advance commonsense reasoning (CSR) beyond English. We collect the Mickey
Corpus, consisting of 561k sentences in 11 different languages, which can be
used for analyzing and improving ML-LMs. We propose Mickey Probe, a
language-agnostic probing task for fairly evaluating the common sense of
popular ML-LMs across different languages. In addition, we also create two new
datasets, X-CSQA and X-CODAH, by translating their English versions to 15 other
languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense
reasoning. To improve the performance beyond English, we propose a simple yet
effective method -- multilingual contrastive pre-training (MCP). It
significantly enhances sentence representations, yielding a large performance
gain on both benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Memory-efficient Transformers via Top-$k$ Attention. (arXiv:2106.06899v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Ankit Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1">Guy Dar</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_S/0/1/0/all/0/1">Shaya Goodman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciprut_D/0/1/0/all/0/1">David Ciprut</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06899">
                                    <div class="article-summary-box-inner">
                                        <span>Following the success of dot-product attention in Transformers, numerous
approximations have been recently proposed to address its quadratic complexity
with respect to the input length. While these variants are memory and compute
efficient, it is not possible to directly use them with popular pre-trained
language models trained using vanilla attention, without an expensive
corrective pre-training stage. In this work, we propose a simple yet highly
accurate approximation for vanilla attention. We process the queries in chunks,
and for each query, compute the top-$k$ scores with respect to the keys. Our
approach offers several advantages: (a) its memory usage is linear in the input
size, similar to linear attention variants, such as Performer and RFA (b) it is
a drop-in replacement for vanilla attention that does not require any
corrective pre-training, and (c) it can also lead to significant memory savings
in the feed-forward layers after casting them into the familiar query-key-value
framework. We evaluate the quality of top-$k$ approximation for multi-head
attention layers on the Long Range Arena Benchmark, and for feed-forward layers
of T5 and UnifiedQA on multiple QA datasets. We show our approach leads to
accuracy that is nearly-identical to vanilla attention in multiple setups
including training from scratch, fine-tuning, and zero-shot inference.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Visualization Techniques to Enhance Automated Event Extraction. (arXiv:2106.06588v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Henn_S/0/1/0/all/0/1">Sophia Henn</a>, <a href="http://arxiv.org/find/cs/1/au:+Sticha_A/0/1/0/all/0/1">Abigail Sticha</a>, <a href="http://arxiv.org/find/cs/1/au:+Burley_T/0/1/0/all/0/1">Timothy Burley</a>, <a href="http://arxiv.org/find/cs/1/au:+Verdeja_E/0/1/0/all/0/1">Ernesto Verdeja</a>, <a href="http://arxiv.org/find/cs/1/au:+Brenner_P/0/1/0/all/0/1">Paul Brenner</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06588">
                                    <div class="article-summary-box-inner">
                                        <span>Robust visualization of complex data is critical for the effective use of NLP
for event classification, as the volume of data is large and the
high-dimensional structure of text makes data challenging to summarize
succinctly. In event extraction tasks in particular, visualization can aid in
understanding and illustrating the textual relationships from which machine
learning tools produce insights. Through our case study which seeks to identify
potential triggers of state-led mass killings from news articles using NLP, we
demonstrate how visualizations can aid in each stage, from exploratory analysis
of raw data, to machine learning training analysis, and finally post-inference
validation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Thinking Like Transformers. (arXiv:2106.06981v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1">Gail Weiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1">Yoav Goldberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1">Eran Yahav</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06981">
                                    <div class="article-summary-box-inner">
                                        <span>What is the computational model behind a Transformer? Where recurrent neural
networks have direct parallels in finite state machines, allowing clear
discussion and thought around architecture variants or trained models,
Transformers have no such familiar parallel. In this paper we aim to change
that, proposing a computational model for the transformer-encoder in the form
of a programming language. We map the basic components of a transformer-encoder
-- attention and feed-forward computation -- into simple primitives, around
which we form a programming language: the Restricted Access Sequence Processing
Language (RASP). We show how RASP can be used to program solutions to tasks
that could conceivably be learned by a Transformer, and how a Transformer can
be trained to mimic a RASP solution. In particular, we provide RASP programs
for histograms, sorting, and Dyck-languages. We further use our model to relate
their difficulty in terms of the number of required layers and attention heads:
analyzing a RASP program implies a maximum number of heads and layers necessary
to encode a task in a transformer. Finally, we see how insights gained from our
abstraction might be used to explain phenomena seen in recent works.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Impact of Encoding and Segmentation Strategies on End-to-End Simultaneous Speech Translation. (arXiv:2104.14470v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Ha Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1">Yannick Est&#xe8;ve</a>, <a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1">Laurent Besacier</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14470">
                                    <div class="article-summary-box-inner">
                                        <span>Boosted by the simultaneous translation shared task at IWSLT 2020, promising
end-to-end online speech translation approaches were recently proposed. They
consist in incrementally encoding a speech input (in a source language) and
decoding the corresponding text (in a target language) with the best possible
trade-off between latency and translation quality. This paper investigates two
key aspects of end-to-end simultaneous speech translation: (a) how to encode
efficiently the continuous speech flow, and (b) how to segment the speech flow
in order to alternate optimally between reading (R: encoding input) and writing
(W: decoding output) operations. We extend our previously proposed end-to-end
online decoding strategy and show that while replacing BLSTM by ULSTM encoding
degrades performance in offline mode, it actually improves both efficiency and
performance in online mode. We also measure the impact of different methods to
segment the speech signal (using fixed interval boundaries, oracle word
boundaries or randomly set boundaries) and show that our best end-to-end online
decoding strategy is surprisingly the one that alternates R/W operations on
fixed size blocks on our English-German speech translation setup.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP. (arXiv:2104.08620v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rozner_J/0/1/0/all/0/1">Josh Rozner</a>, <a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1">Christopher Potts</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1">Kyle Mahowald</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08620">
                                    <div class="article-summary-box-inner">
                                        <span>Cryptic crosswords, the dominant English-language crossword variety in the
United Kingdom, can be solved by expert humans using flexible, creative
intelligence and knowledge of language. Cryptic clues read like fluent natural
language, but they are adversarially composed of two parts: a definition and a
wordplay cipher requiring sub-word or character-level manipulations. As such,
they are a promising target for evaluating and advancing NLP systems that seek
to process language in more creative, human-like ways. We present a dataset of
cryptic crossword clues from a major newspaper that can be used as a benchmark
and train a sequence-to-sequence model to solve them. We also develop related
benchmarks that can guide development of approaches to this challenging task.
We show that performance can be substantially improved using a novel curriculum
learning approach in which the model is pre-trained on related tasks involving,
e.g, unscrambling words, before it is trained to solve cryptics. However, even
this curricular approach does not generalize to novel clue types in the way
that humans can, and so cryptic crosswords remain a challenge for NLP systems
and a potential source of future innovation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Speak or Chat with Me: End-to-End Spoken Language Understanding System with Flexible Inputs. (arXiv:2104.05752v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1">Sujeong Cha</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1">Wangrui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1">Hyun Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Phung_M/0/1/0/all/0/1">My Phung</a>, <a href="http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1">Michael Picheny</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1">Hong-Kwang Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1">Samuel Thomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Morais_E/0/1/0/all/0/1">Edmilson Morais</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05752">
                                    <div class="article-summary-box-inner">
                                        <span>A major focus of recent research in spoken language understanding (SLU) has
been on the end-to-end approach where a single model can predict intents
directly from speech inputs without intermediate transcripts. However, this
approach presents some challenges. First, since speech can be considered as
personally identifiable information, in some cases only automatic speech
recognition (ASR) transcripts are accessible. Second, intent-labeled speech
data is scarce. To address the first challenge, we propose a novel system that
can predict intents from flexible types of inputs: speech, ASR transcripts, or
both. We demonstrate strong performance for either modality separately, and
when both speech and ASR transcripts are available, through system combination,
we achieve better results than using a single input modality. To address the
second challenge, we leverage a semantically robust pre-trained BERT model and
adopt a cross-modal system that co-trains text embeddings and acoustic
embeddings in a shared latent space. We further enhance this system by
utilizing an acoustic module pre-trained on LibriSpeech and domain-adapting the
text module on our target datasets. Our experiments show significant advantages
for these pre-training and fine-tuning strategies, resulting in a system that
achieves competitive intent-classification performance on Snips SLU and Fluent
Speech Commands datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation. (arXiv:2106.03153v2 [eess.AS] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Min_D/0/1/0/all/0/1">Dongchan Min</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1">Dong Bok Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_E/0/1/0/all/0/1">Eunho Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03153">
                                    <div class="article-summary-box-inner">
                                        <span>With rapid progress in neural text-to-speech (TTS) models, personalized
speech generation is now in high demand for many applications. For practical
applicability, a TTS model should generate high-quality speech with only a few
audio samples from the given speaker, that are also short in length. However,
existing methods either require to fine-tune the model or achieve low
adaptation quality without fine-tuning. In this work, we propose StyleSpeech, a
new TTS model which not only synthesizes high-quality speech but also
effectively adapts to new speakers. Specifically, we propose Style-Adaptive
Layer Normalization (SALN) which aligns gain and bias of the text input
according to the style extracted from a reference speech audio. With SALN, our
model effectively synthesizes speech in the style of the target speaker even
from single speech audio. Furthermore, to enhance StyleSpeech&#x27;s adaptation to
speech from new speakers, we extend it to Meta-StyleSpeech by introducing two
discriminators trained with style prototypes, and performing episodic training.
The experimental results show that our models generate high-quality speech
which accurately follows the speaker&#x27;s voice with single short-duration (1-3
sec) speech audio, significantly outperforming baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Librispeech Transducer Model with Internal Language Model Prior Correction. (arXiv:2104.03006v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1">Albert Zeyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Merboldt_A/0/1/0/all/0/1">Andr&#xe9; Merboldt</a>, <a href="http://arxiv.org/find/cs/1/au:+Michel_W/0/1/0/all/0/1">Wilfried Michel</a>, <a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1">Ralf Schl&#xfc;ter</a>, <a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1">Hermann Ney</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03006">
                                    <div class="article-summary-box-inner">
                                        <span>We present our transducer model on Librispeech. We study variants to include
an external language model (LM) with shallow fusion and subtract an estimated
internal LM. This is justified by a Bayesian interpretation where the
transducer model prior is given by the estimated internal LM. The subtraction
of the internal LM gives us over 14% relative improvement over normal shallow
fusion. Our transducer has a separate probability distribution for the
non-blank labels which allows for easier combination with the external LM, and
easier estimation of the internal LM. We additionally take care of including
the end-of-sentence (EOS) probability of the external LM in the last blank
probability which further improves the performance. All our code and setups are
published.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Controllable Generation from Pre-trained Language Models via Inverse Prompting. (arXiv:2103.10685v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xu Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Da Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1">Qingyang Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhilin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10685">
                                    <div class="article-summary-box-inner">
                                        <span>Large-scale pre-trained language models have demonstrated strong capabilities
of generating realistic text. However, it remains challenging to control the
generation results. Previous approaches such as prompting are far from
sufficient, which limits the usage of language models. To tackle this
challenge, we propose an innovative method, inverse prompting, to better
control text generation. The core idea of inverse prompting is to use generated
text to inversely predict the prompt during beam search, which enhances the
relevance between the prompt and the generated text and provides better
controllability. Empirically, we pre-train a large-scale Chinese language model
to perform a systematic study using human evaluation on the tasks of
open-domain poem generation and open-domain long-form question answering. Our
results show that our proposed method substantially outperforms the baselines
and that our generation quality is close to human performance on some of the
tasks.

Narrators can try our poem generation demo at
https://pretrain.aminer.cn/apps/poetry.html, while our QA demo can be found at
https://pretrain.aminer.cn/app/qa. For researchers, the code is provided in
https://github.com/THUDM/InversePrompting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multilingual Neural Semantic Parsing for Low-Resourced Languages. (arXiv:2106.03469v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Menglin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Monti_E/0/1/0/all/0/1">Emilio Monti</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03469">
                                    <div class="article-summary-box-inner">
                                        <span>Multilingual semantic parsing is a cost-effective method that allows a single
model to understand different languages. However, researchers face a great
imbalance of availability of training data, with English being resource rich,
and other languages having much less data. To tackle the data limitation
problem, we propose using machine translation to bootstrap multilingual
training data from the more abundant English data. To compensate for the data
quality of machine translated training data, we utilize transfer learning from
pretrained multilingual encoders to further improve the model. To evaluate our
multilingual models on human-written sentences as opposed to machine translated
ones, we introduce a new multilingual semantic parsing dataset in English,
Italian and Japanese based on the Facebook Task Oriented Parsing (TOP) dataset.
We show that joint multilingual training with pretrained encoders substantially
outperforms our baselines on the TOP dataset and outperforms the
state-of-the-art model on the public NLMaps dataset. We also establish a new
baseline for zero-shot learning on the TOP dataset. We find that a semantic
parser trained only on English data achieves a zero-shot performance of 44.9%
exact-match accuracy on Italian sentences.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Detecting Local Insights from Global Labels: Supervised &amp; Zero-Shot Sequence Labeling via a Convolutional Decomposition. (arXiv:1906.01154v6 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Schmaltz_A/0/1/0/all/0/1">Allen Schmaltz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.01154">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a new, more actionable view of neural network interpretability and
data analysis by leveraging the remarkable matching effectiveness of
representations derived from deep networks, guided by an approach for
class-conditional feature detection. The decomposition of the filter-ngram
interactions of a convolutional neural network and a linear layer over a
pre-trained deep network yields a strong binary sequence labeler, with
flexibility in producing predictions at -- and defining loss functions for --
varying label granularities, from the fully-supervised sequence labeling
setting to the challenging zero-shot sequence labeling setting, in which we
seek token-level predictions but only have document-level labels for training.
From this sequence-labeling layer we derive dense representations of the input
that can then be matched to instances from training, or a support set with
known labels. Such introspection with inference-time decision rules provides a
means, in some settings, of making local updates to the model by altering the
labels or instances in the support set without re-training the full model.
Finally, we construct a particular K-nearest neighbors (K-NN) model from
matched exemplar representations that approximates the original model&#x27;s
predictions and is at least as effective a predictor with respect to the
ground-truth labels. This additionally yields interpretable heuristics at the
token level for determining when predictions are less likely to be reliable,
and for screening input dissimilar to the support set. In effect, we show that
we can transform the deep network into a simple weighting over exemplars and
associated labels, yielding an introspectable -- and modestly updatable --
version of the original model.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Russian News Clustering and Headline Selection Shared Task. (arXiv:2105.00981v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gusev_I/0/1/0/all/0/1">Ilya Gusev</a>, <a href="http://arxiv.org/find/cs/1/au:+Smurov_I/0/1/0/all/0/1">Ivan Smurov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00981">
                                    <div class="article-summary-box-inner">
                                        <span>This paper presents the results of the Russian News Clustering and Headline
Selection shared task. As a part of it, we propose the tasks of Russian news
event detection, headline selection, and headline generation. These tasks are
accompanied by datasets and baselines. The presented datasets for event
detection and headline selection are the first public Russian datasets for
their tasks. The headline generation dataset is based on clustering and
provides multiple reference headlines for every cluster, unlike the previous
datasets. Finally, the approaches proposed by the shared task participants are
reported and analyzed.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection. (arXiv:2104.09261v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Han Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1">Chunyan Miao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09261">
                                    <div class="article-summary-box-inner">
                                        <span>The existence of multiple datasets for sarcasm detection prompts us to apply
transfer learning to exploit their commonality. The adversarial neural transfer
(ANT) framework utilizes multiple loss terms that encourage the source-domain
and the target-domain feature distributions to be similar while optimizing for
domain-specific performance. However, these objectives may be in conflict,
which can lead to optimization difficulties and sometimes diminished transfer.
We propose a generalized latent optimization strategy that allows different
losses to accommodate each other and improves training dynamics. The proposed
method outperforms transfer learning and meta-learning baselines. In
particular, we achieve 10.02% absolute performance gain over the previous state
of the art on the iSarcasm dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SASICM A Multi-Task Benchmark For Subtext Recognition. (arXiv:2106.06944v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1">Hua Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1">Weikang Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1">Feng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1">Furao Shen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06944">
                                    <div class="article-summary-box-inner">
                                        <span>Subtext is a kind of deep semantics which can be acquired after one or more
rounds of expression transformation. As a popular way of expressing one&#x27;s
intentions, it is well worth studying. In this paper, we try to make computers
understand whether there is a subtext by means of machine learning. We build a
Chinese dataset whose source data comes from the popular social media (e.g.
Weibo, Netease Music, Zhihu, and Bilibili). In addition, we also build a
baseline model called SASICM to deal with subtext recognition. The F1 score of
SASICMg, whose pretrained model is GloVe, is as high as 64.37%, which is 3.97%
higher than that of BERT based model, 12.7% higher than that of traditional
methods on average, including support vector machine, logistic regression
classifier, maximum entropy classifier, naive bayes classifier and decision
tree and 2.39% higher than that of the state-of-the-art, including MARIN and
BTM. The F1 score of SASICMBERT, whose pretrained model is BERT, is 65.12%,
which is 0.75% higher than that of SASICMg. The accuracy rates of SASICMg and
SASICMBERT are 71.16% and 70.76%, respectively, which can compete with those of
other methods which are mentioned before.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective. (arXiv:2003.00330v7 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1">Luis C. Lamb</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcez_A/0/1/0/all/0/1">Artur Garcez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1">Marco Gori</a>, <a href="http://arxiv.org/find/cs/1/au:+Prates_M/0/1/0/all/0/1">Marcelo Prates</a>, <a href="http://arxiv.org/find/cs/1/au:+Avelar_P/0/1/0/all/0/1">Pedro Avelar</a>, <a href="http://arxiv.org/find/cs/1/au:+Vardi_M/0/1/0/all/0/1">Moshe Vardi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.00330">
                                    <div class="article-summary-box-inner">
                                        <span>Neural-symbolic computing has now become the subject of interest of both
academic and industry research laboratories. Graph Neural Networks (GNN) have
been widely used in relational and symbolic domains, with widespread
application of GNNs in combinatorial optimization, constraint satisfaction,
relational reasoning and other scientific domains. The need for improved
explainability, interpretability and trust of AI systems in general demands
principled methodologies, as suggested by neural-symbolic computing. In this
paper, we review the state-of-the-art on the use of GNNs as a model of
neural-symbolic computing. This includes the application of GNNs in several
domains as well as its relationship to current developments in neural-symbolic
computing.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue. (arXiv:2101.00151v2 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Hung Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1">Chinnadhurai Sankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1">Seungwhan Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1">Ahmad Beirami</a>, <a href="http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1">Alborz Geramifard</a>, <a href="http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1">Satwik Kottur</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00151">
                                    <div class="article-summary-box-inner">
                                        <span>A video-grounded dialogue system is required to understand both dialogue,
which contains semantic dependencies from turn to turn, and video, which
contains visual cues of spatial and temporal scene variations. Building such
dialogue systems is a challenging problem, involving various reasoning types on
both visual and language inputs. Existing benchmarks do not have enough
annotations to thoroughly analyze dialogue systems and understand their
capabilities and limitations in isolation. These benchmarks are also not
explicitly designed to minimise biases that models can exploit without actual
reasoning. To address these limitations, in this paper, we present DVD, a
Diagnostic Dataset for Video-grounded Dialogues. The dataset is designed to
contain minimal biases and has detailed annotations for the different types of
reasoning over the spatio-temporal space of video. Dialogues are synthesized
over multiple question turns, each of which is injected with a set of
cross-turn semantic relationships. We use DVD to analyze existing approaches,
providing interesting insights into their abilities and limitations. In total,
DVD is built from $11k$ CATER synthetic videos and contains $10$ instances of
$10$-round dialogues for each video, resulting in more than $100k$ dialogues
and $1M$ question-answer pairs. Our code and dataset are publicly available at
https://github.com/facebookresearch/DVDialogues.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Causal Effects of Linguistic Properties. (arXiv:2010.12919v5 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1">Reid Pryzant</a>, <a href="http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1">Dallas Card</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1">Dan Jurafsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1">Victor Veitch</a>, <a href="http://arxiv.org/find/cs/1/au:+Sridhar_D/0/1/0/all/0/1">Dhanya Sridhar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12919">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of using observational data to estimate the causal
effects of linguistic properties. For example, does writing a complaint
politely lead to a faster response time? How much will a positive product
review increase sales? This paper addresses two technical challenges related to
the problem before developing a practical method. First, we formalize the
causal quantity of interest as the effect of a writer&#x27;s intent, and establish
the assumptions necessary to identify this from observational data. Second, in
practice, we only have access to noisy proxies for the linguistic properties of
interest -- e.g., predictions from classifiers and lexicons. We propose an
estimator for this setting and prove that its bias is bounded when we perform
an adjustment for the text. Based on these results, we introduce TextCause, an
algorithm for estimating causal effects of linguistic properties. The method
leverages (1) distant supervision to improve the quality of noisy proxies, and
(2) a pre-trained language model (BERT) to adjust for the text. We show that
the proposed method outperforms related approaches when estimating the effect
of Amazon review sentiment on semi-simulated sales figures. Finally, we present
an applied case study investigating the effects of complaint politeness on
bureaucratic response times.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Can Transformer Language Models Predict Psychometric Properties?. (arXiv:2106.06849v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Laverghetta_A/0/1/0/all/0/1">Antonio Laverghetta Jr.</a>, <a href="http://arxiv.org/find/cs/1/au:+Nighojkar_A/0/1/0/all/0/1">Animesh Nighojkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirzakhalov_J/0/1/0/all/0/1">Jamshidbek Mirzakhalov</a>, <a href="http://arxiv.org/find/cs/1/au:+Licato_J/0/1/0/all/0/1">John Licato</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06849">
                                    <div class="article-summary-box-inner">
                                        <span>Transformer-based language models (LMs) continue to advance state-of-the-art
performance on NLP benchmark tasks, including tasks designed to mimic
human-inspired &quot;commonsense&quot; competencies. To better understand the degree to
which LMs can be said to have certain linguistic reasoning skills, researchers
are beginning to adapt the tools and concepts of the field of psychometrics.
But to what extent can the benefits flow in the other direction? I.e., can LMs
be of use in predicting what the psychometric properties of test items will be
when those items are given to human participants? We gather responses from
numerous human participants and LMs (transformer and non-transformer-based) on
a broad diagnostic test of linguistic competencies. We then use the responses
to calculate standard psychometric properties of the items in the diagnostic
test, using the human responses and the LM responses separately. We then
determine how well these two sets of predictions match. We find cases in which
transformer-based LMs predict psychometric properties consistently well in
certain categories but consistently poorly in others, thus providing new
insights into fundamental similarities and differences between human and LM
reasoning.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">InfoBehavior: Self-supervised Representation Learning for Ultra-long Behavior Sequence via Hierarchical Grouping. (arXiv:2106.06905v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Runshi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_P/0/1/0/all/0/1">Pengda Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Weigao Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1">Kefeng Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qiang Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06905">
                                    <div class="article-summary-box-inner">
                                        <span>E-commerce companies have to face abnormal sellers who sell potentially-risky
products. Typically, the risk can be identified by jointly considering product
content (e.g., title and image) and seller behavior. This work focuses on
behavior feature extraction as behavior sequences can provide valuable clues
for the risk discovery by reflecting the sellers&#x27; operation habits. Traditional
feature extraction techniques heavily depend on domain experts and adapt poorly
to new tasks. In this paper, we propose a self-supervised method InfoBehavior
to automatically extract meaningful representations from ultra-long raw
behavior sequences instead of the costly feature selection procedure.
InfoBehavior utilizes Bidirectional Transformer as feature encoder due to its
excellent capability in modeling long-term dependency. However, it is
intractable for commodity GPUs because the time and memory required by
Transformer grow quadratically with the increase of sequence length. Thus, we
propose a hierarchical grouping strategy to aggregate ultra-long raw behavior
sequences to length-processable high-level embedding sequences. Moreover, we
introduce two types of pretext tasks. Sequence-related pretext task defines a
contrastive-based training objective to correctly select the masked-out
coarse-grained/fine-grained behavior sequences against other &quot;distractor&quot;
behavior sequences; Domain-related pretext task designs a classification
training objective to correctly predict the domain-specific statistical results
of anomalous behavior. We show that behavior representations from the
pre-trained InfoBehavior can be directly used or integrated with features from
other side information to support a wide range of downstream tasks.
Experimental results demonstrate that InfoBehavior significantly improves the
performance of Product Risk Management and Intellectual Property Protection.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Explaining the Deep Natural Language Processing by Mining Textual Interpretable Features. (arXiv:2106.06697v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ventura_F/0/1/0/all/0/1">Francesco Ventura</a>, <a href="http://arxiv.org/find/cs/1/au:+Greco_S/0/1/0/all/0/1">Salvatore Greco</a>, <a href="http://arxiv.org/find/cs/1/au:+Apiletti_D/0/1/0/all/0/1">Daniele Apiletti</a>, <a href="http://arxiv.org/find/cs/1/au:+Cerquitelli_T/0/1/0/all/0/1">Tania Cerquitelli</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06697">
                                    <div class="article-summary-box-inner">
                                        <span>Despite the high accuracy offered by state-of-the-art deep natural-language
models (e.g. LSTM, BERT), their application in real-life settings is still
widely limited, as they behave like a black-box to the end-user. Hence,
explainability is rapidly becoming a fundamental requirement of
future-generation data-driven systems based on deep-learning approaches.
Several attempts to fulfill the existing gap between accuracy and
interpretability have been done. However, robust and specialized xAI
(Explainable Artificial Intelligence) solutions tailored to deep
natural-language models are still missing. We propose a new framework, named
T-EBAnO, which provides innovative prediction-local and class-based
model-global explanation strategies tailored to black-box deep natural-language
models. Given a deep NLP model and the textual input data, T-EBAnO provides an
objective, human-readable, domain-specific assessment of the reasons behind the
automatic decision-making process. Specifically, the framework extracts sets of
interpretable features mining the inner knowledge of the model. Then, it
quantifies the influence of each feature during the prediction process by
exploiting the novel normalized Perturbation Influence Relation index at the
local level and the novel Global Absolute Influence and Global Relative
Influence indexes at the global level. The effectiveness and the quality of the
local and global explanations obtained with T-EBAnO are proved on (i) a
sentiment analysis task performed by a fine-tuned BERT model, and (ii) a toxic
comment classification task performed by an LSTM model.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP. (arXiv:2106.06830v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anthony Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gudipati_P/0/1/0/all/0/1">Pallavi Gudipati</a>, <a href="http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1">Shayne Longpre</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1">Xiao Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sameer Singh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06830">
                                    <div class="article-summary-box-inner">
                                        <span>Retrieval is a core component for open-domain NLP tasks. In open-domain
tasks, multiple entities can share a name, making disambiguation an inherent
yet under-explored problem. We propose an evaluation benchmark for assessing
the entity disambiguation capabilities of these retrievers, which we call
Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection
of entities that share a name along with queries about those entities. By
covering the set of entities for polysemous names, AmbER sets act as a
challenging test of entity disambiguation. We create AmbER sets for three
popular open-domain tasks: fact checking, slot filling, and question answering,
and evaluate a diverse set of retrievers. We find that the retrievers exhibit
popularity bias, significantly under-performing on rarer entities that share a
name, e.g., they are twice as likely to retrieve erroneous documents on queries
for the less popular entity under the same name. These experiments on AmbER
sets show their utility as an evaluation tool and highlight the weaknesses of
popular retrieval systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MultiWOZ 2.3: A multi-domain task-oriented dialogue dataset enhanced with annotation corrections and co-reference annotation. (arXiv:2010.05594v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1">Ting Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Ximing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1">Ryuichi Takanobu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1">Yixin Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chongxuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1">Dazhen Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1">Wei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Minlie Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05594">
                                    <div class="article-summary-box-inner">
                                        <span>Task-oriented dialogue systems have made unprecedented progress with multiple
state-of-the-art (SOTA) models underpinned by a number of publicly available
MultiWOZ datasets. Dialogue state annotations are error-prone, leading to
sub-optimal performance. Various efforts have been put in rectifying the
annotation errors presented in the original MultiWOZ dataset. In this paper, we
introduce MultiWOZ 2.3, in which we differentiate incorrect annotations in
dialogue acts from dialogue states, identifying a lack of co-reference when
publishing the updated dataset. To ensure consistency between dialogue acts and
dialogue states, we implement co-reference features and unify annotations of
dialogue acts and dialogue states. We update the state of the art performance
of natural language understanding and dialogue state tracking on MultiWOZ 2.3,
where the results show significant improvements than on previous versions of
MultiWOZ datasets (2.0-2.2).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Direct Simultaneous Speech-to-Text Translation Assisted by Synchronized Streaming ASR. (arXiv:2106.06636v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junkun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Mingbo Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1">Renjie Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Liang Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06636">
                                    <div class="article-summary-box-inner">
                                        <span>Simultaneous speech-to-text translation is widely useful in many scenarios.
The conventional cascaded approach uses a pipeline of streaming ASR followed by
simultaneous MT, but suffers from error propagation and extra latency. To
alleviate these issues, recent efforts attempt to directly translate the source
speech into target text simultaneously, but this is much harder due to the
combination of two separate tasks. We instead propose a new paradigm with the
advantages of both cascaded and end-to-end approaches. The key idea is to use
two separate, but synchronized, decoders on streaming ASR and direct
speech-to-text translation (ST), respectively, and the intermediate results of
ASR guide the decoding policy of (but is not fed as input to) ST. During
training time, we use multitask learning to jointly learn these two tasks with
a shared encoder. En-to-De and En-to-Es experiments on the MuSTC dataset
demonstrate that our proposed technique achieves substantially better
translation quality at similar levels of latency.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Controllable Model of Grounded Response Generation. (arXiv:2005.00613v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zeqiu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1">Michel Galley</a>, <a href="http://arxiv.org/find/cs/1/au:+Brockett_C/0/1/0/all/0/1">Chris Brockett</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yizhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xiang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Quirk_C/0/1/0/all/0/1">Chris Quirk</a>, <a href="http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1">Rik Koncel-Kedziorski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1">Mari Ostendorf</a>, <a href="http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1">Bill Dolan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00613">
                                    <div class="article-summary-box-inner">
                                        <span>Current end-to-end neural conversation models inherently lack the flexibility
to impose semantic control in the response generation process, often resulting
in uninteresting responses. Attempts to boost informativeness alone come at the
expense of factual accuracy, as attested by pretrained language models&#x27;
propensity to &quot;hallucinate&quot; facts. While this may be mitigated by access to
background knowledge, there is scant guarantee of relevance and informativeness
in generated responses. We propose a framework that we call controllable
grounded response generation (CGRG), in which lexical control phrases are
either provided by a user or automatically extracted by a control phrase
predictor from dialogue context and grounding knowledge. Quantitative and
qualitative results show that, using this framework, a transformer based model
with a novel inductive attention mechanism, trained on a conversation-like
Reddit dataset, outperforms strong generation baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Predicting the Ordering of Characters in Japanese Historical Documents. (arXiv:2106.06786v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1">Alex Lamb</a>, <a href="http://arxiv.org/find/cs/1/au:+Clanuwat_T/0/1/0/all/0/1">Tarin Clanuwat</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Siyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Bober_Irizar_M/0/1/0/all/0/1">Mikel Bober-Irizar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kitamoto_A/0/1/0/all/0/1">Asanobu Kitamoto</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06786">
                                    <div class="article-summary-box-inner">
                                        <span>Japan is a unique country with a distinct cultural heritage, which is
reflected in billions of historical documents that have been preserved.
However, the change in Japanese writing system in 1900 made these documents
inaccessible for the general public. A major research project has been to make
these historical documents accessible and understandable. An increasing amount
of research has focused on the character recognition task and the location of
characters on image, yet less research has focused on how to predict the
sequential ordering of the characters. This is because sequence in classical
Japanese is very different from modern Japanese. Ordering characters into a
sequence is important for making the document text easily readable and
searchable. Additionally, it is a necessary step for any kind of natural
language processing on the data (e.g. machine translation, language modeling,
and word embeddings). We explore a few approaches to the task of predicting the
sequential ordering of the characters: one using simple hand-crafted rules,
another using hand-crafted rules with adaptive thresholds, and another using a
deep recurrent sequence model trained with teacher forcing. We provide a
quantitative and qualitative comparison of these techniques as well as their
distinct trade-offs. Our best-performing system has an accuracy of 98.65\% and
has a perfect accuracy on 49\% of the books in our dataset, suggesting that the
technique is able to predict the order of the characters well enough for many
tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Compression of Deep Learning Models for Text: A Survey. (arXiv:2008.05221v4 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1">Manish Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Puneet Agrawal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.05221">
                                    <div class="article-summary-box-inner">
                                        <span>In recent years, the fields of natural language processing (NLP) and
information retrieval (IR) have made tremendous progress thanksto deep learning
models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and
Long Short-Term Memory (LSTMs)networks, and Transformer [120] based models like
Bidirectional Encoder Representations from Transformers (BERT) [24],
GenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network
(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer
transformer (T5) [95], T-NLG [98] and GShard [63]. But these models are
humongous in size. On the other hand,real world applications demand small model
size, low response times and low computational power wattage. In this survey,
wediscuss six different types of methods (Pruning, Quantization, Knowledge
Distillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic
Transformer based methods) for compression of such models to enable their
deployment in real industry NLP projects.Given the critical need of building
applications with efficient and small models, and the large amount of recently
published work inthis area, we believe that this survey organizes the plethora
of work done by the &#x27;deep learning for NLP&#x27; community in the past fewyears and
presents it as a coherent story.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. (arXiv:2106.06909v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guoguo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_S/0/1/0/all/0/1">Shuzhou Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guanbo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1">Jiayu Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei-Qiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1">Chao Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1">Dan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Povey_D/0/1/0/all/0/1">Daniel Povey</a>, <a href="http://arxiv.org/find/cs/1/au:+Trmal_J/0/1/0/all/0/1">Jan Trmal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Mingjie Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Khudanpur_S/0/1/0/all/0/1">Sanjeev Khudanpur</a>, <a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1">Shinji Watanabe</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shuaijiang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1">Wei Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiangang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1">Xuchen Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yujun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1">Zhao You</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zhiyong Yan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06909">
                                    <div class="article-summary-box-inner">
                                        <span>This paper introduces GigaSpeech, an evolving, multi-domain English speech
recognition corpus with 10,000 hours of high quality labeled audio suitable for
supervised training, and 40,000 hours of total audio suitable for
semi-supervised and unsupervised training. Around 40,000 hours of transcribed
audio is first collected from audiobooks, podcasts and YouTube, covering both
read and spontaneous speaking styles, and a variety of topics, such as arts,
science, sports, etc. A new forced alignment and segmentation pipeline is
proposed to create sentence segments suitable for speech recognition training,
and to filter out segments with low-quality transcription. For system training,
GigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h,
and 10000h. For our 10,000-hour XL training subset, we cap the word error rate
at 4% during the filtering/validation stage, and for all our other smaller
training subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the
other hand, are re-processed by professional human transcribers to ensure high
transcription quality. Baseline systems are provided for popular speech
recognition toolkits, namely Athena, ESPnet, Kaldi and Pika.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Leveraging Pre-trained Language Model for Speech Sentiment Analysis. (arXiv:2106.06598v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shon_S/0/1/0/all/0/1">Suwon Shon</a>, <a href="http://arxiv.org/find/cs/1/au:+Brusco_P/0/1/0/all/0/1">Pablo Brusco</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jing Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1">Kyu J. Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1">Shinji Watanabe</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06598">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we explore the use of pre-trained language models to learn
sentiment information of written texts for speech sentiment analysis. First, we
investigate how useful a pre-trained language model would be in a 2-step
pipeline approach employing Automatic Speech Recognition (ASR) and
transcripts-based sentiment analysis separately. Second, we propose a pseudo
label-based semi-supervised training strategy using a language model on an
end-to-end speech sentiment approach to take advantage of a large, but
unlabeled speech dataset for training. Although spoken and written texts have
different linguistic characteristics, they can complement each other in
understanding sentiment. Therefore, the proposed system can not only model
acoustic characteristics to bear sentiment-specific information in speech
signals, but learn latent information to carry sentiments in the text
representation. In these experiments, we demonstrate the proposed approaches
improve F1 scores consistently compared to systems without a language model.
Moreover, we also show that the proposed framework can reduce 65% of human
supervision by leveraging a large amount of data without human sentiment
annotation and boost performance in a low-resource condition where the human
sentiment annotation is not available enough.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sample-efficient Linguistic Generalizations through Program Synthesis: Experiments with Phonology Problems. (arXiv:2106.06566v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vaduguru_S/0/1/0/all/0/1">Saujas Vaduguru</a>, <a href="http://arxiv.org/find/cs/1/au:+Sathe_A/0/1/0/all/0/1">Aalok Sathe</a>, <a href="http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1">Monojit Choudhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1">Dipti Misra Sharma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06566">
                                    <div class="article-summary-box-inner">
                                        <span>Neural models excel at extracting statistical patterns from large amounts of
data, but struggle to learn patterns or reason about language from only a few
examples. In this paper, we ask: Can we learn explicit rules that generalize
well from only a few examples? We explore this question using program
synthesis. We develop a synthesis model to learn phonology rules as programs in
a domain-specific language. We test the ability of our models to generalize
from few training examples using our new dataset of problems from the
Linguistics Olympiad, a challenging set of tasks that require strong linguistic
reasoning ability. In addition to being highly sample-efficient, our approach
generates human-readable programs, and allows control over the generalizability
of the learnt programs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces. (arXiv:2106.06964v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1">Alexey Tikhonov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06964">
                                    <div class="article-summary-box-inner">
                                        <span>Pre-trained word representations became a key component in many NLP tasks.
However, the global geometry of the word embeddings remains poorly understood.
In this paper, we demonstrate that a typical word embeddings cloud is shaped as
a high-dimensional simplex with interpretable vertices and propose a simple yet
effective method for enumeration of these vertices. We show that the proposed
method can detect and describe vertices of the simplex for GloVe and fasttext
spaces.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Extracting Summary Knowledge Graphs from Long Documents. (arXiv:2009.09162v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zeqiu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1">Rik Koncel-Kedziorski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1">Mari Ostendorf</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hannaneh Hajishirzi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09162">
                                    <div class="article-summary-box-inner">
                                        <span>Knowledge graphs capture entities and relations from long documents and can
facilitate reasoning in many downstream applications. Extracting compact
knowledge graphs containing only salient entities and relations is important
but challenging for understanding and summarizing long documents. We introduce
a new text-to-graph task of predicting summarized knowledge graphs from long
documents. We develop a dataset of 200k document/graph pairs using automatic
and human annotations. We also develop strong baselines for this task based on
graph learning and text summarization, and provide quantitative and qualitative
studies of their effect.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Assessing Multilingual Fairness in Pre-trained Multimodal Representations. (arXiv:2106.06683v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jialu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Eric Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06683">
                                    <div class="article-summary-box-inner">
                                        <span>Recently pre-trained multimodal models, such as CLIP, have received a surge
of attention for their exceptional capabilities towards connecting images and
natural language. The textual representations in English can be desirably
transferred to multilingualism and support promising downstream multimodal
tasks for different languages. Nevertheless, previous fairness discourse in
vision-and-language learning mainly focuses on monolingual representational
biases, and rarely scrutinizes the principles of multilingual fairness in this
multimodal setting, where one language is equated to a group of individuals and
images provide the universal grounding for bridging different languages.

In this paper, we provide a nuanced understanding of individual fairness and
group fairness by viewing language as the recipient of fairness notions. We
define new fairness notions within multilingual context and analytically
articulate that, pre-trained vision-and-language representations are
individually fair across languages but not guaranteed to group fairness.
Furthermore, we conduct extensive experiments to explore the prevalent group
disparity across languages and protected groups including race, gender and age.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sentiment Analysis of Covid-19 Tweets using Evolutionary Classification-Based LSTM Model. (arXiv:2106.06910v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1">Arunava Kumar Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Sourav Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolya_A/0/1/0/all/0/1">Anup Kumar Kolya</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06910">
                                    <div class="article-summary-box-inner">
                                        <span>As the Covid-19 outbreaks rapidly all over the world day by day and also
affects the lives of million, a number of countries declared complete lock-down
to check its intensity. During this lockdown period, social media plat-forms
have played an important role to spread information about this pandemic across
the world, as people used to express their feelings through the social
networks. Considering this catastrophic situation, we developed an experimental
approach to analyze the reactions of people on Twitter taking into ac-count the
popular words either directly or indirectly based on this pandemic. This paper
represents the sentiment analysis on collected large number of tweets on
Coronavirus or Covid-19. At first, we analyze the trend of public sentiment on
the topics related to Covid-19 epidemic using an evolutionary classification
followed by the n-gram analysis. Then we calculated the sentiment ratings on
collected tweet based on their class. Finally, we trained the long-short term
network using two types of rated tweets to predict sentiment on Covid-19 data
and obtained an overall accuracy of 84.46%.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1">Michihiro Yasunaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Percy Liang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06600">
                                    <div class="article-summary-box-inner">
                                        <span>We consider repair tasks: given a critic (e.g., compiler) that assesses the
quality of an input, the goal is to train a fixer that converts a bad example
(e.g., code with syntax errors) into a good one (e.g., code with no errors).
Existing works create training data consisting of (bad, good) pairs by
corrupting good examples using heuristics (e.g., dropping tokens). However,
fixers trained on this synthetically-generated data do not extrapolate well to
the real distribution of bad inputs. To bridge this gap, we propose a new
training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use
the critic to check a fixer&#x27;s output on real bad inputs and add good (fixed)
outputs to the training data, and (ii) we train a breaker to generate realistic
bad code from good code. Based on these ideas, we iteratively update the
breaker and the fixer while using them in conjunction to generate more paired
data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new
dataset we introduce where the goal is to repair Python code with AST parse
errors; and DeepFix, where the goal is to repair C code with compiler errors.
BIFI outperforms existing methods, obtaining 90.5% repair accuracy on
GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not
require any labeled data; we hope it will be a strong starting point for
unsupervised learning of various repair tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Combinatory Constituency Parsing. (arXiv:2106.06689v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhousi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Longtu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Imankulova_A/0/1/0/all/0/1">Aizhan Imankulova</a>, <a href="http://arxiv.org/find/cs/1/au:+Komachi_M/0/1/0/all/0/1">Mamoru Komachi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06689">
                                    <div class="article-summary-box-inner">
                                        <span>We propose two fast neural combinatory models for constituency parsing:
binary and multi-branching. Our models decompose the bottom-up parsing process
into 1) classification of tags, labels, and binary orientations or chunks and
2) vector composition based on the computed orientations or chunks. These
models have theoretical sub-quadratic complexity and empirical linear
complexity. The binary model achieves an F1 score of 92.54 on Penn Treebank,
speeding at 1327.2 sents/sec. Both the models with XLNet provide near
state-of-the-art accuracies for English. Syntactic branching tendency and
headedness of a language are observed during the training and inference
processes for Penn Treebank, Chinese Treebank, and Keyaki Treebank (Japanese).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Engineering Knowledge Graph from Patent Database. (arXiv:2106.06739v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Siddharth_L/0/1/0/all/0/1">L Siddharth</a>, <a href="http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1">Lucienne T.M. Blessing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wood_K/0/1/0/all/0/1">Kristin L. Wood</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jianxi Luo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06739">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a large, scalable engineering knowledge graph, comprising sets of
(entity, relationship, entity) triples that are real-world engineering facts
found in the patent database. We apply a set of rules based on the syntactic
and lexical properties of claims in a patent document to extract facts. We
aggregate these facts within each patent document and integrate the aggregated
sets of facts across the patent database to obtain the engineering knowledge
graph. Such a knowledge graph is expected to support inference, reasoning, and
recalling in various engineering tasks. The knowledge graph has a greater size
and coverage in comparison with the previously used knowledge graphs and
semantic networks in the engineering literature.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-sentence Neural Language Models for Conversational Speech Recognition. (arXiv:2106.06922v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1">Shih-Hsuan Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_T/0/1/0/all/0/1">Tien-Hong Lo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Berlin Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06922">
                                    <div class="article-summary-box-inner">
                                        <span>An important research direction in automatic speech recognition (ASR) has
centered around the development of effective methods to rerank the output
hypotheses of an ASR system with more sophisticated language models (LMs) for
further gains. A current mainstream school of thoughts for ASR N-best
hypothesis reranking is to employ a recurrent neural network (RNN)-based LM or
its variants, with performance superiority over the conventional n-gram LMs
across a range of ASR tasks. In real scenarios such as a long conversation, a
sequence of consecutive sentences may jointly contain ample cues of
conversation-level information such as topical coherence, lexical entrainment
and adjacency pairs, which however remains to be underexplored. In view of
this, we first formulate ASR N-best reranking as a prediction problem, putting
forward an effective cross-sentence neural LM approach that reranks the ASR
N-best hypotheses of an upcoming sentence by taking into consideration the word
usage in its precedent sentences. Furthermore, we also explore to extract
task-specific global topical information of the cross-sentence history in an
unsupervised manner for better ASR performance. Extensive experiments conducted
on the AMI conversational benchmark corpus indicate the effectiveness and
feasibility of our methods in comparison to several state-of-the-art reranking
methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Don&#x27;t Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data. (arXiv:2106.06875v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bhatnagar_R/0/1/0/all/0/1">Rajat Bhatnagar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganesh_A/0/1/0/all/0/1">Ananya Ganesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1">Katharina Kann</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06875">
                                    <div class="article-summary-box-inner">
                                        <span>High-performing machine translation (MT) systems can help overcome language
barriers while making it possible for everyone to communicate and use language
technologies in the language of their choice. However, such systems require
large amounts of parallel sentences for training, and translators can be
difficult to find and expensive. Here, we present a data collection strategy
for MT which, in contrast, is cheap and simple, as it does not require
bilingual speakers. Based on the insight that humans pay specific attention to
movements, we use graphics interchange formats (GIFs) as a pivot to collect
parallel sentences from monolingual annotators. We use our strategy to collect
data in Hindi, Tamil and English. As a baseline, we also collect data using
images as a pivot. We perform an intrinsic evaluation by manually evaluating a
subset of the sentence pairs and an extrinsic evaluation by finetuning mBART on
the collected data. We find that sentences collected via GIFs are indeed of
higher quality.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Every Bite Is an Experience: Key Point Analysis of Business Reviews. (arXiv:2106.06758v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bar_Haim_R/0/1/0/all/0/1">Roy Bar-Haim</a>, <a href="http://arxiv.org/find/cs/1/au:+Eden_L/0/1/0/all/0/1">Lilach Eden</a>, <a href="http://arxiv.org/find/cs/1/au:+Kantor_Y/0/1/0/all/0/1">Yoav Kantor</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedman_R/0/1/0/all/0/1">Roni Friedman</a>, <a href="http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1">Noam Slonim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06758">
                                    <div class="article-summary-box-inner">
                                        <span>Previous work on review summarization focused on measuring the sentiment
toward the main aspects of the reviewed product or business, or on creating a
textual summary. These approaches provide only a partial view of the data:
aspect-based sentiment summaries lack sufficient explanation or justification
for the aspect rating, while textual summaries do not quantify the significance
of each element, and are not well-suited for representing conflicting views.
Recently, Key Point Analysis (KPA) has been proposed as a summarization
framework that provides both textual and quantitative summary of the main
points in the data. We adapt KPA to review data by introducing Collective Key
Point Mining for better key point extraction; integrating sentiment analysis
into KPA; identifying good key point candidates for review summaries; and
leveraging the massive amount of available reviews and their metadata. We show
empirically that these novel extensions of KPA substantially improve its
performance. We demonstrate that promising results can be achieved without any
domain-specific annotation, while human supervision can lead to further
improvement.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Incorporating External POS Tagger for Punctuation Restoration. (arXiv:2106.06731v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1">Ning Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Boxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinfeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiangyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhouhan Lin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06731">
                                    <div class="article-summary-box-inner">
                                        <span>Punctuation restoration is an important post-processing step in automatic
speech recognition. Among other kinds of external information, part-of-speech
(POS) taggers provide informative tags, suggesting each input token&#x27;s syntactic
role, which has been shown to be beneficial for the punctuation restoration
task. In this work, we incorporate an external POS tagger and fuse its
predicted labels into the existing language model to provide syntactic
information. Besides, we propose sequence boundary sampling (SBS) to learn
punctuation positions more efficiently as a sequence tagging task. Experimental
results show that our methods can consistently obtain performance gains and
achieve a new state-of-the-art on the common IWSLT benchmark. Further ablation
studies illustrate that both large pre-trained language models and the external
POS tagger take essential parts to improve the model&#x27;s performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Pseudo Label-wise Attention Network for Automatic ICD Coding. (arXiv:2106.06822v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yifan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1">Min Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Ying Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Min Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06822">
                                    <div class="article-summary-box-inner">
                                        <span>Automatic International Classification of Diseases (ICD) coding is defined as
a kind of text multi-label classification problem, which is difficult because
the number of labels is very large and the distribution of labels is
unbalanced. The label-wise attention mechanism is widely used in automatic ICD
coding because it can assign weights to every word in full Electronic Medical
Records (EMR) for different ICD codes. However, the label-wise attention
mechanism is computational redundant and costly. In this paper, we propose a
pseudo label-wise attention mechanism to tackle the problem. Instead of
computing different attention modes for different ICD codes, the pseudo
label-wise attention mechanism automatically merges similar ICD codes and
computes only one attention mode for the similar ICD codes, which greatly
compresses the number of attention modes and improves the predicted accuracy.
In addition, we apply a more convenient and effective way to obtain the ICD
vectors, and thus our model can predict new ICD codes by calculating the
similarities between EMR vectors and ICD vectors. Extensive experiments show
the superior performance of our model. On the public MIMIC-III dataset and
private Xiangya dataset, our model achieves micro f1 of 0.575 and 0.796,
respectively, which outperforms other competing models. Furthermore, we verify
the ability of our model in predicting new ICD codes. The case study shows how
pseudo label-wise attention works, and demonstrates the effectiveness of pseudo
label-wise attention mechanism.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair Coherence Scoring. (arXiv:2106.06719v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1">Linzi Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1">Giuseppe Carenini</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06719">
                                    <div class="article-summary-box-inner">
                                        <span>Dialogue topic segmentation is critical in several dialogue modeling
problems. However, popular unsupervised approaches only exploit surface
features in assessing topical coherence among utterances. In this work, we
address this limitation by leveraging supervisory signals from the
utterance-pair coherence scoring task. First, we present a simple yet effective
strategy to generate a training corpus for utterance-pair coherence scoring.
Then, we train a BERT-based neural utterance-pair coherence model with the
obtained training corpus. Finally, such model is used to measure the topical
relevance between utterances, acting as the basis of the segmentation
inference. Experiments on three public datasets in English and Chinese
demonstrate that our proposal outperforms the state-of-the-art baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Sentence-level Hierarchical BERT Model for Document Classification with Limited Labelled Data. (arXiv:2106.06738v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jinghui Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Henchion_M/0/1/0/all/0/1">Maeve Henchion</a>, <a href="http://arxiv.org/find/cs/1/au:+Bacher_I/0/1/0/all/0/1">Ivan Bacher</a>, <a href="http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1">Brian Mac Namee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06738">
                                    <div class="article-summary-box-inner">
                                        <span>Training deep learning models with limited labelled data is an attractive
scenario for many NLP tasks, including document classification. While with the
recent emergence of BERT, deep learning language models can achieve reasonably
good performance in document classification with few labelled instances, there
is a lack of evidence in the utility of applying BERT-like models on long
document classification. This work introduces a long-text-specific model -- the
Hierarchical BERT Model (HBM) -- that learns sentence-level features of the
text and works well in scenarios with limited labelled data. Various evaluation
experiments have demonstrated that HBM can achieve higher performance in
document classification than the previous state-of-the-art methods with only 50
to 200 labelled instances, especially when documents are long. Also, as an
extra benefit of HBM, the salient sentences identified by learned HBM are
useful as explanations for labelling documents based on a user study.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.CV"">cs.CV updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution Homography Estimation. (arXiv:2106.04067v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1">Ruizhi Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1">Gaochang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuemei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Ying Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yebin Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04067">
                                    <div class="article-summary-box-inner">
                                        <span>Cross-resolution image alignment is a key problem in multiscale gigapixel
photography, which requires to estimate homography matrix using images with
large resolution gap. Existing deep homography methods concatenate the input
images or features, neglecting the explicit formulation of correspondences
between them, which leads to degraded accuracy in cross-resolution challenges.
In this paper, we consider the cross-resolution homography estimation as a
multimodal problem, and propose a local transformer network embedded within a
multiscale structure to explicitly learn correspondences between the multimodal
inputs, namely, input images with different resolutions. The proposed local
transformer adopts a local attention map specifically for each position in the
feature. By combining the local transformer with the multiscale structure, the
network is able to capture long-short range correspondences efficiently and
accurately. Experiments on both the MS-COCO dataset and the real-captured
cross-resolution dataset show that the proposed network outperforms existing
state-of-the-art feature-based and deep-learning-based homography estimation
methods, and is able to accurately align images under $10\times$ resolution
gap.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-Subject Domain Adaptation for Multi-Frame EEG Images. (arXiv:2106.06769v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junfu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bi Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06769">
                                    <div class="article-summary-box-inner">
                                        <span>Working memory (WM) is a basic part of human cognition, which plays an
important role in the study of human cognitive load. Among various brain
imaging techniques, electroencephalography has shown its advantage on easy
access and reliability. However, one of the critical challenges is that
individual difference may cause the ineffective results, especially when the
established model meets an unfamiliar subject. In this work, we propose a
cross-subject deep adaptation model with spatial attention (CS-DASA) to
generalize the workload classifications across subjects. First, we transform
time-series EEG data into multi-frame EEG images incorporating more
spatio-temporal information. First, the subject-shared module in CS-DASA
receives multi-frame EEG image data from both source and target subjects and
learns the common feature representations. Then, in subject-specific module,
the maximum mean discrepancy is implemented to measure the domain distribution
divergence in a reproducing kernel Hilbert space, which can add an effective
penalty loss for domain adaptation. Additionally, the subject-to-subject
spatial attention mechanism is employed to focus on the most discriminative
spatial feature in EEG image data. Experiments conducted on a public WM EEG
dataset containing 13 subjects show that the proposed model is capable of
achieve better performance than existing state-of-the art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An Interaction-based Convolutional Neural Network (ICNN) Towards Better Understanding of COVID-19 X-ray Images. (arXiv:2106.06911v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1">Shaw-Hwa Lo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yiqiao Yin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06911">
                                    <div class="article-summary-box-inner">
                                        <span>The field of Explainable Artificial Intelligence (XAI) aims to build
explainable and interpretable machine learning (or deep learning) methods
without sacrificing prediction performance. Convolutional Neural Networks
(CNNs) have been successful in making predictions, especially in image
classification. However, these famous deep learning models use tens of millions
of parameters based on a large number of pre-trained filters which have been
repurposed from previous data sets. We propose a novel Interaction-based
Convolutional Neural Network (ICNN) that does not make assumptions about the
relevance of local information. Instead, we use a model-free Influence Score
(I-score) to directly extract the influential information from images to form
important variable modules. We demonstrate that the proposed method produces
state-of-the-art prediction performance of 99.8% on a real-world data set
classifying COVID-19 Chest X-ray images without sacrificing the explanatory
power of the model. This proposed design can efficiently screen COVID-19
patients before human diagnosis, and will be the benchmark for addressing
future XAI problems in large-scale data sets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-Ensembling Contrastive Learning for Semi-Supervised Medical Image Segmentation. (arXiv:2105.12924v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1">Jinxi Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuowei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenji Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1">Qing Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaoting Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12924">
                                    <div class="article-summary-box-inner">
                                        <span>Deep learning has demonstrated significant improvements in medical image
segmentation using a sufficiently large amount of training data with manual
labels. Acquiring well-representative labels requires expert knowledge and
exhaustive labors. In this paper, we aim to boost the performance of
semi-supervised learning for medical image segmentation with limited labels
using a self-ensembling contrastive learning technique. To this end, we propose
to train an encoder-decoder network at image-level with small amounts of
labeled images, and more importantly, we learn latent representations directly
at feature-level by imposing contrastive loss on unlabeled images. This method
strengthens intra-class compactness and inter-class separability, so as to get
a better pixel classifier. Moreover, we devise a student encoder for online
learning and an exponential moving average version of it, called teacher
encoder, to improve the performance iteratively in a self-ensembling manner. To
construct contrastive samples with unlabeled images, two sampling strategies
that exploit structure similarity across medical images and utilize
pseudo-labels for construction, termed region-aware and anatomical-aware
contrastive sampling, are investigated. We conduct extensive experiments on an
MRI and a CT segmentation dataset and demonstrate that in a limited label
setting, the proposed method achieves state-of-the-art performance. Moreover,
the anatomical-aware strategy that prepares contrastive samples on-the-fly
using pseudo-labels realizes better contrastive regularization on feature
representations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DyGLIP: A Dynamic Graph Model with Link Prediction for Accurate Multi-Camera Multiple Object Tracking. (arXiv:2106.06856v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Quach_K/0/1/0/all/0/1">Kha Gia Quach</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1">Pha Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Huu Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1">Thanh-Dat Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1">Chi Nhan Duong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1">Minh-Triet Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1">Khoa Luu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06856">
                                    <div class="article-summary-box-inner">
                                        <span>Multi-Camera Multiple Object Tracking (MC-MOT) is a significant computer
vision problem due to its emerging applicability in several real-world
applications. Despite a large number of existing works, solving the data
association problem in any MC-MOT pipeline is arguably one of the most
challenging tasks. Developing a robust MC-MOT system, however, is still highly
challenging due to many practical issues such as inconsistent lighting
conditions, varying object movement patterns, or the trajectory occlusions of
the objects between the cameras. To address these problems, this work,
therefore, proposes a new Dynamic Graph Model with Link Prediction (DyGLIP)
approach to solve the data association task. Compared to existing methods, our
new model offers several advantages, including better feature representations
and the ability to recover from lost tracks during camera transitions.
Moreover, our model works gracefully regardless of the overlapping ratios
between the cameras. Experimental results show that we outperform existing
MC-MOT algorithms by a large margin on several practical datasets. Notably, our
model works favorably on online settings but can be extended to an incremental
approach for large-scale datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">D2C: Diffusion-Denoising Models for Few-shot Conditional Generation. (arXiv:2106.06819v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">Abhishek Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jiaming Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Chenlin Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06819">
                                    <div class="article-summary-box-inner">
                                        <span>Conditional generative models of high-dimensional images have many
applications, but supervision signals from conditions to images can be
expensive to acquire. This paper describes Diffusion-Decoding models with
Contrastive representations (D2C), a paradigm for training unconditional
variational autoencoders (VAEs) for few-shot conditional image generation. D2C
uses a learned diffusion-based prior over the latent representations to improve
generation and contrastive self-supervised learning to improve representation
quality. D2C can adapt to novel generation tasks conditioned on labels or
manipulation constraints, by learning from as few as 100 labeled examples. On
conditional generation from new labels, D2C achieves superior performance over
state-of-the-art VAEs and diffusion models. On conditional image manipulation,
D2C generations are two orders of magnitude faster to produce over StyleGAN2
ones and are preferred by 50% - 60% of the human evaluators in a double-blind
study.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Disrupting Model Training with Adversarial Shortcuts. (arXiv:2106.06654v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1">Ivan Evtimov</a>, <a href="http://arxiv.org/find/cs/1/au:+Covert_I/0/1/0/all/0/1">Ian Covert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1">Aditya Kusupati</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohno_T/0/1/0/all/0/1">Tadayoshi Kohno</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06654">
                                    <div class="article-summary-box-inner">
                                        <span>When data is publicly released for human consumption, it is unclear how to
prevent its unauthorized usage for machine learning purposes. Successful model
training may be preventable with carefully designed dataset modifications, and
we present a proof-of-concept approach for the image classification setting. We
propose methods based on the notion of adversarial shortcuts, which encourage
models to rely on non-robust signals rather than semantic features, and our
experiments demonstrate that these measures successfully prevent deep learning
models from achieving high accuracy on real, unmodified data examples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Orderly Dual-Teacher Knowledge Distillation for Lightweight Human Pose Estimation. (arXiv:2104.10414v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhong-Qiu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yuchen Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_W/0/1/0/all/0/1">Weidong Tian</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10414">
                                    <div class="article-summary-box-inner">
                                        <span>Although deep convolution neural networks (DCNN) have achieved excellent
performance in human pose estimation, these networks often have a large number
of parameters and computations, leading to the slow inference speed. For this
issue, an effective solution is knowledge distillation, which transfers
knowledge from a large pre-trained network (teacher) to a small network
(student). However, there are some defects in the existing approaches: (I) Only
a single teacher is adopted, neglecting the potential that a student can learn
from multiple teachers. (II) The human segmentation mask can be regarded as
additional prior information to restrict the location of keypoints, which is
never utilized. (III) A student with a small number of parameters cannot fully
imitate heatmaps provided by datasets and teachers. (IV) There exists noise in
heatmaps generated by teachers, which causes model degradation. To overcome
these defects, we propose an orderly dual-teacher knowledge distillation (ODKD)
framework, which consists of two teachers with different capabilities.
Specifically, the weaker one (primary teacher, PT) is used to teach keypoints
information, the stronger one (senior teacher, ST) is utilized to transfer
segmentation and keypoints information by adding the human segmentation mask.
Taking dual-teacher together, an orderly learning strategy is proposed to
promote knowledge absorbability. Moreover, we employ a binarization operation
which further improves the learning ability of the student and reduces noise in
heatmaps. Experimental results on COCO and OCHuman keypoints datasets show that
our proposed ODKD can improve the performance of different lightweight models
by a large margin, and HRNet-W16 equipped with ODKD achieves state-of-the-art
performance for lightweight human pose estimation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Graph-based Visual-Semantic Entanglement Network for Zero-shot Image Recognition. (arXiv:2006.04648v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_G/0/1/0/all/0/1">Guihua Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chapman_A/0/1/0/all/0/1">Adriane Chapman</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1">Pei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1">Mingnan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yingxue Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1">Dan Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hall_W/0/1/0/all/0/1">Wendy Hall</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04648">
                                    <div class="article-summary-box-inner">
                                        <span>Zero-shot learning uses semantic attributes to connect the search space of
unseen objects. In recent years, although the deep convolutional network brings
powerful visual modeling capabilities to the ZSL task, its visual features have
severe pattern inertia and lack of representation of semantic relationships,
which leads to severe bias and ambiguity. In response to this, we propose the
Graph-based Visual-Semantic Entanglement Network to conduct graph modeling of
visual features, which is mapped to semantic attributes by using a knowledge
graph, it contains several novel designs: 1. it establishes a multi-path
entangled network with the convolutional neural network (CNN) and the graph
convolutional network (GCN), which input the visual features from CNN to GCN to
model the implicit semantic relations, then GCN feedback the graph modeled
information to CNN features; 2. it uses attribute word vectors as the target
for the graph semantic modeling of GCN, which forms a self-consistent
regression for graph modeling and supervise GCN to learn more personalized
attribute relations; 3. it fuses and supplements the hierarchical
visual-semantic features refined by graph modeling into visual embedding. Our
method outperforms state-of-the-art approaches on multiple representative ZSL
datasets: AwA2, CUB, and SUN by promoting the semantic linkage modelling of
visual features.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial Robustness under Long-Tailed Distribution. (arXiv:2104.02703v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Tong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Ziwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qingqiu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">Dahua Lin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02703">
                                    <div class="article-summary-box-inner">
                                        <span>Adversarial robustness has attracted extensive studies recently by revealing
the vulnerability and intrinsic characteristics of deep networks. However,
existing works on adversarial robustness mainly focus on balanced datasets,
while real-world data usually exhibits a long-tailed distribution. To push
adversarial robustness towards more realistic scenarios, in this work we
investigate the adversarial vulnerability as well as defense under long-tailed
distributions. In particular, we first reveal the negative impacts induced by
imbalanced data on both recognition performance and adversarial robustness,
uncovering the intrinsic challenges of this problem. We then perform a
systematic study on existing long-tailed recognition methods in conjunction
with the adversarial training framework. Several valuable observations are
obtained: 1) natural accuracy is relatively easy to improve, 2) fake gain of
robust accuracy exists under unreliable evaluation, and 3) boundary error
limits the promotion of robustness. Inspired by these observations, we propose
a clean yet effective framework, RoBal, which consists of two dedicated
modules, a scale-invariant classifier and data re-balancing via both margin
engineering at training stage and boundary adjustment during inference.
Extensive experiments demonstrate the superiority of our approach over other
state-of-the-art defense methods. To our best knowledge, we are the first to
tackle adversarial robustness under long-tailed distributions, which we believe
would be a significant step towards real-world robustness. Our code is
available at: https://github.com/wutong16/Adversarial_Long-Tail .</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions. (arXiv:1812.03664v6 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Han-Jia Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hexiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1">De-Chuan Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1">Fei Sha</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.03664">
                                    <div class="article-summary-box-inner">
                                        <span>Learning with limited data is a key challenge for visual recognition. Many
few-shot learning methods address this challenge by learning an instance
embedding function from seen classes and apply the function to instances from
unseen classes with limited labels. This style of transfer learning is
task-agnostic: the embedding function is not learned optimally discriminative
with respect to the unseen classes, where discerning among them leads to the
target task. In this paper, we propose a novel approach to adapt the instance
embeddings to the target classification task with a set-to-set function,
yielding embeddings that are task-specific and are discriminative. We
empirically investigated various instantiations of such set-to-set functions
and observed the Transformer is most effective -- as it naturally satisfies key
properties of our desired model. We denote this model as FEAT (few-shot
embedding adaptation w/ Transformer) and validate it on both the standard
few-shot classification benchmark and four extended few-shot learning settings
with essential use cases, i.e., cross-domain, transductive, generalized
few-shot learning, and low-shot learning. It archived consistent improvements
over baseline models as well as previous methods and established the new
state-of-the-art results on two benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial Segmentation Loss for Sketch Colorization. (arXiv:2102.06192v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hicsonmez_S/0/1/0/all/0/1">Samet Hicsonmez</a>, <a href="http://arxiv.org/find/cs/1/au:+Samet_N/0/1/0/all/0/1">Nermin Samet</a>, <a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1">Emre Akbas</a>, <a href="http://arxiv.org/find/cs/1/au:+Duygulu_P/0/1/0/all/0/1">Pinar Duygulu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06192">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a new method for generating color images from sketches or edge
maps. Current methods either require some form of additional user-guidance or
are limited to the &quot;paired&quot; translation approach. We argue that segmentation
information could provide valuable guidance for sketch colorization. To this
end, we propose to leverage semantic image segmentation, as provided by a
general purpose panoptic segmentation network, to create an additional
adversarial loss function. Our loss function can be integrated to any baseline
GAN model. Our method is not limited to datasets that contain segmentation
labels, and it can be trained for &quot;unpaired&quot; translation tasks. We show the
effectiveness of our method on four different datasets spanning scene level
indoor, outdoor, and children book illustration images using qualitative,
quantitative and user study analysis. Our model improves its baseline up to 35
points on the FID metric. Our code and pretrained models can be found at
https://github.com/giddyyupp/AdvSegLoss.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks. (arXiv:2101.06969v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhengyan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1">Guangxuan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1">Tian Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1">Fanchao Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yasheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06969">
                                    <div class="article-summary-box-inner">
                                        <span>Pre-trained models (PTMs) have been widely used in various downstream tasks.
The parameters of PTMs are distributed on the Internet and may suffer backdoor
attacks. In this work, we demonstrate the universal vulnerability of PTMs,
where fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary
downstream tasks. Specifically, attackers can add a simple pre-training task,
which restricts the output representations of trigger instances to pre-defined
vectors, namely neuron-level backdoor attack (NeuBA). If the backdoor
functionality is not eliminated during fine-tuning, the triggers can make the
fine-tuned model predict fixed labels by pre-defined vectors. In the
experiments of both natural language processing (NLP) and computer vision (CV),
we show that NeuBA absolutely controls the predictions for trigger instances
without any knowledge of downstream tasks. Finally, we apply several defense
methods to NeuBA and find that model pruning is a promising direction to resist
NeuBA by excluding backdoored neurons. Our findings sound a red alarm for the
wide use of PTMs. Our source code and models are available at
\url{https://github.com/thunlp/NeuBA}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Multi-Implicit Neural Representation for Fonts. (arXiv:2106.06866v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Reddy_P/0/1/0/all/0/1">Pradyumna Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhifei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1">Matthew Fisher</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1">Hailin Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaowen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1">Niloy J. Mitra</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06866">
                                    <div class="article-summary-box-inner">
                                        <span>Fonts are ubiquitous across documents and come in a variety of styles. They
are either represented in a native vector format or rasterized to produce fixed
resolution images. In the first case, the non-standard representation prevents
benefiting from latest network architectures for neural representations; while,
in the latter case, the rasterized representation, when encoded via networks,
results in loss of data fidelity, as font-specific discontinuities like edges
and corners are difficult to represent using neural networks. Based on the
observation that complex fonts can be represented by a superposition of a set
of simpler occupancy functions, we introduce \textit{multi-implicits} to
represent fonts as a permutation-invariant set of learned implict functions,
without losing features (e.g., edges and corners). However, while
multi-implicits locally preserve font features, obtaining supervision in the
form of ground truth multi-channel signals is a problem in itself. Instead, we
propose how to train such a representation with only local supervision, while
the proposed neural architecture directly finds globally consistent
multi-implicits for font families. We extensively evaluate the proposed
representation for various tasks including reconstruction, interpolation, and
synthesis to demonstrate clear advantages with existing alternatives.
Additionally, the representation naturally enables glyph completion, wherein a
single characteristic font is used to synthesize a whole font family in the
target style.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">HPNet: Deep Primitive Segmentation Using Hybrid Representations. (arXiv:2105.10620v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Siming Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhenpei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chongyang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haibin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vouga_E/0/1/0/all/0/1">Etienne Vouga</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1">Qixing Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10620">
                                    <div class="article-summary-box-inner">
                                        <span>This paper introduces HPNet, a novel deep-learning approach for segmenting a
3D shape represented as a point cloud into primitive patches. The key to deep
primitive segmentation is learning a feature representation that can separate
points of different primitives. Unlike utilizing a single feature
representation, HPNet leverages hybrid representations that combine one learned
semantic descriptor, two spectral descriptors derived from predicted geometric
parameters, as well as an adjacency matrix that encodes sharp edges. Moreover,
instead of merely concatenating the descriptors, HPNet optimally combines
hybrid representations by learning combination weights. This weighting module
builds on the entropy of input features. The output primitive segmentation is
obtained from a mean-shift clustering module. Experimental results on benchmark
datasets ANSI and ABCParts show that HPNet leads to significant performance
gains from baseline approaches.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On The Radon-Nikodym Spectral Approach With Optimal Clustering. (arXiv:1906.00460v16 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Malyshkin_V/0/1/0/all/0/1">Vladislav Gennadievich Malyshkin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.00460">
                                    <div class="article-summary-box-inner">
                                        <span>Problems of interpolation, classification, and clustering are considered. In
the tenets of Radon--Nikodym approach $\langle f(\mathbf{x})\psi^2 \rangle /
\langle\psi^2\rangle$, where the $\psi(\mathbf{x})$ is a linear function on
input attributes, all the answers are obtained from a generalized eigenproblem
$|f|\psi^{[i]}\rangle &#x3D; \lambda^{[i]} |\psi^{[i]}\rangle$. The solution to the
interpolation problem is a regular Radon-Nikodym derivative. The solution to
the classification problem requires prior and posterior probabilities that are
obtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian
approach new observations change only outcome probabilities, in the
Radon-Nikodym approach not only outcome probabilities but also the probability
space $|\psi^{[i]}\rangle$ change with new observations. This is a remarkable
feature of the approach: both the probabilities and the probability space are
constructed from the data. The Lebesgue quadrature technique can be also
applied to the optimal clustering problem. The problem is solved by
constructing a Gaussian quadrature on the Lebesgue measure. A distinguishing
feature of the Radon-Nikodym approach is the knowledge of the invariant group:
all the answers are invariant relatively any non-degenerated linear transform
of input vector $\mathbf{x}$ components. A software product implementing the
algorithms of interpolation, classification, and optimal clustering is
available from the authors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Federated Learning with Spiking Neural Networks. (arXiv:2106.06579v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Venkatesha_Y/0/1/0/all/0/1">Yeshwanth Venkatesha</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Youngeun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Tassiulas_L/0/1/0/all/0/1">Leandros Tassiulas</a>, <a href="http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1">Priyadarshini Panda</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06579">
                                    <div class="article-summary-box-inner">
                                        <span>As neural networks get widespread adoption in resource-constrained embedded
devices, there is a growing need for low-power neural systems. Spiking Neural
Networks (SNNs)are emerging to be an energy-efficient alternative to the
traditional Artificial Neural Networks (ANNs) which are known to be
computationally intensive. From an application perspective, as federated
learning involves multiple energy-constrained devices, there is a huge scope to
leverage energy efficiency provided by SNNs. Despite its importance, there has
been little attention on training SNNs on a large-scale distributed system like
federated learning. In this paper, we bring SNNs to a more realistic federated
learning scenario. Specifically, we propose a federated learning framework for
decentralized and privacy-preserving training of SNNs. To validate the proposed
federated learning framework, we experimentally evaluate the advantages of SNNs
on various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks.
We observe that SNNs outperform ANNs in terms of overall accuracy by over 15%
when the data is distributed across a large number of clients in the federation
while providing up to5.3x energy efficiency. In addition to efficiency, we also
analyze the sensitivity of the proposed federated SNN framework to data
distribution among the clients, stragglers, and gradient noise and perform a
comprehensive comparison with ANNs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Video Super-Resolution Transformer. (arXiv:2106.06847v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiezhang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yawei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06847">
                                    <div class="article-summary-box-inner">
                                        <span>Video super-resolution (VSR), with the aim to restore a high-resolution video
from its corresponding low-resolution version, is a spatial-temporal sequence
prediction problem. Recently, Transformer has been gaining popularity due to
its parallel computing ability for sequence-to-sequence modeling. Thus, it
seems to be straightforward to apply the vision Transformer to solve VSR.
However, the typical block design of Transformer with a fully connected
self-attention layer and a token-wise feed-forward layer does not fit well for
VSR due to the following two reasons. First, the fully connected self-attention
layer neglects to exploit the data locality because this layer relies on linear
layers to compute attention maps. Second, the token-wise feed-forward layer
lacks the feature alignment which is important for VSR since this layer
independently processes each of the input token embeddings without any
interaction among them. In this paper, we make the first attempt to adapt
Transformer for VSR. Specifically, to tackle the first issue, we present a
spatial-temporal convolutional self-attention layer with a theoretical
understanding to exploit the locality information. For the second issue, we
design a bidirectional optical flow-based feed-forward layer to discover the
correlations across different video frames and also align features. Extensive
experiments on several benchmark datasets demonstrate the effectiveness of our
proposed method. The code will be available at
https://github.com/caojiezhang/VSR-Transformer.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Independent Prototype Propagation for Zero-Shot Compositionality. (arXiv:2106.00305v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ruis_F/0/1/0/all/0/1">Frank Ruis</a>, <a href="http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1">Gertjan Burghouts</a>, <a href="http://arxiv.org/find/cs/1/au:+Bucur_D/0/1/0/all/0/1">Doina Bucur</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00305">
                                    <div class="article-summary-box-inner">
                                        <span>Humans are good at compositional zero-shot reasoning; someone who has never
seen a zebra before could nevertheless recognize one when we tell them it looks
like a horse with black and white stripes. Machine learning systems, on the
other hand, usually leverage spurious correlations in the training data, and
while such correlations can help recognize objects in context, they hurt
generalization. To be able to deal with underspecified datasets while still
leveraging contextual clues during classification, we propose ProtoProp, a
novel prototype propagation graph method. First we learn prototypical
representations of objects (e.g., zebra) that are conditionally independent
w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate
the independent prototypes through a compositional graph, to learn
compositional prototypes of novel attribute-object combinations that reflect
the dependencies of the target distribution. The method does not rely on any
external data, such as class hierarchy graphs or pretrained word embeddings. We
evaluate our approach on AO-Clever, a synthetic and strongly visual dataset
with clean labels, and UT-Zappos, a noisy real-world dataset of fine-grained
shoe types. We show that in the generalized compositional zero-shot setting we
outperform state-of-the-art results, and through ablations we show the
importance of each part of the method and their contribution to the final
results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Lite-FPN for Keypoint-based Monocular 3D Object Detection. (arXiv:2105.00268v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xinyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Li Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Minghan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jun Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00268">
                                    <div class="article-summary-box-inner">
                                        <span>3D object detection with a single image is an essential and challenging task
for autonomous driving. Recently, keypoint-based monocular 3D object detection
has made tremendous progress and achieved great speed-accuracy trade-off.
However, there still exists a huge gap with LIDAR-based methods in terms of
accuracy. To improve their performance without sacrificing efficiency, we
propose a sort of lightweight feature pyramid network called Lite-FPN to
achieve multi-scale feature fusion in an effective and efficient way, which can
boost the multi-scale detection capability of keypoint-based detectors.
Besides, the misalignment between classification score and localization
precision is further relieved by introducing a novel regression loss named
attention loss. With the proposed loss, predictions with high confidence but
poor localization are treated with more attention during the training phase.
Comparative experiments based on several state-of-the-art keypoint-based
detectors on the KITTI dataset show that our proposed methods manage to achieve
significant improvements in both accuracy and frame rate. The code and
pretrained models will be released at
\url{https://github.com/yanglei18/Lite-FPN}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving Co-registration for Sentinel-1 SAR and Sentinel-2 Optical images. (arXiv:2005.11092v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Ye_Y/0/1/0/all/0/1">Yuanxin Ye</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1">Chao Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_B/0/1/0/all/0/1">Bai Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1">Youquan He</a>, <a href="http://arxiv.org/find/eess/1/au:+Jia_H/0/1/0/all/0/1">Huarong Jia</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.11092">
                                    <div class="article-summary-box-inner">
                                        <span>Co-registering the Sentinel-1 SAR and Sentinel-2 optical data of European
Space Agency (ESA) is of great importance for many remote sensing applications.
However, we find that there are evident misregistration shifts between the
Sentinel-1 SAR and Sentinel-2 optical images that are directly downloaded from
the official website. To address that, this paper presents a fast and effective
registration method for the two types of images. In the proposed method, a
block-based scheme is first designed to extract evenly distributed interest
points. Then the correspondences are detected by using the similarity of
structural features between the SAR and optical images, where the three
dimension (3D) phase correlation (PC) is used as the similarity measure for
accelerating image matching. Finally, the obtained correspondences are employed
to measure the misregistration shifts between the images. Moreover, to
eliminate the misregistration, we use some representative geometric
transformation models such as polynomial models, projective models, and
rational function models for the co-registration of the two types of images,
and compare and analyze their registration accuracy under different numbers of
control points and different terrains. Six pairs of the Sentinel-1 SAR L1 and
Sentinel-2 optical L1C images covering three different terrains are tested in
our experiments. Experimental results show that the proposed method can achieve
precise correspondences between the images, and the 3rd. Order polynomial
achieves the most satisfactory registration results. Its registration accuracy
of the flat areas is less than 1.0 10m pixels, and that of the hilly areas is
about 1.5 10m pixels, and that of the mountainous areas is between 1.7 and 2.3
10m pixels, which significantly improves the co-registration accuracy of the
Sentinel-1 SAR and Sentinel-2 optical images.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hippocampus segmentation in magnetic resonance images of Alzheimer&#x27;s patients using Deep machine learning. (arXiv:2106.06743v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Varmazyar_H/0/1/0/all/0/1">Hadi Varmazyar</a>, <a href="http://arxiv.org/find/eess/1/au:+Yousefi_Banaem_H/0/1/0/all/0/1">Hossein Yousefi-Banaem</a>, <a href="http://arxiv.org/find/eess/1/au:+Malekzadeh_S/0/1/0/all/0/1">Saber Malekzadeh</a>, <a href="http://arxiv.org/find/eess/1/au:+Gharehaghaji_N/0/1/0/all/0/1">Nahideh Gharehaghaji</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06743">
                                    <div class="article-summary-box-inner">
                                        <span>Background: Alzheimers disease is a progressive neurodegenerative disorder
and the main cause of dementia in aging. Hippocampus is prone to changes in the
early stages of Alzheimers disease. Detection and observation of the
hippocampus changes using magnetic resonance imaging (MRI) before the onset of
Alzheimers disease leads to the faster preventive and therapeutic measures.
Objective: The aim of this study was the segmentation of the hippocampus in
magnetic resonance (MR) images of Alzheimers patients using deep machine
learning method. Methods: U-Net architecture of convolutional neural network
was proposed to segment the hippocampus in the real MRI data. The MR images of
the 100 and 35 patients available in Alzheimers disease Neuroimaging Initiative
(ADNI) dataset, was used for the train and test of the model, respectively. The
performance of the proposed method was compared with manual segmentation by
measuring the similarity metrics. Results: The desired segmentation achieved
after 10 iterations. A Dice similarity coefficient (DSC) &#x3D; 92.3%, sensitivity &#x3D;
96.5%, positive predicted value (PPV) &#x3D; 90.4%, and Intersection over Union
(IoU) value for the train 92.94 and test 92.93 sets were obtained which are
acceptable. Conclusion: The proposed approach is promising and can be extended
in the prognosis of Alzheimers disease by the prediction of the hippocampus
volume changes in the early stage of the disease.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Explaining the Black-box Smoothly- A Counterfactual Approach. (arXiv:2101.04230v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1">Sumedha Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Pollack_B/0/1/0/all/0/1">Brian Pollack</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallace_S/0/1/0/all/0/1">Stephen Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1">Kayhan Batmanghelich</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04230">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a BlackBox \emph{Counterfactual Explainer} that is explicitly
developed for medical imaging applications. Classical approaches (e.g. saliency
maps) assessing feature importance do not explain \emph{how} and \emph{why}
variations in a particular anatomical region is relevant to the outcome, which
is crucial for transparent decision making in healthcare application. Our
framework explains the outcome by gradually \emph{exaggerating} the semantic
effect of the given outcome label. Given a query input to a classifier,
Generative Adversarial Networks produce a progressive set of perturbations to
the query image that gradually changes the posterior probability from its
original class to its negation. We design the loss function to ensure that
essential and potentially relevant details, such as support devices, are
preserved in the counterfactually generated images. We provide an extensive
evaluation of different classification tasks on the chest X-Ray images. Our
experiments show that a counterfactually generated visual explanation is
consistent with the disease&#x27;s clinical relevant measurements, both
quantitatively and qualitatively.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep manifold learning reveals hidden dynamics of proteasome autoregulation. (arXiv:2012.12854v2 [q-bio.QM] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/q-bio/1/au:+Wu_Z/0/1/0/all/0/1">Zhaolong Wu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_S/0/1/0/all/0/1">Shuwen Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_W/0/1/0/all/0/1">Wei Li Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ma_Y/0/1/0/all/0/1">Yinping Ma</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Dong_Y/0/1/0/all/0/1">Yuanchen Dong</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mao_Y/0/1/0/all/0/1">Youdong Mao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.12854">
                                    <div class="article-summary-box-inner">
                                        <span>The 2.5-MDa 26S proteasome maintains proteostasis and regulates myriad
cellular processes. How polyubiquitylated substrate interactions regulate
proteasome activity is not understood. Here we introduce a deep manifold
learning framework, named AlphaCryo4D, which enables atomic-level cryogenic
electron microscopy (cryo-EM) reconstructions of nonequilibrium conformational
continuum and reconstitutes hidden dynamics of proteasome autoregulation in the
act of substrate degradation. AlphaCryo4D integrates 3D deep residual learning
with manifold embedding of free-energy landscapes, which directs 3D clustering
via an energy-based particle-voting algorithm. In blind assessments using
simulated heterogeneous cryo-EM datasets, AlphaCryo4D achieved 3D
classification accuracy three times that of conventional method and
reconstructed continuous conformational changes of a 130-kDa protein at
sub-3-angstrom resolution. By using AlphaCryo4D to analyze a single
experimental cryo-EM dataset, we identified 64 conformers of the
substrate-bound human 26S proteasome, revealing conformational entanglement of
two regulatory particles in the doubly capped holoenzymes and their energetic
differences with singly capped ones. Novel ubiquitin-binding sites are
discovered on the RPN2, RPN10 and Alpha5 subunits to remodel polyubiquitin
chains for deubiquitylation and recycle. Importantly, AlphaCryo4D choreographs
single-nucleotide-exchange dynamics of proteasomal AAA-ATPase motor during
translocation initiation, which upregulates proteolytic activity by
allosterically promoting nucleophilic attack. Our systemic analysis illuminates
a grand hierarchical allostery for proteasome autoregulation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers. (arXiv:2106.06560v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Mingyu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1">Xiaochen Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Linjie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaojie Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhiwu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06560">
                                    <div class="article-summary-box-inner">
                                        <span>High-resolution representations (HR) are essential for dense prediction tasks
such as segmentation, detection, and pose estimation. Learning HR
representations is typically ignored in previous Neural Architecture Search
(NAS) methods that focus on image classification. This work proposes a novel
NAS method, called HR-NAS, which is able to find efficient and accurate
networks for different tasks, by effectively encoding multiscale contextual
information while maintaining high-resolution representations. In HR-NAS, we
renovate the NAS search space as well as its searching strategy. To better
encode multiscale image contexts in the search space of HR-NAS, we first
carefully design a lightweight transformer, whose computational complexity can
be dynamically changed with respect to different objective functions and
computation budgets. To maintain high-resolution representations of the learned
networks, HR-NAS adopts a multi-branch architecture that provides convolutional
encoding of multiple feature resolutions, inspired by HRNet. Last, we proposed
an efficient fine-grained search strategy to train HR-NAS, which effectively
explores the search space, and finds optimal architectures given various tasks
and computation resources. HR-NAS is capable of achieving state-of-the-art
trade-offs between performance and FLOPs for three dense prediction tasks and
an image classification task, given only small computational budgets. For
example, HR-NAS surpasses SqueezeNAS that is specially designed for semantic
segmentation while improving efficiency by 45.9%. Code is available at
https://github.com/dingmyu/HR-NAS</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Evaluating Foveated Video Quality Using Entropic Differencing. (arXiv:2106.06817v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yize Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Patney_A/0/1/0/all/0/1">Anjul Patney</a>, <a href="http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1">Alan Bovik</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06817">
                                    <div class="article-summary-box-inner">
                                        <span>Virtual Reality is regaining attention due to recent advancements in hardware
technology. Immersive images / videos are becoming widely adopted to carry
omnidirectional visual information. However, due to the requirements for higher
spatial and temporal resolution of real video data, immersive videos require
significantly larger bandwidth consumption. To reduce stresses on bandwidth,
foveated video compression is regaining popularity, whereby the space-variant
spatial resolution of the retina is exploited. Towards advancing the progress
of foveated video compression, we propose a full reference (FR) foveated image
quality assessment algorithm, which we call foveated entropic differencing
(FED), which employs the natural scene statistics of bandpass responses by
applying differences of local entropies weighted by a foveation-based error
sensitivity function. We evaluate the proposed algorithm by measuring the
correlations of the predictions that FED makes against human judgements on the
newly created 2D and 3D LIVE-FBT-FCVR databases for Virtual Reality (VR). The
performance of the proposed algorithm yields state-of-the-art as compared with
other existing full reference algorithms. Software for FED has been made
available at: this http URL</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PMP-Net: Point Cloud Completion by Learning Multi-step Point Moving Paths. (arXiv:2012.03408v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1">Xin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1">Peng Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhizhong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yan-Pei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1">Pengfei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu-Shen Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03408">
                                    <div class="article-summary-box-inner">
                                        <span>The task of point cloud completion aims to predict the missing part for an
incomplete 3D shape. A widely used strategy is to generate a complete point
cloud from the incomplete one. However, the unordered nature of point clouds
will degrade the generation of high-quality 3D shapes, as the detailed topology
and structure of discrete points are hard to be captured by the generative
process only using a latent code. In this paper, we address the above problem
by reconsidering the completion task from a new perspective, where we formulate
the prediction as a point cloud deformation process. Specifically, we design a
novel neural network, named PMP-Net, to mimic the behavior of an earth mover.
It moves each point of the incomplete input to complete the point cloud, where
the total distance of point moving paths (PMP) should be shortest. Therefore,
PMP-Net predicts a unique point moving path for each point according to the
constraint of total point moving distances. As a result, the network learns a
strict and unique correspondence on point-level, which can capture the detailed
topology and structure relationships between the incomplete shape and the
complete target, and thus improves the quality of the predicted complete shape.
We conduct comprehensive experiments on Completion3D and PCN datasets, which
demonstrate our advantages over the state-of-the-art point cloud completion
methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">RobustBench: a standardized adversarial robustness benchmark. (arXiv:2010.09670v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1">Francesco Croce</a>, <a href="http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1">Maksym Andriushchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1">Vikash Sehwag</a>, <a href="http://arxiv.org/find/cs/1/au:+Debenedetti_E/0/1/0/all/0/1">Edoardo Debenedetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1">Nicolas Flammarion</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1">Mung Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1">Prateek Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1">Matthias Hein</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09670">
                                    <div class="article-summary-box-inner">
                                        <span>As a research community, we are still lacking a systematic understanding of
the progress on adversarial robustness, which often makes it hard to identify
the most promising ideas in training robust models. A key challenge in
benchmarking robustness is that its evaluation is often error-prone, leading to
overestimation of the true robustness of models. While adaptive attacks
designed for a particular defense are a potential solution, they have to be
highly customized for particular models, which makes it difficult to compare
different methods. Our goal is to instead establish a standardized benchmark of
adversarial robustness, which as accurately as possible reflects the robustness
of the considered models within a reasonable computational budget. To evaluate
the robustness of models for our benchmark, we consider AutoAttack, an ensemble
of white- and black-box attacks which was recently shown in a large-scale study
to improve almost all robustness evaluations compared to the original
publications. We also impose some restrictions on the admitted models to rule
out defenses that only make gradient-based attacks ineffective without
improving actual robustness. Our leaderboard, hosted at
https://robustbench.github.io/, contains evaluations of 90+ models and aims at
reflecting the current state of the art on a set of well-defined tasks in
$\ell_\infty$- and $\ell_2$-threat models and on common corruptions, with
possible extensions in the future. Additionally, we open-source the library
https://github.com/RobustBench/robustbench that provides unified access to 60+
robust models to facilitate their downstream applications. Finally, based on
the collected models, we analyze the impact of robustness on the performance on
distribution shifts, calibration, out-of-distribution detection, fairness,
privacy leakage, smoothness, and transferability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Task Transformer Network for Joint MRI Reconstruction and Super-Resolution. (arXiv:2106.06742v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1">Chun-Mei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yunlu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Huazhu Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Li Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yong Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06742">
                                    <div class="article-summary-box-inner">
                                        <span>The core problem of Magnetic Resonance Imaging (MRI) is the trade off between
acceleration and image quality. Image reconstruction and super-resolution are
two crucial techniques in Magnetic Resonance Imaging (MRI). Current methods are
designed to perform these tasks separately, ignoring the correlations between
them. In this work, we propose an end-to-end task transformer network
(T$^2$Net) for joint MRI reconstruction and super-resolution, which allows
representations and feature transmission to be shared between multiple task to
achieve higher-quality, super-resolved and motion-artifacts-free images from
highly undersampled and degenerated MRI data. Our framework combines both
reconstruction and super-resolution, divided into two sub-branches, whose
features are expressed as queries and keys. Specifically, we encourage joint
feature learning between the two tasks, thereby transferring accurate task
information. We first use two separate CNN branches to extract task-specific
features. Then, a task transformer module is designed to embed and synthesize
the relevance between the two tasks. Experimental results show that our
multi-task model significantly outperforms advanced sequential methods, both
quantitatively and qualitatively.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">What Can I Do Here? Learning New Skills by Imagining Visual Affordances. (arXiv:2106.00671v2 [cs.RO] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Khazatsky_A/0/1/0/all/0/1">Alexander Khazatsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1">Ashvin Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_D/0/1/0/all/0/1">Daniel Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00671">
                                    <div class="article-summary-box-inner">
                                        <span>A generalist robot equipped with learned skills must be able to perform many
tasks in many different environments. However, zero-shot generalization to new
settings is not always possible. When the robot encounters a new environment or
object, it may need to finetune some of its previously learned skills to
accommodate this change. But crucially, previously learned behaviors and models
should still be suitable to accelerate this relearning. In this paper, we aim
to study how generative models of possible outcomes can allow a robot to learn
visual representations of affordances, so that the robot can sample potentially
possible outcomes in new situations, and then further train its policy to
achieve those outcomes. In effect, prior data is used to learn what kinds of
outcomes may be possible, such that when the robot encounters an unfamiliar
setting, it can sample potential outcomes from its model, attempt to reach
them, and thereby update both its skills and its outcome model. This approach,
visuomotor affordance learning (VAL), can be used to train goal-conditioned
policies that operate on raw image inputs, and can rapidly learn to manipulate
new objects via our proposed affordance-directed exploration scheme. We show
that VAL can utilize prior data to solve real-world tasks such drawer opening,
grasping, and placing objects in new scenes with only five minutes of online
experience in the new scene.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-Scale Hourglass Hierarchical Fusion Network for Single Image Deraining. (arXiv:2104.12100v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xiang Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1">Yufeng Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1">Lei Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12100">
                                    <div class="article-summary-box-inner">
                                        <span>Rain streaks bring serious blurring and visual quality degradation, which
often vary in size, direction and density. Current CNN-based methods achieve
encouraging performance, while are limited to depict rain characteristics and
recover image details in the poor visibility environment. To address these
issues, we present a Multi-scale Hourglass Hierarchical Fusion Network
(MH2F-Net) in end-to-end manner, to exactly captures rain streak features with
multi-scale extraction, hierarchical distillation and information aggregation.
For better extracting the features, a novel Multi-scale Hourglass Extraction
Block (MHEB) is proposed to get local and global features across different
scales through down- and up-sample process. Besides, a Hierarchical Attentive
Distillation Block (HADB) then employs the dual attention feature responses to
adaptively recalibrate the hierarchical features and eliminate the redundant
ones. Further, we introduce a Residual Projected Feature Fusion (RPFF) strategy
to progressively discriminate feature learning and aggregate different features
instead of directly concatenating or adding. Extensive experiments on both
synthetic and real rainy datasets demonstrate the effectiveness of the designed
MH2F-Net by comparing with recent state-of-the-art deraining algorithms. Our
source code will be available on the GitHub:
https://github.com/cxtalk/MH2F-Net.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">BigEarthNet Dataset with A New Class-Nomenclature for Remote Sensing Image Understanding. (arXiv:2001.06372v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1">Gencer Sumbul</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1">Jian Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreuziger_T/0/1/0/all/0/1">Tristan Kreuziger</a>, <a href="http://arxiv.org/find/cs/1/au:+Marcelino_F/0/1/0/all/0/1">Filipe Marcelino</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_H/0/1/0/all/0/1">Hugo Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Benevides_P/0/1/0/all/0/1">Pedro Benevides</a>, <a href="http://arxiv.org/find/cs/1/au:+Caetano_M/0/1/0/all/0/1">Mario Caetano</a>, <a href="http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1">Beg&#xfc;m Demir</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.06372">
                                    <div class="article-summary-box-inner">
                                        <span>This paper presents BigEarthNet that is a large-scale Sentinel-2
multispectral image dataset with a new class nomenclature to advance deep
learning (DL) studies in remote sensing (RS). BigEarthNet is made up of 590,326
image patches annotated with multi-labels provided by the CORINE Land Cover
(CLC) map of 2018 based on its most thematic detailed Level-3 class
nomenclature. Initial research demonstrates that some CLC classes are
challenging to be accurately described by considering only Sentinel-2 images.
To increase the effectiveness of BigEarthNet, in this paper we introduce an
alternative class-nomenclature to allow DL models for better learning and
describing the complex spatial and spectral information content of the
Sentinel-2 images. This is achieved by interpreting and arranging the CLC
Level-3 nomenclature based on the properties of Sentinel-2 images in a new
nomenclature of 19 classes. Then, the new class-nomenclature of BigEarthNet is
used within state-of-the-art DL models in the context of multi-label
classification. Results show that the models trained from scratch on
BigEarthNet outperform those pre-trained on ImageNet, especially in relation to
some complex classes including agriculture, other vegetated and natural
environments. All DL models are made publicly available at
this http URL, offering an important resource to guide future
progress on RS image analysis.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Is Perfect Filtering Enough Leading to Perfect Phase Correction for dMRI data?. (arXiv:2106.06992v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Feihong_L/0/1/0/all/0/1">Liu Feihong</a>, <a href="http://arxiv.org/find/cs/1/au:+Junwei_Y/0/1/0/all/0/1">Yang Junwei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiaowei_H/0/1/0/all/0/1">He Xiaowei</a>, <a href="http://arxiv.org/find/cs/1/au:+Luping_Z/0/1/0/all/0/1">Zhou Luping</a>, <a href="http://arxiv.org/find/cs/1/au:+Jun_F/0/1/0/all/0/1">Feng Jun</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinggang_S/0/1/0/all/0/1">Shen Dinggang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06992">
                                    <div class="article-summary-box-inner">
                                        <span>Being complex-valued and low in signal-to-noise ratios, magnitude-based
diffusion MRI is confounded by the noise-floor that falsely elevates signal
magnitude and incurs bias to the commonly used diffusion indices, such as
fractional anisotropy (FA). To avoid noise-floor, most existing phase
correction methods explore improving filters to estimate the noise-free
background phase. In this work, after diving into the phase correction
procedures, we argue that even a perfect filter is insufficient for phase
correction because the correction procedures are incapable of distinguishing
sign-symbols of noise, resulting in artifacts (\textit{i.e.}, arbitrary signal
loss). With this insight, we generalize the definition of noise-floor to a
complex polar coordinate system and propose a calibration procedure that could
conveniently distinguish noise sign symbols. The calibration procedure is
conceptually simple and easy to implement without relying on any external
technique while keeping distinctly effective.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Reconstruction of turbulent data with deep generative models for semantic inpainting from TURB-Rot database. (arXiv:2006.09179v2 [physics.flu-dyn] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/physics/1/au:+Buzzicotti_M/0/1/0/all/0/1">M. Buzzicotti</a>, <a href="http://arxiv.org/find/physics/1/au:+Bonaccorso_F/0/1/0/all/0/1">F. Bonaccorso</a>, <a href="http://arxiv.org/find/physics/1/au:+Leoni_P/0/1/0/all/0/1">P. Clark Di Leoni</a>, <a href="http://arxiv.org/find/physics/1/au:+Biferale_L/0/1/0/all/0/1">L. Biferale</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.09179">
                                    <div class="article-summary-box-inner">
                                        <span>We study the applicability of tools developed by the computer vision
community for features learning and semantic image inpainting to perform data
reconstruction of fluid turbulence configurations. The aim is twofold. First,
we explore on a quantitative basis, the capability of Convolutional Neural
Networks embedded in a Deep Generative Adversarial Model (Deep-GAN) to generate
missing data in turbulence, a paradigmatic high dimensional chaotic system. In
particular, we investigate their use in reconstructing two-dimensional damaged
snapshots extracted from a large database of numerical configurations of 3d
turbulence in the presence of rotation, a case with multi-scale random features
where both large-scale organised structures and small-scale highly intermittent
and non-Gaussian fluctuations are present. Second, following a reverse
engineering approach, we aim to rank the input flow properties (features) in
terms of their qualitative and quantitative importance to obtain a better set
of reconstructed fields. We present two approaches both based on Context
Encoders. The first one infers the missing data via a minimization of the L2
pixel-wise reconstruction loss, plus a small adversarial penalisation. The
second searches for the closest encoding of the corrupted flow configuration
from a previously trained generator. Finally, we present a comparison with a
different data assimilation tool, based on Nudging, an equation-informed
unbiased protocol, well known in the numerical weather prediction community.
The TURB-Rot database, this http URL, of roughly 300K 2d
turbulent images is released and details on how to download it are given.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An Approach Towards Physics Informed Lung Ultrasound Image Scoring Neural Network for Diagnostic Assistance in COVID-19. (arXiv:2106.06980v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Panicker_M/0/1/0/all/0/1">Mahesh Raveendranatha Panicker</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yale Tung Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+M_G/0/1/0/all/0/1">Gayathri M</a>, <a href="http://arxiv.org/find/eess/1/au:+N_M/0/1/0/all/0/1">Madhavanunni A N</a>, <a href="http://arxiv.org/find/eess/1/au:+Narayan_K/0/1/0/all/0/1">Kiran Vishnu Narayan</a>, <a href="http://arxiv.org/find/eess/1/au:+Kesavadas_C/0/1/0/all/0/1">C Kesavadas</a>, <a href="http://arxiv.org/find/eess/1/au:+Vinod_A/0/1/0/all/0/1">A P Vinod</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06980">
                                    <div class="article-summary-box-inner">
                                        <span>Ultrasound is fast becoming an inevitable diagnostic tool for regular and
continuous monitoring of the lung with the recent outbreak of COVID-19. In this
work, a novel approach is presented to extract acoustic propagation-based
features to automatically highlight the region below pleura, which is an
important landmark in lung ultrasound (LUS). Subsequently, a multichannel input
formed by using the acoustic physics-based feature maps is fused to train a
neural network, referred to as LUSNet, to classify the LUS images into five
classes of varying severity of lung infection to track the progression of
COVID-19. In order to ensure that the proposed approach is agnostic to the type
of acquisition, the LUSNet, which consists of a U-net architecture is trained
in an unsupervised manner with the acoustic feature maps to ensure that the
encoder-decoder architecture is learning features in the pleural region of
interest. A novel combination of the U-net output and the U-net encoder output
is employed for the classification of severity of infection in the lung. A
detailed analysis of the proposed approach on LUS images over the infection to
full recovery period of ten confirmed COVID-19 subjects shows an average
five-fold cross-validation accuracy, sensitivity, and specificity of 97%, 93%,
and 98% respectively over 5000 frames of COVID-19 videos. The analysis also
shows that, when the input dataset is limited and diverse as in the case of
COVID-19 pandemic, an aided effort of combining acoustic propagation-based
features along with the gray scale images, as proposed in this work, improves
the performance of the neural network significantly and also aids the labelling
and triaging process.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-Contrast MRI Super-Resolution via a Multi-Stage Integration Network. (arXiv:2105.08949v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1">Chun-Mei Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1">Huazhu Fu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yuan_S/0/1/0/all/0/1">Shuhao Yuan</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1">Yong Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08949">
                                    <div class="article-summary-box-inner">
                                        <span>Super-resolution (SR) plays a crucial role in improving the image quality of
magnetic resonance imaging (MRI). MRI produces multi-contrast images and can
provide a clear display of soft tissues. However, current super-resolution
methods only employ a single contrast, or use a simple multi-contrast fusion
mechanism, ignoring the rich relations among different contrasts, which are
valuable for improving SR. In this work, we propose a multi-stage integration
network (i.e., MINet) for multi-contrast MRI SR, which explicitly models the
dependencies between multi-contrast images at different stages to guide image
SR. In particular, our MINet first learns a hierarchical feature representation
from multiple convolutional stages for each of different-contrast image.
Subsequently, we introduce a multi-stage integration module to mine the
comprehensive relations between the representations of the multi-contrast
images. Specifically, the module matches each representation with all other
features, which are integrated in terms of their similarities to obtain an
enriched representation. Extensive experiments on fastMRI and real-world
clinical datasets demonstrate that 1) our MINet outperforms state-of-the-art
multi-contrast SR methods in terms of various metrics and 2) our multi-stage
integration module is able to excavate complex interactions among
multi-contrast features at different stages, leading to improved target-image
quality.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1">Seyed Saeed Changiz Rezaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1">Fred X. Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1">Di Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1">Mohammad Salameh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1">Keith Mills</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1">Shuo Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Wei Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1">Shangling Jui</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09356">
                                    <div class="article-summary-box-inner">
                                        <span>Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Go Small and Similar: A Simple Output Decay Brings Better Performance. (arXiv:2106.06726v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tianshu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaomin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jiali Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minghui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Ming Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06726">
                                    <div class="article-summary-box-inner">
                                        <span>Regularization and data augmentation methods have been widely used and become
increasingly indispensable in deep learning training. Researchers who devote
themselves to this have considered various possibilities. But so far, there has
been little discussion about regularizing outputs of the model. This paper
begins with empirical observations that better performances are significantly
associated with output distributions, that have smaller average values and
variances. By audaciously assuming there is causality involved, we propose a
novel regularization term, called Output Decay, that enforces the model to
assign smaller and similar output values on each class. Though being
counter-intuitive, such a small modification result in a remarkable improvement
on performance. Extensive experiments demonstrate the wide applicability,
versatility, and compatibility of Output Decay.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cycle4Completion: Unpaired Point Cloud Completion using Cycle Transformation with Missing Region Coding. (arXiv:2103.07838v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1">Xin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhizhong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yan-Pei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1">Pengfei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu-Shen Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07838">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we present a novel unpaired point cloud completion network,
named Cycle4Completion, to infer the complete geometries from a partial 3D
object. Previous unpaired completion methods merely focus on the learning of
geometric correspondence from incomplete shapes to complete shapes, and ignore
the learning in the reverse direction, which makes them suffer from low
completion accuracy due to the limited 3D shape understanding ability. To
address this problem, we propose two simultaneous cycle transformations between
the latent spaces of complete shapes and incomplete ones. The insight of cycle
transformation is to promote networks to understand 3D shapes by learning to
generate complete or incomplete shapes from their complementary ones.
Specifically, the first cycle transforms shapes from incomplete domain to
complete domain, and then projects them back to the incomplete domain. This
process learns the geometric characteristic of complete shapes, and maintains
the shape consistency between the complete prediction and the incomplete input.
Similarly, the inverse cycle transformation starts from complete domain to
incomplete domain, and goes back to complete domain to learn the characteristic
of incomplete shapes. We provide a comprehensive evaluation in experiments,
which shows that our model with the learned bidirectional geometry
correspondence outperforms state-of-the-art unpaired completion methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards annotation-efficient segmentation via image-to-image translation. (arXiv:1904.01636v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vorontsov_E/0/1/0/all/0/1">Eugene Vorontsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1">Pavlo Molchanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Beckham_C/0/1/0/all/0/1">Christopher Beckham</a>, <a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1">Jan Kautz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadoury_S/0/1/0/all/0/1">Samuel Kadoury</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.01636">
                                    <div class="article-summary-box-inner">
                                        <span>Often in medical imaging, it is prohibitively challenging to produce enough
boundary annotations to train deep neural networks for accurate tumor
segmentation. We propose the use of weak labels about whether an image presents
tumor or whether it is absent to extend training over images that lack these
annotations. Specifically, we propose a semi-supervised framework that employs
unpaired image-to-image translation between two domains, presence vs. absence
of cancer, as the unsupervised objective. We conjecture that translation helps
segmentation -- both require the target to be separated from the background. We
encode images into two codes: one that is common to both domains and one that
is unique to the presence domain. Decoding from the common code yields healthy
images; decoding with the addition of the unique code produces a residual
change to this image that adds cancer. Translation proceeds from presence to
absence and vice versa. In the first case, the tumor is re-added to the image
and we successfully exploit the residual decoder to also perform segmentation.
In the second case, unique codes are sampled, producing a distribution of
possible tumors. To validate the method, we created challenging synthetic tasks
and tumor segmentation datasets from public BRATS (brain, MRI) and LitS (liver,
CT) datasets. We show a clear improvement (0.83 Dice on brain, 0.74 on liver)
over baseline semi-supervised training with autoencoding (0.73, 0.66) and a
mean teacher approach (0.75, 0.69), demonstrating the ability to generalize
from smaller distributions of annotated samples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Understanding self-supervised Learning Dynamics without Contrastive Pairs. (arXiv:2102.06810v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuandong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinlei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1">Surya Ganguli</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06810">
                                    <div class="article-summary-box-inner">
                                        <span>While contrastive approaches of self-supervised learning (SSL) learn
representations by minimizing the distance between two augmented views of the
same data point (positive pairs) and maximizing views from different data
points (negative pairs), recent \emph{non-contrastive} SSL (e.g., BYOL and
SimSiam) show remarkable performance {\it without} negative pairs, with an
extra learnable predictor and a stop-gradient operation. A fundamental question
arises: why do these methods not collapse into trivial representations? We
answer this question via a simple theoretical study and propose a novel
approach, DirectPred, that \emph{directly} sets the linear predictor based on
the statistics of its inputs, without gradient training. On ImageNet, it
performs comparably with more complex two-layer non-linear predictors that
employ BatchNorm and outperforms a linear predictor by $2.5\%$ in 300-epoch
training (and $5\%$ in 60-epoch). DirectPred is motivated by our theoretical
study of the nonlinear learning dynamics of non-contrastive SSL in simple
linear networks. Our study yields conceptual insights into how non-contrastive
SSL methods learn, how they avoid representational collapse, and how multiple
factors, like predictor networks, stop-gradients, exponential moving averages,
and weight decay all come into play. Our simple theory recapitulates the
results of real-world ablation studies in both STL-10 and ImageNet. Code is
released\footnote{\url{https://github.com/facebookresearch/luckmatters/tree/master/ssl}}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DS-TransUNet:Dual Swin Transformer U-Net for Medical Image Segmentation. (arXiv:2106.06716v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1">Ailiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bingzhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiayu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1">Guangming Lu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06716">
                                    <div class="article-summary-box-inner">
                                        <span>Automatic medical image segmentation has made great progress benefit from the
development of deep learning. However, most existing methods are based on
convolutional neural networks (CNNs), which fail to build long-range
dependencies and global context connections due to the limitation of receptive
field in convolution operation. Inspired by the success of Transformer in
modeling the long-range contextual information, some researchers have expended
considerable efforts in designing the robust variants of Transformer-based
U-Net. Moreover, the patch division used in vision transformers usually ignores
the pixel-level intrinsic structural features inside each patch. To alleviate
these problems, we propose a novel deep medical image segmentation framework
called Dual Swin Transformer U-Net (DS-TransUNet), which might be the first
attempt to concurrently incorporate the advantages of hierarchical Swin
Transformer into both encoder and decoder of the standard U-shaped architecture
to enhance the semantic segmentation quality of varying medical images. Unlike
many prior Transformer-based solutions, the proposed DS-TransUNet first adopts
dual-scale encoder subnetworks based on Swin Transformer to extract the coarse
and fine-grained feature representations of different semantic scales. As the
core component for our DS-TransUNet, a well-designed Transformer Interactive
Fusion (TIF) module is proposed to effectively establish global dependencies
between features of different scales through the self-attention mechanism.
Furthermore, we also introduce the Swin Transformer block into decoder to
further explore the long-range contextual information during the up-sampling
process. Extensive experiments across four typical tasks for medical image
segmentation demonstrate the effectiveness of DS-TransUNet, and show that our
approach significantly outperforms the state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">D3DLO: Deep 3D LiDAR Odometry. (arXiv:2101.12242v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Adis_P/0/1/0/all/0/1">Philipp Adis</a>, <a href="http://arxiv.org/find/cs/1/au:+Horst_N/0/1/0/all/0/1">Nicolas Horst</a>, <a href="http://arxiv.org/find/cs/1/au:+Wien_M/0/1/0/all/0/1">Mathias Wien</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12242">
                                    <div class="article-summary-box-inner">
                                        <span>LiDAR odometry (LO) describes the task of finding an alignment of subsequent
LiDAR point clouds. This alignment can be used to estimate the motion of the
platform where the LiDAR sensor is mounted on. Currently, on the well-known
KITTI Vision Benchmark Suite state-of-the-art algorithms are non-learning
approaches. We propose a network architecture that learns LO by directly
processing 3D point clouds. It is trained on the KITTI dataset in an end-to-end
manner without the necessity of pre-defining corresponding pairs of points. An
evaluation on the KITTI Vision Benchmark Suite shows similar performance to a
previously published work, DeepCLR [1], even though our model uses only around
3.56% of the number of network parameters thereof. Furthermore, a plane point
extraction is applied which leads to a marginal performance decrease while
simultaneously reducing the input size by up to 50%.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CNN-based Lung CT Registration with Multiple Anatomical Constraints. (arXiv:2011.14372v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hering_A/0/1/0/all/0/1">Alessa Hering</a>, <a href="http://arxiv.org/find/cs/1/au:+Hager_S/0/1/0/all/0/1">Stephanie H&#xe4;ger</a>, <a href="http://arxiv.org/find/cs/1/au:+Moltz_J/0/1/0/all/0/1">Jan Moltz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lessmann_N/0/1/0/all/0/1">Nikolas Lessmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Heldmann_S/0/1/0/all/0/1">Stefan Heldmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1">Bram van Ginneken</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14372">
                                    <div class="article-summary-box-inner">
                                        <span>Deep-learning-based registration methods emerged as a fast alternative to
conventional registration methods. However, these methods often still cannot
achieve the same performance as conventional registration methods because they
are either limited to small deformation or they fail to handle a superposition
of large and small deformations without producing implausible deformation
fields with foldings inside.

In this paper, we identify important strategies of conventional registration
methods for lung registration and successfully developed the deep-learning
counterpart. We employ a Gaussian-pyramid-based multilevel framework that can
solve the image registration optimization in a coarse-to-fine fashion.
Furthermore, we prevent foldings of the deformation field and restrict the
determinant of the Jacobian to physiologically meaningful values by combining a
volume change penalty with a curvature regularizer in the loss function.
Keypoint correspondences are integrated to focus on the alignment of smaller
structures.

We perform an extensive evaluation to assess the accuracy, the robustness,
the plausibility of the estimated deformation fields, and the transferability
of our registration approach. We show that it achieves state-of-the-art results
on the COPDGene dataset compared to conventional registration method with much
shorter execution time. In our experiments on the DIRLab exhale to inhale lung
registration, we demonstrate substantial improvements (TRE below $1.2$ mm) over
other deep learning methods. Our algorithm is publicly available at
https://grand-challenge.org/algorithms/deep-learning-based-ct-lung-registration/.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multistream ValidNet: Improving 6D Object Pose Estimation by Automatic Multistream Validation. (arXiv:2106.06684v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mazumder_J/0/1/0/all/0/1">Joy Mazumder</a>, <a href="http://arxiv.org/find/cs/1/au:+Zand_M/0/1/0/all/0/1">Mohsen Zand</a>, <a href="http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1">Michael Greenspan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06684">
                                    <div class="article-summary-box-inner">
                                        <span>This work presents a novel approach to improve the results of pose estimation
by detecting and distinguishing between the occurrence of True and False
Positive results. It achieves this by training a binary classifier on the
output of an arbitrary pose estimation algorithm, and returns a binary label
indicating the validity of the result. We demonstrate that our approach
improves upon a state-of-the-art pose estimation result on the Sil\&#x27;eane
dataset, outperforming a variation of the alternative CullNet method by 4.15%
in average class accuracy and 0.73% in overall accuracy at validation. Applying
our method can also improve the pose estimation average precision results of
Op-Net by 6.06% on average.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Learning for Reversible Steganography: Principles and Insights. (arXiv:2106.06924v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Ching-Chun Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sisheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1">Isao Echizen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1">Victor Sanchez</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chang-Tsun Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06924">
                                    <div class="article-summary-box-inner">
                                        <span>Deep-learning\textendash{centric} reversible steganography has emerged as a
promising research paradigm. A direct way of applying deep learning to
reversible steganography is to construct a pair of encoder and decoder, whose
parameters are trained jointly, thereby learning the steganographic system as a
whole. This end-to-end framework, however, falls short of the reversibility
requirement because it is difficult for this kind of monolithic system, as a
black box, to create or duplicate intricate reversible mechanisms. In response
to this issue, a recent approach is to carve up the steganographic system and
work on modules independently. In particular, neural networks are deployed in
an analytics module to learn the data distribution, while an established
mechanism is called upon to handle the remaining tasks. In this paper, we
investigate the modular framework and deploy deep neural networks in a
reversible steganographic scheme referred to as prediction-error modulation, in
which an analytics module serves the purpose of pixel intensity prediction. The
primary focus of this study is on deep-learning\textendash{based} context-aware
pixel intensity prediction. We address the unsolved issues reported in related
literature, including the impact of pixel initialisation on prediction accuracy
and the influence of uncertainty propagation in dual-layer embedding.
Furthermore, we establish a connection between context-aware pixel intensity
prediction and low-level computer vision and analyse the performance of several
advanced neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Shared Cross-Modal Trajectory Prediction for Autonomous Driving. (arXiv:2004.00202v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1">Chiho Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Joon Hee Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1">Srikanth Malla</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiachen Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.00202">
                                    <div class="article-summary-box-inner">
                                        <span>Predicting future trajectories of traffic agents in highly interactive
environments is an essential and challenging problem for the safe operation of
autonomous driving systems. On the basis of the fact that self-driving vehicles
are equipped with various types of sensors (e.g., LiDAR scanner, RGB camera,
radar, etc.), we propose a Cross-Modal Embedding framework that aims to benefit
from the use of multiple input modalities. At training time, our model learns
to embed a set of complementary features in a shared latent space by jointly
optimizing the objective functions across different types of input data. At
test time, a single input modality (e.g., LiDAR data) is required to generate
predictions from the input perspective (i.e., in the LiDAR space), while taking
advantages from the model trained with multiple sensor modalities. An extensive
evaluation is conducted to show the efficacy of the proposed framework using
two benchmark driving datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Anisotropic Stroke Control for Multiple Artists Style Transfer. (arXiv:2010.08175v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuanhong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xirui Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Naiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1">Ting Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1">Bingbing Ni</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08175">
                                    <div class="article-summary-box-inner">
                                        <span>Though significant progress has been made in artistic style transfer,
semantic information is usually difficult to be preserved in a fine-grained
locally consistent manner by most existing methods, especially when multiple
artists styles are required to transfer within one single model. To circumvent
this issue, we propose a Stroke Control Multi-Artist Style Transfer framework.
On the one hand, we develop a multi-condition single-generator structure which
first performs multi-artist style transfer. On the one hand, we design an
Anisotropic Stroke Module (ASM) which realizes the dynamic adjustment of
style-stroke between the non-trivial and the trivial regions. ASM endows the
network with the ability of adaptive semantic-consistency among various styles.
On the other hand, we present an novel Multi-Scale Projection Discriminator} to
realize the texture-level conditional generation. In contrast to the
single-scale conditional discriminator, our discriminator is able to capture
multi-scale texture clue to effectively distinguish a wide range of artistic
styles. Extensive experimental results well demonstrate the feasibility and
effectiveness of our approach. Our framework can transform a photograph into
different artistic style oil painting via only ONE single model. Furthermore,
the results are with distinctive artistic style and retain the anisotropic
semantic information. The code is already available on github:
https://github.com/neuralchen/ASMAGAN.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Understanding Failures of Deep Networks via Robust Feature Extraction. (arXiv:2012.01750v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1">Sahil Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Nushi_B/0/1/0/all/0/1">Besmira Nushi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1">Shital Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1">Ece Kamar</a>, <a href="http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1">Eric Horvitz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01750">
                                    <div class="article-summary-box-inner">
                                        <span>Traditional evaluation metrics for learned models that report aggregate
scores over a test set are insufficient for surfacing important and informative
patterns of failure over features and instances. We introduce and study a
method aimed at characterizing and explaining failures by identifying visual
attributes whose presence or absence results in poor performance. In
distinction to previous work that relies upon crowdsourced labels for visual
attributes, we leverage the representation of a separate robust model to
extract interpretable features and then harness these features to identify
failure modes. We further propose a visualization method aimed at enabling
humans to understand the meaning encoded in such features and we test the
comprehensibility of the features. An evaluation of the methods on the ImageNet
dataset demonstrates that: (i) the proposed workflow is effective for
discovering important failure modes, (ii) the visualization techniques help
humans to understand the extracted features, and (iii) the extracted insights
can assist engineers with error analysis and debugging.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deception Detection and Remote Physiological Monitoring: A Dataset and Baseline Experimental Results. (arXiv:2106.06583v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Speth_J/0/1/0/all/0/1">Jeremy Speth</a>, <a href="http://arxiv.org/find/cs/1/au:+Vance_N/0/1/0/all/0/1">Nathan Vance</a>, <a href="http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1">Adam Czajka</a>, <a href="http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1">Kevin W. Bowyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Wright_D/0/1/0/all/0/1">Diane Wright</a>, <a href="http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1">Patrick Flynn</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06583">
                                    <div class="article-summary-box-inner">
                                        <span>We present the Deception Detection and Physiological Monitoring (DDPM)
dataset and initial baseline results on this dataset. Our application context
is an interview scenario in which the interviewee attempts to deceive the
interviewer on selected responses. The interviewee is recorded in RGB,
near-infrared, and long-wave infrared, along with cardiac pulse, blood
oxygenation, and audio. After collection, data were annotated for
interviewer/interviewee, curated, ground-truthed, and organized into train /
test parts for a set of canonical deception detection experiments. Baseline
experiments found random accuracy for micro-expressions as an indicator of
deception, but that saccades can give a statistically significant response. We
also estimated subject heart rates from face videos (remotely) with a mean
absolute error as low as 3.16 bpm. The database contains almost 13 hours of
recordings of 70 subjects, and over 8 million visible-light, near-infrared, and
thermal video frames, along with appropriate meta, audio and pulse oximeter
data. To our knowledge, this is the only collection offering recordings of five
modalities in an interview scenario that can be used in both deception
detection and remote photoplethysmography research.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dynamic Clone Transformer for Efficient Convolutional Neural Netwoks. (arXiv:2106.06778v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1">Longqing Ye</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06778">
                                    <div class="article-summary-box-inner">
                                        <span>Convolutional networks (ConvNets) have shown impressive capability to solve
various vision tasks. Nevertheless, the trade-off between performance and
efficiency is still a challenge for a feasible model deployment on
resource-constrained platforms. In this paper, we introduce a novel concept
termed multi-path fully connected pattern (MPFC) to rethink the
interdependencies of topology pattern, accuracy and efficiency for ConvNets.
Inspired by MPFC, we further propose a dual-branch module named dynamic clone
transformer (DCT) where one branch generates multiple replicas from inputs and
another branch reforms those clones through a series of difference vectors
conditional on inputs itself to produce more variants. This operation allows
the self-expansion of channel-wise information in a data-driven way with little
computational cost while providing sufficient learning capacity, which is a
potential unit to replace computationally expensive pointwise convolution as an
expansion layer in the bottleneck structure.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unsupervised Place Recognition with Deep Embedding Learning over Radar Videos. (arXiv:2106.06703v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gadd_M/0/1/0/all/0/1">Matthew Gadd</a>, <a href="http://arxiv.org/find/cs/1/au:+Martini_D/0/1/0/all/0/1">Daniele De Martini</a>, <a href="http://arxiv.org/find/cs/1/au:+Newman_P/0/1/0/all/0/1">Paul Newman</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06703">
                                    <div class="article-summary-box-inner">
                                        <span>We learn, in an unsupervised way, an embedding from sequences of radar images
that is suitable for solving place recognition problem using complex radar
data. We experiment on 280 km of data and show performance exceeding
state-of-the-art supervised approaches, localising correctly 98.38% of the time
when using just the nearest database candidate.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up. (arXiv:2102.07074v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yifan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1">Shiyu Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07074">
                                    <div class="article-summary-box-inner">
                                        <span>The recent explosive interest on transformers has suggested their potential
to become powerful &#x60;&#x60;universal&quot; models for computer vision tasks, such as
classification, detection, and segmentation. While those attempts mainly study
the discriminative models, we explore transformers on some more notoriously
difficult vision tasks, e.g., generative adversarial networks (GANs). Our goal
is to conduct the first pilot study in building a GAN completely free of
convolutions, using only pure transformer-based architectures. Our vanilla GAN
architecture, dubbed TransGAN, consists of a memory-friendly transformer-based
generator that progressively increases feature resolution, and correspondingly
a multi-scale discriminator to capture simultaneously semantic contexts and
low-level textures. On top of them, we introduce the new module of grid
self-attention for alleviating the memory bottleneck further, in order to scale
up TransGAN to high-resolution generation. We also develop a unique training
recipe including a series of techniques that can mitigate the training
instability issues of TransGAN, such as data augmentation, modified
normalization, and relative position encoding. Our best architecture achieves
highly competitive performance compared to current state-of-the-art GANs using
convolutional backbones. Specifically, TransGAN sets new state-of-the-art
inception score of 10.43 and FID of 18.28 on STL-10, outperforming StyleGAN-V2.
When it comes to higher-resolution (e.g. 256 x 256) generation tasks, such as
on CelebA-HQ and LSUN-Church, TransGAN continues to produce diverse visual
examples with high fidelity and impressive texture details. In addition, we
dive deep into the transformer-based generation models to understand how their
behaviors differ from convolutional ones, by visualizing training dynamics. The
code is available at https://github.com/VITA-Group/TransGAN.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability. (arXiv:2006.14512v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1">Kaizhao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jacky Y. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1">Oluwasanmi Koyejo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14512">
                                    <div class="article-summary-box-inner">
                                        <span>Knowledge transferability, or transfer learning, has been widely adopted to
allow a pre-trained model in the source domain to be effectively adapted to
downstream tasks in the target domain. It is thus important to explore and
understand the factors affecting knowledge transferability. In this paper, as
the first work, we analyze and demonstrate the connections between knowledge
transferability and another important phenomenon--adversarial transferability,
\emph{i.e.}, adversarial examples generated against one model can be
transferred to attack other models. Our theoretical studies show that
adversarial transferability indicates knowledge transferability and vice versa.
Moreover, based on the theoretical insights, we propose two practical
adversarial transferability metrics to characterize this process, serving as
bidirectional indicators between adversarial and knowledge transferability. We
conduct extensive experiments for different scenarios on diverse datasets,
showing a positive correlation between adversarial transferability and
knowledge transferability. Our findings will shed light on future research
about effective knowledge transfer learning and adversarial transferability
analyses.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">NLHD: A Pixel-Level Non-Local Retinex Model for Low-Light Image Enhancement. (arXiv:2106.06971v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1">Hou Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yingkun_H/0/1/0/all/0/1">Hou Yingkun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuxuan_S/0/1/0/all/0/1">Shi Yuxuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Benzheng_W/0/1/0/all/0/1">Wei Benzheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jun_X/0/1/0/all/0/1">Xu Jun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06971">
                                    <div class="article-summary-box-inner">
                                        <span>Retinex model has been applied to low-light image enhancement in many
existing methods. More appropriate decomposition of a low-light image can help
achieve better image enhancement. In this paper, we propose a new pixel-level
non-local Haar transform based illumination and reflectance decomposition
method (NLHD). The unique low-frequency coefficient of Haar transform on each
similar pixel group is used to reconstruct the illumination component, and the
rest of all high-frequency coefficients are employed to reconstruct the
reflectance component. The complete similarity of pixels in a matched similar
pixel group and the simple separable Haar transform help to obtain more
appropriate image decomposition; thus, the image is hardly sharpened in the
image brightness enhancement procedure. The exponential transform and
logarithmic transform are respectively implemented on the illumination
component. Then a minimum fusion strategy on the results of these two
transforms is utilized to achieve more natural illumination component
enhancement. It can alleviate the mosaic artifacts produced in the darker
regions by the exponential transform with a gamma value less than 1 and reduce
information loss caused by excessive enhancement of the brighter regions due to
the logarithmic transform. Finally, the Retinex model is applied to the
enhanced illumination and reflectance to achieve image enhancement. We also
develop a local noise level estimation based noise suppression method and a
non-local saturation reduction based color deviation correction method. These
two methods can respectively attenuate noise or color deviation usually
presented in the enhanced results of the extremely dark low-light images.
Experiments on benchmark datasets show that the proposed method can achieve
better low-light image enhancement results on subjective and objective
evaluations than most existing methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Spatio-Temporal Poisson Point Process: A Simple Model for the Alignment of Event Camera Data. (arXiv:2106.06887v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1">Cheng Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Learned_Miller_E/0/1/0/all/0/1">Erik Learned-Miller</a>, <a href="http://arxiv.org/find/cs/1/au:+Sheldon_D/0/1/0/all/0/1">Daniel Sheldon</a>, <a href="http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1">Guillermo Gallego</a>, <a href="http://arxiv.org/find/cs/1/au:+Bideau_P/0/1/0/all/0/1">Pia Bideau</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06887">
                                    <div class="article-summary-box-inner">
                                        <span>Event cameras, inspired by biological vision systems, provide a natural and
data efficient representation of visual information. Visual information is
acquired in the form of events that are triggered by local brightness changes.
Each pixel location of the camera&#x27;s sensor records events asynchronously and
independently with very high temporal resolution. However, because most
brightness changes are triggered by relative motion of the camera and the
scene, the events recorded at a single sensor location seldom correspond to the
same world point. To extract meaningful information from event cameras, it is
helpful to register events that were triggered by the same underlying world
point. In this work we propose a new model of event data that captures its
natural spatio-temporal structure. We start by developing a model for aligned
event data. That is, we develop a model for the data as though it has been
perfectly registered already. In particular, we model the aligned data as a
spatio-temporal Poisson point process. Based on this model, we develop a
maximum likelihood approach to registering events that are not yet aligned.
That is, we find transformations of the observed events that make them as
likely as possible under our model. In particular we extract the camera
rotation that leads to the best event alignment. We show new state of the art
accuracy for rotational velocity estimation on the DAVIS 240C dataset. In
addition, our method is also faster and has lower computational complexity than
several competing methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">LE-NAS: Learning-based Ensenble with NAS for Dose Prediction. (arXiv:2106.06733v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yanfei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingguang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guocai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1">Kai Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06733">
                                    <div class="article-summary-box-inner">
                                        <span>Radiation therapy treatment planning is a complex process, as the target dose
prescription and normal tissue sparing are conflicting objectives. Automated
and accurate dose prediction for radiation therapy planning is in high demand.
In this study, we propose a novel learning-based ensemble approach, named
LE-NAS, which integrates neural architecture search (NAS) with knowledge
distillation for 3D radiotherapy dose prediction. Specifically, the prediction
network first exhaustively searches each block from enormous architecture
space. Then, multiple architectures are selected with promising performance and
diversity. To reduce the inference time, we adopt the teacher-student paradigm
by treating the combination of diverse outputs from multiple searched networks
as supervisions to guide the student network training. In addition, we apply
adversarial learning to optimize the student network to recover the knowledge
in teacher networks. To the best of our knowledge, we are the first to
investigate the combination of NAS and knowledge distillation. The proposed
method has been evaluated on the public OpenKBP dataset, and experimental
results demonstrate the effectiveness of our method and its superior
performance to the state-of-the-art method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1">Pietro Barbiero</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1">Gabriele Ciravegna</a>, <a href="http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1">Francesco Giannini</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf3;</a>, <a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1">Marco Gori</a>, <a href="http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1">Stefano Melacci</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06804">
                                    <div class="article-summary-box-inner">
                                        <span>Explainable artificial intelligence has rapidly emerged since lawmakers have
started requiring interpretable models for safety-critical domains.
Concept-based neural networks have arisen as explainable-by-design methods as
they leverage human-understandable symbols (i.e. concepts) to predict class
memberships. However, most of these approaches focus on the identification of
the most relevant concepts but do not provide concise, formal explanations of
how such concepts are leveraged by the classifier to make predictions. In this
paper, we propose a novel end-to-end differentiable approach enabling the
extraction of logic explanations from neural networks using the formalism of
First-Order Logic. The method relies on an entropy-based criterion which
automatically identifies the most relevant concepts. We consider four different
case studies to demonstrate that: (i) this entropy-based criterion enables the
distillation of concise logic explanations in safety-critical domains from
clinical data to computer vision; (ii) the proposed approach outperforms
state-of-the-art white-box models in terms of classification accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Descent for Visual 3D Human Pose and Shape. (arXiv:2008.06910v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zanfir_A/0/1/0/all/0/1">Andrei Zanfir</a>, <a href="http://arxiv.org/find/cs/1/au:+Bazavan_E/0/1/0/all/0/1">Eduard Gabriel Bazavan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanfir_M/0/1/0/all/0/1">Mihai Zanfir</a>, <a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1">William T. Freeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Sukthankar_R/0/1/0/all/0/1">Rahul Sukthankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1">Cristian Sminchisescu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.06910">
                                    <div class="article-summary-box-inner">
                                        <span>We present deep neural network methodology to reconstruct the 3d pose and
shape of people, given an input RGB image. We rely on a recently introduced,
expressivefull body statistical 3d human model, GHUM, trained end-to-end, and
learn to reconstruct its pose and shape state in a self-supervised regime.
Central to our methodology, is a learning to learn and optimize approach,
referred to as HUmanNeural Descent (HUND), which avoids both second-order
differentiation when training the model parameters,and expensive state gradient
descent in order to accurately minimize a semantic differentiable rendering
loss at test time. Instead, we rely on novel recurrent stages to update the
pose and shape parameters such that not only losses are minimized effectively,
but the process is meta-regularized in order to ensure end-progress. HUND&#x27;s
symmetry between training and testing makes it the first 3d human sensing
architecture to natively support different operating regimes including
self-supervised ones. In diverse tests, we show that HUND achieves very
competitive results in datasets like H3.6M and 3DPW, aswell as good quality 3d
reconstructions for complex imagery collected in-the-wild.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CAR-Net: Unsupervised Co-Attention Guided Registration Network for Joint Registration and Structure Learning. (arXiv:2106.06637v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1">Nishant Ravikumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1">Alejandro F Frangi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06637">
                                    <div class="article-summary-box-inner">
                                        <span>Image registration is a fundamental building block for various applications
in medical image analysis. To better explore the correlation between the fixed
and moving images and improve registration performance, we propose a novel deep
learning network, Co-Attention guided Registration Network (CAR-Net). CAR-Net
employs a co-attention block to learn a new representation of the inputs, which
drives the registration of the fixed and moving images. Experiments on UK
Biobank cardiac cine-magnetic resonance image data demonstrate that CAR-Net
obtains higher registration accuracy and smoother deformation fields than
state-of-the-art unsupervised registration methods, while achieving comparable
or better registration performance than corresponding weakly-supervised
variants. In addition, our approach can provide critical structural information
of the input fixed and moving images simultaneously in a completely
unsupervised manner.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sparse PointPillars: Exploiting Sparsity in Birds-Eye-View Object Detection. (arXiv:2106.06882v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vedder_K/0/1/0/all/0/1">Kyle Vedder</a>, <a href="http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1">Eric Eaton</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06882">
                                    <div class="article-summary-box-inner">
                                        <span>Bird&#x27;s Eye View (BEV) is a popular representation for processing 3D point
clouds, and by its nature is fundamentally sparse. Motivated by the
computational limitations of mobile robot platforms, we take a fast
high-performance BEV 3D object detector - PointPillars - and modify its
backbone to exploit this sparsity, leading to decreased runtimes. We present
preliminary results demonstrating decreased runtimes with either the same
performance or a modest decrease in performance, which we anticipate will be
remedied by model specific hyperparameter tuning. Our work is a first step
towards a new class of 3D object detectors that exploit sparsity throughout
their entire pipeline in order to reduce runtime and resource usage while
maintaining good detection performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Large-Scale Unsupervised Object Discovery. (arXiv:2106.06650v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1">Huy V. Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Sizikova_E/0/1/0/all/0/1">Elena Sizikova</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1">Cordelia Schmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1">Patrick P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1">Jean Ponce</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06650">
                                    <div class="article-summary-box-inner">
                                        <span>Existing approaches to unsupervised object discovery (UOD) do not scale up to
large datasets without approximations which compromise their performance. We
propose a novel formulation of UOD as a ranking problem, amenable to the
arsenal of distributed methods available for eigenvalue problems and link
analysis. Extensive experiments with COCO and OpenImages demonstrate that, in
the single-object discovery setting where a single prominent object is sought
in each image, the proposed LOD (Large-scale Object Discovery) approach is on
par with, or better than the state of the art for medium-scale datasets (up to
120K images), and over 37% better than the only other algorithms capable of
scaling up to 1.7M images. In the multi-object discovery setting where multiple
objects are sought in each image, the proposed LOD is over 14% better in
average precision (AP) than all other methods for datasets ranging from 20K to
1.7M images.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cluster-to-Conquer: A Framework for End-to-End Multi-Instance Learning for Whole Slide Image Classification. (arXiv:2103.10626v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Sharma_Y/0/1/0/all/0/1">Yash Sharma</a>, <a href="http://arxiv.org/find/eess/1/au:+Shrivastava_A/0/1/0/all/0/1">Aman Shrivastava</a>, <a href="http://arxiv.org/find/eess/1/au:+Ehsan_L/0/1/0/all/0/1">Lubaina Ehsan</a>, <a href="http://arxiv.org/find/eess/1/au:+Moskaluk_C/0/1/0/all/0/1">Christopher A. Moskaluk</a>, <a href="http://arxiv.org/find/eess/1/au:+Syed_S/0/1/0/all/0/1">Sana Syed</a>, <a href="http://arxiv.org/find/eess/1/au:+Brown_D/0/1/0/all/0/1">Donald E. Brown</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10626">
                                    <div class="article-summary-box-inner">
                                        <span>In recent years, the availability of digitized Whole Slide Images (WSIs) has
enabled the use of deep learning-based computer vision techniques for automated
disease diagnosis. However, WSIs present unique computational and algorithmic
challenges. WSIs are gigapixel-sized ($\sim$100K pixels), making them
infeasible to be used directly for training deep neural networks. Also, often
only slide-level labels are available for training as detailed annotations are
tedious and can be time-consuming for experts. Approaches using
multiple-instance learning (MIL) frameworks have been shown to overcome these
challenges. Current state-of-the-art approaches divide the learning framework
into two decoupled parts: a convolutional neural network (CNN) for encoding the
patches followed by an independent aggregation approach for slide-level
prediction. In this approach, the aggregation step has no bearing on the
representations learned by the CNN encoder. We have proposed an end-to-end
framework that clusters the patches from a WSI into ${k}$-groups, samples
${k}&#x27;$ patches from each group for training, and uses an adaptive attention
mechanism for slide level prediction; Cluster-to-Conquer (C2C). We have
demonstrated that dividing a WSI into clusters can improve the model training
by exposing it to diverse discriminative features extracted from the patches.
We regularized the clustering mechanism by introducing a KL-divergence loss
between the attention weights of patches in a cluster and the uniform
distribution. The framework is optimized end-to-end on slide-level
cross-entropy, patch-level cross-entropy, and KL-divergence loss
(Implementation: https://github.com/YashSharma/C2C).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Alpha Matte Generation from Single Input for Portrait Matting. (arXiv:2106.03210v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1">Dogucan Yaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1">Haz&#x131;m Kemal Ekenel</a>, <a href="http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1">Alexander Waibel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03210">
                                    <div class="article-summary-box-inner">
                                        <span>Portrait matting is an important research problem with a wide range of
applications, such as video conference app, image/video editing, and
post-production. The goal is to predict an alpha matte that identifies the
effect of each pixel on the foreground subject. Traditional approaches and most
of the existing works utilized an additional input, e.g., trimap, background
image, to predict alpha matte. However, providing additional input is not
always practical. Besides, models are too sensitive to these additional inputs.
In this paper, we introduce an additional input-free approach to perform
portrait matting using Generative Adversarial Nets (GANs). We divide the main
task into two subtasks. For this, we propose a segmentation network for the
person segmentation and the alpha generation network for alpha matte
prediction. While the segmentation network takes an input image and produces a
coarse segmentation map, the alpha generation network utilizes the same input
image as well as a coarse segmentation map that is produced by the segmentation
network to predict the alpha matte. Besides, we present a segmentation encoding
block to downsample the coarse segmentation map and provide feature
representation to the residual block. Furthermore, we propose border loss to
penalize only the borders of the subject separately which is more likely to be
challenging and we also adapt perceptual loss for portrait matting. To train
the proposed system, we combine two different popular training datasets to
improve the amount of data as well as diversity to address domain shift
problems in the inference time. We tested our model on three different
benchmark datasets, namely Adobe Image Matting dataset, Portrait Matting
dataset, and Distinctions dataset. The proposed method outperformed the MODNet
method that also takes a single input.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Barlow Twins: Self-Supervised Learning via Redundancy Reduction. (arXiv:2103.03230v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zbontar_J/0/1/0/all/0/1">Jure Zbontar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1">Li Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1">Ishan Misra</a>, <a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1">Yann LeCun</a>, <a href="http://arxiv.org/find/cs/1/au:+Deny_S/0/1/0/all/0/1">St&#xe9;phane Deny</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03230">
                                    <div class="article-summary-box-inner">
                                        <span>Self-supervised learning (SSL) is rapidly closing the gap with supervised
methods on large computer vision benchmarks. A successful approach to SSL is to
learn embeddings which are invariant to distortions of the input sample.
However, a recurring issue with this approach is the existence of trivial
constant solutions. Most current methods avoid such solutions by careful
implementation details. We propose an objective function that naturally avoids
collapse by measuring the cross-correlation matrix between the outputs of two
identical networks fed with distorted versions of a sample, and making it as
close to the identity matrix as possible. This causes the embedding vectors of
distorted versions of a sample to be similar, while minimizing the redundancy
between the components of these vectors. The method is called Barlow Twins,
owing to neuroscientist H. Barlow&#x27;s redundancy-reduction principle applied to a
pair of identical networks. Barlow Twins does not require large batches nor
asymmetry between the network twins such as a predictor network, gradient
stopping, or a moving average on the weight updates. Intriguingly it benefits
from very high-dimensional output vectors. Barlow Twins outperforms previous
methods on ImageNet for semi-supervised classification in the low-data regime,
and is on par with current state of the art for ImageNet classification with a
linear classifier head, and for transfer tasks of classification and object
detection.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Probabilistic Embeddings for Cross-Modal Retrieval. (arXiv:2101.05068v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1">Sanghyuk Chun</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Seong Joon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezende_R/0/1/0/all/0/1">Rafael Sampaio de Rezende</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalantidis_Y/0/1/0/all/0/1">Yannis Kalantidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Larlus_D/0/1/0/all/0/1">Diane Larlus</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05068">
                                    <div class="article-summary-box-inner">
                                        <span>Cross-modal retrieval methods build a common representation space for samples
from multiple modalities, typically from the vision and the language domains.
For images and their captions, the multiplicity of the correspondences makes
the task particularly challenging. Given an image (respectively a caption),
there are multiple captions (respectively images) that equally make sense. In
this paper, we argue that deterministic functions are not sufficiently powerful
to capture such one-to-many correspondences. Instead, we propose to use
Probabilistic Cross-Modal Embedding (PCME), where samples from the different
modalities are represented as probabilistic distributions in the common
embedding space. Since common benchmarks such as COCO suffer from
non-exhaustive annotations for cross-modal matches, we propose to additionally
evaluate retrieval on the CUB dataset, a smaller yet clean database where all
possible image-caption pairs are annotated. We extensively ablate PCME and
demonstrate that it not only improves the retrieval performance over its
deterministic counterpart but also provides uncertainty estimates that render
the embeddings more interpretable. Code is available at
https://github.com/naver-ai/pcme</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DONet: Dual-Octave Network for Fast MR Image Reconstruction. (arXiv:2105.05980v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1">Chun-Mei Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1">Zhanyuan Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1">Huazhu Fu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1">Yong Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1">Ling Shao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05980">
                                    <div class="article-summary-box-inner">
                                        <span>Magnetic resonance (MR) image acquisition is an inherently prolonged process,
whose acceleration has long been the subject of research. This is commonly
achieved by obtaining multiple undersampled images, simultaneously, through
parallel imaging. In this paper, we propose the Dual-Octave Network (DONet),
which is capable of learning multi-scale spatial-frequency features from both
the real and imaginary components of MR data, for fast parallel MR image
reconstruction. More specifically, our DONet consists of a series of
Dual-Octave convolutions (Dual-OctConv), which are connected in a dense manner
for better reuse of features. In each Dual-OctConv, the input feature maps and
convolutional kernels are first split into two components (ie, real and
imaginary), and then divided into four groups according to their spatial
frequencies. Then, our Dual-OctConv conducts intra-group information updating
and inter-group information exchange to aggregate the contextual information
across different groups. Our framework provides three appealing benefits: (i)
It encourages information interaction and fusion between the real and imaginary
components at various spatial frequencies to achieve richer representational
capacity. (ii) The dense connections between the real and imaginary groups in
each Dual-OctConv make the propagation of features more efficient by feature
reuse. (iii) DONet enlarges the receptive field by learning multiple
spatial-frequency features of both the real and imaginary components. Extensive
experiments on two popular datasets (ie, clinical knee and fastMRI), under
different undersampling patterns and acceleration factors, demonstrate the
superiority of our model in accelerated parallel MR image reconstruction.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">NDPNet: A novel non-linear data projection network for few-shot fine-gained image classification. (arXiv:2106.06988v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhangy_W/0/1/0/all/0/1">Weichuan Zhangy</a>, <a href="http://arxiv.org/find/cs/1/au:+Liuy_X/0/1/0/all/0/1">Xuefang Liuy</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1">Zhe Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yongsheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Changming Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06988">
                                    <div class="article-summary-box-inner">
                                        <span>Metric-based few-shot fine-grained image classification (FSFGIC) aims to
learn a transferable feature embedding network by estimating the similarities
between query images and support classes from very few examples. In this work,
we propose, for the first time, to introduce the non-linear data projection
concept into the design of FSFGIC architecture in order to address the limited
sample problem in few-shot learning and at the same time to increase the
discriminability of the model for fine-grained image classification.
Specifically, we first design a feature re-abstraction embedding network that
has the ability to not only obtain the required semantic features for effective
metric learning but also re-enhance such features with finer details from input
images. Then the descriptors of the query images and the support classes are
projected into different non-linear spaces in our proposed similarity metric
learning network to learn discriminative projection factors. This design can
effectively operate in the challenging and restricted condition of a FSFGIC
task for making the distance between the samples within the same class smaller
and the distance between samples from different classes larger and for reducing
the coupling relationship between samples from different categories.
Furthermore, a novel similarity measure based on the proposed non-linear data
project is presented for evaluating the relationships of feature information
between a query image and a support set. It is worth to note that our proposed
architecture can be easily embedded into any episodic training mechanisms for
end-to-end training from scratch. Extensive experiments on FSFGIC tasks
demonstrate the superiority of the proposed methods over the state-of-the-art
benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning. (arXiv:2105.15134v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1">Zixin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhi Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15134">
                                    <div class="article-summary-box-inner">
                                        <span>How can neural networks trained by contrastive learning extract features from
the unlabeled data? Why does contrastive learning usually need much stronger
data augmentations than supervised learning to ensure good representations?
These questions involve both the optimization and statistical aspects of deep
learning, but can hardly be answered by analyzing supervised learning, where
the target functions are the highest pursuit. Indeed, in self-supervised
learning, it is inevitable to relate to the optimization/generalization of
neural networks to how they can encode the latent structures in the data, which
we refer to as the feature learning process.

In this work, we formally study how contrastive learning learns the feature
representations for neural networks by analyzing its feature learning process.
We consider the case where our data are comprised of two types of features: the
more semantically aligned sparse features which we want to learn from, and the
other dense features we want to avoid. Theoretically, we prove that contrastive
learning using $\mathbf{ReLU}$ networks provably learns the desired sparse
features if proper augmentations are adopted. We present an underlying
principle called $\textbf{feature decoupling}$ to explain the effects of
augmentations, where we theoretically characterize how augmentations can reduce
the correlations of dense features between positive samples while keeping the
correlations of sparse features intact, thereby forcing the neural networks to
learn from the self-supervision of sparse features. Empirically, we verified
that the feature decoupling principle matches the underlying mechanism of
contrastive learning in practice.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PVRED: A Position-Velocity Recurrent Encoder-Decoder for Human Motion Prediction. (arXiv:1906.06514v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongsong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jian Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1">Bin Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiashi Feng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.06514">
                                    <div class="article-summary-box-inner">
                                        <span>Human motion prediction, which aims to predict future human poses given past
poses, has recently seen increased interest. Many recent approaches are based
on Recurrent Neural Networks (RNN) which model human poses with exponential
maps. These approaches neglect the pose velocity as well as temporal relation
of different poses, and tend to converge to the mean pose or fail to generate
natural-looking poses. We therefore propose a novel Position-Velocity Recurrent
Encoder-Decoder (PVRED) for human motion prediction, which makes full use of
pose velocities and temporal positional information. A temporal position
embedding method is presented and a Position-Velocity RNN (PVRNN) is proposed.
We also emphasize the benefits of quaternion parameterization of poses and
design a novel trainable Quaternion Transformation (QT) layer, which is
combined with a robust loss function during training. We provide quantitative
results for both short-term prediction in the future 0.5 seconds and long-term
prediction in the future 0.5 to 1 seconds. Experiments on several benchmarks
show that our approach considerably outperforms the state-of-the-art methods.
In addition, qualitative visualizations in the future 4 seconds show that our
approach could predict future human-like and meaningful poses in very long time
horizons. Code is publicly available on GitHub:
\textcolor{red}{https://github.com/hongsong-wang/PVRNN}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v2 [cs.LG] CROSS LISTED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1">Antoine Labatie</a>, <a href="http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1">Dominic Masters</a>, <a href="http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1">Zach Eaton-Rosen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1">Carlo Luschi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03743">
                                    <div class="article-summary-box-inner">
                                        <span>We investigate the reasons for the performance degradation incurred with
batch-independent normalization. We find that the prototypical techniques of
layer normalization and instance normalization both induce the appearance of
failure modes in the neural network&#x27;s pre-activations: (i) layer normalization
induces a collapse towards channel-wise constant functions; (ii) instance
normalization induces a lack of variability in instance statistics, symptomatic
of an alteration of the expressivity. To alleviate failure mode (i) without
aggravating failure mode (ii), we introduce the technique &quot;Proxy Normalization&quot;
that normalizes post-activations using a proxy distribution. When combined with
layer normalization or group normalization, this batch-independent
normalization emulates batch normalization&#x27;s behavior and consistently matches
or exceeds its performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Few-Shot Learning with Class Imbalance. (arXiv:2101.02523v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ochal_M/0/1/0/all/0/1">Mateusz Ochal</a>, <a href="http://arxiv.org/find/cs/1/au:+Patacchiola_M/0/1/0/all/0/1">Massimiliano Patacchiola</a>, <a href="http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1">Amos Storkey</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazquez_J/0/1/0/all/0/1">Jose Vazquez</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sen Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02523">
                                    <div class="article-summary-box-inner">
                                        <span>Few-Shot Learning (FSL) algorithms are commonly trained through Meta-Learning
(ML), which exposes models to batches of tasks sampled from a meta-dataset to
mimic tasks seen during evaluation. However, the standard training procedures
overlook the real-world dynamics where classes commonly occur at different
frequencies. While it is generally understood that class imbalance harms the
performance of supervised methods, limited research examines the impact of
imbalance on the FSL evaluation task. Our analysis compares 10 state-of-the-art
meta-learning and FSL methods on different imbalance distributions and
rebalancing techniques. Our results reveal that 1) some FSL methods display a
natural disposition against imbalance while most other approaches produce a
performance drop by up to 17\% compared to the balanced task without the
appropriate mitigation; 2) contrary to popular belief, many meta-learning
algorithms will not automatically learn to balance from exposure to imbalanced
training tasks; 3) classical rebalancing strategies, such as random
oversampling, can still be very effective, leading to state-of-the-art
performances and should not be overlooked; 4) FSL methods are more robust
against meta-dataset imbalance than imbalance at the task-level with a similar
imbalance ratio ($\rho&lt;20$), with the effect holding even in long-tail datasets
under a larger imbalance ($\rho&#x3D;65$).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Information Obfuscation of Graph Neural Networks. (arXiv:2009.13504v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liao_P/0/1/0/all/0/1">Peiyuan Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Han Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Keyulu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1">Tommi Jaakkola</a>, <a href="http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1">Geoffrey Gordon</a>, <a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1">Stefanie Jegelka</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Ruslan Salakhutdinov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13504">
                                    <div class="article-summary-box-inner">
                                        <span>While the advent of Graph Neural Networks (GNNs) has greatly improved node
and graph representation learning in many applications, the neighborhood
aggregation scheme exposes additional vulnerabilities to adversaries seeking to
extract node-level information about sensitive attributes. In this paper, we
study the problem of protecting sensitive attributes by information obfuscation
when learning with graph structured data. We propose a framework to locally
filter out pre-determined sensitive attributes via adversarial training with
the total variation and the Wasserstein distance. Our method creates a strong
defense against inference attacks, while only suffering small loss in task
performance. Theoretically, we analyze the effectiveness of our framework
against a worst-case adversary, and characterize an inherent trade-off between
maximizing predictive accuracy and minimizing information leakage. Experiments
across multiple datasets from recommender systems, knowledge graphs and quantum
chemistry demonstrate that the proposed approach provides a robust defense
across various graph structures and tasks, while producing competitive GNN
encoders for downstream tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Novel Interaction-based Methodology Towards Explainable AI with Better Understanding of Pneumonia Chest X-ray Images. (arXiv:2104.12672v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1">Shaw-Hwa Lo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yiqiao Yin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12672">
                                    <div class="article-summary-box-inner">
                                        <span>In the field of eXplainable AI (XAI), robust &#x60;&#x60;blackbox&#x27;&#x27; algorithms such as
Convolutional Neural Networks (CNNs) are known for making high prediction
performance. However, the ability to explain and interpret these algorithms
still require innovation in the understanding of influential and, more
importantly, explainable features that directly or indirectly impact the
performance of predictivity. A number of methods existing in literature focus
on visualization techniques but the concepts of explainability and
interpretability still require rigorous definition. In view of the above needs,
this paper proposes an interaction-based methodology -- Influence Score
(I-score) -- to screen out the noisy and non-informative variables in the
images hence it nourishes an environment with explainable and interpretable
features that are directly associated to feature predictivity. We apply the
proposed method on a real world application in Pneumonia Chest X-ray Image data
set and produced state-of-the-art results. We demonstrate how to apply the
proposed approach for more general big data problems by improving the
explainability and interpretability without sacrificing the prediction
performance. The contribution of this paper opens a novel angle that moves the
community closer to the future pipelines of XAI problems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CoPE: Conditional image generation using Polynomial Expansions. (arXiv:2104.05077v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1">Grigorios G Chrysos</a>, <a href="http://arxiv.org/find/cs/1/au:+Georgopoulos_M/0/1/0/all/0/1">Markos Georgopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1">Yannis Panagakis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05077">
                                    <div class="article-summary-box-inner">
                                        <span>Generative modeling has evolved to a notable field of machine learning. Deep
polynomial neural networks (PNNs) have demonstrated impressive results in
unsupervised image generation, where the task is to map an input vector (i.e.,
noise) to a synthesized image. However, the success of PNNs has not been
replicated in conditional generation tasks, such as super-resolution. Existing
PNNs focus on single-variable polynomial expansions which do not fare well to
two-variable inputs, i.e., the noise variable and the conditional variable. In
this work, we introduce a general framework, called CoPE, that enables a
polynomial expansion of two input variables and captures their auto- and
cross-correlations. We exhibit how CoPE can be trivially augmented to accept an
arbitrary number of input variables. CoPE is evaluated in five tasks
(class-conditional generation, inverse problems, edges-to-image translation,
image-to-image translation, attribute-guided generation) involving eight
datasets. The thorough evaluation suggests that CoPE can be useful for tackling
diverse conditional generation tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Revisiting Classification Perspective on Scene Text Recognition. (arXiv:2102.10884v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Hongxiang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jun Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yichao Xiong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10884">
                                    <div class="article-summary-box-inner">
                                        <span>The prevalent perspectives of scene text recognition are from sequence to
sequence (seq2seq) and segmentation. Nevertheless, the former is composed of
many components which makes implementation and deployment complicated, while
the latter requires character level annotations that is expensive. In this
paper, we revisit classification perspective that models scene text recognition
as an image classification problem. Classification perspective has a simple
pipeline and only needs word level annotations. We revive classification
perspective by devising a scene text recognition model named as CSTR, which
performs as well as methods from other perspectives. The CSTR model consists of
CPNet (classification perspective network) and SPPN (separated conv with global
average pooling prediction network). CSTR is as simple as image classification
model like ResNet \cite{he2016deep} which makes it easy to implement and
deploy. We demonstrate the effectiveness of the classification perspective on
scene text recognition with extensive experiments. Futhermore, CSTR achieves
nearly state-of-the-art performance on six public benchmarks including regular
text, irregular text. The code will be available at
https://github.com/Media-Smart/vedastr.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SPADE: A Spectral Method for Black-Box Adversarial Robustness Evaluation. (arXiv:2102.03716v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wuxinlin Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Chenhui Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhiqiang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yaohui Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiru Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhuo Feng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03716">
                                    <div class="article-summary-box-inner">
                                        <span>A black-box spectral method is introduced for evaluating the adversarial
robustness of a given machine learning (ML) model. Our approach, named SPADE,
exploits bijective distance mapping between the input/output graphs constructed
for approximating the manifolds corresponding to the input/output data. By
leveraging the generalized Courant-Fischer theorem, we propose a SPADE score
for evaluating the adversarial robustness of a given model, which is proved to
be an upper bound of the best Lipschitz constant under the manifold setting. To
reveal the most non-robust data samples highly vulnerable to adversarial
attacks, we develop a spectral graph embedding procedure leveraging dominant
generalized eigenvectors. This embedding step allows assigning each data sample
a robustness score that can be further harnessed for more effective adversarial
training. Our experiments show the proposed SPADE method leads to promising
empirical results for neural network models that are adversarially trained with
the MNIST and CIFAR-10 data sets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes. (arXiv:2012.05522v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiashun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huazhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sifei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05522">
                                    <div class="article-summary-box-inner">
                                        <span>Synthesizing 3D human motion plays an important role in many graphics
applications as well as understanding human activity. While many efforts have
been made on generating realistic and natural human motion, most approaches
neglect the importance of modeling human-scene interactions and affordance. On
the other hand, affordance reasoning (e.g., standing on the floor or sitting on
the chair) has mainly been studied with static human pose and gestures, and it
has rarely been addressed with human motion. In this paper, we propose to
bridge human motion synthesis and scene affordance reasoning. We present a
hierarchical generative framework to synthesize long-term 3D human motion
conditioning on the 3D scene structure. Building on this framework, we further
enforce multiple geometry constraints between the human mesh and scene point
clouds via optimization to improve realistic synthesis. Our experiments show
significant improvements over previous approaches on generating natural and
physically plausible human motion in a scene.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fast and Robust Certifiable Estimation of the Relative Pose Between Two Calibrated Cameras. (arXiv:2101.08524v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Garcia_Salguero_M/0/1/0/all/0/1">Mercedes Garcia-Salguero</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Jimenez_J/0/1/0/all/0/1">Javier Gonzalez-Jimenez</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08524">
                                    <div class="article-summary-box-inner">
                                        <span>This work contributes an efficient algorithm to compute the Relative Pose
problem (RPp) between calibrated cameras and certify the optimality of the
solution, given a set of pair-wise feature correspondences affected by noise
and probably corrupted by wrong matches. We propose a family of certifiers that
is shown to increase the ratio of detected optimal solutions. This set of
certifiers is incorporated into a fast essential matrix estimation pipeline
that, given any initial guess for the RPp, refines it iteratively on the
product space of 3D rotations and 2-sphere. In addition, this fast certifiable
pipeline is integrated into a robust framework that combines Graduated
Non-convexity and the Black-Rangarajan duality between robust functions and
line processes.

We proved through extensive experiments on synthetic and real data that the
proposed framework provides a fast and robust relative pose estimation. We make
the code publicly available
\url{https://github.com/mergarsal/FastCertRelPose.git}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-Disease Classification of 13,667 Body CT Scans Using Weakly Supervised Deep Learning. (arXiv:2008.01158v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tushar_F/0/1/0/all/0/1">Fakrul Islam Tushar</a>, <a href="http://arxiv.org/find/cs/1/au:+DAnniballe_V/0/1/0/all/0/1">Vincent M. D&#x27;Anniballe</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1">Rui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1">Maciej A. Mazurowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1">Wanyi Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Samei_E/0/1/0/all/0/1">Ehsan Samei</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubin_G/0/1/0/all/0/1">Geoffrey D. Rubin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_J/0/1/0/all/0/1">Joseph Y. Lo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01158">
                                    <div class="article-summary-box-inner">
                                        <span>Background: Training deep learning classifiers typically requires massive
amounts of manual annotation. Weak supervision may leverage existing medical
data to classify multiple diseases and organ systems. Purpose: To design
multi-disease classifiers for body computed tomography (CT) scans using
automatically extracted labels from radiology text reports. Materials &amp;
Methods: This retrospective study deployed rule-based algorithms to extract
19,255 disease labels from reports of 13,667 body CT scans of 12,092 subjects
for training. Using a 3D DenseVNet, three organ systems were segmented:
lungs/pleura, liver/gallbladder, and kidneys/ureters. For each organ, a 3D
convolutional neural network classified normality versus four common diseases.
Testing was performed on an additional 2,158 CT volumes relative to 2,875
manually derived reference labels. Results: Manual validation of the extracted
labels confirmed 91 to 99% accuracy. Performance using the receiver operating
characteristic area under the curve (AUC) for lungs/pleura labels were as
follows: atelectasis 0.77 (95% CI: 0.74 to 0.81), nodule 0.65 (0.61 to 0.69),
emphysema 0.89 (0.86 to 0.92), effusion 0.97 (0.96 to 0.98), and normal 0.89
(0.87 to 0.91). For liver/gallbladder: stone 0.62 (0.56 to 0.67), lesion 0.73
(0.69 to 0.77), dilation 0.87 (0.84 to 0.90), fatty 0.89 (0.86 to 0.92), and
normal 0.82 (0.78 to 0.85). For kidneys/ureters: stone 0.83 (0.79 to 0.87),
atrophy 0.92 (0.89 to 0.94), lesion 0.68 (0.64 to 0.72), cyst 0.70 (0.66 to
0.73), and normal 0.79 (0.75 to 0.83). Conclusion: Weakly supervised deep
learning classifiers leveraged massive amounts of unannotated body CT data to
classify multiple organ systems and diverse diseases.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Diffusion Probabilistic Models for 3D Point Cloud Generation. (arXiv:2103.01458v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Shitong Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wei Hu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01458">
                                    <div class="article-summary-box-inner">
                                        <span>We present a probabilistic model for point cloud generation, which is
fundamental for various 3D vision tasks such as shape completion, upsampling,
synthesis and data augmentation. Inspired by the diffusion process in
non-equilibrium thermodynamics, we view points in point clouds as particles in
a thermodynamic system in contact with a heat bath, which diffuse from the
original distribution to a noise distribution. Point cloud generation thus
amounts to learning the reverse diffusion process that transforms the noise
distribution to the distribution of a desired shape. Specifically, we propose
to model the reverse diffusion process for point clouds as a Markov chain
conditioned on certain shape latent. We derive the variational bound in closed
form for training and provide implementations of the model. Experimental
results demonstrate that our model achieves competitive performance in point
cloud generation and auto-encoding. The code is available at
\url{https://github.com/luost26/diffusion-point-cloud}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fenglin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1">Changchang Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1">Shen Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Ping Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xu Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06965">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, chest X-ray report generation, which aims to automatically generate
descriptions of given chest X-ray images, has received growing research
interests. The key challenge of chest X-ray report generation is to accurately
capture and describe the abnormal regions. In most cases, the normal regions
dominate the entire chest X-ray image, and the corresponding descriptions of
these normal regions dominate the final report. Due to such data bias,
learning-based models may fail to attend to abnormal regions. In this work, to
effectively capture and describe abnormal regions, we propose the Contrastive
Attention (CA) model. Instead of solely focusing on the current input image,
the CA model compares the current input image with normal images to distill the
contrastive information. The acquired contrastive information can better
represent the visual features of abnormal regions. According to the experiments
on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into
several existing models can boost their performance across most metrics. In
addition, according to the analysis, the CA model can help existing models
better attend to the abnormal regions and provide more accurate descriptions
which are crucial for an interpretable diagnosis. Specifically, we achieve the
state-of-the-art results on the two public datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An Integrated Approach to Produce Robust Models with High Efficiency. (arXiv:2008.13305v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhijian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1">Jack Xin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.13305">
                                    <div class="article-summary-box-inner">
                                        <span>Deep Neural Networks (DNNs) needs to be both efficient and robust for
practical uses. Quantization and structure simplification are promising ways to
adapt DNNs to mobile devices, and adversarial training is the most popular
method to make DNNs robust. In this work, we try to obtain both features by
applying a convergent relaxation quantization algorithm, Binary-Relax (BR), to
a robust adversarial-trained model, ResNets Ensemble via Feynman-Kac Formalism
(EnResNet). We also discover that high precision, such as ternary (tnn) and
4-bit, quantization will produce sparse DNNs. However, this sparsity is
unstructured under advarsarial training. To solve the problems that adversarial
training jeopardizes DNNs&#x27; accuracy on clean images and the struture of
sparsity, we design a trade-off loss function that helps DNNs preserve their
natural accuracy and improve the channel sparsity. With our trade-off loss
function, we achieve both goals with no reduction of resistance under weak
attacks and very minor reduction of resistance under strong attcks. Together
with quantized EnResNet with trade-off loss function, we provide robust models
that have high efficiency.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning from Crowds by Modeling Common Confusions. (arXiv:2012.13052v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1">Zhendong Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jing Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongning Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13052">
                                    <div class="article-summary-box-inner">
                                        <span>Crowdsourcing provides a practical way to obtain large amounts of labeled
data at a low cost. However, the annotation quality of annotators varies
considerably, which imposes new challenges in learning a high-quality model
from the crowdsourced annotations. In this work, we provide a new perspective
to decompose annotation noise into common noise and individual noise and
differentiate the source of confusion based on instance difficulty and
annotator expertise on a per-instance-annotator basis. We realize this new
crowdsourcing model by an end-to-end learning solution with two types of noise
adaptation layers: one is shared across annotators to capture their commonly
shared confusions, and the other one is pertaining to each annotator to realize
individual confusion. To recognize the source of noise in each annotation, we
use an auxiliary network to choose the two noise adaptation layers with respect
to both instances and annotators. Extensive experiments on both synthesized and
real-world benchmarks demonstrate the effectiveness of our proposed common
noise adaptation solution.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning the Imaging Landmarks: Unsupervised Key point Detection in Lung Ultrasound Videos. (arXiv:2106.06987v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Tripathi_A/0/1/0/all/0/1">Arpan Tripathi</a>, <a href="http://arxiv.org/find/eess/1/au:+Panicker_M/0/1/0/all/0/1">Mahesh Raveendranatha Panicker</a>, <a href="http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1">Abhilash R Hareendranathan</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yale Tung Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Jaremko_J/0/1/0/all/0/1">Jacob L Jaremko</a>, <a href="http://arxiv.org/find/eess/1/au:+Narayan_K/0/1/0/all/0/1">Kiran Vishnu Narayan</a>, <a href="http://arxiv.org/find/eess/1/au:+C_K/0/1/0/all/0/1">Kesavadas C</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06987">
                                    <div class="article-summary-box-inner">
                                        <span>Lung ultrasound (LUS) is an increasingly popular diagnostic imaging modality
for continuous and periodic monitoring of lung infection, given its advantages
of non-invasiveness, non-ionizing nature, portability and easy disinfection.
The major landmarks assessed by clinicians for triaging using LUS are pleura, A
and B lines. There have been many efforts for the automatic detection of these
landmarks. However, restricting to a few pre-defined landmarks may not reveal
the actual imaging biomarkers particularly in case of new pathologies like
COVID-19. Rather, the identification of key landmarks should be driven by data
given the availability of a plethora of neural network algorithms. This work is
a first of its kind attempt towards unsupervised detection of the key LUS
landmarks in LUS videos of COVID-19 subjects during various stages of
infection. We adapted the relatively newer approach of transporter neural
networks to automatically mark and track pleura, A and B lines based on their
periodic motion and relatively stable appearance in the videos. Initial results
on unsupervised pleura detection show an accuracy of 91.8% employing 1081 LUS
video frames.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Compression of Deep Learning Models for Text: A Survey. (arXiv:2008.05221v4 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1">Manish Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Puneet Agrawal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.05221">
                                    <div class="article-summary-box-inner">
                                        <span>In recent years, the fields of natural language processing (NLP) and
information retrieval (IR) have made tremendous progress thanksto deep learning
models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and
Long Short-Term Memory (LSTMs)networks, and Transformer [120] based models like
Bidirectional Encoder Representations from Transformers (BERT) [24],
GenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network
(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer
transformer (T5) [95], T-NLG [98] and GShard [63]. But these models are
humongous in size. On the other hand,real world applications demand small model
size, low response times and low computational power wattage. In this survey,
wediscuss six different types of methods (Pruning, Quantization, Knowledge
Distillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic
Transformer based methods) for compression of such models to enable their
deployment in real industry NLP projects.Given the critical need of building
applications with efficient and small models, and the large amount of recently
published work inthis area, we believe that this survey organizes the plethora
of work done by the &#x27;deep learning for NLP&#x27; community in the past fewyears and
presents it as a coherent story.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Capsule Attention for Multimodal EEG-EOG Representation Learning with Application to Driver Vigilance Estimation. (arXiv:1912.07812v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guangyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1">Ali Etemad</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.07812">
                                    <div class="article-summary-box-inner">
                                        <span>Driver vigilance estimation is an important task for transportation safety.
Wearable and portable brain-computer interface devices provide a powerful means
for real-time monitoring of the vigilance level of drivers to help with
avoiding distracted or impaired driving. In this paper, we propose a novel
multimodal architecture for in-vehicle vigilance estimation from
Electroencephalogram and Electrooculogram. To enable the system to focus on the
most salient parts of the learned multimodal representations, we propose an
architecture composed of a capsule attention mechanism following a deep Long
Short-Term Memory (LSTM) network. Our model learns hierarchical dependencies in
the data through the LSTM and capsule feature representation layers. To better
explore the discriminative ability of the learned representations, we study the
effect of the proposed capsule attention mechanism including the number of
dynamic routing iterations as well as other parameters. Experiments show the
robustness of our method by outperforming other solutions and baseline
techniques, setting a new state-of-the-art. We then provide an analysis on
different frequency bands and brain regions to evaluate their suitability for
driver vigilance estimation. Lastly, an analysis on the role of capsule
attention, multimodality, and robustness to noise is performed, highlighting
the advantages of our approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust Image Classification Using A Low-Pass Activation Function and DCT Augmentation. (arXiv:2007.09453v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1">Md Tahmid Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1">Shyh Wei Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohel_F/0/1/0/all/0/1">Ferdous Sohel</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1">Guojun Lu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.09453">
                                    <div class="article-summary-box-inner">
                                        <span>Convolutional Neural Network&#x27;s (CNN&#x27;s) performance disparity on clean and
corrupted datasets has recently come under scrutiny. In this work, we analyse
common corruptions in the frequency domain, i.e., High Frequency corruptions
(HFc, e.g., noise) and Low Frequency corruptions (LFc, e.g., blur). Although a
simple solution to HFc is low-pass filtering, ReLU -- a widely used Activation
Function (AF), does not have any filtering mechanism. In this work, we instill
low-pass filtering into the AF (LP-ReLU) to improve robustness against HFc. To
deal with LFc, we complement LP-ReLU with Discrete Cosine Transform based
augmentation. LP-ReLU, coupled with DCT augmentation, enables a deep network to
tackle the entire spectrum of corruption. We use CIFAR-10-C and Tiny ImageNet-C
for evaluation and demonstrate improvements of 5% and 7.3% in accuracy
respectively, compared to the State-Of-The-Art (SOTA). We further evaluate our
method&#x27;s stability on a variety of perturbations in CIFAR-10-P and Tiny
ImageNet-P, achieving new SOTA in these experiments as well. To further
strengthen our understanding regarding CNN&#x27;s lack of robustness, a decision
space visualisation process is proposed and presented in this work.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fenglin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1">Shen Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1">Wei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yuexian Zou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06963">
                                    <div class="article-summary-box-inner">
                                        <span>Automatically generating radiology reports can improve current clinical
practice in diagnostic radiology. On one hand, it can relieve radiologists from
the heavy burden of report writing; On the other hand, it can remind
radiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.
Yet, this task remains a challenging job for data-driven neural networks, due
to the serious visual and textual data biases. To this end, we propose a
Posterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to
imitate the working patterns of radiologists, who will first examine the
abnormal regions and assign the disease topic tags to the abnormal regions, and
then rely on the years of prior medical knowledge and prior working experience
accumulations to write reports. Thus, the PPKED includes three modules:
Posterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and
Multi-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior
knowledge, which provides explicit abnormal visual regions to alleviate visual
data bias; PrKE explores the prior knowledge from the prior medical knowledge
graph (medical knowledge) and prior radiology reports (working experience) to
alleviate textual data bias. The explored knowledge is distilled by the MKD to
generate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our
method is able to outperform previous state-of-the-art models on these two
datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Feedback Pyramid Attention Networks for Single Image Super-Resolution. (arXiv:2106.06966v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Huapeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_J/0/1/0/all/0/1">Jie Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1">James T. Kwok</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhihui Wei</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06966">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, convolutional neural network (CNN) based image super-resolution
(SR) methods have achieved significant performance improvement. However, most
CNN-based methods mainly focus on feed-forward architecture design and neglect
to explore the feedback mechanism, which usually exists in the human visual
system. In this paper, we propose feedback pyramid attention networks (FPAN) to
fully exploit the mutual dependencies of features. Specifically, a novel
feedback connection structure is developed to enhance low-level feature
expression with high-level information. In our method, the output of each layer
in the first stage is also used as the input of the corresponding layer in the
next state to re-update the previous low-level filters. Moreover, we introduce
a pyramid non-local structure to model global contextual information in
different scales and improve the discriminative representation of the network.
Extensive experimental results on various datasets demonstrate the superiority
of our FPAN in comparison with the state-of-the-art SR methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DeepMMSA: A Novel Multimodal Deep Learning Method for Non-small Cell Lung Cancer Survival Analysis. (arXiv:2106.06744v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yujiao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jie Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaoshui Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1">Sai Ho Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1">Steven Weidong Su</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06744">
                                    <div class="article-summary-box-inner">
                                        <span>Lung cancer is the leading cause of cancer death worldwide. The critical
reason for the deaths is delayed diagnosis and poor prognosis. With the
accelerated development of deep learning techniques, it has been successfully
applied extensively in many real-world applications, including health sectors
such as medical image interpretation and disease diagnosis. By combining more
modalities that being engaged in the processing of information, multimodal
learning can extract better features and improve predictive ability. The
conventional methods for lung cancer survival analysis normally utilize
clinical data and only provide a statistical probability. To improve the
survival prediction accuracy and help prognostic decision-making in clinical
practice for medical experts, we for the first time propose a multimodal deep
learning method for non-small cell lung cancer (NSCLC) survival analysis, named
DeepMMSA. This method leverages CT images in combination with clinical data,
enabling the abundant information hold within medical images to be associate
with lung cancer survival information. We validate our method on the data of
422 NSCLC patients from The Cancer Imaging Archive (TCIA). Experimental results
support our hypothesis that there is an underlying relationship between
prognostic information and radiomic images. Besides, quantitative results
showing that the established multimodal model can be applied to traditional
method and has the potential to break bottleneck of existing methods and
increase the the percentage of concordant pairs(right predicted pairs) in
overall population by 4%.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adaptive Dynamic Pruning for Non-IID Federated Learning. (arXiv:2106.06921v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Sixing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1">Phuong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1">Ali Anwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1">Ali Jannesari</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06921">
                                    <div class="article-summary-box-inner">
                                        <span>Federated Learning~(FL) has emerged as a new paradigm of training machine
learning models without sacrificing data security and privacy. Learning models
at edge devices such as cell phones is one of the most common use case of FL.
However, the limited computing power and energy constraints of edge devices
hinder the adoption of FL for both model training and deployment, especially
for the resource-hungry Deep Neural Networks~(DNNs). To this end, many model
compression methods have been proposed and network pruning is among the most
well-known. However, a pruning policy for a given model is highly
dataset-dependent, which is not suitable for non-Independent and Identically
Distributed~(Non-IID) FL edge devices. In this paper, we present an adaptive
pruning scheme for edge devices in an FL system, which applies dataset-aware
dynamic pruning for inference acceleration on Non-IID datasets. Our evaluation
shows that the proposed method accelerates inference by $2\times$~($50\%$ FLOPs
reduction) while maintaining the model&#x27;s quality on edge devices.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dirty Road Can Attack: Security of Deep Learning based Automated Lane Centering under Physical-World Attack. (arXiv:2009.06701v2 [cs.CR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1">Takami Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Junjie Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Ningfei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yunhan Jack Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xue Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qi Alfred Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06701">
                                    <div class="article-summary-box-inner">
                                        <span>Automated Lane Centering (ALC) systems are convenient and widely deployed
today, but also highly security and safety critical. In this work, we are the
first to systematically study the security of state-of-the-art deep learning
based ALC systems in their designed operational domains under physical-world
adversarial attacks. We formulate the problem with a safety-critical attack
goal, and a novel and domain-specific attack vector: dirty road patches. To
systematically generate the attack, we adopt an optimization-based approach and
overcome domain-specific design challenges such as camera frame
inter-dependencies due to attack-influenced vehicle control, and the lack of
objective function design for lane detection models.

We evaluate our attack on a production ALC using 80 scenarios from real-world
driving traces. The results show that our attack is highly effective with over
97.5% success rates and less than 0.903 sec average success time, which is
substantially lower than the average driver reaction time. This attack is also
found (1) robust to various real-world factors such as lighting conditions and
view angles, (2) general to different model designs, and (3) stealthy from the
driver&#x27;s view. To understand the safety impacts, we conduct experiments using
software-in-the-loop simulation and attack trace injection in a real vehicle.
The results show that our attack can cause a 100% collision rate in different
scenarios, including when tested with common safety features such as automatic
emergency braking. We also evaluate and discuss defenses.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">1st Place Solution for YouTubeVOS Challenge 2021:Video Instance Segmentation. (arXiv:2106.06649v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thuy C. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1">Tuan N. Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1">Nam LH. Phan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Chuong H. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamazaki_M/0/1/0/all/0/1">Masayuki Yamazaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamanaka_M/0/1/0/all/0/1">Masao Yamanaka</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06649">
                                    <div class="article-summary-box-inner">
                                        <span>Video Instance Segmentation (VIS) is a multi-task problem performing
detection, segmentation, and tracking simultaneously. Extended from image set
applications, video data additionally induces the temporal information, which,
if handled appropriately, is very useful to identify and predict object
motions. In this work, we design a unified model to mutually learn these tasks.
Specifically, we propose two modules, named Temporally Correlated Instance
Segmentation (TCIS) and Bidirectional Tracking (BiTrack), to take the benefit
of the temporal correlation between the object&#x27;s instance masks across adjacent
frames. On the other hand, video data is often redundant due to the frame&#x27;s
overlap. Our analysis shows that this problem is particularly severe for the
YoutubeVOS-VIS2021 data. Therefore, we propose a Multi-Source Data (MSD)
training mechanism to compensate for the data deficiency. By combining these
techniques with a bag of tricks, the network performance is significantly
boosted compared to the baseline, and outperforms other methods by a
considerable margin on the YoutubeVOS-VIS 2019 and 2021 datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yunhao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengmeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianbu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Weiwei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1">Ran Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1">Qian Du</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06896">
                                    <div class="article-summary-box-inner">
                                        <span>The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Using Convolutional Neural Networks for the Helicity Classification of Magnetic Fields. (arXiv:2106.06718v1 [astro-ph.HE])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/astro-ph/1/au:+Vago_N/0/1/0/all/0/1">Nicol&#xf2; Oreste Pinciroli Vago</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Hameed_I/0/1/0/all/0/1">Ibrahim A. Hameed</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Kachelriess_M/0/1/0/all/0/1">Michael Kachelriess</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06718">
                                    <div class="article-summary-box-inner">
                                        <span>The presence of non-zero helicity in intergalactic magnetic fields is a
smoking gun for their primordial origin since they have to be generated by
processes that break CP invariance. As an experimental signature for the
presence of helical magnetic fields, an estimator $Q$ based on the triple
scalar product of the wave-vectors of photons generated in electromagnetic
cascades from, e.g., TeV blazars, has been suggested previously. We propose to
apply deep learning to helicity classification employing Convolutional Neural
Networks and show that this method outperforms the $Q$ estimator.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rojas_Gomez_R/0/1/0/all/0/1">Renan A. Rojas-Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1">Raymond A. Yeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1">Minh N. Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06927">
                                    <div class="article-summary-box-inner">
                                        <span>Recent research in adversarially robust classifiers suggests their
representations tend to be aligned with human perception, which makes them
attractive for image synthesis and restoration applications. Despite favorable
empirical results on a few downstream tasks, their advantages are limited to
slow and sensitive optimization-based techniques. Moreover, their use on
generative models remains unexplored. This work proposes the use of robust
representations as a perceptual primitive for feature inversion models, and
show its benefits with respect to standard non-robust image features. We
empirically show that adopting robust representations as an image prior
significantly improves the reconstruction accuracy of CNN-based feature
inversion models. Furthermore, it allows reconstructing images at multiple
scales out-of-the-box. Following these findings, we propose an
encoding-decoding network based on robust representations and show its
advantages for applications such as anomaly detection, style transfer and image
denoising.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation. (arXiv:2106.06801v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1">Prashant Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1">Ajey Pai</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1">Nisarg Bhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1">Prasenjit Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Makharia_G/0/1/0/all/0/1">Govind Makharia</a>, <a href="http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1">Prathosh AP</a>, <a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1">Mausam</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06801">
                                    <div class="article-summary-box-inner">
                                        <span>Contrastive Learning (CL) is a recent representation learning approach, which
achieves promising results by encouraging inter-class separability and
intra-class compactness in learned image representations. Because medical
images often contain multiple classes of interest per image, a standard
image-level CL for these images is not applicable. In this work, we present a
novel semi-supervised 2D medical segmentation solution that applies CL on image
patches, instead of full images. These patches are meaningfully constructed
using the semantic information of different classes obtained via pseudo
labeling. We also propose a novel consistency regularization scheme, which
works in synergy with contrastive learning. It addresses the problem of
confirmation bias often observed in semi-supervised settings, and encourages
better clustering in the feature space. We evaluate our method on four public
medical segmentation datasets along with a novel histopathology dataset that we
introduce. Our method obtains consistent improvements over the state-of-the-art
semi-supervised segmentation approaches for all datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Boosting Randomized Smoothing with Variance Reduced Classifiers. (arXiv:2106.06946v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Horvath_M/0/1/0/all/0/1">Mikl&#xf3;s Z. Horv&#xe1;th</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1">Mark Niklas M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1">Marc Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1">Martin Vechev</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06946">
                                    <div class="article-summary-box-inner">
                                        <span>Randomized Smoothing (RS) is a promising method for obtaining robustness
certificates by evaluating a base model under noise. In this work we: (i)
theoretically motivate why ensembles are a particularly suitable choice as base
models for RS, and (ii) empirically confirm this choice, obtaining state of the
art results in multiple settings. The key insight of our work is that the
reduced variance of ensembles over the perturbations introduced in RS leads to
significantly more consistent classifications for a given input, in turn
leading to substantially increased certifiable radii for difficult samples. We
also introduce key optimizations which enable an up to 50-fold decrease in
sample complexity of RS, thus drastically reducing its computational overhead.
Experimentally, we show that ensembles of only 3 to 10 classifiers consistently
improve on the strongest single model with respect to their average certified
radius (ACR) by 5% to 21% on both CIFAR-10 and ImageNet. On the latter, we
achieve a state-of-the-art ACR of 1.11. We release all code and models required
to reproduce our results upon publication.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A One-Shot Texture-Perceiving Generative Adversarial Network for Unsupervised Surface Inspection. (arXiv:2106.06792v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1">Lingyun Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaokui Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06792">
                                    <div class="article-summary-box-inner">
                                        <span>Visual surface inspection is a challenging task owing to the highly diverse
appearance of target surfaces and defective regions. Previous attempts heavily
rely on vast quantities of training examples with manual annotation. However,
in some practical cases, it is difficult to obtain a large number of samples
for inspection. To combat it, we propose a hierarchical texture-perceiving
generative adversarial network (HTP-GAN) that is learned from the one-shot
normal image in an unsupervised scheme. Specifically, the HTP-GAN contains a
pyramid of convolutional GANs that can capture the global structure and
fine-grained representation of an image simultaneously. This innovation helps
distinguishing defective surface regions from normal ones. In addition, in the
discriminator, a texture-perceiving module is devised to capture the spatially
invariant representation of normal image via directional convolutions, making
it more sensitive to defective areas. Experiments on a variety of datasets
consistently demonstrate the effectiveness of our method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Stronger Baseline for Ego-Centric Action Detection. (arXiv:2106.06942v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1">Zhiwu Qing</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Ziyuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shiwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jianwen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Mingqian Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Changxin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1">Marcelo H. Ang Jr</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1">Nong Sang</a>,
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06942">
                                    <div class="article-summary-box-inner">
                                        <span>This technical report analyzes an egocentric video action detection method we
used in the 2021 EPIC-KITCHENS-100 competition hosted in CVPR2021 Workshop. The
goal of our task is to locate the start time and the end time of the action in
the long untrimmed video, and predict action category. We adopt sliding window
strategy to generate proposals, which can better adapt to short-duration
actions. In addition, we show that classification and proposals are conflict in
the same network. The separation of the two tasks boost the detection
performance with high efficiency. By simply employing these strategy, we
achieved 16.10\% performance on the test set of EPIC-KITCHENS-100 Action
Detection challenge using a single model, surpassing the baseline method by
11.7\% in terms of average mAP.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Structure-Regularized Attention for Deformable Object Representation. (arXiv:2106.06672v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shenao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhifeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06672">
                                    <div class="article-summary-box-inner">
                                        <span>Capturing contextual dependencies has proven useful to improve the
representational power of deep neural networks. Recent approaches that focus on
modeling global context, such as self-attention and non-local operation,
achieve this goal by enabling unconstrained pairwise interactions between
elements. In this work, we consider learning representations for deformable
objects which can benefit from context exploitation by modeling the structural
dependencies that the data intrinsically possesses. To this end, we provide a
novel structure-regularized attention mechanism, which formalizes feature
interaction as structural factorization through the use of a pair of
light-weight operations. The instantiated building blocks can be directly
incorporated into modern convolutional neural networks, to boost the
representational power in an efficient manner. Comprehensive studies on
multiple tasks and empirical comparisons with modern attention mechanisms
demonstrate the gains brought by our method in terms of both performance and
model complexity. We further investigate its effect on feature representations,
showing that our trained models can capture diversified representations
characterizing object parts without resorting to extra supervision.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-level Attention Fusion Network for Audio-visual Event Recognition. (arXiv:2106.06736v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Brousmiche_M/0/1/0/all/0/1">Mathilde Brousmiche</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1">Jean Rouat</a>, <a href="http://arxiv.org/find/cs/1/au:+Dupont_S/0/1/0/all/0/1">St&#xe9;phane Dupont</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06736">
                                    <div class="article-summary-box-inner">
                                        <span>Event classification is inherently sequential and multimodal. Therefore, deep
neural models need to dynamically focus on the most relevant time window and/or
modality of a video. In this study, we propose the Multi-level Attention Fusion
network (MAFnet), an architecture that can dynamically fuse visual and audio
information for event recognition. Inspired by prior studies in neuroscience,
we couple both modalities at different levels of visual and audio paths.
Furthermore, the network dynamically highlights a modality at a given time
window relevant to classify events. Experimental results in AVE (Audio-Visual
Event), UCF51, and Kinetics-Sounds datasets show that the approach can
effectively improve the accuracy in audio-visual event classification. Code is
available at: https://github.com/numediart/MAFnet</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust Representation Learning via Perceptual Similarity Metrics. (arXiv:2106.06620v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Taghanaki_S/0/1/0/all/0/1">Saeid Asgari Taghanaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1">Kristy Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1">Amir Khasahmadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Anirudh Goyal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06620">
                                    <div class="article-summary-box-inner">
                                        <span>A fundamental challenge in artificial intelligence is learning useful
representations of data that yield good performance on a downstream task,
without overfitting to spurious input features. Extracting such task-relevant
predictive information is particularly difficult for real-world datasets. In
this work, we propose Contrastive Input Morphing (CIM), a representation
learning framework that learns input-space transformations of the data to
mitigate the effect of irrelevant input features on downstream performance. Our
method leverages a perceptual similarity metric via a triplet loss to ensure
that the transformation preserves task-relevant information.Empirically, we
demonstrate the efficacy of our approach on tasks which typically suffer from
the presence of spurious correlations: classification with nuisance
information, out-of-distribution generalization, and preservation of subgroup
accuracies. We additionally show that CIM is complementary to other mutual
information-based representation learning techniques, and demonstrate that it
improves the performance of variational information bottleneck (VIB) when used
together.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Representation and Correlation Enhanced Encoder-Decoder Framework for Scene Text Recognition. (arXiv:2106.06960v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1">Mengmeng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jinjin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06960">
                                    <div class="article-summary-box-inner">
                                        <span>Attention-based encoder-decoder framework is widely used in the scene text
recognition task. However, for the current state-of-the-art(SOTA) methods,
there is room for improvement in terms of the efficient usage of local visual
and global context information of the input text image, as well as the robust
correlation between the scene processing module(encoder) and the text
processing module(decoder). In this paper, we propose a Representation and
Correlation Enhanced Encoder-Decoder Framework(RCEED) to address these
deficiencies and break performance bottleneck. In the encoder module, local
visual feature, global context feature, and position information are aligned
and fused to generate a small-size comprehensive feature map. In the decoder
module, two methods are utilized to enhance the correlation between scene and
text feature space. 1) The decoder initialization is guided by the holistic
feature and global glimpse vector exported from the encoder. 2) The feature
enriched glimpse vector produced by the Multi-Head General Attention is used to
assist the RNN iteration and the character prediction at each time step.
Meanwhile, we also design a Layernorm-Dropout LSTM cell to improve model&#x27;s
generalization towards changeable texts. Extensive experiments on the
benchmarks demonstrate the advantageous performance of RCEED in scene text
recognition tasks, especially the irregular ones.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Toward Accurate and Realistic Outfits Visualization with Attention to Details. (arXiv:2106.06593v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kedan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1">Min jin Chong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jeffrey Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingen Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06593">
                                    <div class="article-summary-box-inner">
                                        <span>Virtual try-on methods aim to generate images of fashion models wearing
arbitrary combinations of garments. This is a challenging task because the
generated image must appear realistic and accurately display the interaction
between garments. Prior works produce images that are filled with artifacts and
fail to capture important visual details necessary for commercial applications.
We propose Outfit Visualization Net (OVNet) to capture these important details
(e.g. buttons, shading, textures, realistic hemlines, and interactions between
garments) and produce high quality multiple-garment virtual try-on images.
OVNet consists of 1) a semantic layout generator and 2) an image generation
pipeline using multiple coordinated warps. We train the warper to output
multiple warps using a cascade loss, which refines each successive warp to
focus on poorly generated regions of a previous warp and yields consistent
improvements in detail. In addition, we introduce a method for matching outfits
with the most suitable model and produce significant improvements for both our
and other previous try-on methods. Through quantitative and qualitative
analysis, we demonstrate our method generates substantially higher-quality
studio images compared to prior works for multi-garment outfits. An interactive
interface powered by this method has been deployed on fashion e-commerce
websites and received overwhelmingly positive feedback.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial Robustness via Fisher-Rao Regularization. (arXiv:2106.06685v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Picot_M/0/1/0/all/0/1">Marine Picot</a>, <a href="http://arxiv.org/find/cs/1/au:+Messina_F/0/1/0/all/0/1">Francisco Messina</a>, <a href="http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1">Malik Boudiaf</a>, <a href="http://arxiv.org/find/cs/1/au:+Labeau_F/0/1/0/all/0/1">Fabrice Labeau</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1">Ismail Ben Ayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1">Pablo Piantanida</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06685">
                                    <div class="article-summary-box-inner">
                                        <span>Adversarial robustness has become a topic of growing interest in machine
learning since it was observed that neural networks tend to be brittle. We
propose an information-geometric formulation of adversarial defense and
introduce FIRE, a new Fisher-Rao regularization for the categorical
cross-entropy loss, which is based on the geodesic distance between natural and
perturbed input features. Based on the information-geometric properties of the
class of softmax distributions, we derive an explicit characterization of the
Fisher-Rao Distance (FRD) for the binary and multiclass cases, and draw some
interesting properties as well as connections with standard regularization
metrics. Furthermore, for a simple linear and Gaussian model, we show that all
Pareto-optimal points in the accuracy-robustness region can be reached by FIRE
while other state-of-the-art methods fail. Empirically, we evaluate the
performance of various classifiers trained with the proposed loss on standard
datasets, showing up to 2\% of improvements in terms of robustness while
reducing the training time by 20\% over the best-performing methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs. (arXiv:2106.06959v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jaewoong Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1">Changyeon Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Junho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jung Ho Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1">Geonho Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Myungjoo Kang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06959">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose a method to find local-geometry-aware traversal
directions on the intermediate latent space of Generative Adversarial Networks
(GANs). These directions are defined as an ordered basis of tangent space at a
latent code. Motivated by the intrinsic sparsity of the latent space, the basis
is discovered by solving the low-rank approximation problem of the differential
of the partial network. Moreover, the local traversal basis leads to a natural
iterative traversal on the latent space. Iterative Curve-Traversal shows stable
traversal on images, since the trajectory of latent code stays close to the
latent space even under the strong perturbations compared to the linear
traversal. This stability provides far more diverse variations of the given
image. Although the proposed method can be applied to various GAN models, we
focus on the W-space of the StyleGAN2, which is renowned for showing the better
disentanglement of the latent factors of variation. Our quantitative and
qualitative analysis provides evidence showing that the W-space is still
globally warped while showing a certain degree of global consistency of
interpretable variation. In particular, we introduce some metrics on the
Grassmannian manifolds to quantify the global warpage of the W-space and the
subspace traversal to test the stability of traversal directions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Pay Attention with Focus: A Novel Learning Scheme for Classification of Whole Slide Images. (arXiv:2106.06623v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kalra_S/0/1/0/all/0/1">Shivam Kalra</a>, <a href="http://arxiv.org/find/cs/1/au:+Adnan_M/0/1/0/all/0/1">Mohammed Adnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemati_S/0/1/0/all/0/1">Sobhan Hemati</a>, <a href="http://arxiv.org/find/cs/1/au:+Dehkharghanian_T/0/1/0/all/0/1">Taher Dehkharghanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahnamayan_S/0/1/0/all/0/1">Shahryar Rahnamayan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1">Hamid Tizhoosh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06623">
                                    <div class="article-summary-box-inner">
                                        <span>Deep learning methods such as convolutional neural networks (CNNs) are
difficult to directly utilize to analyze whole slide images (WSIs) due to the
large image dimensions. We overcome this limitation by proposing a novel
two-stage approach. First, we extract a set of representative patches (called
mosaic) from a WSI. Each patch of a mosaic is encoded to a feature vector using
a deep network. The feature extractor model is fine-tuned using hierarchical
target labels of WSIs, i.e., anatomic site and primary diagnosis. In the second
stage, a set of encoded patch-level features from a WSI is used to compute the
primary diagnosis probability through the proposed Pay Attention with Focus
scheme, an attention-weighted averaging of predicted probabilities for all
patches of a mosaic modulated by a trainable focal factor. Experimental results
show that the proposed model can be robust, and effective for the
classification of WSIs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Rapid COVID-19 Risk Screening by Eye-region Manifestations. (arXiv:2106.06664v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1">Yanwei Fu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1">Lei Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1">Haojie Zheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_Q/0/1/0/all/0/1">Qiang Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1">Li Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1">Hong Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Xie_J/0/1/0/all/0/1">Jiao Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Xue_X/0/1/0/all/0/1">Xiangyang Xue</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1">Feng Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yuan Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Pei_Y/0/1/0/all/0/1">Yantao Pei</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jianmin Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xiuqi Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1">Yanhua Zheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Gu1_H/0/1/0/all/0/1">Hongxia Tian Mengwei Gu1</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06664">
                                    <div class="article-summary-box-inner">
                                        <span>It is still nontrivial to develop a new fast COVID-19 screening method with
the easier access and lower cost, due to the technical and cost limitations of
the current testing methods in the medical resource-poor districts. On the
other hand, there are more and more ocular manifestations that have been
reported in the COVID-19 patients as growing clinical evidence[1]. This
inspired this project. We have conducted the joint clinical research since
January 2021 at the ShiJiaZhuang City, Heibei province, China, which approved
by the ethics committee of The fifth hospital of ShiJiaZhuang of Hebei Medical
University. We undertake several blind tests of COVID-19 patients by Union
Hospital, Tongji Medical College, Huazhong University of Science and
Technology, Wuhan, China. Meantime as an important part of the ongoing globally
COVID-19 eye test program by AIMOMICS since February 2020, we propose a new
fast screening method of analyzing the eye-region images, captured by common
CCD and CMOS cameras. This could reliably make a rapid risk screening of
COVID-19 with the sustainable stable high performance in different countries
and races. Our model for COVID-19 rapid prescreening have the merits of the
lower cost, fully self-performed, non-invasive, importantly real-time, and thus
enables the continuous health surveillance. We further implement it as the open
accessible APIs, and provide public service to the world. Our pilot experiments
show that our model is ready to be usable to all kinds of surveillance
scenarios, such as infrared temperature measurement device at airports and
stations, or directly pushing to the target people groups smartphones as a
packaged application.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dise\~no y desarrollo de aplicaci\&#x27;on m\&#x27;ovil para la clasificaci\&#x27;on de flora nativa chilena utilizando redes neuronales convolucionales. (arXiv:2106.06592v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Munoz_I/0/1/0/all/0/1">Ignacio Mu&#xf1;oz</a>, <a href="http://arxiv.org/find/cs/1/au:+Bolt_A/0/1/0/all/0/1">Alfredo Bolt</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06592">
                                    <div class="article-summary-box-inner">
                                        <span>Introduction: Mobile apps, through artificial vision, are capable of
recognizing vegetable species in real time. However, the existing species
recognition apps do not take in consideration the wide variety of endemic and
native (Chilean) species, which leads to wrong species predictions. This study
introduces the development of a chilean species dataset and an optimized
classification model implemented to a mobile app. Method: the data set was
built by putting together pictures of several species captured on the field and
by selecting some pictures available from other datasets available online.
Convolutional neural networks were used in order to develop the images
prediction models. The networks were trained by performing a sensitivity
analysis, validating with k-fold cross validation and performing tests with
different hyper-parameters, optimizers, convolutional layers, and learning
rates in order to identify and choose the best models and then put them
together in one classification model. Results: The final data set was
compounded by 46 species, including native species, endemic and exotic from
Chile, with 6120 training pictures and 655 testing pictures. The best models
were implemented on a mobile app, obtaining a 95% correct prediction rate with
respect to the set of tests. Conclusion: The app developed in this study is
capable of classifying species with a high level of accuracy, depending on the
state of the art of the artificial vision and it can also show relevant
information related to the classified species.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-Modal Attention Consistency for Video-Audio Unsupervised Learning. (arXiv:2106.06939v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1">Shaobo Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1">Qi Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hongtao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chuang Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongdong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06939">
                                    <div class="article-summary-box-inner">
                                        <span>Cross-modal correlation provides an inherent supervision for video
unsupervised representation learning. Existing methods focus on distinguishing
different video clips by visual and audio representations. We human visual
perception could attend to regions where sounds are made, and our auditory
perception could also ground their frequencies of sounding objects, which we
call bidirectional local correspondence. Such supervision is intuitive but not
well explored in the contrastive learning framework. This paper introduces a
pretext task, Cross-Modal Attention Consistency (CMAC), for exploring the
bidirectional local correspondence property. The CMAC approach aims to align
the regional attention generated purely from the visual signal with the target
attention generated under the guidance of acoustic signal, and do a similar
alignment for frequency grounding on the acoustic attention. Accompanied by a
remoulded cross-modal contrastive loss where we consider additional
within-modal interactions, the CMAC approach works effectively for enforcing
the bidirectional alignment. Extensive experiments on six downstream benchmarks
demonstrate that CMAC can improve the state-of-the-art performance on both
visual and audio modalities.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GANs N&#x27; Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!). (arXiv:2106.06561v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1">Min Jin Chong</a>, <a href="http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1">David Forsyth</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06561">
                                    <div class="article-summary-box-inner">
                                        <span>We show how to learn a map that takes a content code, derived from a face
image, and a randomly chosen style code to an anime image. We derive an
adversarial loss from our simple and effective definitions of style and
content. This adversarial loss guarantees the map is diverse -- a very wide
range of anime can be produced from a single content code. Under plausible
assumptions, the map is not just diverse, but also correctly represents the
probability of an anime, conditioned on an input face. In contrast, current
multimodal generation procedures cannot capture the complex styles that appear
in anime. Extensive quantitative experiments support the idea the map is
correct. Extensive qualitative results show that the method can generate a much
more diverse range of styles than SOTA comparisons. Finally, we show that our
formalization of content and style allows us to perform video to video
translation without ever training on videos.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Mirror3D: Depth Refinement for Mirror Surfaces. (arXiv:2106.06629v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1">Jiaqi Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Weijie Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1">Angel X. Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1">Manolis Savva</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06629">
                                    <div class="article-summary-box-inner">
                                        <span>Despite recent progress in depth sensing and 3D reconstruction, mirror
surfaces are a significant source of errors. To address this problem, we create
the Mirror3D dataset: a 3D mirror plane dataset based on three RGBD datasets
(Matterport3D, NYUv2 and ScanNet) containing 7,011 mirror instance masks and 3D
planes. We then develop Mirror3DNet: a module that refines raw sensor depth or
estimated depth to correct errors on mirror surfaces. Our key idea is to
estimate the 3D mirror plane based on RGB input and surrounding depth context,
and use this estimate to directly regress mirror surface depth. Our experiments
show that Mirror3DNet significantly mitigates errors from a variety of input
depth data, including raw sensor depth and depth estimation or completion
methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Reverse-engineer the Distributional Structure of Infant Egocentric Views for Training Generalizable Image Classifiers. (arXiv:2106.06694v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tsutsui_S/0/1/0/all/0/1">Satoshi Tsutsui</a>, <a href="http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1">David Crandall</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chen Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06694">
                                    <div class="article-summary-box-inner">
                                        <span>We analyze egocentric views of attended objects from infants. This paper
shows 1) empirical evidence that children&#x27;s egocentric views have more diverse
distributions compared to adults&#x27; views, 2) we can computationally simulate the
infants&#x27; distribution, and 3) the distribution is beneficial for training more
generalized image classifiers not only for infant egocentric vision but for
third-person computer vision.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Domain Generalization on Medical Imaging Classification using Episodic Training with Task Augmentation. (arXiv:2106.06908v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1">Qi Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1">Xinghao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1">Dong Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yizhou Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06908">
                                    <div class="article-summary-box-inner">
                                        <span>Medical imaging datasets usually exhibit domain shift due to the variations
of scanner vendors, imaging protocols, etc. This raises the concern about the
generalization capacity of machine learning models. Domain generalization (DG),
which aims to learn a model from multiple source domains such that it can be
directly generalized to unseen test domains, seems particularly promising to
medical imaging community. To address DG, recent model-agnostic meta-learning
(MAML) has been introduced, which transfers the knowledge from previous
training tasks to facilitate the learning of novel testing tasks. However, in
clinical practice, there are usually only a few annotated source domains
available, which decreases the capacity of training task generation and thus
increases the risk of overfitting to training tasks in the paradigm. In this
paper, we propose a novel DG scheme of episodic training with task augmentation
on medical imaging classification. Based on meta-learning, we develop the
paradigm of episodic training to construct the knowledge transfer from episodic
training-task simulation to the real testing task of DG. Motivated by the
limited number of source domains in real-world medical deployment, we consider
the unique task-level overfitting and we propose task augmentation to enhance
the variety during training task generation to alleviate it. With the
established learning framework, we further exploit a novel meta-objective to
regularize the deep embedding of training domains. To validate the
effectiveness of the proposed method, we perform experiments on
histopathological images and abdominal CT images.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Knowledge Consolidation based Class Incremental Online Learning with Limited Data. (arXiv:2106.06795v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1">Mohammed Asad Karim</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1">Vinay Kumar Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1">Pravendra Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1">Vinay Namboodiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1">Piyush Rai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06795">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a novel approach for class incremental online learning in a
limited data setting. This problem setting is challenging because of the
following constraints: (1) Classes are given incrementally, which necessitates
a class incremental learning approach; (2) Data for each class is given in an
online fashion, i.e., each training example is seen only once during training;
(3) Each class has very few training examples; and (4) We do not use or assume
access to any replay/memory to store data from previous classes. Therefore, in
this setting, we have to handle twofold problems of catastrophic forgetting and
overfitting. In our approach, we learn robust representations that are
generalizable across tasks without suffering from the problems of catastrophic
forgetting and overfitting to accommodate future classes with limited samples.
Our proposed method leverages the meta-learning framework with knowledge
consolidation. The meta-learning framework helps the model for rapid learning
when samples appear in an online fashion. Simultaneously, knowledge
consolidation helps to learn a robust representation against forgetting under
online updates to facilitate future learning. Our approach significantly
outperforms other methods on several benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.IR"">cs.IR updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1">Geand Trindade Pereira</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1">Moises Rocha dos Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1">Andre Carlos Ponce de Leon Ferreira de Carvalho</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03954">
                                    <div class="article-summary-box-inner">
                                        <span>With the popularity of Machine Learning (ML) solutions, algorithms and data
have been released faster than the capacity of processing them. In this
context, the problem of Algorithm Recommendation (AR) is receiving a
significant deal of attention recently. This problem has been addressed in the
literature as a learning task, often as a Meta-Learning problem where the aim
is to recommend the best alternative for a specific dataset. For such, datasets
encoded by meta-features are explored by ML algorithms that try to learn the
mapping between meta-representations and the best technique to be used. One of
the challenges for the successful use of ML is to define which features are the
most valuable for a specific dataset since several meta-features can be used,
which increases the meta-feature dimension. This paper presents an empirical
analysis of Feature Selection and Feature Extraction in the meta-level for the
AR problem. The present study was focused on three criteria: predictive
performance, dimensionality reduction, and pipeline runtime. As we verified,
applying Dimensionality Reduction (DR) methods did not improve predictive
performances in general. However, DR solutions reduced about 80% of the
meta-features, obtaining pretty much the same performance as the original setup
but with lower runtimes. The only exception was PCA, which presented about the
same runtime as the original meta-features. Experimental results also showed
that various datasets have many non-informative meta-features and that it is
possible to obtain high predictive performance using around 20% of the original
meta-features. Therefore, due to their natural trend for high dimensionality,
DR methods should be used for Meta-Feature Selection and Meta-Feature
Extraction.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v3 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1">Liyi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Junqi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haoqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhenzhe Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhiye Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhizhuang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1">Fei Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1">Lvyin Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haiyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chuan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuning Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaoqiang Zhu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14188">
                                    <div class="article-summary-box-inner">
                                        <span>Advertising expenditures have become the major source of revenue for
e-commerce platforms. Providing good advertising experiences for advertisers by
reducing their costs of trial and error in discovering the optimal advertising
strategies is crucial for the long-term prosperity of online advertising. To
achieve this goal, the advertising platform needs to identify the advertiser&#x27;s
optimization objectives, and then recommend the corresponding strategies to
fulfill the objectives. In this work, we first deploy a prototype of strategy
recommender system on Taobao display advertising platform, which indeed
increases the advertisers&#x27; performance and the platform&#x27;s revenue, indicating
the effectiveness of strategy recommendation for online advertising. We further
augment this prototype system by explicitly learning the advertisers&#x27;
preferences over various advertising performance indicators and then
optimization objectives through their adoptions of different recommending
advertising strategies. We use contextual bandit algorithms to efficiently
learn the advertisers&#x27; preferences and maximize the recommendation adoption,
simultaneously. Simulation experiments based on Taobao online bidding data show
that the designed algorithms can effectively optimize the strategy adoption
rate of advertisers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Socially-Aware Self-Supervised Tri-Training for Recommendation. (arXiv:2106.03569v2 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Junliang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hongzhi Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1">Min Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1">Xin Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hung_N/0/1/0/all/0/1">Nguyen Quoc Viet Hung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03569">
                                    <div class="article-summary-box-inner">
                                        <span>Self-supervised learning (SSL), which can automatically generate ground-truth
samples from raw data, holds vast potential to improve recommender systems.
Most existing SSL-based methods perturb the raw data graph with uniform
node/edge dropout to generate new data views and then conduct the
self-discrimination based contrastive learning over different views to learn
generalizable representations. Under this scheme, only a bijective mapping is
built between nodes in two different views, which means that the
self-supervision signals from other nodes are being neglected. Due to the
widely observed homophily in recommender systems, we argue that the supervisory
signals from other nodes are also highly likely to benefit the representation
learning for recommendation. To capture these signals, a general socially-aware
SSL framework that integrates tri-training is proposed in this paper.
Technically, our framework first augments the user data views with the user
social information. And then under the regime of tri-training for multi-view
encoding, the framework builds three graph encoders (one for recommendation)
upon the augmented views and iteratively improves each encoder with
self-supervision signals from other users, generated by the other two encoders.
Since the tri-training operates on the augmented views of the same data sources
for self-supervision signals, we name it self-supervised tri-training.
Extensive experiments on multiple real-world datasets consistently validate the
effectiveness of the self-supervised tri-training framework for improving
recommendation. The code is released at https://github.com/Coder-Yu/QRec.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Reinforcement Learning based Group Recommender System. (arXiv:2106.06900v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zefang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1">Shuran Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Quan_Y/0/1/0/all/0/1">Yinzhu Quan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06900">
                                    <div class="article-summary-box-inner">
                                        <span>Group recommender systems are widely used in current web applications. In
this paper, we propose a novel group recommender system based on the deep
reinforcement learning. We introduce the MovieLens data at first and generate
one random group dataset, MovieLens-Rand, from it. This randomly generated
dataset is described and analyzed. We also present experimental settings and
two state-of-art baselines, AGREE and GroupIM. The framework of our novel
model, the Deep Reinforcement learning based Group Recommender system (DRGR),
is proposed. Actor-critic networks are implemented with the deep deterministic
policy gradient algorithm. The DRGR model is applied on the MovieLens-Rand
dataset with two baselines. Compared with baselines, we conclude that DRGR
performs better than GroupIM due to long interaction histories but worse than
AGREE because of the self-attention mechanism. We express advantages and
shortcomings of DRGR and also give future improvement directions at the end.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sentiment Analysis of Covid-19 Tweets using Evolutionary Classification-Based LSTM Model. (arXiv:2106.06910v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1">Arunava Kumar Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Sourav Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolya_A/0/1/0/all/0/1">Anup Kumar Kolya</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06910">
                                    <div class="article-summary-box-inner">
                                        <span>As the Covid-19 outbreaks rapidly all over the world day by day and also
affects the lives of million, a number of countries declared complete lock-down
to check its intensity. During this lockdown period, social media plat-forms
have played an important role to spread information about this pandemic across
the world, as people used to express their feelings through the social
networks. Considering this catastrophic situation, we developed an experimental
approach to analyze the reactions of people on Twitter taking into ac-count the
popular words either directly or indirectly based on this pandemic. This paper
represents the sentiment analysis on collected large number of tweets on
Coronavirus or Covid-19. At first, we analyze the trend of public sentiment on
the topics related to Covid-19 epidemic using an evolutionary classification
followed by the n-gram analysis. Then we calculated the sentiment ratings on
collected tweet based on their class. Finally, we trained the long-short term
network using two types of rated tweets to predict sentiment on Covid-19 data
and obtained an overall accuracy of 84.46%.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CHECKED: Chinese COVID-19 Fake News Dataset. (arXiv:2010.09029v2 [cs.SI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xinyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zafarani_R/0/1/0/all/0/1">Reza Zafarani</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09029">
                                    <div class="article-summary-box-inner">
                                        <span>COVID-19 has impacted all lives. To maintain social distancing and avoiding
exposure, works and lives have gradually moved online. Under this trend, social
media usage to obtain COVID-19 news has increased. Also, misinformation on
COVID-19 is frequently spread on social media. In this work, we develop
CHECKED, the first Chinese dataset on COVID-19 misinformation. CHECKED provides
a total 2,104 verified microblogs related to COVID-19 from December 2019 to
August 2020, identified by using a specific list of keywords. Correspondingly,
CHECKED includes 1,868,175 reposts, 1,185,702 comments, and 56,852,736 likes
that reveal how these verified microblogs are spread and reacted on Weibo. The
dataset contains a rich set of multimedia information for each microblog
including ground-truth label, textual, visual, temporal, and network
information. Extensive experiments have been conducted to analyze CHECKED data
and to provide benchmark results for well-established methods when predicting
fake news using CHECKED. We hope that CHECKED can facilitate studies that
target misinformation on coronavirus. The dataset is available at
https://github.com/cyang03/CHECKED.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Engineering Knowledge Graph from Patent Database. (arXiv:2106.06739v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Siddharth_L/0/1/0/all/0/1">L Siddharth</a>, <a href="http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1">Lucienne T.M. Blessing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wood_K/0/1/0/all/0/1">Kristin L. Wood</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jianxi Luo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06739">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a large, scalable engineering knowledge graph, comprising sets of
(entity, relationship, entity) triples that are real-world engineering facts
found in the patent database. We apply a set of rules based on the syntactic
and lexical properties of claims in a patent document to extract facts. We
aggregate these facts within each patent document and integrate the aggregated
sets of facts across the patent database to obtain the engineering knowledge
graph. Such a knowledge graph is expected to support inference, reasoning, and
recalling in various engineering tasks. The knowledge graph has a greater size
and coverage in comparison with the previously used knowledge graphs and
semantic networks in the engineering literature.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Curriculum Pre-Training Heterogeneous Subgraph Transformer for Top-$N$ Recommendation. (arXiv:2106.06722v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wayne Xin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Ji-Rong Wen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06722">
                                    <div class="article-summary-box-inner">
                                        <span>Due to the flexibility in modelling data heterogeneity, heterogeneous
information network (HIN) has been adopted to characterize complex and
heterogeneous auxiliary data in top-$N$ recommender systems, called
\emph{HIN-based recommendation}. HIN characterizes complex, heterogeneous data
relations, containing a variety of information that may not be related to the
recommendation task. Therefore, it is challenging to effectively leverage
useful information from HINs for improving the recommendation performance. To
address the above issue, we propose a Curriculum pre-training based
HEterogeneous Subgraph Transformer (called \emph{CHEST}) with new \emph{data
characterization}, \emph{representation model} and \emph{learning algorithm}.

Specifically, we consider extracting useful information from HIN to compose
the interaction-specific heterogeneous subgraph, containing both sufficient and
relevant context information for recommendation. Then we capture the rich
semantics (\eg graph structure and path semantics) within the subgraph via a
heterogeneous subgraph Transformer, where we encode the subgraph with
multi-slot sequence representations. Besides, we design a curriculum
pre-training strategy to provide an elementary-to-advanced learning process, by
which we smoothly transfer basic semantics in HIN for modeling user-item
interaction relation.

Extensive experiments conducted on three real-world datasets demonstrate the
superiority of our proposed method over a number of competitive baselines,
especially when only limited training data is available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AutoLoss: Automated Loss Function Search in Recommendations. (arXiv:2106.06713v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiangyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haochen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1">Wenqi Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chong Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06713">
                                    <div class="article-summary-box-inner">
                                        <span>Designing an effective loss function plays a crucial role in training deep
recommender systems. Most existing works often leverage a predefined and fixed
loss function that could lead to suboptimal recommendation quality and training
efficiency. Some recent efforts rely on exhaustively or manually searched
weights to fuse a group of candidate loss functions, which is exceptionally
costly in computation and time. They also neglect the various convergence
behaviors of different data examples. In this work, we propose an AutoLoss
framework that can automatically and adaptively search for the appropriate loss
function from a set of candidates. To be specific, we develop a novel
controller network, which can dynamically adjust the loss probabilities in a
differentiable manner. Unlike existing algorithms, the proposed controller can
adaptively generate the loss probabilities for different data examples
according to their varied convergence behaviors. Such design improves the
model&#x27;s generalizability and transferability between deep recommender systems
and datasets. We evaluate the proposed framework on two benchmark datasets. The
results show that AutoLoss outperforms representative baselines. Further
experiments have been conducted to deepen our understandings of AutoLoss,
including its transferability, components and training efficiency.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.LG"">cs.LG updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Evaluating Meta-Feature Selection for the Algorithm Recommendation Problem. (arXiv:2106.03954v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1">Geand Trindade Pereira</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1">Moises Rocha dos Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1">Andre Carlos Ponce de Leon Ferreira de Carvalho</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03954">
                                    <div class="article-summary-box-inner">
                                        <span>With the popularity of Machine Learning (ML) solutions, algorithms and data
have been released faster than the capacity of processing them. In this
context, the problem of Algorithm Recommendation (AR) is receiving a
significant deal of attention recently. This problem has been addressed in the
literature as a learning task, often as a Meta-Learning problem where the aim
is to recommend the best alternative for a specific dataset. For such, datasets
encoded by meta-features are explored by ML algorithms that try to learn the
mapping between meta-representations and the best technique to be used. One of
the challenges for the successful use of ML is to define which features are the
most valuable for a specific dataset since several meta-features can be used,
which increases the meta-feature dimension. This paper presents an empirical
analysis of Feature Selection and Feature Extraction in the meta-level for the
AR problem. The present study was focused on three criteria: predictive
performance, dimensionality reduction, and pipeline runtime. As we verified,
applying Dimensionality Reduction (DR) methods did not improve predictive
performances in general. However, DR solutions reduced about 80% of the
meta-features, obtaining pretty much the same performance as the original setup
but with lower runtimes. The only exception was PCA, which presented about the
same runtime as the original meta-features. Experimental results also showed
that various datasets have many non-informative meta-features and that it is
possible to obtain high predictive performance using around 20% of the original
meta-features. Therefore, due to their natural trend for high dimensionality,
DR methods should be used for Meta-Feature Selection and Meta-Feature
Extraction.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robustifying $\ell_\infty$ Adversarial Training to the Union of Perturbation Models. (arXiv:2105.14710v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1">Ameya D. Patil</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuttle_M/0/1/0/all/0/1">Michael Tuttle</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1">Alexander G. Schwing</a>, <a href="http://arxiv.org/find/cs/1/au:+Shanbhag_N/0/1/0/all/0/1">Naresh R. Shanbhag</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14710">
                                    <div class="article-summary-box-inner">
                                        <span>Classical adversarial training (AT) frameworks are designed to achieve high
adversarial accuracy against a single attack type, typically $\ell_\infty$
norm-bounded perturbations. Recent extensions in AT have focused on defending
against the union of multiple perturbations but this benefit is obtained at the
expense of a significant (up to $10\times$) increase in training complexity
over single-attack $\ell_\infty$ AT. In this work, we expand the capabilities
of widely popular single-attack $\ell_\infty$ AT frameworks to provide
robustness to the union of ($\ell_\infty, \ell_2, \ell_1$) perturbations while
preserving their training efficiency. Our technique, referred to as Shaped
Noise Augmented Processing (SNAP), exploits a well-established byproduct of
single-attack AT frameworks -- the reduction in the curvature of the decision
boundary of networks. SNAP prepends a given deep net with a shaped noise
augmentation layer whose distribution is learned along with network parameters
using any standard single-attack AT. As a result, SNAP enhances adversarial
accuracy of ResNet-18 on CIFAR-10 against the union of ($\ell_\infty, \ell_2,
\ell_1$) perturbations by 14%-to-20% for four state-of-the-art (SOTA)
single-attack $\ell_\infty$ AT frameworks, and, for the first time, establishes
a benchmark for ResNet-50 and ResNet-101 on ImageNet.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Signal Transformer: Complex-valued Attention and Meta-Learning for Signal Recognition. (arXiv:2106.04392v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yihong Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Ying Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Muqiao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Songtao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1">Qingjiang Shi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04392">
                                    <div class="article-summary-box-inner">
                                        <span>Deep neural networks have been shown as a class of useful tools for
addressing signal recognition issues in recent years, especially for
identifying the nonlinear feature structures of signals. However, this power of
most deep learning techniques heavily relies on an abundant amount of training
data, so the performance of classic neural nets decreases sharply when the
number of training data samples is small or unseen data are presented in the
testing phase. This calls for an advanced strategy, i.e., model-agnostic
meta-learning (MAML), which is able to capture the invariant representation of
the data samples or signals. In this paper, inspired by the special structure
of the signal, i.e., real and imaginary parts consisted in practical
time-series signals, we propose a Complex-valued Attentional MEta Learner
(CAMEL) for the problem of few-shot signal recognition by leveraging attention
and meta-learning in the complex domain. To the best of our knowledge, this is
also the first complex-valued MAML that can find the first-order stationary
points of general nonconvex problems with theoretical convergence guarantees.
Extensive experiments results showcase the superiority of the proposed CAMEL
compared with the state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised Classification. (arXiv:2106.04527v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sellars_P/0/1/0/all/0/1">Philip Sellars</a>, <a href="http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1">Angelica I. Aviles-Rivero</a>, <a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1">Carola-Bibiane Sch&#xf6;nlieb</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04527">
                                    <div class="article-summary-box-inner">
                                        <span>Semi-supervised learning has received a lot of recent attention as it
alleviates the need for large amounts of labelled data which can often be
expensive, requires expert knowledge and be time consuming to collect. Recent
developments in deep semi-supervised classification have reached unprecedented
performance and the gap between supervised and semi-supervised learning is
ever-decreasing. This improvement in performance has been based on the
inclusion of numerous technical tricks, strong augmentation techniques and
costly optimisation schemes with multi-term loss functions. We propose a new
framework, LaplaceNet, for deep semi-supervised classification that has a
greatly reduced model complexity. We utilise a hybrid energy-neural network
where graph based pseudo-labels, generated by minimising the graphical
Laplacian, are used to iteratively improve a neural-network backbone. Our model
outperforms state-of-the-art methods for deep semi-supervised classification,
over several benchmark datasets. Furthermore, we consider the application of
strong-augmentations to neural networks theoretically and justify the use of a
multi-sampling approach for semi-supervised learning. We demonstrate, through
rigorous experimentation, that a multi-sampling augmentation approach improves
generalisation and reduces the sensitivity of the network to augmentation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation. (arXiv:2106.04563v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1">Subhabrata Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1">Ahmed Hassan Awadallah</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jianfeng Gao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04563">
                                    <div class="article-summary-box-inner">
                                        <span>While deep and large pre-trained models are the state-of-the-art for various
natural language processing tasks, their huge size poses significant challenges
for practical uses in resource constrained settings. Recent works in knowledge
distillation propose task-agnostic as well as task-specific methods to compress
these models, with task-specific ones often yielding higher compression rate.
In this work, we develop a new task-agnostic distillation framework
XtremeDistilTransformers that leverages the advantage of task-specific methods
for learning a small universal model that can be applied to arbitrary tasks and
languages. To this end, we study the transferability of several source tasks,
augmentation resources and model architecture for distillation. We evaluate our
model performance on multiple tasks, including the General Language
Understanding Evaluation (GLUE) benchmark, SQuAD question answering dataset and
a massive multi-lingual NER dataset with 41 languages. We release three
distilled task-agnostic checkpoints with 13MM, 22MM and 33MM parameters
obtaining SOTA performance in several tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards a Theoretical Framework of Out-of-Distribution Generalization. (arXiv:2106.04496v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Haotian Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1">Chuanlong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Tianle Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruichen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04496">
                                    <div class="article-summary-box-inner">
                                        <span>Generalization to out-of-distribution (OOD) data, or domain generalization,
is one of the central problems in modern machine learning. Recently, there is a
surge of attempts to propose algorithms for OOD that mainly build upon the idea
of extracting invariant features. Although intuitively reasonable, theoretical
understanding of what kind of invariance can guarantee OOD generalization is
still limited, and generalization to arbitrary out-of-distribution is clearly
impossible. In this work, we take the first step towards rigorous and
quantitative definitions of 1) what is OOD; and 2) what does it mean by saying
an OOD problem is learnable. We also introduce a new concept of expansion
function, which characterizes to what extent the variance is amplified in the
test domains over the training domains, and therefore give a quantitative
meaning of invariant features. Based on these, we prove OOD generalization
error bounds. It turns out that OOD generalization largely depends on the
expansion function. As recently pointed out by Gulrajani and Lopez-Paz (2020),
any OOD learning algorithm without a model selection module is incomplete. Our
theory naturally induces a model selection criterion. Extensive experiments on
benchmark OOD datasets demonstrate that our model selection criterion has a
significant advantage over baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Principled Hyperedge Prediction with Structural Spectral Features and Neural Networks. (arXiv:2106.04292v4 [cs.SI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1">Changlin Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Muhan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1">Wei Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1">Sha Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04292">
                                    <div class="article-summary-box-inner">
                                        <span>Hypergraph offers a framework to depict the multilateral relationships in
real-world complex data. Predicting higher-order relationships, i.e hyperedge,
becomes a fundamental problem for the full understanding of complicated
interactions. The development of graph neural network (GNN) has greatly
advanced the analysis of ordinary graphs with pair-wise relations. However,
these methods could not be easily extended to the case of hypergraph. In this
paper, we generalize the challenges of GNN in representing higher-order data in
principle, which are edge- and node-level ambiguities. To overcome the
challenges, we present SNALS that utilizes bipartite graph neural network with
structural features to collectively tackle the two ambiguity issues. SNALS
captures the joint interactions of a hyperedge by its local environment, which
is retrieved by collecting the spectrum information of their connections. As a
result, SNALS achieves nearly 30% performance increase compared with most
recent GNN-based models. In addition, we applied SNALS to predict genetic
higher-order interactions on 3D genome organization data. SNALS showed
consistently high prediction accuracy across different chromosomes, and
generated novel findings on 4-way gene interaction, which is further validated
by existing literature.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Heavy-Tail Phenomenon in SGD. (arXiv:2006.04740v5 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1">Mert Gurbuzbalaban</a>, <a href="http://arxiv.org/find/math/1/au:+Simsekli_U/0/1/0/all/0/1">Umut &#x15e;im&#x15f;ekli</a>, <a href="http://arxiv.org/find/math/1/au:+Zhu_L/0/1/0/all/0/1">Lingjiong Zhu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04740">
                                    <div class="article-summary-box-inner">
                                        <span>In recent years, various notions of capacity and complexity have been
proposed for characterizing the generalization properties of stochastic
gradient descent (SGD) in deep learning. Some of the popular notions that
correlate well with the performance on unseen data are (i) the &#x60;flatness&#x27; of
the local minimum found by SGD, which is related to the eigenvalues of the
Hessian, (ii) the ratio of the stepsize $\eta$ to the batch-size $b$, which
essentially controls the magnitude of the stochastic gradient noise, and (iii)
the &#x60;tail-index&#x27;, which measures the heaviness of the tails of the network
weights at convergence. In this paper, we argue that these three seemingly
unrelated perspectives for generalization are deeply linked to each other. We
claim that depending on the structure of the Hessian of the loss at the
minimum, and the choices of the algorithm parameters $\eta$ and $b$, the SGD
iterates will converge to a \emph{heavy-tailed} stationary distribution. We
rigorously prove this claim in the setting of quadratic optimization: we show
that even in a simple linear regression problem with independent and
identically distributed data whose distribution has finite moments of all
order, the iterates can be heavy-tailed with infinite variance. We further
characterize the behavior of the tails with respect to algorithm parameters,
the dimension, and the curvature. We then translate our results into insights
about the behavior of SGD in deep learning. We support our theory with
experiments conducted on synthetic data, fully connected, and convolutional
neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">We Know What You Want: An Advertising Strategy Recommender System for Online Advertising. (arXiv:2105.14188v3 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1">Liyi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1">Junqi Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haoqi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhenzhe Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhiye Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1">Zhizhuang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1">Fei Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1">Lvyin Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haiyang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chuan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yuning Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaoqiang Zhu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14188">
                                    <div class="article-summary-box-inner">
                                        <span>Advertising expenditures have become the major source of revenue for
e-commerce platforms. Providing good advertising experiences for advertisers by
reducing their costs of trial and error in discovering the optimal advertising
strategies is crucial for the long-term prosperity of online advertising. To
achieve this goal, the advertising platform needs to identify the advertiser&#x27;s
optimization objectives, and then recommend the corresponding strategies to
fulfill the objectives. In this work, we first deploy a prototype of strategy
recommender system on Taobao display advertising platform, which indeed
increases the advertisers&#x27; performance and the platform&#x27;s revenue, indicating
the effectiveness of strategy recommendation for online advertising. We further
augment this prototype system by explicitly learning the advertisers&#x27;
preferences over various advertising performance indicators and then
optimization objectives through their adoptions of different recommending
advertising strategies. We use contextual bandit algorithms to efficiently
learn the advertisers&#x27; preferences and maximize the recommendation adoption,
simultaneously. Simulation experiments based on Taobao online bidding data show
that the designed algorithms can effectively optimize the strategy adoption
rate of advertisers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Non-Transferable Learning: A New Approach for Model Verification and Authorization. (arXiv:2106.06916v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lixu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shichao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruiqi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1">Qi Zhu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06916">
                                    <div class="article-summary-box-inner">
                                        <span>As Artificial Intelligence as a Service gains popularity, protecting
well-trained models as intellectual property is becoming increasingly
important. Generally speaking, there are two common protection methods:
ownership verification and usage authorization. In this paper, we propose
Non-Transferable Learning (NTL), a novel approach that captures the exclusive
data representation in the learned model and restricts the model generalization
ability to certain domains. This approach provides effective solutions to both
model verification and authorization. For ownership verification, watermarking
techniques are commonly used but are often vulnerable to sophisticated
watermark removal methods. Our NTL-based model verification approach instead
provides robust resistance to state-of-the-art watermark removal methods, as
shown in extensive experiments for four of such methods over the digits,
CIFAR10 &amp; STL10, and VisDA datasets. For usage authorization, prior solutions
focus on authorizing specific users to use the model, but authorized users can
still apply the model to any data without restriction. Our NTL-based
authorization approach instead provides data-centric usage protection by
significantly degrading the performance of usage on unauthorized data. Its
effectiveness is also shown through experiments on a variety of datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">LE-NAS: Learning-based Ensenble with NAS for Dose Prediction. (arXiv:2106.06733v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1">Yi Lin</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1">Yanfei Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1">Jingguang Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_G/0/1/0/all/0/1">Guocai Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1">Kai Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06733">
                                    <div class="article-summary-box-inner">
                                        <span>Radiation therapy treatment planning is a complex process, as the target dose
prescription and normal tissue sparing are conflicting objectives. Automated
and accurate dose prediction for radiation therapy planning is in high demand.
In this study, we propose a novel learning-based ensemble approach, named
LE-NAS, which integrates neural architecture search (NAS) with knowledge
distillation for 3D radiotherapy dose prediction. Specifically, the prediction
network first exhaustively searches each block from enormous architecture
space. Then, multiple architectures are selected with promising performance and
diversity. To reduce the inference time, we adopt the teacher-student paradigm
by treating the combination of diverse outputs from multiple searched networks
as supervisions to guide the student network training. In addition, we apply
adversarial learning to optimize the student network to recover the knowledge
in teacher networks. To the best of our knowledge, we are the first to
investigate the combination of NAS and knowledge distillation. The proposed
method has been evaluated on the public OpenKBP dataset, and experimental
results demonstrate the effectiveness of our method and its superior
performance to the state-of-the-art method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-Subject Domain Adaptation for Multi-Frame EEG Images. (arXiv:2106.06769v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junfu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bi Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06769">
                                    <div class="article-summary-box-inner">
                                        <span>Working memory (WM) is a basic part of human cognition, which plays an
important role in the study of human cognitive load. Among various brain
imaging techniques, electroencephalography has shown its advantage on easy
access and reliability. However, one of the critical challenges is that
individual difference may cause the ineffective results, especially when the
established model meets an unfamiliar subject. In this work, we propose a
cross-subject deep adaptation model with spatial attention (CS-DASA) to
generalize the workload classifications across subjects. First, we transform
time-series EEG data into multi-frame EEG images incorporating more
spatio-temporal information. First, the subject-shared module in CS-DASA
receives multi-frame EEG image data from both source and target subjects and
learns the common feature representations. Then, in subject-specific module,
the maximum mean discrepancy is implemented to measure the domain distribution
divergence in a reproducing kernel Hilbert space, which can add an effective
penalty loss for domain adaptation. Additionally, the subject-to-subject
spatial attention mechanism is employed to focus on the most discriminative
spatial feature in EEG image data. Experiments conducted on a public WM EEG
dataset containing 13 subjects show that the proposed model is capable of
achieve better performance than existing state-of-the art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bayesian Neural Networks for Virtual Flow Metering: An Empirical Study. (arXiv:2102.01391v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Grimstad_B/0/1/0/all/0/1">Bjarne Grimstad</a>, <a href="http://arxiv.org/find/cs/1/au:+Hotvedt_M/0/1/0/all/0/1">Mathilde Hotvedt</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandnes_A/0/1/0/all/0/1">Anders T. Sandnes</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolbjornsen_O/0/1/0/all/0/1">Odd Kolbj&#xf8;rnsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Imsland_L/0/1/0/all/0/1">Lars S. Imsland</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01391">
                                    <div class="article-summary-box-inner">
                                        <span>Recent works have presented promising results from the application of machine
learning (ML) to the modeling of flow rates in oil and gas wells. Encouraging
results and advantageous properties of ML models, such as computationally cheap
evaluation and ease of calibration to new data, have sparked optimism for the
development of data-driven virtual flow meters (VFMs). Data-driven VFMs are
developed in the small data regime, where it is important to question the
uncertainty and robustness of models. The modeling of uncertainty may help to
build trust in models, which is a prerequisite for industrial applications. The
contribution of this paper is the introduction of a probabilistic VFM based on
Bayesian neural networks. Uncertainty in the model and measurements is
described, and the paper shows how to perform approximate Bayesian inference
using variational inference. The method is studied by modeling on a large and
heterogeneous dataset, consisting of 60 wells across five different oil and gas
assets. The predictive performance is analyzed on historical and future test
data, where an average error of 4-6% and 8-13% is achieved for the 50% best
performing models, respectively. Variational inference appears to provide more
robust predictions than the reference approach on future data. Prediction
performance and uncertainty calibration is explored in detail and discussed in
light of four data challenges. The findings motivate the development of
alternative strategies to improve the robustness of data-driven VFMs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization. (arXiv:2006.16205v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1">Sang Michael Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tengyu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Percy Liang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.16205">
                                    <div class="article-summary-box-inner">
                                        <span>We focus on prediction problems with structured outputs that are subject to
output validity constraints, e.g. pseudocode-to-code translation where the code
must compile. While labeled input-output pairs are expensive to obtain,
&quot;unlabeled&quot; outputs, i.e. outputs without corresponding inputs, are freely
available (e.g. code on GitHub) and provide information about output validity.
Pre-training captures this structure by training a denoiser to denoise
corrupted versions of unlabeled outputs. We first show that standard
fine-tuning after pre-training destroys some of this structure. We then propose
composed fine-tuning, which trains a predictor composed with the pre-trained
denoiser. Importantly, the denoiser is fixed to preserve output structure. Like
standard fine-tuning, the predictor is also initialized with the pre-trained
denoiser. We prove for two-layer ReLU networks that composed fine-tuning
significantly reduces the complexity of the predictor, thus improving
generalization. Empirically, we show that composed fine-tuning improves over
standard fine-tuning on two pseudocode-to-code translation datasets (3% and 6%
relative). The improvement is magnified on out-of-distribution (OOD) examples
(4% and 25% relative), suggesting that reducing predictor complexity improves
OOD extrapolation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Singular Dynamic Mode Decompositions. (arXiv:2106.02639v2 [eess.SY] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Rosenfeld_J/0/1/0/all/0/1">Joel A. Rosenfeld</a>, <a href="http://arxiv.org/find/eess/1/au:+Kamalapurkar_R/0/1/0/all/0/1">Rushikesh Kamalapurkar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02639">
                                    <div class="article-summary-box-inner">
                                        <span>This manuscript is aimed at addressing several long standing limitations of
dynamic mode decompositions in the application of Koopman analysis. Principle
among these limitations are the convergence of associated Dynamic Mode
Decomposition algorithms and the existence of Koopman modes. To address these
limitations, two major modifications are made, where Koopman operators are
removed from the analysis in light of Liouville operators (known as Koopman
generators in special cases), and these operators are shown to be compact for
certain pairs of Hilbert spaces selected separately as the domain and range of
the operator. While eigenfunctions are discarded in the general analysis, a
viable reconstruction algorithm is still demonstrated, and the sacrifice of
eigenfunctions realizes the theoretical goals of DMD analysis that have yet to
be achieved in other contexts. However, in the case where the domain is
embedded in the range, an eigenfunction approach is still achievable, where a
more typical DMD routine is established, but that leverages a finite rank
representation that converges in norm. The manuscript concludes with the
description of two Dynamic Mode Decomposition algorithms that converges when a
dense collection of occupation kernels, arising from the data, are leveraged in
the analysis.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Characterizing the Gap Between Actor-Critic and Policy Gradient. (arXiv:2106.06932v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1">Junfeng Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Saurabh Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Gummadi_R/0/1/0/all/0/1">Ramki Gummadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1">Dale Schuurmans</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06932">
                                    <div class="article-summary-box-inner">
                                        <span>Actor-critic (AC) methods are ubiquitous in reinforcement learning. Although
it is understood that AC methods are closely related to policy gradient (PG),
their precise connection has not been fully characterized previously. In this
paper, we explain the gap between AC and PG methods by identifying the exact
adjustment to the AC objective/gradient that recovers the true policy gradient
of the cumulative reward objective (PG). Furthermore, by viewing the AC method
as a two-player Stackelberg game between the actor and critic, we show that the
Stackelberg policy gradient can be recovered as a special case of our more
general analysis. Based on these results, we develop practical algorithms,
Residual Actor-Critic and Stackelberg Actor-Critic, for estimating the
correction between AC and PG and use these to modify the standard AC algorithm.
Experiments on popular tabular and continuous environments show the proposed
corrections can improve both the sample efficiency and final performance of
existing AC methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Hybrid Variance-Reduced Method for Decentralized Stochastic Non-Convex Optimization. (arXiv:2102.06752v2 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Xin_R/0/1/0/all/0/1">Ran Xin</a>, <a href="http://arxiv.org/find/math/1/au:+Khan_U/0/1/0/all/0/1">Usman A. Khan</a>, <a href="http://arxiv.org/find/math/1/au:+Kar_S/0/1/0/all/0/1">Soummya Kar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06752">
                                    <div class="article-summary-box-inner">
                                        <span>This paper considers decentralized stochastic optimization over a network of
$n$ nodes, where each node possesses a smooth non-convex local cost function
and the goal of the networked nodes is to find an $\epsilon$-accurate
first-order stationary point of the sum of the local costs. We focus on an
online setting, where each node accesses its local cost only by means of a
stochastic first-order oracle that returns a noisy version of the exact
gradient. In this context, we propose a novel single-loop decentralized hybrid
variance-reduced stochastic gradient method, called GT-HSGD, that outperforms
the existing approaches in terms of both the oracle complexity and practical
implementation. The GT-HSGD algorithm implements specialized local hybrid
stochastic gradient estimators that are fused over the network to track the
global gradient. Remarkably, GT-HSGD achieves a network topology-independent
oracle complexity of $O(n^{-1}\epsilon^{-3})$ when the required error tolerance
$\epsilon$ is small enough, leading to a linear speedup with respect to the
centralized optimal online variance-reduced approaches that operate on a single
node. Numerical experiments are provided to illustrate our main technical
results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection. (arXiv:2104.09261v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Han Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1">Chunyan Miao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09261">
                                    <div class="article-summary-box-inner">
                                        <span>The existence of multiple datasets for sarcasm detection prompts us to apply
transfer learning to exploit their commonality. The adversarial neural transfer
(ANT) framework utilizes multiple loss terms that encourage the source-domain
and the target-domain feature distributions to be similar while optimizing for
domain-specific performance. However, these objectives may be in conflict,
which can lead to optimization difficulties and sometimes diminished transfer.
We propose a generalized latent optimization strategy that allows different
losses to accommodate each other and improves training dynamics. The proposed
method outperforms transfer learning and meta-learning baselines. In
particular, we achieve 10.02% absolute performance gain over the previous state
of the art on the iSarcasm dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Barlow Twins: Self-Supervised Learning via Redundancy Reduction. (arXiv:2103.03230v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zbontar_J/0/1/0/all/0/1">Jure Zbontar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1">Li Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1">Ishan Misra</a>, <a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1">Yann LeCun</a>, <a href="http://arxiv.org/find/cs/1/au:+Deny_S/0/1/0/all/0/1">St&#xe9;phane Deny</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03230">
                                    <div class="article-summary-box-inner">
                                        <span>Self-supervised learning (SSL) is rapidly closing the gap with supervised
methods on large computer vision benchmarks. A successful approach to SSL is to
learn embeddings which are invariant to distortions of the input sample.
However, a recurring issue with this approach is the existence of trivial
constant solutions. Most current methods avoid such solutions by careful
implementation details. We propose an objective function that naturally avoids
collapse by measuring the cross-correlation matrix between the outputs of two
identical networks fed with distorted versions of a sample, and making it as
close to the identity matrix as possible. This causes the embedding vectors of
distorted versions of a sample to be similar, while minimizing the redundancy
between the components of these vectors. The method is called Barlow Twins,
owing to neuroscientist H. Barlow&#x27;s redundancy-reduction principle applied to a
pair of identical networks. Barlow Twins does not require large batches nor
asymmetry between the network twins such as a predictor network, gradient
stopping, or a moving average on the weight updates. Intriguingly it benefits
from very high-dimensional output vectors. Barlow Twins outperforms previous
methods on ImageNet for semi-supervised classification in the low-data regime,
and is on par with current state of the art for ImageNet classification with a
linear classifier head, and for transfer tasks of classification and object
detection.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">LogME: Practical Assessment of Pre-trained Models for Transfer Learning. (arXiv:2102.11005v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1">Kaichao You</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1">Mingsheng Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianmin Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11005">
                                    <div class="article-summary-box-inner">
                                        <span>This paper studies task adaptive pre-trained model selection, an
underexplored problem of assessing pre-trained models for the target task and
select best ones from the model zoo \emph{without fine-tuning}. A few pilot
works addressed the problem in transferring supervised pre-trained models to
classification tasks, but they cannot handle emerging unsupervised pre-trained
models or regression tasks. In pursuit of a practical assessment method, we
propose to estimate the maximum value of label evidence given features
extracted by pre-trained models. Unlike the maximum likelihood, the maximum
evidence is \emph{immune to over-fitting}, while its expensive computation can
be dramatically reduced by our carefully designed algorithm. The Logarithm of
Maximum Evidence (LogME) can be used to assess pre-trained models for transfer
learning: a pre-trained model with a high LogME value is likely to have good
transfer performance. LogME is \emph{fast, accurate, and general},
characterizing itself as the first practical method for assessing pre-trained
models. Compared with brute-force fine-tuning, LogME brings at most
$3000\times$ speedup in wall-clock time and requires only $1\%$ memory
footprint. It outperforms prior methods by a large margin in their setting and
is applicable to new settings. It is general enough for diverse pre-trained
models (supervised pre-trained and unsupervised pre-trained), downstream tasks
(classification and regression), and modalities (vision and language). Code is
available at this repository:
\href{https://github.com/thuml/LogME}{https://github.com/thuml/LogME}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Time Series Classification via Topological Data Analysis. (arXiv:2102.01956v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Karan_A/0/1/0/all/0/1">Alperen Karan</a>, <a href="http://arxiv.org/find/stat/1/au:+Kaygun_A/0/1/0/all/0/1">Atabey Kaygun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01956">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we develop topological data analysis methods for
classification tasks on univariate time series. As an application, we perform
binary and ternary classification tasks on two public datasets that consist of
physiological signals collected under stress and non-stress conditions. We
accomplish our goal by using persistent homology to engineer stable topological
features after we use a time delay embedding of the signals and perform a
subwindowing instead of using windows of fixed length. The combination of
methods we use can be applied to any univariate time series and in this
application allows us to reduce noise and use long window sizes without
incurring an extra computational cost. We then use machine learning models on
the features we algorithmically engineered to obtain higher accuracies with
fewer features.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">RobustBench: a standardized adversarial robustness benchmark. (arXiv:2010.09670v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Croce_F/0/1/0/all/0/1">Francesco Croce</a>, <a href="http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1">Maksym Andriushchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1">Vikash Sehwag</a>, <a href="http://arxiv.org/find/cs/1/au:+Debenedetti_E/0/1/0/all/0/1">Edoardo Debenedetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1">Nicolas Flammarion</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1">Mung Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1">Prateek Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1">Matthias Hein</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09670">
                                    <div class="article-summary-box-inner">
                                        <span>As a research community, we are still lacking a systematic understanding of
the progress on adversarial robustness, which often makes it hard to identify
the most promising ideas in training robust models. A key challenge in
benchmarking robustness is that its evaluation is often error-prone, leading to
overestimation of the true robustness of models. While adaptive attacks
designed for a particular defense are a potential solution, they have to be
highly customized for particular models, which makes it difficult to compare
different methods. Our goal is to instead establish a standardized benchmark of
adversarial robustness, which as accurately as possible reflects the robustness
of the considered models within a reasonable computational budget. To evaluate
the robustness of models for our benchmark, we consider AutoAttack, an ensemble
of white- and black-box attacks which was recently shown in a large-scale study
to improve almost all robustness evaluations compared to the original
publications. We also impose some restrictions on the admitted models to rule
out defenses that only make gradient-based attacks ineffective without
improving actual robustness. Our leaderboard, hosted at
https://robustbench.github.io/, contains evaluations of 90+ models and aims at
reflecting the current state of the art on a set of well-defined tasks in
$\ell_\infty$- and $\ell_2$-threat models and on common corruptions, with
possible extensions in the future. Additionally, we open-source the library
https://github.com/RobustBench/robustbench that provides unified access to 60+
robust models to facilitate their downstream applications. Finally, based on
the collected models, we analyze the impact of robustness on the performance on
distribution shifts, calibration, out-of-distribution detection, fairness,
privacy leakage, smoothness, and transferability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The DEformer: An Order-Agnostic Distribution Estimating Transformer. (arXiv:2106.06989v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Alcorn_M/0/1/0/all/0/1">Michael A. Alcorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06989">
                                    <div class="article-summary-box-inner">
                                        <span>Order-agnostic autoregressive distribution estimation (OADE), i.e.,
autoregressive distribution estimation where the features can occur in an
arbitrary order, is a challenging problem in generative machine learning. Prior
work on OADE has encoded feature identity (e.g., pixel location) by assigning
each feature to a distinct fixed position in an input vector. As a result,
architectures built for these inputs must strategically mask either the input
or model weights to learn the various conditional distributions necessary for
inferring the full joint distribution of the dataset in an order-agnostic way.
In this paper, we propose an alternative approach for encoding feature
identities, where each feature&#x27;s identity is included alongside its value in
the input. This feature identity encoding strategy allows neural architectures
designed for sequential data to be applied to the OADE task without
modification. As a proof of concept, we show that a Transformer trained on this
input (which we refer to as &quot;the DEformer&quot;, i.e., the distribution estimating
Transformer) can effectively model binarized-MNIST, approaching the average
negative log-likelihood of fixed order autoregressive distribution estimating
algorithms while still being entirely order-agnostic.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A New Formalism, Method and Open Issues for Zero-Shot Coordination. (arXiv:2106.06613v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Treutlein_J/0/1/0/all/0/1">Johannes Treutlein</a>, <a href="http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1">Michael Dennis</a>, <a href="http://arxiv.org/find/cs/1/au:+Oesterheld_C/0/1/0/all/0/1">Caspar Oesterheld</a>, <a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1">Jakob Foerster</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06613">
                                    <div class="article-summary-box-inner">
                                        <span>In many coordination problems, independently reasoning humans are able to
discover mutually compatible policies. In contrast, independently trained
self-play policies are often mutually incompatible. Zero-shot coordination
(ZSC) has recently been proposed as a new frontier in multi-agent reinforcement
learning to address this fundamental issue. Prior work approaches the ZSC
problem by assuming players can agree on a shared learning algorithm but not on
labels for actions and observations, and proposes other-play as an optimal
solution. However, until now, this &quot;label-free&quot; problem has only been
informally defined. We formalize this setting as the label-free coordination
(LFC) problem by defining the label-free coordination game. We show that
other-play is not an optimal solution to the LFC problem as it fails to
consistently break ties between incompatible maximizers of the other-play
objective. We introduce an extension of the algorithm, other-play with
tie-breaking, and prove that it is optimal in the LFC problem and an
equilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the
ZSC setting aims to prevent, we conclude that the LFC problem does not reflect
the aims of ZSC. To address this, we introduce an alternative informal
operationalization of ZSC as a starting point for future work.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ECG-TCN: Wearable Cardiac Arrhythmia Detection with a Temporal Convolutional Network. (arXiv:2103.13740v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ingolfsson_T/0/1/0/all/0/1">Thorir Mar Ingolfsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaying Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hersche_M/0/1/0/all/0/1">Michael Hersche</a>, <a href="http://arxiv.org/find/cs/1/au:+Burrello_A/0/1/0/all/0/1">Alessio Burrello</a>, <a href="http://arxiv.org/find/cs/1/au:+Cavigelli_L/0/1/0/all/0/1">Lukas Cavigelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1">Luca Benini</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13740">
                                    <div class="article-summary-box-inner">
                                        <span>Personalized ubiquitous healthcare solutions require energy-efficient
wearable platforms that provide an accurate classification of bio-signals while
consuming low average power for long-term battery-operated use. Single lead
electrocardiogram (ECG) signals provide the ability to detect, classify, and
even predict cardiac arrhythmia. In this paper, we propose a novel temporal
convolutional network (TCN) that achieves high accuracy while still being
feasible for wearable platform use. Experimental results on the ECG5000 dataset
show that the TCN has a similar accuracy (94.2%) score as the state-of-the-art
(SoA) network while achieving an improvement of 16.5% in the balanced accuracy
score. This accurate classification is done with 27 times fewer parameters and
37 times less multiply-accumulate operations. We test our implementation on two
publicly available platforms, the STM32L475, which is based on ARM Cortex M4F,
and the GreenWaves Technologies GAP8 on the GAPuino board, based on 1+8 RISC-V
CV32E40P cores. Measurements show that the GAP8 implementation respects the
real-time constraints while consuming 0.10 mJ per inference. With 9.91
GMAC/s/W, it is 23.0 times more energy-efficient and 46.85 times faster than an
implementation on the ARM Cortex M4F (0.43 GMAC/s/W). Overall, we obtain 8.1%
higher accuracy while consuming 19.6 times less energy and being 35.1 times
faster compared to a previous SoA embedded implementation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Semi-Supervised Data Programming with Subset Selection. (arXiv:2008.09887v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1">Ayush Maheshwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_O/0/1/0/all/0/1">Oishik Chatterjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1">KrishnaTeja Killamsetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1">Ganesh Ramakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1">Rishabh Iyer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09887">
                                    <div class="article-summary-box-inner">
                                        <span>The paradigm of data programming, which uses weak supervision in the form of
rules/labelling functions, and semi-supervised learning, which augments small
amounts of labelled data with a large unlabelled dataset, have shown great
promise in several text classification scenarios. In this work, we argue that
by not using any labelled data, data programming based approaches can yield
sub-optimal performances, particularly when the labelling functions are noisy.
The first contribution of this work is an introduction of a framework, \model
which is a semi-supervised data programming paradigm that learns a \emph{joint
model} that effectively uses the rules/labelling functions along with
semi-supervised loss functions on the feature space. Next, we also study
\modelss which additionally does subset selection on top of the joint
semi-supervised data programming objective and \emph{selects} a set of examples
that can be used as the labelled set by \model. The goal of \modelss is to
ensure that the labelled data can \emph{complement} the labelling functions,
thereby benefiting from both data-programming as well as appropriately selected
data for human labelling. We demonstrate that by effectively combining
semi-supervision, data-programming, and subset selection paradigms, we
significantly outperform the current state-of-the-art on seven publicly
available datasets. \footnote{The source code is available at
\url{https://github.com/ayushbits/Semi-Supervised-LFs-Subset-Selection}}</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models. (arXiv:2104.10683v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Koeppe_A/0/1/0/all/0/1">Arnd Koeppe</a>, <a href="http://arxiv.org/find/cs/1/au:+Bamer_F/0/1/0/all/0/1">Franz Bamer</a>, <a href="http://arxiv.org/find/cs/1/au:+Selzer_M/0/1/0/all/0/1">Michael Selzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Nestler_B/0/1/0/all/0/1">Britta Nestler</a>, <a href="http://arxiv.org/find/cs/1/au:+Markert_B/0/1/0/all/0/1">Bernd Markert</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10683">
                                    <div class="article-summary-box-inner">
                                        <span>(Artificial) neural networks have become increasingly popular in mechanics as
means to accelerate computations with model order reduction techniques and as
universal models for a wide variety of materials. However, the major
disadvantage of neural networks remains: their numerous parameters are
challenging to interpret and explain. Thus, neural networks are often labeled
as black boxes, and their results often elude human interpretation. In
mechanics, the new and active field of physics-informed neural networks
attempts to mitigate this disadvantage by designing deep neural networks on the
basis of mechanical knowledge. By using this a priori knowledge, deeper and
more complex neural networks became feasible, since the mechanical assumptions
could be explained. However, the internal reasoning and explanation of neural
network parameters remain mysterious.

Complementary to the physics-informed approach, we propose a first step
towards a physics-informing approach, which explains neural networks trained on
mechanical data a posteriori. This novel explainable artificial intelligence
approach aims at elucidating the black box of neural networks and their
high-dimensional representations. Therein, the principal component analysis
decorrelates the distributed representations in cell states of RNNs and allows
the comparison to known and fundamental functions. The novel approach is
supported by a systematic hyperparameter search strategy that identifies the
best neural network architectures and training parameters. The findings of
three case studies on fundamental constitutive models (hyperelasticity,
elastoplasticity, and viscoelasticity) imply that the proposed strategy can
help identify numerical and analytical closed-form solutions to characterize
new materials.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Double-descent curves in neural networks: a new perspective using Gaussian processes. (arXiv:2102.07238v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Harzli_O/0/1/0/all/0/1">Ouns El Harzli</a>, <a href="http://arxiv.org/find/stat/1/au:+Valle_Perez_G/0/1/0/all/0/1">Guillermo Valle-P&#xe9;rez</a>, <a href="http://arxiv.org/find/stat/1/au:+Louis_A/0/1/0/all/0/1">Ard A. Louis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07238">
                                    <div class="article-summary-box-inner">
                                        <span>Double-descent curves in neural networks describe the phenomenon that the
generalisation error initially descends with increasing parameters, then grows
after reaching an optimal number of parameters which is less than the number of
data points, but then descends again in the overparameterised regime. Here we
use a neural network Gaussian process (NNGP) which maps exactly to a fully
connected network (FCN) in the infinite width limit, combined with techniques
from random matrix theory, to calculate this generalisation behaviour, with a
particular focus on the overparameterised regime. An advantage of our NNGP
approach is that the analytical calculations are easier to interpret. We argue
that neural network generalization performance improves in the
overparameterised regime precisely because that is where they converge to their
equivalent Gaussian process.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The zoo of Fairness metrics in Machine Learning. (arXiv:2106.00467v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Castelnovo_A/0/1/0/all/0/1">Alessandro Castelnovo</a>, <a href="http://arxiv.org/find/cs/1/au:+Crupi_R/0/1/0/all/0/1">Riccardo Crupi</a>, <a href="http://arxiv.org/find/cs/1/au:+Greco_G/0/1/0/all/0/1">Greta Greco</a>, <a href="http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1">Daniele Regoli</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00467">
                                    <div class="article-summary-box-inner">
                                        <span>In recent years, the problem of addressing fairness in Machine Learning (ML)
and automatic decision-making has attracted a lot of attention in the
scientific communities dealing with Artificial Intelligence. A plethora of
different definitions of fairness in ML have been proposed, that consider
different notions of what is a &quot;fair decision&quot; in situations impacting
individuals in the population. The precise differences, implications and
&quot;orthogonality&quot; between these notions have not yet been fully analyzed in the
literature. In this work, we try to make some order out of this zoo of
definitions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Musical Prosody-Driven Emotion Classification: Interpreting Vocalists Portrayal of Emotions Through Machine Learning. (arXiv:2106.02556v2 [cs.SD] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Farris_N/0/1/0/all/0/1">Nicholas Farris</a>, <a href="http://arxiv.org/find/cs/1/au:+Model_B/0/1/0/all/0/1">Brian Model</a>, <a href="http://arxiv.org/find/cs/1/au:+Savery_R/0/1/0/all/0/1">Richard Savery</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinberg_G/0/1/0/all/0/1">Gil Weinberg</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02556">
                                    <div class="article-summary-box-inner">
                                        <span>The task of classifying emotions within a musical track has received
widespread attention within the Music Information Retrieval (MIR) community.
Music emotion recognition has traditionally relied on the use of acoustic
features, verbal features, and metadata-based filtering. The role of musical
prosody remains under-explored despite several studies demonstrating a strong
connection between prosody and emotion. In this study, we restrict the input of
traditional machine learning algorithms to the features of musical prosody.
Furthermore, our proposed approach builds upon the prior by classifying
emotions under an expanded emotional taxonomy, using the Geneva Wheel of
Emotion. We utilize a methodology for individual data collection from
vocalists, and personal ground truth labeling by the artist themselves. We
found that traditional machine learning algorithms when limited to the features
of musical prosody (1) achieve high accuracies for a single singer, (2)
maintain high accuracy when the dataset is expanded to multiple singers, and
(3) achieve high accuracies when trained on a reduced subset of the total
features.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Defending against Backdoors in Federated Learning with Robust Learning Rate. (arXiv:2007.03767v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ozdayi_M/0/1/0/all/0/1">Mustafa Safa Ozdayi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kantarcioglu_M/0/1/0/all/0/1">Murat Kantarcioglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gel_Y/0/1/0/all/0/1">Yulia R. Gel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.03767">
                                    <div class="article-summary-box-inner">
                                        <span>Federated learning (FL) allows a set of agents to collaboratively train a
model without sharing their potentially sensitive data. This makes FL suitable
for privacy-preserving applications. At the same time, FL is susceptible to
adversarial attacks due to decentralized and unvetted data. One important line
of attacks against FL is the backdoor attacks. In a backdoor attack, an
adversary tries to embed a backdoor functionality to the model during training
that can later be activated to cause a desired misclassification. To prevent
backdoor attacks, we propose a lightweight defense that requires minimal change
to the FL protocol. At a high level, our defense is based on carefully
adjusting the aggregation server&#x27;s learning rate, per dimension and per round,
based on the sign information of agents&#x27; updates. We first conjecture the
necessary steps to carry a successful backdoor attack in FL setting, and then,
explicitly formulate the defense based on our conjecture. Through experiments,
we provide empirical evidence that supports our conjecture, and we test our
defense against backdoor attacks under different settings. We observe that
either backdoor is completely eliminated, or its accuracy is significantly
reduced. Overall, our experiments suggest that our defense significantly
outperforms some of the recently proposed defenses in the literature. We
achieve this by having minimal influence over the accuracy of the trained
models. In addition, we also provide convergence rate analysis for our proposed
scheme.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Statistical Analysis from the Fourier Integral Theorem. (arXiv:2106.06608v1 [stat.ME])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1">Nhat Ho</a>, <a href="http://arxiv.org/find/stat/1/au:+Walker_S/0/1/0/all/0/1">Stephen G. Walker</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06608">
                                    <div class="article-summary-box-inner">
                                        <span>Taking the Fourier integral theorem as our starting point, in this paper we
focus on natural Monte Carlo and fully nonparametric estimators of multivariate
distributions and conditional distribution functions. We do this without the
need for any estimated covariance matrix or dependence structure between
variables. These aspects arise immediately from the integral theorem. Being
able to model multivariate data sets using conditional distribution functions
we can study a number of problems, such as prediction for Markov processes,
estimation of mixing distribution functions which depend on covariates, and
general multivariate data. Estimators are explicit Monte Carlo based and
require no recursive or iterative algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Locally Adaptive Label Smoothing for Predictive Churn. (arXiv:2102.05140v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1">Dara Bahri</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Heinrich Jiang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05140">
                                    <div class="article-summary-box-inner">
                                        <span>Training modern neural networks is an inherently noisy process that can lead
to high \emph{prediction churn} -- disagreements between re-trainings of the
same model due to factors such as randomization in the parameter initialization
and mini-batches -- even when the trained models all attain similar accuracies.
Such prediction churn can be very undesirable in practice. In this paper, we
present several baselines for reducing churn and show that training on soft
labels obtained by adaptively smoothing each example&#x27;s label based on the
example&#x27;s neighboring labels often outperforms the baselines on churn while
improving accuracy on a variety of benchmark classification tasks and model
architectures.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PGDOT -- Perturbed Gradient Descent Adapted with Occupation Time. (arXiv:2005.04507v2 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Guo_X/0/1/0/all/0/1">Xin Guo</a>, <a href="http://arxiv.org/find/math/1/au:+Han_J/0/1/0/all/0/1">Jiequn Han</a>, <a href="http://arxiv.org/find/math/1/au:+Tajrobehkar_M/0/1/0/all/0/1">Mahan Tajrobehkar</a>, <a href="http://arxiv.org/find/math/1/au:+Tang_W/0/1/0/all/0/1">Wenpin Tang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.04507">
                                    <div class="article-summary-box-inner">
                                        <span>This paper develops further the idea of perturbed gradient descent (PGD), by
adapting perturbation with the history of states via the notion of occupation
time. The proposed algorithm, perturbed gradient descent adapted with
occupation time (PGDOT), is shown to converge at least as fast as the PGD
algorithm and is guaranteed to avoid getting stuck at saddle points. The
analysis is corroborated by empirical studies, in which a mini-batch version of
PGDOT is shown to outperform alternatives such as mini-batch gradient descent,
Adam, AMSGrad, and RMSProp in training multilayer perceptrons (MLPs). In
particular, the mini-batch PGDOT manages to escape saddle points whereas these
alternatives fail.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Controllable Generation from Pre-trained Language Models via Inverse Prompting. (arXiv:2103.10685v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xu Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1">Da Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1">Qingyang Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhilin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10685">
                                    <div class="article-summary-box-inner">
                                        <span>Large-scale pre-trained language models have demonstrated strong capabilities
of generating realistic text. However, it remains challenging to control the
generation results. Previous approaches such as prompting are far from
sufficient, which limits the usage of language models. To tackle this
challenge, we propose an innovative method, inverse prompting, to better
control text generation. The core idea of inverse prompting is to use generated
text to inversely predict the prompt during beam search, which enhances the
relevance between the prompt and the generated text and provides better
controllability. Empirically, we pre-train a large-scale Chinese language model
to perform a systematic study using human evaluation on the tasks of
open-domain poem generation and open-domain long-form question answering. Our
results show that our proposed method substantially outperforms the baselines
and that our generation quality is close to human performance on some of the
tasks.

Narrators can try our poem generation demo at
https://pretrain.aminer.cn/apps/poetry.html, while our QA demo can be found at
https://pretrain.aminer.cn/app/qa. For researchers, the code is provided in
https://github.com/THUDM/InversePrompting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Meta-Learning Dynamics Forecasting Using Task Inference. (arXiv:2102.10271v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1">Robin Walters</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1">Rose Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10271">
                                    <div class="article-summary-box-inner">
                                        <span>Current deep learning models for dynamics forecasting struggle with
generalization. They can only forecast in a specific domain and fail when
applied to systems with different parameters, external forces, or boundary
conditions. We propose a model-based meta-learning method called DyAd which can
generalize across heterogeneous domains by partitioning them into different
tasks. DyAd has two parts: an encoder which infers the time-invariant hidden
features of the task with weak supervision, and a forecaster which learns the
shared dynamics of the entire domain. The encoder adapts and controls the
forecaster during inference using adaptive instance normalization and adaptive
padding. Theoretically, we prove that the generalization error of such
procedure is related to the task relatedness in the source domain, as well as
the domain differences between source and target. Experimentally, we
demonstrate that our model outperforms state-of-the-art approaches on both
turbulent flow and real-world ocean data forecasting tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning MDPs from Features: Predict-Then-Optimize for Sequential Decision Problems by Reinforcement Learning. (arXiv:2106.03279v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1">Sanket Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haipeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Perrault_A/0/1/0/all/0/1">Andrew Perrault</a>, <a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1">Finale Doshi-Velez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1">Milind Tambe</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03279">
                                    <div class="article-summary-box-inner">
                                        <span>In the predict-then-optimize framework, the objective is to train a
predictive model, mapping from environment features to parameters of an
optimization problem, which maximizes decision quality when the optimization is
subsequently solved. Recent work on decision-focused learning shows that
embedding the optimization problem in the training pipeline can improve
decision quality and help generalize better to unseen tasks compared to relying
on an intermediate loss function for evaluating prediction quality. We study
the predict-then-optimize framework in the context of sequential decision
problems (formulated as MDPs) that are solved via reinforcement learning. In
particular, we are given environment features and a set of trajectories from
training MDPs, which we use to train a predictive model that generalizes to
unseen test MDPs without trajectories. Two significant computational challenges
arise in applying decision-focused learning to MDPs: (i) large state and action
spaces make it infeasible for existing techniques to differentiate through MDP
problems, and (ii) the high-dimensional policy space, as parameterized by a
neural network, makes differentiating through a policy expensive. We resolve
the first challenge by sampling provably unbiased derivatives to approximate
and differentiate through optimality conditions, and the second challenge by
using a low-rank approximation to the high-dimensional sample-based
derivatives. We implement both Bellman--based and policy gradient--based
decision-focused learning on three different MDP problems with missing
parameters, and show that decision-focused learning performs better in
generalization to unseen tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Quantifying Ignorance in Individual-Level Causal-Effect Estimates under Hidden Confounding. (arXiv:2103.04850v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jesson_A/0/1/0/all/0/1">Andrew Jesson</a>, <a href="http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1">S&#xf6;ren Mindermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1">Yarin Gal</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1">Uri Shalit</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04850">
                                    <div class="article-summary-box-inner">
                                        <span>We study the problem of learning conditional average treatment effects (CATE)
from high-dimensional, observational data with unobserved confounders.
Unobserved confounders introduce ignorance -- a level of unidentifiability --
about an individual&#x27;s response to treatment by inducing bias in CATE estimates.
We present a new parametric interval estimator suited for high-dimensional
data, that estimates a range of possible CATE values when given a predefined
bound on the level of hidden confounding. Further, previous interval estimators
do not account for ignorance about the CATE associated with samples that may be
underrepresented in the original study, or samples that violate the overlap
assumption. Our interval estimator also incorporates model uncertainty so that
practitioners can be made aware of out-of-distribution data. We prove that our
estimator converges to tight bounds on CATE when there may be unobserved
confounding, and assess it using semi-synthetic, high-dimensional datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PDE-constrained Models with Neural Network Terms: Optimization and Global Convergence. (arXiv:2105.08633v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sirignano_J/0/1/0/all/0/1">Justin Sirignano</a>, <a href="http://arxiv.org/find/cs/1/au:+MacArt_J/0/1/0/all/0/1">Jonathan MacArt</a>, <a href="http://arxiv.org/find/cs/1/au:+Spiliopoulos_K/0/1/0/all/0/1">Konstantinos Spiliopoulos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08633">
                                    <div class="article-summary-box-inner">
                                        <span>Recent research has used deep learning to develop partial differential
equation (PDE) models in science and engineering. The functional form of the
PDE is determined by a neural network, and the neural network parameters are
calibrated to available data. Calibration of the embedded neural network can be
performed by optimizing over the PDE. Motivated by these applications, we
rigorously study the optimization of a class of linear elliptic PDEs with
neural network terms. The neural network parameters in the PDE are optimized
using gradient descent, where the gradient is evaluated using an adjoint PDE.
As the number of parameters become large, the PDE and adjoint PDE converge to a
non-local PDE system. Using this limit PDE system, we are able to prove
convergence of the neural network-PDE to a global minimum during the
optimization. The limit PDE system contains a non-local linear operator whose
eigenvalues are positive but become arbitrarily small. The lack of a spectral
gap for the eigenvalues poses the main challenge for the global convergence
proof. Careful analysis of the spectral decomposition of the coupled PDE and
adjoint PDE system is required. Finally, we use this adjoint method to train a
neural network model for an application in fluid mechanics, in which the neural
network functions as a closure model for the Reynolds-averaged Navier-Stokes
(RANS) equations. The RANS neural network model is trained on several datasets
for turbulent channel flow and is evaluated out-of-sample at different Reynolds
numbers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Integer Programming for Causal Structure Learning in the Presence of Latent Variables. (arXiv:2102.03129v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Rui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1">Sanjeeb Dash</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Tian Gao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03129">
                                    <div class="article-summary-box-inner">
                                        <span>The problem of finding an ancestral acyclic directed mixed graph (ADMG) that
represents the causal relationships between a set of variables is an important
area of research on causal inference. Most existing score-based structure
learning methods focus on learning directed acyclic graph (DAG) models without
latent variables. A number of score-based methods have recently been proposed
for the ADMG learning, yet they are heuristic in nature and do not guarantee an
optimal solution. We propose a novel exact score-based method that solves an
integer programming (IP) formulation and returns a score-maximizing ancestral
ADMG for a set of continuous variables that follow a multivariate Gaussian
distribution. We generalize the state-of-the-art IP model for DAG learning
problems and derive new classes of valid inequalities to formulate an IP model
for ADMG learning. Empirically, our model can be solved efficiently for
medium-sized problems and achieves better accuracy than state-of-the-art
score-based methods as well as benchmark constraint-based methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Randomized Stochastic Variance-Reduced Methods for Multi-Task Stochastic Bilevel Optimization. (arXiv:2105.02266v2 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Guo_Z/0/1/0/all/0/1">Zhishuai Guo</a>, <a href="http://arxiv.org/find/math/1/au:+Hu_Q/0/1/0/all/0/1">Quanqi Hu</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_L/0/1/0/all/0/1">Lijun Zhang</a>, <a href="http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1">Tianbao Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02266">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we consider non-convex stochastic bilevel optimization (SBO)
problems that have many applications in machine learning. Although numerous
studies have proposed stochastic algorithms for solving these problems, they
are limited in two perspectives: (i) their sample complexities are high, which
do not match the state-of-the-art result for non-convex stochastic
optimization; (ii) their algorithms are tailored to problems with only one
lower-level problem. When there are many lower-level problems, it could be
prohibitive to process all these lower-level problems at each iteration. To
address these limitations, this paper proposes fast randomized stochastic
algorithms for non-convex SBO problems. First, we present a stochastic method
for non-convex SBO with only one lower problem and establish its sample
complexity of $O(1/\epsilon^3)$ for finding an $\epsilon$-stationary point
under Lipschitz continuous conditions of stochastic oracles, matching the lower
bound for stochastic smooth non-convex optimization. Second, we present a
randomized stochastic method for non-convex SBO with $m&gt;1$ lower level problems
(multi-task SBO) by processing a constant number of lower problems at each
iteration, and establish its sample complexity no worse than $O(m/\epsilon^3)$,
which could be a better complexity than that of simply processing all $m$ lower
problems at each iteration. Lastly, we establish even faster convergence
results for gradient-dominant functions. To the best of our knowledge, this is
the first work considering multi-task SBO and developing state-of-the-art
sample complexity results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1">An Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Junyang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1">Rui Men</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1">Chang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Le Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xianyan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Ang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiamang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Di Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Wei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Lin Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongxia Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15082">
                                    <div class="article-summary-box-inner">
                                        <span>Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Quantifying Uncertainty in Deep Spatiotemporal Forecasting. (arXiv:2105.11982v2 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Dongxia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Liyao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1">Xinyue Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chinazzi_M/0/1/0/all/0/1">Matteo Chinazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vespignani_A/0/1/0/all/0/1">Alessandro Vespignani</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi-An Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1">Rose Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11982">
                                    <div class="article-summary-box-inner">
                                        <span>Deep learning is gaining increasing popularity for spatiotemporal
forecasting. However, prior works have mostly focused on point estimates
without quantifying the uncertainty of the predictions. In high stakes domains,
being able to generate probabilistic forecasts with confidence intervals is
critical to risk assessment and decision making. Hence, a systematic study of
uncertainty quantification (UQ) methods for spatiotemporal forecasting is
missing in the community. In this paper, we describe two types of
spatiotemporal forecasting problems: regular grid-based and graph-based. Then
we analyze UQ methods from both the Bayesian and the frequentist point of view,
casting in a unified framework via statistical decision theory. Through
extensive experiments on real-world road network traffic, epidemics, and air
quality forecasting tasks, we reveal the statistical and computational
trade-offs for different UQ methods: Bayesian methods are typically more robust
in mean prediction, while confidence levels obtained from frequentist methods
provide more extensive coverage over data variations. Computationally, quantile
regression type methods are cheaper for a single confidence interval but
require re-training for different intervals. Sampling based methods generate
samples that can form multiple confidence intervals, albeit at a higher
computational cost.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Globally-Robust Neural Networks. (arXiv:2102.08452v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Leino_K/0/1/0/all/0/1">Klas Leino</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1">Matt Fredrikson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08452">
                                    <div class="article-summary-box-inner">
                                        <span>The threat of adversarial examples has motivated work on training certifiably
robust neural networks to facilitate efficient verification of local robustness
at inference time. We formalize a notion of global robustness, which captures
the operational properties of on-line local robustness certification while
yielding a natural learning objective for robust training. We show that
widely-used architectures can be easily adapted to this objective by
incorporating efficient global Lipschitz bounds into the network, yielding
certifiably-robust models by construction that achieve state-of-the-art
verifiable accuracy. Notably, this approach requires significantly less time
and memory than recent certifiable training methods, and leads to negligible
costs when certifying points on-line; for example, our evaluation shows that it
is possible to train a large robust Tiny-Imagenet model in a matter of hours.
Our models effectively leverage inexpensive global Lipschitz bounds for
real-time certification, despite prior suggestions that tighter local bounds
are needed for good performance; we posit this is possible because our models
are specifically trained to achieve tighter global bounds. Namely, we prove
that the maximum achievable verifiable accuracy for a given dataset is not
improved by using a local bound.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Foundations and modelling of dynamic networks using Dynamic Graph Neural Networks: A survey. (arXiv:2005.07496v2 [cs.SI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Skarding_J/0/1/0/all/0/1">Joakim Skarding</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabrys_B/0/1/0/all/0/1">Bogdan Gabrys</a>, <a href="http://arxiv.org/find/cs/1/au:+Musial_K/0/1/0/all/0/1">Katarzyna Musial</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.07496">
                                    <div class="article-summary-box-inner">
                                        <span>Dynamic networks are used in a wide range of fields, including social network
analysis, recommender systems, and epidemiology. Representing complex networks
as structures changing over time allow network models to leverage not only
structural but also temporal patterns. However, as dynamic network literature
stems from diverse fields and makes use of inconsistent terminology, it is
challenging to navigate. Meanwhile, graph neural networks (GNNs) have gained a
lot of attention in recent years for their ability to perform well on a range
of network science tasks, such as link prediction and node classification.
Despite the popularity of graph neural networks and the proven benefits of
dynamic network models, there has been little focus on graph neural networks
for dynamic networks. To address the challenges resulting from the fact that
this research crosses diverse fields as well as to survey dynamic graph neural
networks, this work is split into two main parts. First, to address the
ambiguity of the dynamic network terminology we establish a foundation of
dynamic networks with consistent, detailed terminology and notation. Second, we
present a comprehensive survey of dynamic graph neural network models using the
proposed terminology</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An error analysis of generative adversarial networks for learning distributions. (arXiv:2105.13010v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jian Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1">Yuling Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yunfei Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13010">
                                    <div class="article-summary-box-inner">
                                        <span>This paper studies how well generative adversarial networks (GANs) learn
probability distributions from finite samples. Our main results establish the
convergence rates of GANs under a collection of integral probability metrics
defined through H\&quot;older classes, including the Wasserstein distance as a
special case. We also show that GANs are able to adaptively learn data
distributions with low-dimensional structures or have H\&quot;older densities, when
the network architectures are chosen properly. In particular, for distributions
concentrated around a low-dimensional set, we show that the learning rates of
GANs do not depend on the high ambient dimension, but on the lower intrinsic
dimension. Our analysis is based on a new oracle inequality decomposing the
estimation error into the generator and discriminator approximation error and
the statistical error, which may be of independent interest.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GRAD-MATCH: Gradient Matching based Data Subset Selection for Efficient Deep Model Training. (arXiv:2103.00123v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1">Krishnateja Killamsetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1">Durga Sivasubramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1">Ganesh Ramakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1">Abir De</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1">Rishabh Iyer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00123">
                                    <div class="article-summary-box-inner">
                                        <span>The great success of modern machine learning models on large datasets is
contingent on extensive computational resources with high financial and
environmental costs. One way to address this is by extracting subsets that
generalize on par with the full data. In this work, we propose a general
framework, GRAD-MATCH, which finds subsets that closely match the gradient of
the training or validation set. We find such subsets effectively using an
orthogonal matching pursuit algorithm. We show rigorous theoretical and
convergence guarantees of the proposed algorithm and, through our extensive
experiments on real-world datasets, show the effectiveness of our proposed
framework. We show that GRAD-MATCH significantly and consistently outperforms
several recent data-selection algorithms and achieves the best
accuracy-efficiency trade-off. GRAD-MATCH is available as a part of the CORDS
toolkit: \url{https://github.com/decile-team/cords}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Graph Inference Representation: Learning Graph Positional Embeddings with Anchor Path Encoding. (arXiv:2105.03821v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yuheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jinpeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">ChuXiong Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jie Hu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03821">
                                    <div class="article-summary-box-inner">
                                        <span>Learning node representations that incorporate information from graph
structure benefits wide range of tasks on graph. The majority of existing graph
neural networks (GNNs) have limited power in capturing position information for
a given node. The idea of positioning nodes with selected anchors has been
exploited, yet mainly relying on explicit labeling of distance information.
Here we propose Graph Inference Representation (GIR), an anchor based GNN model
encoding path information related to pre-selected anchors for each node.
Abilities to get position-aware embeddings are theoretically and experimentally
investigated on GIR and its core variants. Further, the complementarity between
GIRs and typical GNNs is demonstrated. We show that GIRs get outperformed
results in position-aware scenarios, and performances on typical GNNs could be
improved by fusing GIR embeddings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Skew Orthogonal Convolutions. (arXiv:2105.11417v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1">Sahil Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11417">
                                    <div class="article-summary-box-inner">
                                        <span>Training convolutional neural networks with a Lipschitz constraint under the
$l_{2}$ norm is useful for provable adversarial robustness, interpretable
gradients, stable training, etc. While 1-Lipschitz networks can be designed by
imposing a 1-Lipschitz constraint on each layer, training such networks
requires each layer to be gradient norm preserving (GNP) to prevent gradients
from vanishing. However, existing GNP convolutions suffer from slow training,
lead to significant reduction in accuracy and provide no guarantees on their
approximations. In this work, we propose a GNP convolution layer called Skew
Orthogonal Convolution (SOC) that uses the following mathematical property:
when a matrix is {\it Skew-Symmetric}, its exponential function is an {\it
orthogonal} matrix. To use this property, we first construct a convolution
filter whose Jacobian is Skew-Symmetric. Then, we use the Taylor series
expansion of the Jacobian exponential to construct the SOC layer that is
orthogonal. To efficiently implement SOC, we keep a finite number of terms from
the Taylor series and provide a provable guarantee on the approximation error.
Our experiments on CIFAR-10 and CIFAR-100 show that SOC allows us to train
provably Lipschitz, large convolutional neural networks significantly faster
than prior works while achieving significant improvements for both standard and
certified robust accuracies.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fantastic Four: Differentiable Bounds on Singular Values of Convolution Layers. (arXiv:1911.10258v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1">Sahil Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.10258">
                                    <div class="article-summary-box-inner">
                                        <span>In deep neural networks, the spectral norm of the Jacobian of a layer bounds
the factor by which the norm of a signal changes during forward/backward
propagation. Spectral norm regularizations have been shown to improve
generalization, robustness and optimization of deep learning methods. Existing
methods to compute the spectral norm of convolution layers either rely on
heuristics that are efficient in computation but lack guarantees or are
theoretically-sound but computationally expensive. In this work, we obtain the
best of both worlds by deriving {\it four} provable upper bounds on the
spectral norm of a standard 2D multi-channel convolution layer. These bounds
are differentiable and can be computed efficiently during training with
negligible overhead. One of these bounds is in fact the popular heuristic
method of Miyato et al. (multiplied by a constant factor depending on filter
sizes). Each of these four bounds can achieve the tightest gap depending on
convolution filters. Thus, we propose to use the minimum of these four bounds
as a tight, differentiable and efficient upper bound on the spectral norm of
convolution layers. We show that our spectral bound is an effective regularizer
and can be used to bound either the lipschitz constant or curvature values
(eigenvalues of the Hessian) of neural networks. Through experiments on MNIST
and CIFAR-10, we demonstrate the effectiveness of our spectral bound in
improving generalization and provable robustness of deep networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Understanding and Mitigating Accuracy Disparity in Regression. (arXiv:2102.12013v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1">Jianfeng Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuan Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1">Geoffrey J. Gordon</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Han Zhao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12013">
                                    <div class="article-summary-box-inner">
                                        <span>With the widespread deployment of large-scale prediction systems in
high-stakes domains, e.g., face recognition, criminal justice, etc., disparity
in prediction accuracy between different demographic subgroups has called for
fundamental understanding on the source of such disparity and algorithmic
intervention to mitigate it. In this paper, we study the accuracy disparity
problem in regression. To begin with, we first propose an error decomposition
theorem, which decomposes the accuracy disparity into the distance between
marginal label distributions and the distance between conditional
representations, to help explain why such accuracy disparity appears in
practice. Motivated by this error decomposition and the general idea of
distribution alignment with statistical distances, we then propose an algorithm
to reduce this disparity, and analyze its game-theoretic optima of the proposed
objective functions. To corroborate our theoretical findings, we also conduct
experiments on five benchmark datasets. The experimental results suggest that
our proposed algorithms can effectively mitigate accuracy disparity while
maintaining the predictive power of the regression models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sinkhorn Label Allocation: Semi-Supervised Classification via Annealed Self-Training. (arXiv:2102.08622v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tai_K/0/1/0/all/0/1">Kai Sheng Tai</a>, <a href="http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1">Peter Bailis</a>, <a href="http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1">Gregory Valiant</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08622">
                                    <div class="article-summary-box-inner">
                                        <span>Self-training is a standard approach to semi-supervised learning where the
learner&#x27;s own predictions on unlabeled data are used as supervision during
training. In this paper, we reinterpret this label assignment process as an
optimal transportation problem between examples and classes, wherein the cost
of assigning an example to a class is mediated by the current predictions of
the classifier. This formulation facilitates a practical annealing strategy for
label assignment and allows for the inclusion of prior knowledge on class
proportions via flexible upper bound constraints. The solutions to these
assignment problems can be efficiently approximated using Sinkhorn iteration,
thus enabling their use in the inner loop of standard stochastic optimization
algorithms. We demonstrate the effectiveness of our algorithm on the CIFAR-10,
CIFAR-100, and SVHN datasets in comparison with FixMatch, a state-of-the-art
self-training algorithm. Our code is available at
https://github.com/stanford-futuredata/sinkhorn-label-allocation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Urysohn Forest for Aleatoric Uncertainty Quantification. (arXiv:2104.01714v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Polar_A/0/1/0/all/0/1">Andrew Polar</a>, <a href="http://arxiv.org/find/cs/1/au:+Poluektov_M/0/1/0/all/0/1">Michael Poluektov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01714">
                                    <div class="article-summary-box-inner">
                                        <span>The terms tree and forest are normally associated with an ensemble of
classifiers. In this article Urysohn tree is a regression model representing
multiple discrete Urysohn operators connected as a tree, where the inputs of
one operator are outputs of the others. This structure, referred as Urysohn
tree, is not completely new. One example of such tree is known for more than
half a century. It is Kolmogorov-Arnold representation. The authors of this
paper in their recently published research offered the new computational
technique for generating of Kolmogorov-Arnold representation as a deep machine
learning process. This article is two steps further into this research. First
is a Urysohn tree with multiple hidden layers which is generalization of
Kolmogorov-Arnold model and second is a boosting algorithm for building of the
forest of such trees for modeling of aleatoric uncertainty of the data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Acceleration via Fractal Learning Rate Schedules. (arXiv:2103.01338v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1">Naman Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1">Surbhi Goel</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cyril Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01338">
                                    <div class="article-summary-box-inner">
                                        <span>In practical applications of iterative first-order optimization, the learning
rate schedule remains notoriously difficult to understand and expensive to
tune. We demonstrate the presence of these subtleties even in the innocuous
case when the objective is a convex quadratic. We reinterpret an iterative
algorithm from the numerical analysis literature as what we call the Chebyshev
learning rate schedule for accelerating vanilla gradient descent, and show that
the problem of mitigating instability leads to a fractal ordering of step
sizes. We provide some experiments to challenge conventional beliefs about
stable learning rates in deep learning: the fractal schedule enables training
to converge with locally unstable updates which make negative progress on the
objective.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. (arXiv:2106.02549v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Frank_T/0/1/0/all/0/1">Thorben Frank</a>, <a href="http://arxiv.org/find/cs/1/au:+Chmiela_S/0/1/0/all/0/1">Stefan Chmiela</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02549">
                                    <div class="article-summary-box-inner">
                                        <span>Attention mechanisms are developing into a viable alternative to
convolutional layers as elementary building block of NNs. Their main advantage
is that they are not restricted to capture local dependencies in the input, but
can draw arbitrary connections. This unprecedented capability coincides with
the long-standing problem of modeling global atomic interactions in molecular
force fields and other many-body problems. In its original formulation,
however, attention is not applicable to the continuous domains in which the
atoms live. For this purpose we propose a variant to describe geometric
relations for arbitrary atomic configurations in Euclidean space that also
respects all relevant physical symmetries. We furthermore demonstrate, how the
successive application of our learned attention matrices effectively translates
the molecular geometry into a set of individual atomic contributions
on-the-fly.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Synthesized Difference in Differences. (arXiv:2105.00455v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Strobl_E/0/1/0/all/0/1">Eric V. Strobl</a>, <a href="http://arxiv.org/find/stat/1/au:+Lasko_T/0/1/0/all/0/1">Thomas A. Lasko</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00455">
                                    <div class="article-summary-box-inner">
                                        <span>We consider estimating the conditional average treatment effect for everyone
by eliminating confounding and selection bias. Unfortunately, randomized
clinical trials (RCTs) eliminate confounding but impose strict exclusion
criteria that prevent sampling of the entire clinical population. Observational
datasets are more inclusive but suffer from confounding. We therefore analyze
RCT and observational data simultaneously in order to extract the strengths of
each. Our solution builds upon Difference in Differences (DD), an algorithm
that eliminates confounding from observational data by comparing outcomes
before and after treatment administration. DD requires a parallel slopes
assumption that may not apply in practice when confounding shifts across time.
We instead propose Synthesized Difference in Differences (SDD) that infers the
correct (possibly non-parallel) slopes by linearly adjusting a conditional
version of DD using additional RCT data. The algorithm achieves state of the
art performance across multiple synthetic and real datasets even when the RCT
excludes the majority of patients.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Novel Interaction-based Methodology Towards Explainable AI with Better Understanding of Pneumonia Chest X-ray Images. (arXiv:2104.12672v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1">Shaw-Hwa Lo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1">Yiqiao Yin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12672">
                                    <div class="article-summary-box-inner">
                                        <span>In the field of eXplainable AI (XAI), robust &#x60;&#x60;blackbox&#x27;&#x27; algorithms such as
Convolutional Neural Networks (CNNs) are known for making high prediction
performance. However, the ability to explain and interpret these algorithms
still require innovation in the understanding of influential and, more
importantly, explainable features that directly or indirectly impact the
performance of predictivity. A number of methods existing in literature focus
on visualization techniques but the concepts of explainability and
interpretability still require rigorous definition. In view of the above needs,
this paper proposes an interaction-based methodology -- Influence Score
(I-score) -- to screen out the noisy and non-informative variables in the
images hence it nourishes an environment with explainable and interpretable
features that are directly associated to feature predictivity. We apply the
proposed method on a real world application in Pneumonia Chest X-ray Image data
set and produced state-of-the-art results. We demonstrate how to apply the
proposed approach for more general big data problems by improving the
explainability and interpretability without sacrificing the prediction
performance. The contribution of this paper opens a novel angle that moves the
community closer to the future pipelines of XAI problems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks. (arXiv:2103.03212v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bodnar_C/0/1/0/all/0/1">Cristian Bodnar</a>, <a href="http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1">Fabrizio Frasca</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Guang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Otter_N/0/1/0/all/0/1">Nina Otter</a>, <a href="http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1">Guido Mont&#xfa;far</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1">Michael Bronstein</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03212">
                                    <div class="article-summary-box-inner">
                                        <span>The pairwise interaction paradigm of graph machine learning has predominantly
governed the modelling of relational systems. However, graphs alone cannot
capture the multi-level interactions present in many complex systems and the
expressive power of such schemes was proven to be limited. To overcome these
limitations, we propose Message Passing Simplicial Networks (MPSNs), a class of
models that perform message passing on simplicial complexes (SCs). To
theoretically analyse the expressivity of our model we introduce a Simplicial
Weisfeiler-Lehman (SWL) colouring procedure for distinguishing non-isomorphic
SCs. We relate the power of SWL to the problem of distinguishing non-isomorphic
graphs and show that SWL and MPSNs are strictly more powerful than the WL test
and not less powerful than the 3-WL test. We deepen the analysis by comparing
our model with traditional graph neural networks (GNNs) with ReLU activations
in terms of the number of linear regions of the functions they can represent.
We empirically support our theoretical claims by showing that MPSNs can
distinguish challenging strongly regular graphs for which GNNs fail and, when
equipped with orientation equivariant layers, they can improve classification
accuracy in oriented SCs compared to a GNN baseline.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">BoolNet: Minimizing The Energy Consumption of Binary Neural Networks. (arXiv:2106.06991v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1">Nianhui Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Bethge_J/0/1/0/all/0/1">Joseph Bethge</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haojin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1">Kai Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1">Xuefei Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1">Christoph Meinel</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06991">
                                    <div class="article-summary-box-inner">
                                        <span>Recent works on Binary Neural Networks (BNNs) have made promising progress in
narrowing the accuracy gap of BNNs to their 32-bit counterparts. However, the
accuracy gains are often based on specialized model designs using additional
32-bit components. Furthermore, almost all previous BNNs use 32-bit for feature
maps and the shortcuts enclosing the corresponding binary convolution blocks,
which helps to effectively maintain the accuracy, but is not friendly to
hardware accelerators with limited memory, energy, and computing resources.
Thus, we raise the following question: How can accuracy and energy consumption
be balanced in a BNN network design? We extensively study this fundamental
problem in this work and propose a novel BNN architecture without most commonly
used 32-bit components: \textit{BoolNet}. Experimental results on ImageNet
demonstrate that BoolNet can achieve 4.6x energy reduction coupled with 1.2\%
higher accuracy than the commonly used BNN architecture Bi-RealNet. Code and
trained models are available at: https://github.com/hpi-xnor/BoolNet.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">COVID-19 Cough Classification using Machine Learning and Global Smartphone Recordings. (arXiv:2012.01926v2 [cs.SD] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pahar_M/0/1/0/all/0/1">Madhurananda Pahar</a>, <a href="http://arxiv.org/find/cs/1/au:+Klopper_M/0/1/0/all/0/1">Marisa Klopper</a>, <a href="http://arxiv.org/find/cs/1/au:+Warren_R/0/1/0/all/0/1">Robin Warren</a>, <a href="http://arxiv.org/find/cs/1/au:+Niesler_T/0/1/0/all/0/1">Thomas Niesler</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01926">
                                    <div class="article-summary-box-inner">
                                        <span>We present a machine learning based COVID-19 cough classifier which can
discriminate COVID-19 positive coughs from both COVID-19 negative and healthy
coughs recorded on a smartphone. This type of screening is non-contact, easy to
apply, and can reduce the workload in testing centres as well as limit
transmission by recommending early self-isolation to those who have a cough
suggestive of COVID-19. The datasets used in this study include subjects from
all six continents and contain both forced and natural coughs, indicating that
the approach is widely applicable. The publicly available Coswara dataset
contains 92 COVID-19 positive and 1079 healthy subjects, while the second
smaller dataset was collected mostly in South Africa and contains 18 COVID-19
positive and 26 COVID-19 negative subjects who have undergone a SARS-CoV
laboratory test. Both datasets indicate that COVID-19 positive coughs are
15\%-20\% shorter than non-COVID coughs. Dataset skew was addressed by applying
the synthetic minority oversampling technique (SMOTE). A leave-$p$-out
cross-validation scheme was used to train and evaluate seven machine learning
classifiers: LR, KNN, SVM, MLP, CNN, LSTM and Resnet50. Our results show that
although all classifiers were able to identify COVID-19 coughs, the best
performance was exhibited by the Resnet50 classifier, which was best able to
discriminate between the COVID-19 positive and the healthy coughs with an area
under the ROC curve (AUC) of 0.98. An LSTM classifier was best able to
discriminate between the COVID-19 positive and COVID-19 negative coughs, with
an AUC of 0.94 after selecting the best 13 features from a sequential forward
selection (SFS). Since this type of cough audio classification is
cost-effective and easy to deploy, it is potentially a useful and viable means
of non-contact COVID-19 screening.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Shared Cross-Modal Trajectory Prediction for Autonomous Driving. (arXiv:2004.00202v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1">Chiho Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Joon Hee Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1">Srikanth Malla</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiachen Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.00202">
                                    <div class="article-summary-box-inner">
                                        <span>Predicting future trajectories of traffic agents in highly interactive
environments is an essential and challenging problem for the safe operation of
autonomous driving systems. On the basis of the fact that self-driving vehicles
are equipped with various types of sensors (e.g., LiDAR scanner, RGB camera,
radar, etc.), we propose a Cross-Modal Embedding framework that aims to benefit
from the use of multiple input modalities. At training time, our model learns
to embed a set of complementary features in a shared latent space by jointly
optimizing the objective functions across different types of input data. At
test time, a single input modality (e.g., LiDAR data) is required to generate
predictions from the input perspective (i.e., in the LiDAR space), while taking
advantages from the model trained with multiple sensor modalities. An extensive
evaluation is conducted to show the efficacy of the proposed framework using
two benchmark driving datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Federated Continual Learning with Weighted Inter-client Transfer. (arXiv:2003.03196v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jaehong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1">Wonyong Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1">Giwoong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1">Eunho Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.03196">
                                    <div class="article-summary-box-inner">
                                        <span>There has been a surge of interest in continual learning and federated
learning, both of which are important in deep neural networks in real-world
scenarios. Yet little research has been done regarding the scenario where each
client learns on a sequence of tasks from a private local data stream. This
problem of federated continual learning poses new challenges to continual
learning, such as utilizing knowledge from other clients, while preventing
interference from irrelevant knowledge. To resolve these issues, we propose a
novel federated continual learning framework, Federated Weighted Inter-client
Transfer (FedWeIT), which decomposes the network weights into global federated
parameters and sparse task-specific parameters, and each client receives
selective knowledge from other clients by taking a weighted combination of
their task-specific parameters. FedWeIT minimizes interference between
incompatible tasks, and also allows positive knowledge transfer across clients
during learning. We validate our FedWeIT against existing federated learning
and continual learning methods under varying degrees of task similarity across
clients, and our model significantly outperforms them with a large reduction in
the communication cost. Code is available at https://github.com/wyjeong/FedWeIT</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Using Convolutional Neural Networks for the Helicity Classification of Magnetic Fields. (arXiv:2106.06718v1 [astro-ph.HE])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/astro-ph/1/au:+Vago_N/0/1/0/all/0/1">Nicol&#xf2; Oreste Pinciroli Vago</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Hameed_I/0/1/0/all/0/1">Ibrahim A. Hameed</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Kachelriess_M/0/1/0/all/0/1">Michael Kachelriess</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06718">
                                    <div class="article-summary-box-inner">
                                        <span>The presence of non-zero helicity in intergalactic magnetic fields is a
smoking gun for their primordial origin since they have to be generated by
processes that break CP invariance. As an experimental signature for the
presence of helical magnetic fields, an estimator $Q$ based on the triple
scalar product of the wave-vectors of photons generated in electromagnetic
cascades from, e.g., TeV blazars, has been suggested previously. We propose to
apply deep learning to helicity classification employing Convolutional Neural
Networks and show that this method outperforms the $Q$ estimator.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Active Testing: Sample-Efficient Model Evaluation. (arXiv:2103.05331v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Kossen_J/0/1/0/all/0/1">Jannik Kossen</a>, <a href="http://arxiv.org/find/stat/1/au:+Farquhar_S/0/1/0/all/0/1">Sebastian Farquhar</a>, <a href="http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1">Yarin Gal</a>, <a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1">Tom Rainforth</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05331">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a new framework for sample-efficient model evaluation that we
call active testing. While approaches like active learning reduce the number of
labels needed for model training, existing literature largely ignores the cost
of labeling test data, typically unrealistically assuming large test sets for
model evaluation. This creates a disconnect to real applications, where test
labels are important and just as expensive, e.g. for optimizing
hyperparameters. Active testing addresses this by carefully selecting the test
points to label, ensuring model evaluation is sample-efficient. To this end, we
derive theoretically-grounded and intuitive acquisition strategies that are
specifically tailored to the goals of active testing, noting these are distinct
to those of active learning. As actively selecting labels introduces a bias; we
further show how to remove this bias while reducing the variance of the
estimator at the same time. Active testing is easy to implement and can be
applied to any supervised machine learning method. We demonstrate its
effectiveness on models including WideResNets and Gaussian processes on
datasets including Fashion-MNIST and CIFAR-100.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Recoverability Landscape of Tree Structured Markov Random Fields under Symmetric Noise. (arXiv:2102.08554v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Katiyar_A/0/1/0/all/0/1">Ashish Katiyar</a>, <a href="http://arxiv.org/find/stat/1/au:+Basu_S/0/1/0/all/0/1">Soumya Basu</a>, <a href="http://arxiv.org/find/stat/1/au:+Shah_V/0/1/0/all/0/1">Vatsal Shah</a>, <a href="http://arxiv.org/find/stat/1/au:+Caramanis_C/0/1/0/all/0/1">Constantine Caramanis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08554">
                                    <div class="article-summary-box-inner">
                                        <span>We study the problem of learning tree-structured Markov random fields (MRF)
on discrete random variables with common support when the observations are
corrupted by a $k$-ary symmetric noise channel with unknown probability of
error. For Ising models (support size &#x3D; 2), past work has shown that graph
structure can only be recovered up to the leaf clusters (a leaf node, its
parent, and its siblings form a leaf cluster) and exact recovery is impossible.
No prior work has addressed the setting of support size of 3 or more, and
indeed this setting is far richer. As we show, when the support size is 3 or
more, the structure of the leaf clusters may be partially or fully
identifiable. We provide a precise characterization of this phenomenon and show
that the extent of recoverability is dictated by the joint PMF of the random
variables. In particular, we provide necessary and sufficient conditions for
exact recoverability. Furthermore, we present a polynomial time, sample
efficient algorithm that recovers the exact tree when this is possible, or up
to the unidentifiability as promised by our characterization, when full
recoverability is impossible. Finally, we demonstrate the efficacy of our
algorithm experimentally.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage. (arXiv:2106.03207v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1">Jonathan D. Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1">Masatoshi Uehara</a>, <a href="http://arxiv.org/find/cs/1/au:+Sreenivas_D/0/1/0/all/0/1">Dhruv Sreenivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kidambi_R/0/1/0/all/0/1">Rahul Kidambi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wen Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03207">
                                    <div class="article-summary-box-inner">
                                        <span>This paper studies offline Imitation Learning (IL) where an agent learns to
imitate an expert demonstrator without additional online environment
interactions. Instead, the learner is presented with a static offline dataset
of state-action-next state transition triples from a potentially less
proficient behavior policy. We introduce Model-based IL from Offline data
(MILO): an algorithmic framework that utilizes the static dataset to solve the
offline IL problem efficiently both in theory and in practice. In theory, even
if the behavior policy is highly sub-optimal compared to the expert, we show
that as long as the data from the behavior policy provides sufficient coverage
on the expert state-action traces (and with no necessity for a global coverage
over the entire state-action space), MILO can provably combat the covariate
shift issue in IL. Complementing our theory results, we also demonstrate that a
practical implementation of our approach mitigates covariate shift on benchmark
MuJoCo continuous control tasks. We demonstrate that with behavior policies
whose performances are less than half of that of the expert, MILO still
successfully imitates with an extremely low number of expert state-action pairs
while traditional offline IL method such as behavior cloning (BC) fails
completely. Source code is provided at https://github.com/jdchang1/milo.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Correlated Weights in Infinite Limits of Deep Convolutional Neural Networks. (arXiv:2101.04097v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Garriga_Alonso_A/0/1/0/all/0/1">Adri&#xe0; Garriga-Alonso</a>, <a href="http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1">Mark van der Wilk</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04097">
                                    <div class="article-summary-box-inner">
                                        <span>Infinite width limits of deep neural networks often have tractable forms.
They have been used to analyse the behaviour of finite networks, as well as
being useful methods in their own right. When investigating infinitely wide
convolutional neural networks (CNNs), it was observed that the correlations
arising from spatial weight sharing disappear in the infinite limit. This is
undesirable, as spatial correlation is the main motivation behind CNNs. We show
that the loss of this property is not a consequence of the infinite limit, but
rather of choosing an independent weight prior. Correlating the weights
maintains the correlations in the activations. Varying the amount of
correlation interpolates between independent-weight limits and mean-pooling.
Empirical evaluation of the infinitely wide network shows that optimal
performance is achieved between the extremes, indicating that correlations can
be useful.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Differentiable Particle Filtering via Entropy-Regularized Optimal Transport. (arXiv:2102.07850v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Corenflos_A/0/1/0/all/0/1">Adrien Corenflos</a>, <a href="http://arxiv.org/find/stat/1/au:+Thornton_J/0/1/0/all/0/1">James Thornton</a>, <a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1">Arnaud Doucet</a>, <a href="http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1">George Deligiannidis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07850">
                                    <div class="article-summary-box-inner">
                                        <span>Particle Filtering (PF) methods are an established class of procedures for
performing inference in non-linear state-space models. Resampling is a key
ingredient of PF, necessary to obtain low variance likelihood and states
estimates. However, traditional resampling methods result in PF-based loss
functions being non-differentiable with respect to model and PF parameters. In
a variational inference context, resampling also yields high variance gradient
estimates of the PF-based evidence lower bound. By leveraging optimal transport
ideas, we introduce a principled differentiable particle filter and provide
convergence results. We demonstrate this novel method on a variety of
applications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment. (arXiv:2104.05632v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ball_P/0/1/0/all/0/1">Philip J. Ball</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Cong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1">Jack Parker-Holder</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1">Stephen Roberts</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05632">
                                    <div class="article-summary-box-inner">
                                        <span>Reinforcement learning from large-scale offline datasets provides us with the
ability to learn policies without potentially unsafe or impractical
exploration. Significant progress has been made in the past few years in
dealing with the challenge of correcting for differing behavior between the
data collection and learned policies. However, little attention has been paid
to potentially changing dynamics when transferring a policy to the online
setting, where performance can be up to 90% reduced for existing methods. In
this paper we address this problem with Augmented World Models (AugWM). We
augment a learned dynamics model with simple transformations that seek to
capture potential changes in physical properties of the robot, leading to more
robust policies. We not only train our policy in this new setting, but also
provide it with the sampled augmentation as a context, allowing it to adapt to
changes in the environment. At test time we learn the context in a
self-supervised fashion by approximating the augmentation which corresponds to
the new environment. We rigorously evaluate our approach on over 100 different
changed dynamics settings, and show that this simple approach can significantly
improve the zero-shot generalization of a recent state-of-the-art baseline,
often achieving successful policies where the baseline fails.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Thinking Like Transformers. (arXiv:2106.06981v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1">Gail Weiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1">Yoav Goldberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1">Eran Yahav</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06981">
                                    <div class="article-summary-box-inner">
                                        <span>What is the computational model behind a Transformer? Where recurrent neural
networks have direct parallels in finite state machines, allowing clear
discussion and thought around architecture variants or trained models,
Transformers have no such familiar parallel. In this paper we aim to change
that, proposing a computational model for the transformer-encoder in the form
of a programming language. We map the basic components of a transformer-encoder
-- attention and feed-forward computation -- into simple primitives, around
which we form a programming language: the Restricted Access Sequence Processing
Language (RASP). We show how RASP can be used to program solutions to tasks
that could conceivably be learned by a Transformer, and how a Transformer can
be trained to mimic a RASP solution. In particular, we provide RASP programs
for histograms, sorting, and Dyck-languages. We further use our model to relate
their difficulty in terms of the number of required layers and attention heads:
analyzing a RASP program implies a maximum number of heads and layers necessary
to encode a task in a transformer. Finally, we see how insights gained from our
abstraction might be used to explain phenomena seen in recent works.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">BoMb-OT: On Batch of Mini-batches Optimal Transport. (arXiv:2102.05912v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Nguyen_K/0/1/0/all/0/1">Khai Nguyen</a>, <a href="http://arxiv.org/find/stat/1/au:+Nguyen_Q/0/1/0/all/0/1">Quoc Nguyen</a>, <a href="http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1">Nhat Ho</a>, <a href="http://arxiv.org/find/stat/1/au:+Pham_T/0/1/0/all/0/1">Tung Pham</a>, <a href="http://arxiv.org/find/stat/1/au:+Bui_H/0/1/0/all/0/1">Hung Bui</a>, <a href="http://arxiv.org/find/stat/1/au:+Phung_D/0/1/0/all/0/1">Dinh Phung</a>, <a href="http://arxiv.org/find/stat/1/au:+Le_T/0/1/0/all/0/1">Trung Le</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05912">
                                    <div class="article-summary-box-inner">
                                        <span>Mini-batch optimal transport (m-OT) has been successfully used in practical
applications that involve probability measures with intractable density, or
probability measures with a very high number of supports. The m-OT solves
several sparser optimal transport problems and then returns the average of
their costs and transportation plans. Despite its scalability advantage, the
m-OT does not consider the relationship between mini-batches which leads to
undesirable estimation. Moreover, the m-OT does not approximate a proper metric
between probability measures since the identity property is not satisfied. To
address these problems, we propose a novel mini-batching scheme for optimal
transport, named Batch of Mini-batches Optimal Transport (BoMb-OT), that finds
the optimal coupling between mini-batches and it can be seen as an
approximation to a well-defined distance on the space of probability measures.
Furthermore, we show that the m-OT is a limit of the entropic regularized
version of the BoMb-OT when the regularized parameter goes to infinity.
Finally, we carry out extensive experiments to show that the BoMb-OT can
estimate a better transportation plan between two original measures than the
m-OT. It leads to a favorable performance of the BoMb-OT in the matching and
color transfer tasks. Furthermore, we observe that the BoMb-OT also provides a
better objective loss than the m-OT for doing approximate Bayesian computation,
estimating parameters of interest in parametric generative models, and learning
non-parametric generative models with gradient flow.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information. (arXiv:2009.01454v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dai_E/0/1/0/all/0/1">Enyan Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Suhang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.01454">
                                    <div class="article-summary-box-inner">
                                        <span>Graph neural networks (GNNs) have achieved state-of-the-art performance in
modeling graphs. Despite its great success, as with many other models, GNNs
have the risk to inherit the bias from the training data. In addition, the bias
of GNN can be magnified by the graph structures and message-passing mechanism
of GNNs. The risk of discrimination limits the adoption of GNNs in sensitive
domains such as credit score estimation. Though extensive studies of fair
classification have been conducted on i.i.d data, methods to address the
problem of discrimination on non-i.i.d data are rather limited. Furthermore,
the practical scenario of sparse annotations in sensitive attributes is rarely
considered in existing works. Therefore, we study the novel and important
problem of learning fair GNNs with limited sensitive information. We propose a
novel framework called FairGNN, which is able to reduce the bias of GNNs and
maintain high node classification accuracy by leveraging graph structured data
and sensitive information. Theoretical analysis is conducted to show that
FairGNN can ensure fairness under mild conditions given limited nodes with
known sensitive attributes. Experiments on real-world datasets demonstrated the
effectiveness of the proposed framework in eliminating discrimination while
maintaining high node classification accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Quantifying the Conceptual Error in Dimensionality Reduction. (arXiv:2106.06815v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1">Tom Hanika</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirth_J/0/1/0/all/0/1">Johannes Hirth</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06815">
                                    <div class="article-summary-box-inner">
                                        <span>Dimension reduction of data sets is a standard problem in the realm of
machine learning and knowledge reasoning. They affect patterns in and
dependencies on data dimensions and ultimately influence any decision-making
processes. Therefore, a wide variety of reduction procedures are in use, each
pursuing different objectives. A so far not considered criterion is the
conceptual continuity of the reduction mapping, i.e., the preservation of the
conceptual structure with respect to the original data set. Based on the notion
scale-measure from formal concept analysis we present in this work a) the
theoretical foundations to detect and quantify conceptual errors in data
scalings; b) an experimental investigation of our approach on eleven data sets
that were respectively treated with a variant of non-negative matrix
factorization.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Minimalist Approach to Offline Reinforcement Learning. (arXiv:2106.06860v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fujimoto_S/0/1/0/all/0/1">Scott Fujimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1">Shixiang Shane Gu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06860">
                                    <div class="article-summary-box-inner">
                                        <span>Offline reinforcement learning (RL) defines the task of learning from a fixed
batch of data. Due to errors in value estimation from out-of-distribution
actions, most offline RL algorithms take the approach of constraining or
regularizing the policy with the actions contained in the dataset. Built on
pre-existing RL algorithms, modifications to make an RL algorithm work offline
comes at the cost of additional complexity. Offline RL algorithms introduce new
hyperparameters and often leverage secondary components such as generative
models, while adjusting the underlying RL algorithm. In this paper we aim to
make a deep RL algorithm work while making minimal changes. We find that we can
match the performance of state-of-the-art offline RL algorithms by simply
adding a behavior cloning term to the policy update of an online RL algorithm
and normalizing the data. The resulting algorithm is a simple to implement and
tune baseline, while more than halving the overall run time by removing the
additional computational overheads of previous methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Domain Adaptation for Time Series Forecasting via Attention Sharing. (arXiv:2102.06828v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaoyong Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1">Youngsuk Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Maddix_D/0/1/0/all/0/1">Danielle C. Maddix</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xifeng Yan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06828">
                                    <div class="article-summary-box-inner">
                                        <span>Recent years have witnessed deep neural networks gaining increasing
popularity in the field of time series forecasting. A primary reason of their
success is their ability to effectively capture complex temporal dynamics
across multiple related time series. However, the advantages of these deep
forecasters only start to emerge in the presence of a sufficient amount of
data. This poses a challenge for typical forecasting problems in practice,
where one either has a small number of time series, or limited observations per
time series, or both. To cope with the issue of data scarcity, we propose a
novel domain adaptation framework, Domain Adaptation Forecaster (DAF), that
leverages the statistical strengths from another relevant domain with abundant
data samples (source) to improve the performance on the domain of interest with
limited data (target). In particular, we propose an attention-based shared
module with a domain discriminator across domains as well as private modules
for individual domains. This allows us to jointly train the source and target
domains by generating domain-invariant latent features while retraining
domain-specific features. Extensive experiments on various domains demonstrate
that our proposed method outperforms state-of-the-art baselines on synthetic
and real-world datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An Empirical Comparison of Off-policy Prediction Learning Algorithms on the Collision Task. (arXiv:2106.00922v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ghiassian_S/0/1/0/all/0/1">Sina Ghiassian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1">Richard S. Sutton</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00922">
                                    <div class="article-summary-box-inner">
                                        <span>Off-policy prediction -- learning the value function for one policy from data
generated while following another policy -- is one of the most challenging
subproblems in reinforcement learning. This paper presents empirical results
with eleven prominent off-policy learning algorithms that use linear function
approximation: five Gradient-TD methods, two Emphatic-TD methods, Off-policy
TD($\lambda$), Vtrace, and versions of Tree Backup and ABQ modified to apply to
a prediction setting. Our experiments used the Collision task, a small
idealized off-policy problem analogous to that of an autonomous car trying to
predict whether it will collide with an obstacle. We assessed the performance
of the algorithms according to their learning rate, asymptotic error level, and
sensitivity to step-size and bootstrapping parameters. By these measures, the
eleven algorithms can be partially ordered on the Collision task. In the top
tier, the two Emphatic-TD algorithms learned the fastest, reached the lowest
errors, and were robust to parameter settings. In the middle tier, the five
Gradient-TD algorithms and Off-policy TD($\lambda$) were more sensitive to the
bootstrapping parameter. The bottom tier comprised Vtrace, Tree Backup, and
ABQ; these algorithms were no faster and had higher asymptotic error than the
others. Our results are definitive for this task, though of course experiments
with more tasks are needed before an overall assessment of the algorithms&#x27;
merits can be made.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SoundDet: Polyphonic Sound Event Detection and Localization from Raw Waveform. (arXiv:2106.06969v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yuhang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1">Niki Trigoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1">Andrew Markham</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06969">
                                    <div class="article-summary-box-inner">
                                        <span>We present a new framework SoundDet, which is an end-to-end trainable and
light-weight framework, for polyphonic moving sound event detection and
localization. Prior methods typically approach this problem by preprocessing
raw waveform into time-frequency representations, which is more amenable to
process with well-established image processing pipelines. Prior methods also
detect in segment-wise manner, leading to incomplete and partial detections.
SoundDet takes a novel approach and directly consumes the raw, multichannel
waveform and treats the spatio-temporal sound event as a complete
&#x60;&#x60;sound-object&quot; to be detected. Specifically, SoundDet consists of a backbone
neural network and two parallel heads for temporal detection and spatial
localization, respectively. Given the large sampling rate of raw waveform, the
backbone network first learns a set of phase-sensitive and frequency-selective
bank of filters to explicitly retain direction-of-arrival information, whilst
being highly computationally and parametrically efficient than standard 1D/2D
convolution. A dense sound event proposal map is then constructed to handle the
challenges of predicting events with large varying temporal duration.
Accompanying the dense proposal map are a temporal overlapness map and a motion
smoothness map that measure a proposal&#x27;s confidence to be an event from
temporal detection accuracy and movement consistency perspective. Involving the
two maps guarantees SoundDet to be trained in a spatio-temporally unified
manner. Experimental results on the public DCASE dataset show the advantage of
SoundDet on both segment-based and our newly proposed event-based evaluation
system.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hedging with Linear Regressions and Neural Networks. (arXiv:2004.08891v3 [q-fin.RM] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/q-fin/1/au:+Ruf_J/0/1/0/all/0/1">Johannes Ruf</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Wang_W/0/1/0/all/0/1">Weiguan Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.08891">
                                    <div class="article-summary-box-inner">
                                        <span>We study neural networks as nonparametric estimation tools for the hedging of
options. To this end, we design a network, named HedgeNet, that directly
outputs a hedging strategy. This network is trained to minimise the hedging
error instead of the pricing error. Applied to end-of-day and tick prices of
S&amp;P 500 and Euro Stoxx 50 options, the network is able to reduce the mean
squared hedging error of the Black-Scholes benchmark significantly. However, a
similar benefit arises by simple linear regressions that incorporate the
leverage effect.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Predicting Higher Education Throughput in South Africa Using a Tree-Based Ensemble Technique. (arXiv:2106.06805v1 [stat.AP])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Mbuvha_R/0/1/0/all/0/1">Rendani Mbuvha</a>, <a href="http://arxiv.org/find/stat/1/au:+Zondo_P/0/1/0/all/0/1">Patience Zondo</a>, <a href="http://arxiv.org/find/stat/1/au:+Mauda_A/0/1/0/all/0/1">Aluwani Mauda</a>, <a href="http://arxiv.org/find/stat/1/au:+Marwala_T/0/1/0/all/0/1">Tshilidzi Marwala</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06805">
                                    <div class="article-summary-box-inner">
                                        <span>We use gradient boosting machines and logistic regression to predict academic
throughput at a South African university. The results highlight the significant
influence of socio-economic factors and field of study as predictors of
throughput. We further find that socio-economic factors become less of a
predictor relative to the field of study as the time to completion increases.
We provide recommendations on interventions to counteract the identified
effects, which include academic, psychosocial and financial support.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP. (arXiv:2106.06830v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anthony Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gudipati_P/0/1/0/all/0/1">Pallavi Gudipati</a>, <a href="http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1">Shayne Longpre</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1">Xiao Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sameer Singh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06830">
                                    <div class="article-summary-box-inner">
                                        <span>Retrieval is a core component for open-domain NLP tasks. In open-domain
tasks, multiple entities can share a name, making disambiguation an inherent
yet under-explored problem. We propose an evaluation benchmark for assessing
the entity disambiguation capabilities of these retrievers, which we call
Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection
of entities that share a name along with queries about those entities. By
covering the set of entities for polysemous names, AmbER sets act as a
challenging test of entity disambiguation. We create AmbER sets for three
popular open-domain tasks: fact checking, slot filling, and question answering,
and evaluate a diverse set of retrievers. We find that the retrievers exhibit
popularity bias, significantly under-performing on rarer entities that share a
name, e.g., they are twice as likely to retrieve erroneous documents on queries
for the less popular entity under the same name. These experiments on AmbER
sets show their utility as an evaluation tool and highlight the weaknesses of
popular retrieval systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Functional optimal transport: map estimation and domain adaptation for functional data. (arXiv:2102.03895v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1">Jiacheng Zhu</a>, <a href="http://arxiv.org/find/stat/1/au:+Guha_A/0/1/0/all/0/1">Aritra Guha</a>, <a href="http://arxiv.org/find/stat/1/au:+Do_D/0/1/0/all/0/1">Dat Do</a>, <a href="http://arxiv.org/find/stat/1/au:+Xu_M/0/1/0/all/0/1">Mengdi Xu</a>, <a href="http://arxiv.org/find/stat/1/au:+Nguyen_X/0/1/0/all/0/1">XuanLong Nguyen</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1">Ding Zhao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03895">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a formulation of optimal transport problem for distributions on
function spaces, where the stochastic map between functional domains can be
partially represented in terms of an (infinite-dimensional) Hilbert-Schmidt
operator mapping a Hilbert space of functions to another. For numerous machine
learning tasks, data can be naturally viewed as samples drawn from spaces of
functions, such as curves and surfaces, in high dimensions. Optimal transport
for functional data analysis provides a useful framework of treatment for such
domains. In this work, we develop an efficient algorithm for finding the
stochastic transport map between functional domains and provide theoretical
guarantees on the existence, uniqueness, and consistency of our estimate for
the Hilbert-Schmidt operator. We validate our method on synthetic datasets and
study the geometric properties of the transport map. Experiments on real-world
datasets of robot arm trajectories further demonstrate the effectiveness of our
method on applications in domain adaptation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions. (arXiv:1812.03664v6 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Han-Jia Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hexiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1">De-Chuan Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1">Fei Sha</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.03664">
                                    <div class="article-summary-box-inner">
                                        <span>Learning with limited data is a key challenge for visual recognition. Many
few-shot learning methods address this challenge by learning an instance
embedding function from seen classes and apply the function to instances from
unseen classes with limited labels. This style of transfer learning is
task-agnostic: the embedding function is not learned optimally discriminative
with respect to the unseen classes, where discerning among them leads to the
target task. In this paper, we propose a novel approach to adapt the instance
embeddings to the target classification task with a set-to-set function,
yielding embeddings that are task-specific and are discriminative. We
empirically investigated various instantiations of such set-to-set functions
and observed the Transformer is most effective -- as it naturally satisfies key
properties of our desired model. We denote this model as FEAT (few-shot
embedding adaptation w/ Transformer) and validate it on both the standard
few-shot classification benchmark and four extended few-shot learning settings
with essential use cases, i.e., cross-domain, transductive, generalized
few-shot learning, and low-shot learning. It archived consistent improvements
over baseline models as well as previous methods and established the new
state-of-the-art results on two benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Zero-Cost Proxies Meet Differentiable Architecture Search. (arXiv:2106.06799v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1">Lichuan Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dudziak_L/0/1/0/all/0/1">&#x141;ukasz Dudziak</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdelfattah_M/0/1/0/all/0/1">Mohamed S. Abdelfattah</a>, <a href="http://arxiv.org/find/cs/1/au:+Chau_T/0/1/0/all/0/1">Thomas Chau</a>, <a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1">Nicholas D. Lane</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hongkai Wen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06799">
                                    <div class="article-summary-box-inner">
                                        <span>Differentiable neural architecture search (NAS) has attracted significant
attention in recent years due to its ability to quickly discover promising
architectures of deep neural networks even in very large search spaces. Despite
its success, DARTS lacks robustness in certain cases, e.g. it may degenerate to
trivial architectures with excessive parametric-free operations such as skip
connection or random noise, leading to inferior performance. In particular,
operation selection based on the magnitude of architectural parameters was
recently proven to be fundamentally wrong showcasing the need to rethink this
aspect. On the other hand, zero-cost proxies have been recently studied in the
context of sample-based NAS showing promising results -- speeding up the search
process drastically in some cases but also failing on some of the large search
spaces typical for differentiable NAS. In this work we propose a novel
operation selection paradigm in the context of differentiable NAS which
utilises zero-cost proxies. Our perturbation-based zero-cost operation
selection (Zero-Cost-PT) improves searching time and, in many cases, accuracy
compared to the best available differentiable architecture search, regardless
of the search space size. Specifically, we are able to find comparable
architectures to DARTS-PT on the DARTS CNN search space while being over 40x
faster (total searching time 25 minutes on a single GPU).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Corruption-Robust Offline Reinforcement Learning. (arXiv:2106.06630v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuezhou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiding Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jerry Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wen Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06630">
                                    <div class="article-summary-box-inner">
                                        <span>We study the adversarial robustness in offline reinforcement learning. Given
a batch dataset consisting of tuples $(s, a, r, s&#x27;)$, an adversary is allowed
to arbitrarily modify $\epsilon$ fraction of the tuples. From the corrupted
dataset the learner aims to robustly identify a near-optimal policy. We first
show that a worst-case $\Omega(d\epsilon)$ optimality gap is unavoidable in
linear MDP of dimension $d$, even if the adversary only corrupts the reward
element in a tuple. This contrasts with dimension-free results in robust
supervised learning and best-known lower-bound in the online RL setting with
corruption. Next, we propose robust variants of the Least-Square Value
Iteration (LSVI) algorithm utilizing robust supervised learning oracles, which
achieve near-matching performances in cases both with and without full data
coverage. The algorithm requires the knowledge of $\epsilon$ to design the
pessimism bonus in the no-coverage case. Surprisingly, in this case, the
knowledge of $\epsilon$ is necessary, as we show that being adaptive to unknown
$\epsilon$ is impossible.This again contrasts with recent results on
corruption-robust online RL and implies that robust offline RL is a strictly
harder problem.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Independent Prototype Propagation for Zero-Shot Compositionality. (arXiv:2106.00305v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ruis_F/0/1/0/all/0/1">Frank Ruis</a>, <a href="http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1">Gertjan Burghouts</a>, <a href="http://arxiv.org/find/cs/1/au:+Bucur_D/0/1/0/all/0/1">Doina Bucur</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00305">
                                    <div class="article-summary-box-inner">
                                        <span>Humans are good at compositional zero-shot reasoning; someone who has never
seen a zebra before could nevertheless recognize one when we tell them it looks
like a horse with black and white stripes. Machine learning systems, on the
other hand, usually leverage spurious correlations in the training data, and
while such correlations can help recognize objects in context, they hurt
generalization. To be able to deal with underspecified datasets while still
leveraging contextual clues during classification, we propose ProtoProp, a
novel prototype propagation graph method. First we learn prototypical
representations of objects (e.g., zebra) that are conditionally independent
w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate
the independent prototypes through a compositional graph, to learn
compositional prototypes of novel attribute-object combinations that reflect
the dependencies of the target distribution. The method does not rely on any
external data, such as class hierarchy graphs or pretrained word embeddings. We
evaluate our approach on AO-Clever, a synthetic and strongly visual dataset
with clean labels, and UT-Zappos, a noisy real-world dataset of fine-grained
shoe types. We show that in the generalized compositional zero-shot setting we
outperform state-of-the-art results, and through ablations we show the
importance of each part of the method and their contribution to the final
results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learnable Hypergraph Laplacian for Hypergraph Learning. (arXiv:2106.06666v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuzhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xi Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1">Runiu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shu-Tao Xia</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06666">
                                    <div class="article-summary-box-inner">
                                        <span>HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their
potential in modeling high-order relations preserved in graph structured data.
However, most existing convolution filters are localized and determined by the
pre-defined initial hypergraph topology, neglecting to explore implicit and
long-ange relations in real-world data. In this paper, we propose the first
learning-based method tailored for constructing adaptive hypergraph structure,
termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic
plug-in-play module for improving the representational power of HGCNNs.
Specifically, HERALD adaptively optimizes the adjacency relationship between
hypernodes and hyperedges in an end-to-end manner and thus the task-aware
hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism
to capture the non-local paired-nodes relation. Extensive experiments on
various popular hypergraph datasets for node classification and graph
classification tasks demonstrate that our approach obtains consistent and
considerable performance enhancement, proving its effectiveness and
generalization ability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Deep Reinforcement Learning Approach to Marginalized Importance Sampling with the Successor Representation. (arXiv:2106.06854v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fujimoto_S/0/1/0/all/0/1">Scott Fujimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1">David Meger</a>, <a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06854">
                                    <div class="article-summary-box-inner">
                                        <span>Marginalized importance sampling (MIS), which measures the density ratio
between the state-action occupancy of a target policy and that of a sampling
distribution, is a promising approach for off-policy evaluation. However,
current state-of-the-art MIS methods rely on complex optimization tricks and
succeed mostly on simple toy problems. We bridge the gap between MIS and deep
reinforcement learning by observing that the density ratio can be computed from
the successor representation of the target policy. The successor representation
can be trained through deep reinforcement learning methodology and decouples
the reward optimization from the dynamics of the environment, making the
resulting algorithm stable and applicable to high-dimensional domains. We
evaluate the empirical performance of our approach on a variety of challenging
Atari and MuJoCo environments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Probabilistic Generating Circuits. (arXiv:2102.09768v2 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Honghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Juba_B/0/1/0/all/0/1">Brendan Juba</a>, <a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1">Guy Van den Broeck</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09768">
                                    <div class="article-summary-box-inner">
                                        <span>Generating functions, which are widely used in combinatorics and probability
theory, encode function values into the coefficients of a polynomial. In this
paper, we explore their use as a tractable probabilistic model, and propose
probabilistic generating circuits (PGCs) for their efficient representation.
PGCs are strictly more expressive efficient than many existing tractable
probabilistic models, including determinantal point processes (DPPs),
probabilistic circuits (PCs) such as sum-product networks, and tractable
graphical models. We contend that PGCs are not just a theoretical framework
that unifies vastly different existing models, but also show great potential in
modeling realistic data. We exhibit a simple class of PGCs that are not
trivially subsumed by simple combinations of PCs and DPPs, and obtain
competitive performance on a suite of density estimation benchmarks. We also
highlight PGCs&#x27; connection to the theory of strongly Rayleigh distributions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Link Prediction with Persistent Homology: An Interactive View. (arXiv:2102.10255v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zuoyu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tengfei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Liangcai Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10255">
                                    <div class="article-summary-box-inner">
                                        <span>Link prediction is an important learning task for graph-structured data. In
this paper, we propose a novel topological approach to characterize
interactions between two nodes. Our topological feature, based on the extended
persistent homology, encodes rich structural information regarding the
multi-hop paths connecting nodes. Based on this feature, we propose a graph
neural network method that outperforms state-of-the-arts on different
benchmarks. As another contribution, we propose a novel algorithm to more
efficiently compute the extended persistence diagrams for graphs. This
algorithm can be generally applied to accelerate many other topological methods
for graph learning tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sparse PointPillars: Exploiting Sparsity in Birds-Eye-View Object Detection. (arXiv:2106.06882v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vedder_K/0/1/0/all/0/1">Kyle Vedder</a>, <a href="http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1">Eric Eaton</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06882">
                                    <div class="article-summary-box-inner">
                                        <span>Bird&#x27;s Eye View (BEV) is a popular representation for processing 3D point
clouds, and by its nature is fundamentally sparse. Motivated by the
computational limitations of mobile robot platforms, we take a fast
high-performance BEV 3D object detector - PointPillars - and modify its
backbone to exploit this sparsity, leading to decreased runtimes. We present
preliminary results demonstrating decreased runtimes with either the same
performance or a modest decrease in performance, which we anticipate will be
remedied by model specific hyperparameter tuning. Our work is a first step
towards a new class of 3D object detectors that exploit sparsity throughout
their entire pipeline in order to reduce runtime and resource usage while
maintaining good detection performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Simple Combinatorial Algorithms for Combinatorial Bandits: Corruptions and Approximations. (arXiv:2106.06712v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haike Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06712">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the stochastic combinatorial semi-bandit problem with adversarial
corruptions. We provide a simple combinatorial algorithm that can achieve a
regret of $\tilde{O}\left(C+d^2K/\Delta_{min}\right)$ where $C$ is the total
amount of corruptions, $d$ is the maximal number of arms one can play in each
round, $K$ is the number of arms. If one selects only one arm in each round, we
achieves a regret of $\tilde{O}\left(C+\sum_{\Delta_i&gt;0}(1/\Delta_i)\right)$.
Our algorithm is combinatorial and improves on the previous combinatorial
algorithm by [Gupta et al., COLT2019] (their bound is
$\tilde{O}\left(KC+\sum_{\Delta_i&gt;0}(1/\Delta_i)\right)$), and almost matches
the best known bounds obtained by [Zimmert et al., ICML2019] and [Zimmert and
Seldin, AISTATS2019] (up to logarithmic factor). Note that the algorithms in
[Zimmert et al., ICML2019] and [Zimmert and Seldin, AISTATS2019] require one to
solve complex convex programs while our algorithm is combinatorial, very easy
to implement, requires weaker assumptions and has very low oracle complexity
and running time. We also study the setting where we only get access to an
approximation oracle for the stochastic combinatorial semi-bandit problem. Our
algorithm achieves an (approximation) regret bound of
$\tilde{O}\left(d\sqrt{KT}\right)$. Our algorithm is very simple, only worse
than the best known regret bound by $\sqrt{d}$, and has much lower oracle
complexity than previous work.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A3C-S: Automated Agent Accelerator Co-Search towards Efficient Deep Reinforcement Learning. (arXiv:2106.06577v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yonggan Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chaojian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhongzhi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yingyan Lin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06577">
                                    <div class="article-summary-box-inner">
                                        <span>Driven by the explosive interest in applying deep reinforcement learning
(DRL) agents to numerous real-time control and decision-making applications,
there has been a growing demand to deploy DRL agents to empower daily-life
intelligent devices, while the prohibitive complexity of DRL stands at odds
with limited on-device resources. In this work, we propose an Automated Agent
Accelerator Co-Search (A3C-S) framework, which to our best knowledge is the
first to automatically co-search the optimally matched DRL agents and
accelerators that maximize both test scores and hardware efficiency. Extensive
experiments consistently validate the superiority of our A3C-S over
state-of-the-art techniques.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust Representation Learning via Perceptual Similarity Metrics. (arXiv:2106.06620v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Taghanaki_S/0/1/0/all/0/1">Saeid Asgari Taghanaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1">Kristy Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1">Amir Khasahmadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Anirudh Goyal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06620">
                                    <div class="article-summary-box-inner">
                                        <span>A fundamental challenge in artificial intelligence is learning useful
representations of data that yield good performance on a downstream task,
without overfitting to spurious input features. Extracting such task-relevant
predictive information is particularly difficult for real-world datasets. In
this work, we propose Contrastive Input Morphing (CIM), a representation
learning framework that learns input-space transformations of the data to
mitigate the effect of irrelevant input features on downstream performance. Our
method leverages a perceptual similarity metric via a triplet loss to ensure
that the transformation preserves task-relevant information.Empirically, we
demonstrate the efficacy of our approach on tasks which typically suffer from
the presence of spurious correlations: classification with nuisance
information, out-of-distribution generalization, and preservation of subgroup
accuracies. We additionally show that CIM is complementary to other mutual
information-based representation learning techniques, and demonstrate that it
improves the performance of variational information bottleneck (VIB) when used
together.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PerSim: Data-Efficient Offline Reinforcement Learning with Heterogeneous Agents via Personalized Simulators. (arXiv:2102.06961v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1">Anish Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Alomar_A/0/1/0/all/0/1">Abdullah Alomar</a>, <a href="http://arxiv.org/find/cs/1/au:+Alumootil_V/0/1/0/all/0/1">Varkey Alumootil</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1">Devavrat Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1">Dennis Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cindy Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06961">
                                    <div class="article-summary-box-inner">
                                        <span>We consider offline reinforcement learning (RL) with heterogeneous agents
under severe data scarcity, i.e., we only observe a single historical
trajectory for every agent under an unknown, potentially sub-optimal policy. We
find that the performance of state-of-the-art offline and model-based RL
methods degrade significantly given such limited data availability, even for
commonly perceived &quot;solved&quot; benchmark settings such as &quot;MountainCar&quot; and
&quot;CartPole&quot;. To address this challenge, we propose PerSim, a model-based offline
RL approach which first learns a personalized simulator for each agent by
collectively using the historical trajectories across all agents, prior to
learning a policy. We do so by positing that the transition dynamics across
agents can be represented as a latent function of latent factors associated
with agents, states, and actions; subsequently, we theoretically establish that
this function is well-approximated by a &quot;low-rank&quot; decomposition of separable
agent, state, and action latent functions. This representation suggests a
simple, regularized neural network architecture to effectively learn the
transition dynamics per agent, even with scarce, offline data. We perform
extensive experiments across several benchmark environments and RL methods. The
consistent improvement of our approach, measured in terms of both state
dynamics prediction and eventual reward, confirms the efficacy of our framework
in leveraging limited historical data to simultaneously learn personalized
policies across agents.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction. (arXiv:2106.06935v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhaocheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zuobai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xhonneux_L/0/1/0/all/0/1">Louis-Pascal Xhonneux</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jian Tang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06935">
                                    <div class="article-summary-box-inner">
                                        <span>Link prediction is a very fundamental task on graphs. Inspired by traditional
path-based methods, in this paper we propose a general and flexible
representation learning framework based on paths for link prediction.
Specifically, we define the representation of a pair of nodes as the
generalized sum of all path representations, with each path representation as
the generalized product of the edge representations in the path. Motivated by
the Bellman-Ford algorithm for solving the shortest path problem, we show that
the proposed path formulation can be efficiently solved by the generalized
Bellman-Ford algorithm. To further improve the capacity of the path
formulation, we propose the Neural Bellman-Ford Network (NBFNet), a general
graph neural network framework that solves the path formulation with learned
operators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes
the generalized Bellman-Ford algorithm with 3 neural components, namely
INDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary
condition, multiplication operator, and summation operator respectively. The
NBFNet is very general, covers many traditional path-based methods, and can be
applied to both homogeneous graphs and multi-relational graphs (e.g., knowledge
graphs) in both transductive and inductive settings. Experiments on both
homogeneous graphs and knowledge graphs show that the proposed NBFNet
outperforms existing methods by a large margin in both transductive and
inductive settings, achieving new state-of-the-art results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DANCE: Enhancing saliency maps using decoys. (arXiv:2002.00526v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wenbo Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1">Xinyu Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Noble_W/0/1/0/all/0/1">William Stafford Noble</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.00526">
                                    <div class="article-summary-box-inner">
                                        <span>Saliency methods can make deep neural network predictions more interpretable
by identifying a set of critical features in an input sample, such as pixels
that contribute most strongly to a prediction made by an image classifier.
Unfortunately, recent evidence suggests that many saliency methods poorly
perform, especially in situations where gradients are saturated, inputs contain
adversarial perturbations, or predictions rely upon inter-feature dependence.
To address these issues, we propose a framework that improves the robustness of
saliency methods by following a two-step procedure. First, we introduce a
perturbation mechanism that subtly varies the input sample without changing its
intermediate representations. Using this approach, we can gather a corpus of
perturbed data samples while ensuring that the perturbed and original input
samples follow the same distribution. Second, we compute saliency maps for the
perturbed samples and propose a new method to aggregate saliency maps. With
this design, we offset the gradient saturation influence upon interpretation.
From a theoretical perspective, we show the aggregated saliency map could not
only capture inter-feature dependence but, more importantly, robustify
interpretation against previously described adversarial perturbation methods.
Following our theoretical analysis, we present experimental results suggesting
that, both qualitatively and quantitatively, our saliency method outperforms
existing methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards a Privacy-preserving Deep Learning-based Network Intrusion Detection in Data Distribution Services. (arXiv:2106.06765v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Abaimov_S/0/1/0/all/0/1">Stanislav Abaimov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06765">
                                    <div class="article-summary-box-inner">
                                        <span>Data Distribution Service (DDS) is an innovative approach towards
communication in ICS/IoT infrastructure and robotics. Being based on the
cross-platform and cross-language API to be applicable in any computerised
device, it offers the benefits of modern programming languages and the
opportunities to develop more complex and advanced systems. However, the DDS
complexity equally increases its vulnerability, while the existing security
measures are limited to plug-ins and static rules, with the rest of the
security provided by third-party applications and operating system.
Specifically, traditional intrusion detection systems (IDS) do not detect any
anomalies in the publish/subscribe method. With the exponentially growing
global communication exchange, securing DDS is of the utmost importance to
futureproofing industrial, public, and even personal devices and systems. This
report presents an experimental work on the simulation of several specific
attacks against DDS, and the application of Deep Learning for their detection.
The findings show that even though Deep Learning allows to detect all simulated
attacks using only metadata analysis, their detection level varies, with some
of the advanced attacks being harder to detect. The limitations imposed by the
attempts to preserve privacy significantly decrease the detection rate. The
report also reviews the drawbacks and limitations of the Deep Learning approach
and proposes a set of selected solutions and configurations, that can further
improve the DDS security.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Achieving Near Instance-Optimality and Minimax-Optimality in Stochastic and Adversarial Linear Bandits Simultaneously. (arXiv:2102.05858v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Chung-Wei Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Haipeng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Chen-Yu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengxiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaojin Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05858">
                                    <div class="article-summary-box-inner">
                                        <span>In this work, we develop linear bandit algorithms that automatically adapt to
different environments. By plugging a novel loss estimator into the
optimization problem that characterizes the instance-optimal strategy, our
first algorithm not only achieves nearly instance-optimal regret in stochastic
environments, but also works in corrupted environments with additional regret
being the amount of corruption, while the state-of-the-art (Li et al., 2019)
achieves neither instance-optimality nor the optimal dependence on the
corruption amount. Moreover, by equipping this algorithm with an adversarial
component and carefully-designed testings, our second algorithm additionally
enjoys minimax-optimal regret in completely adversarial environments, which is
the first of this kind to our knowledge. Finally, all our guarantees hold with
high probability, while existing instance-optimal guarantees only hold in
expectation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Label Inference Attacks from Log-loss Scores. (arXiv:2105.08266v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_A/0/1/0/all/0/1">Abhinav Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasiviswanathan_S/0/1/0/all/0/1">Shiva Prasad Kasiviswanathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zekun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feyisetan_O/0/1/0/all/0/1">Oluwaseyi Feyisetan</a>, <a href="http://arxiv.org/find/cs/1/au:+Teissier_N/0/1/0/all/0/1">Nathanael Teissier</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08266">
                                    <div class="article-summary-box-inner">
                                        <span>Log-loss (also known as cross-entropy loss) metric is ubiquitously used
across machine learning applications to assess the performance of
classification algorithms. In this paper, we investigate the problem of
inferring the labels of a dataset from single (or multiple) log-loss score(s),
without any other access to the dataset. Surprisingly, we show that for any
finite number of label classes, it is possible to accurately infer the labels
of the dataset from the reported log-loss score of a single carefully
constructed prediction vector if we allow arbitrary precision arithmetic.
Additionally, we present label inference algorithms (attacks) that succeed even
under addition of noise to the log-loss scores and under limited precision
arithmetic. All our algorithms rely on ideas from number theory and
combinatorics and require no model training. We run experimental simulations on
some real datasets to demonstrate the ease of running these attacks in
practice.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Lattice protein design using Bayesian learning. (arXiv:2003.06601v5 [physics.bio-ph] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/physics/1/au:+Takahashi_T/0/1/0/all/0/1">Tomoei Takahashi</a>, <a href="http://arxiv.org/find/physics/1/au:+Chikenji_G/0/1/0/all/0/1">George Chikenji</a>, <a href="http://arxiv.org/find/physics/1/au:+Tokita_K/0/1/0/all/0/1">Kei Tokita</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06601">
                                    <div class="article-summary-box-inner">
                                        <span>Protein design is the inverse approach of the three-dimensional (3D)
structure prediction for elucidating the relationship between the 3D structures
and amino acid sequences. In general, the computation of the protein design
involves a double loop: a loop for amino acid sequence changes and a loop for
an exhaustive conformational search for each amino acid sequence. Herein, we
propose a novel statistical mechanical design method using Bayesian learning,
which can design lattice proteins without the exhaustive conformational search.
We consider a thermodynamic hypothesis of the evolution of proteins and apply
it to the prior distribution of amino acid sequences. Furthermore, we take the
water effect into account in view of the grand canonical picture. As a result,
on applying the 2D lattice hydrophobic-polar (HP) model, our design method
successfully finds an amino acid sequence for which the target conformation has
a unique ground state. However, the performance was not as good for the 3D
lattice HP models compared to the 2D models. The performance of the 3D model
improves on using a 20-letter lattice proteins. Furthermore, we find a strong
linearity between the chemical potential of water and the number of surface
residues, thereby revealing the relationship between protein structure and the
effect of water molecules. The advantage of our method is that it greatly
reduces computation time, because it does not require long calculations for the
partition function corresponding to an exhaustive conformational search. As our
method uses a general form of Bayesian learning and statistical mechanics and
is not limited to lattice proteins, the results presented here elucidate some
heuristics used successfully in previous protein design methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Short-term forecasting of global solar irradiance with incomplete data. (arXiv:2106.06868v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hoyos_Gomez_L/0/1/0/all/0/1">Laura S. Hoyos-G&#xf3;mez</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruiz_Munoz_J/0/1/0/all/0/1">Jose F. Ruiz-Mu&#xf1;oz</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruiz_Mendoza_B/0/1/0/all/0/1">Belizza J. Ruiz-Mendoza</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06868">
                                    <div class="article-summary-box-inner">
                                        <span>Accurate mechanisms for forecasting solar irradiance and insolation provide
important information for the planning of renewable energy and agriculture
projects as well as for environmental and socio-economical studies. This
research introduces a pipeline for the one-day ahead forecasting of solar
irradiance and insolation that only requires solar irradiance historical data
for training. Furthermore, our approach is able to deal with missing data since
it includes a data imputation state. In the prediction stage, we consider four
data-driven approaches: Autoregressive Integrated Moving Average (ARIMA),
Single Layer Feed Forward Network (SL-FNN), Multiple Layer Feed Forward Network
(FL-FNN), and Long Short-Term Memory (LSTM). The experiments are performed in a
real-world dataset collected with 12 Automatic Weather Stations (AWS) located
in the Nari\~no - Colombia. The results show that the neural network-based
models outperform ARIMA in most cases. Furthermore, LSTM exhibits better
performance in cloudy environments (where more randomness is expected).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Compression of Deep Learning Models for Text: A Survey. (arXiv:2008.05221v4 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1">Manish Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Puneet Agrawal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.05221">
                                    <div class="article-summary-box-inner">
                                        <span>In recent years, the fields of natural language processing (NLP) and
information retrieval (IR) have made tremendous progress thanksto deep learning
models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and
Long Short-Term Memory (LSTMs)networks, and Transformer [120] based models like
Bidirectional Encoder Representations from Transformers (BERT) [24],
GenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network
(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer
transformer (T5) [95], T-NLG [98] and GShard [63]. But these models are
humongous in size. On the other hand,real world applications demand small model
size, low response times and low computational power wattage. In this survey,
wediscuss six different types of methods (Pruning, Quantization, Knowledge
Distillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic
Transformer based methods) for compression of such models to enable their
deployment in real industry NLP projects.Given the critical need of building
applications with efficient and small models, and the large amount of recently
published work inthis area, we believe that this survey organizes the plethora
of work done by the &#x27;deep learning for NLP&#x27; community in the past fewyears and
presents it as a coherent story.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Benchmarks, Algorithms, and Metrics for Hierarchical Disentanglement. (arXiv:2102.05185v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1">Andrew Slavin Ross</a>, <a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1">Finale Doshi-Velez</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05185">
                                    <div class="article-summary-box-inner">
                                        <span>In representation learning, there has been recent interest in developing
algorithms to disentangle the ground-truth generative factors behind a dataset,
and metrics to quantify how fully this occurs. However, these algorithms and
metrics often assume that both representations and ground-truth factors are
flat, continuous, and factorized, whereas many real-world generative processes
involve rich hierarchical structure, mixtures of discrete and continuous
variables with dependence between them, and even varying intrinsic
dimensionality. In this work, we develop benchmarks, algorithms, and metrics
for learning such hierarchical representations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns. (arXiv:2106.06586v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1">Susheel Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Budde_V/0/1/0/all/0/1">Vinith Budde</a>, <a href="http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1">Jennifer Neville</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jianzhu Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06586">
                                    <div class="article-summary-box-inner">
                                        <span>Graph neural networks (GNNs) have achieved tremendous success on multiple
graph-based learning tasks by fusing network structure and node features.
Modern GNN models are built upon iterative aggregation of neighbor&#x27;s/proximity
features by message passing. Its prediction performance has been shown to be
strongly bounded by assortative mixing in the graph, a key property wherein
nodes with similar attributes mix/connect with each other. We observe that real
world networks exhibit heterogeneous or diverse mixing patterns and the
conventional global measurement of assortativity, such as global assortativity
coefficient, may not be a representative statistic in quantifying this mixing.
We adopt a generalized concept, node-level assortativity, one that is based at
the node level to better represent the diverse patterns and accurately quantify
the learnability of GNNs. We find that the prediction performance of a wide
range of GNN models is highly correlated with the node level assortativity. To
break this limit, in this work, we focus on transforming the input graph into a
computation graph which contains both proximity and structural information as
distinct type of edges. The resulted multi-relational graph has an enhanced
level of assortativity and, more importantly, preserves rich information from
the original graph. We then propose to run GNNs on this computation graph and
show that adaptively choosing between structure and proximity leads to improved
performance under diverse mixing. Empirically, we show the benefits of adopting
our transformation framework for semi-supervised node classification task on a
variety of real world graph learning benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FedScale: Benchmarking Model and System Performance of Federated Learning. (arXiv:2105.11367v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1">Fan Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yinwei Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiangfeng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1">Mosharaf Chowdhury</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11367">
                                    <div class="article-summary-box-inner">
                                        <span>We present FedScale, a diverse set of challenging and realistic benchmark
datasets to facilitate scalable, comprehensive, and reproducible federated
learning (FL) research. FedScale datasets are large-scale, encompassing a
diverse range of important FL tasks, such as image classification, object
detection, language modeling, speech recognition, and reinforcement learning.
For each dataset, we provide a unified evaluation protocol using realistic data
splits and evaluation metrics. To meet the pressing need for reproducing
realistic FL at scale, we have also built an efficient evaluation platform to
simplify and standardize the process of FL experimental setup and model
evaluation. Our evaluation platform provides flexible APIs to implement new FL
algorithms and includes new execution backends with minimal developer efforts.
Finally, we perform indepth benchmark experiments on these datasets. Our
experiments suggest fruitful opportunities in heterogeneity-aware
co-optimizations of the system and statistical efficiency under realistic FL
characteristics. FedScale is open-source with permissive licenses and actively
maintained,1 and we welcome feedback and contributions from the community.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search. (arXiv:2010.07003v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Gyuwan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1">Kyunghyun Cho</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07003">
                                    <div class="article-summary-box-inner">
                                        <span>Despite transformers&#x27; impressive accuracy, their computational cost is often
prohibitive to use with limited computational resources. Most previous
approaches to improve inference efficiency require a separate model for each
possible computational budget. In this paper, we extend PoWER-BERT (Goyal et
al., 2020) and propose Length-Adaptive Transformer that can be used for various
inference scenarios after one-shot training. We train a transformer with
LengthDrop, a structural variant of dropout, which stochastically determines a
sequence length at each layer. We then conduct a multi-objective evolutionary
search to find a length configuration that maximizes the accuracy and minimizes
the efficiency metric under any given computational budget. Additionally, we
significantly extend the applicability of PoWER-BERT beyond sequence-level
classification into token-level classification with Drop-and-Restore process
that drops word-vectors temporarily in intermediate layers and restores at the
last layer if necessary. We empirically verify the utility of the proposed
approach by demonstrating the superior accuracy-efficiency trade-off under
various setups, including span-based question answering and text
classification. Code is available at
https://github.com/clovaai/length-adaptive-transformer.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Review of Graph Neural Networks and Their Applications in Power Systems. (arXiv:2101.10025v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1">Wenlong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bak_Jensen_B/0/1/0/all/0/1">Birgitte Bak-Jensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pillai_J/0/1/0/all/0/1">Jayakrishnan Radhakrishna Pillai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuelong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yusen Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10025">
                                    <div class="article-summary-box-inner">
                                        <span>Deep neural networks have revolutionized many machine learning tasks in power
systems, ranging from pattern recognition to signal processing. The data in
these tasks is typically represented in Euclidean domains. Nevertheless, there
is an increasing number of applications in power systems, where data are
collected from non-Euclidean domains and represented as graph-structured data
with high dimensional features and interdependency among nodes. The complexity
of graph-structured data has brought significant challenges to the existing
deep neural networks defined in Euclidean domains. Recently, many publications
generalizing deep neural networks for graph-structured data in power systems
have emerged. In this paper, a comprehensive overview of graph neural networks
(GNNs) in power systems is proposed. Specifically, several classical paradigms
of GNNs structures (e.g., graph convolutional networks) are summarized, and key
applications in power systems, such as fault scenario application, time series
prediction, power flow calculation, and data generation are reviewed in detail.
Furthermore, main issues and some research trends about the applications of
GNNs in power systems are discussed.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Continuous Coordination As a Realistic Scenario for Lifelong Learning. (arXiv:2103.03216v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nekoei_H/0/1/0/all/0/1">Hadi Nekoei</a>, <a href="http://arxiv.org/find/cs/1/au:+Badrinaaraayanan_A/0/1/0/all/0/1">Akilesh Badrinaaraayanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1">Aaron Courville</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1">Sarath Chandar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03216">
                                    <div class="article-summary-box-inner">
                                        <span>Current deep reinforcement learning (RL) algorithms are still highly
task-specific and lack the ability to generalize to new environments. Lifelong
learning (LLL), however, aims at solving multiple tasks sequentially by
efficiently transferring and using knowledge between tasks. Despite a surge of
interest in lifelong RL in recent years, the lack of a realistic testbed makes
robust evaluation of LLL algorithms difficult. Multi-agent RL (MARL), on the
other hand, can be seen as a natural scenario for lifelong RL due to its
inherent non-stationarity, since the agents&#x27; policies change over time. In this
work, we introduce a multi-agent lifelong learning testbed that supports both
zero-shot and few-shot settings. Our setup is based on Hanabi -- a
partially-observable, fully cooperative multi-agent game that has been shown to
be challenging for zero-shot coordination. Its large strategy space makes it a
desirable environment for lifelong RL tasks. We evaluate several recent MARL
methods, and benchmark state-of-the-art LLL algorithms in limited memory and
computation regimes to shed light on their strengths and weaknesses. This
continual learning paradigm also provides us with a pragmatic way of going
beyond centralized training which is the most commonly used training protocol
in MARL. We empirically show that the agents trained in our setup are able to
coordinate well with unseen agents, without any additional assumptions made by
previous works. The code and all pre-trained models are available at
https://github.com/chandar-lab/Lifelong-Hanabi.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dirty Road Can Attack: Security of Deep Learning based Automated Lane Centering under Physical-World Attack. (arXiv:2009.06701v2 [cs.CR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1">Takami Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Junjie Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Ningfei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yunhan Jack Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1">Xue Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qi Alfred Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06701">
                                    <div class="article-summary-box-inner">
                                        <span>Automated Lane Centering (ALC) systems are convenient and widely deployed
today, but also highly security and safety critical. In this work, we are the
first to systematically study the security of state-of-the-art deep learning
based ALC systems in their designed operational domains under physical-world
adversarial attacks. We formulate the problem with a safety-critical attack
goal, and a novel and domain-specific attack vector: dirty road patches. To
systematically generate the attack, we adopt an optimization-based approach and
overcome domain-specific design challenges such as camera frame
inter-dependencies due to attack-influenced vehicle control, and the lack of
objective function design for lane detection models.

We evaluate our attack on a production ALC using 80 scenarios from real-world
driving traces. The results show that our attack is highly effective with over
97.5% success rates and less than 0.903 sec average success time, which is
substantially lower than the average driver reaction time. This attack is also
found (1) robust to various real-world factors such as lighting conditions and
view angles, (2) general to different model designs, and (3) stealthy from the
driver&#x27;s view. To understand the safety impacts, we conduct experiments using
software-in-the-loop simulation and attack trace injection in a real vehicle.
The results show that our attack can cause a 100% collision rate in different
scenarios, including when tested with common safety features such as automatic
emergency braking. We also evaluate and discuss defenses.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Explainable Artificial Intelligence for Manufacturing Cost Estimation and Machining Feature Visualization. (arXiv:2010.14824v2 [cs.CG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1">Soyoung Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_N/0/1/0/all/0/1">Namwoo Kang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.14824">
                                    <div class="article-summary-box-inner">
                                        <span>Studies on manufacturing cost prediction based on deep learning have begun in
recent years, but the cost prediction rationale cannot be explained because the
models are still used as a black box. This study aims to propose a
manufacturing cost prediction process for 3D computer-aided design (CAD) models
using explainable artificial intelligence. The proposed process can visualize
the machining features of the 3D CAD model that are influencing the increase in
manufacturing costs. The proposed process consists of (1) data collection and
pre-processing, (2) 3D deep learning architecture exploration, and (3)
visualization to explain the prediction results. The proposed deep learning
model shows high predictability of manufacturing cost for the computer
numerical control (CNC) machined parts. In particular, using 3D
gradient-weighted class activation mapping proves that the proposed model not
only can detect the CNC machining features but also can differentiate the
machining difficulty for the same feature. Using the proposed process, we can
provide a design guidance to engineering designers in reducing manufacturing
costs during the conceptual design phase. We can also provide real-time
quotations and redesign proposals to online manufacturing platform customers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Quantitative Understanding of VAE as a Non-linearly Scaled Isometric Embedding. (arXiv:2007.15190v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Nakagawa_A/0/1/0/all/0/1">Akira Nakagawa</a>, <a href="http://arxiv.org/find/stat/1/au:+Kato_K/0/1/0/all/0/1">Keizo Kato</a>, <a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1">Taiji Suzuki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.15190">
                                    <div class="article-summary-box-inner">
                                        <span>Variational autoencoder (VAE) estimates the posterior parameters (mean and
variance) of latent variables corresponding to each input data. While it is
used for many tasks, the transparency of the model is still an underlying
issue. This paper provides a quantitative understanding of VAE property through
the differential geometric and information-theoretic interpretations of VAE.
According to the Rate-distortion theory, the optimal transform coding is
achieved by using an orthonormal transform with PCA basis where the transform
space is isometric to the input. Considering the analogy of transform coding to
VAE, we clarify theoretically and experimentally that VAE can be mapped to an
implicit isometric embedding with a scale factor derived from the posterior
parameter. As a result, we can estimate the data probabilities in the input
space from the prior, loss metrics, and corresponding posterior parameters, and
further, the quantitative importance of each latent variable can be evaluated
like the eigenvalue of PCA.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network. (arXiv:2106.06555v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lovelace_J/0/1/0/all/0/1">Justin Lovelace</a>, <a href="http://arxiv.org/find/cs/1/au:+Newman_Griffis_D/0/1/0/all/0/1">Denis Newman-Griffis</a>, <a href="http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1">Shikhar Vashishth</a>, <a href="http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1">Jill Fain Lehman</a>, <a href="http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1">Carolyn Penstein Ros&#xe9;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06555">
                                    <div class="article-summary-box-inner">
                                        <span>Knowledge Graph (KG) completion research usually focuses on densely connected
benchmark datasets that are not representative of real KGs. We curate two KG
datasets that include biomedical and encyclopedic knowledge and use an existing
commonsense KG dataset to explore KG completion in the more realistic setting
where dense connectivity is not guaranteed. We develop a deep convolutional
network that utilizes textual entity representations and demonstrate that our
model outperforms recent KG completion methods in this challenging setting. We
find that our model&#x27;s performance improvements stem primarily from its
robustness to sparsity. We then distill the knowledge from the convolutional
network into a student network that re-ranks promising candidate entities. This
re-ranking stage leads to further improvements in performance and demonstrates
the effectiveness of entity re-ranking for KG completion.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Decreasing scaling transition from adaptive gradient descent to stochastic gradient descent. (arXiv:2106.06749v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1">Kun Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinlan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhixia Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dongpo Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06749">
                                    <div class="article-summary-box-inner">
                                        <span>Currently, researchers have proposed the adaptive gradient descent algorithm
and its variants, such as AdaGrad, RMSProp, Adam, AmsGrad, etc. Although these
algorithms have a faster speed in the early stage, the generalization ability
in the later stage of training is often not as good as the stochastic gradient
descent. Recently, some researchers have combined the adaptive gradient descent
and stochastic gradient descent to obtain the advantages of both and achieved
good results. Based on this research, we propose a decreasing scaling
transition from adaptive gradient descent to stochastic gradient descent
method(DSTAda). For the training stage of the stochastic gradient descent, we
use a learning rate that decreases linearly with the number of iterations
instead of a constant learning rate. We achieve a smooth and stable transition
from adaptive gradient descent to stochastic gradient descent through scaling.
At the same time, we give a theoretical proof of the convergence of DSTAda
under the framework of online learning. Our experimental results show that the
DSTAda algorithm has a faster convergence speed, higher accuracy, and better
stability and robustness. Our implementation is available at:
https://github.com/kunzeng/DSTAdam.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Tracking Peaceful Tractors on Social Media -- XAI-enabled analysis of Red Fort Riots 2021. (arXiv:2104.13352v2 [cs.SI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1">Ajay Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_B/0/1/0/all/0/1">Basant Agarwal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13352">
                                    <div class="article-summary-box-inner">
                                        <span>On 26 January 2021, India witnessed a national embarrassment from the
demographic least expected from - farmers. People across the nation watched in
horror as a pseudo-patriotic mob of farmers stormed capital Delhi and
vandalized the national pride- Red Fort. Investigations that followed the event
revealed the existence of a social media trail that led to the likes of such an
event. Consequently, it became essential and necessary to archive this trail
for social media analysis - not only to understand the bread-crumbs that are
dispersed across the trail but also to visualize the role played by
misinformation and fake news in this event. In this paper, we propose the
tractor2twitter dataset which contains around 0.05 million tweets that were
posted before, during, and after this event. Also, we benchmark our dataset
with an Explainable AI ML model for classification of each tweet into either of
the three categories - disinformation, misinformation, and opinion.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scaling transition from momentum stochastic gradient descent to plain stochastic gradient descent. (arXiv:2106.06753v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1">Kun Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jinlan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhixia Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1">Dongpo Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06753">
                                    <div class="article-summary-box-inner">
                                        <span>The plain stochastic gradient descent and momentum stochastic gradient
descent have extremely wide applications in deep learning due to their simple
settings and low computational complexity. The momentum stochastic gradient
descent uses the accumulated gradient as the updated direction of the current
parameters, which has a faster training speed. Because the direction of the
plain stochastic gradient descent has not been corrected by the accumulated
gradient. For the parameters that currently need to be updated, it is the
optimal direction, and its update is more accurate. We combine the advantages
of the momentum stochastic gradient descent with fast training speed and the
plain stochastic gradient descent with high accuracy, and propose a scaling
transition from momentum stochastic gradient descent to plain stochastic
gradient descent(TSGD) method. At the same time, a learning rate that decreases
linearly with the iterations is used instead of a constant learning rate. The
TSGD algorithm has a larger step size in the early stage to speed up the
training, and training with a smaller step size in the later stage can steadily
converge. Our experimental results show that the TSGD algorithm has faster
training speed, higher accuracy and better stability. Our implementation is
available at: https://github.com/kunzeng/TSGD.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Omnidirectional Transfer for Quasilinear Lifelong Learning. (arXiv:2004.12908v7 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1">Joshua T. Vogelstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Dey_J/0/1/0/all/0/1">Jayanta Dey</a>, <a href="http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1">Hayden S. Helm</a>, <a href="http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1">Will LeVine</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1">Ronak D. Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Geisa_A/0/1/0/all/0/1">Ali Geisa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1">Gido M. van de Ven</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1">Emily Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chenyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Weiwei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tower_B/0/1/0/all/0/1">Bryan Tower</a>, <a href="http://arxiv.org/find/cs/1/au:+Larson_J/0/1/0/all/0/1">Jonathan Larson</a>, <a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1">Christopher M. White</a>, <a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1">Carey E. Priebe</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12908">
                                    <div class="article-summary-box-inner">
                                        <span>In biological learning, data are used to improve performance not only on the
current task, but also on previously encountered and as yet unencountered
tasks. In contrast, classical machine learning starts from a blank slate, or
tabula rasa, using data only for the single task at hand. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
catastrophic forgetting). Many recent approaches for continual or lifelong
learning have attempted to maintain performance given new tasks. But striving
to avoid forgetting sets the goal unnecessarily low: the goal of lifelong
learning, whether biological or artificial, should be to improve performance on
all tasks (including past and future) with any new data. We propose
omnidirectional transfer learning algorithms, which includes two special cases
of interest: decision forests and deep networks. Our key insight is the
development of the omni-voter layer, which ensembles representations learned
independently on all tasks to jointly decide how to proceed on any given new
data point, thereby improving performance on both past and future tasks. Our
algorithms demonstrate omnidirectional transfer in a variety of simulated and
real data scenarios, including tabular data, image data, spoken data, and
adversarial tasks. Moreover, they do so with quasilinear space and time
complexity.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving weakly supervised sound event detection with self-supervised auxiliary tasks. (arXiv:2106.06858v1 [eess.AS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Deshmukh_S/0/1/0/all/0/1">Soham Deshmukh</a>, <a href="http://arxiv.org/find/eess/1/au:+Raj_B/0/1/0/all/0/1">Bhiksha Raj</a>, <a href="http://arxiv.org/find/eess/1/au:+Singh_R/0/1/0/all/0/1">Rita Singh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06858">
                                    <div class="article-summary-box-inner">
                                        <span>While multitask and transfer learning has shown to improve the performance of
neural networks in limited data settings, they require pretraining of the model
on large datasets beforehand. In this paper, we focus on improving the
performance of weakly supervised sound event detection in low data and noisy
settings simultaneously without requiring any pretraining task. To that extent,
we propose a shared encoder architecture with sound event detection as a
primary task and an additional secondary decoder for a self-supervised
auxiliary task. We empirically evaluate the proposed framework for weakly
supervised sound event detection on a remix dataset of the DCASE 2019 task 1
acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20
dB SNR. To ensure we retain the localisation information of multiple sound
events, we propose a two-step attention pooling mechanism that provides a
time-frequency localisation of multiple audio events in the clip. The proposed
framework with two-step attention outperforms existing benchmark models by
22.3%, 12.8%, 5.9% on 0, 10 and 20 dB SNR respectively. We carry out an
ablation study to determine the contribution of the auxiliary task and two-step
attention pooling to the SED performance improvement.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Comprehensive Overview on 5G-and-Beyond Networks with UAVs: From Communications to Sensing and Intelligence. (arXiv:2010.09317v2 [cs.IT] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qingqing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yong Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1">Derrick Wing Kwan Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Dhahir_N/0/1/0/all/0/1">Naofal Al-Dhahir</a>, <a href="http://arxiv.org/find/cs/1/au:+Schober_R/0/1/0/all/0/1">Robert Schober</a>, <a href="http://arxiv.org/find/cs/1/au:+Swindlehurst_A/0/1/0/all/0/1">A. Lee Swindlehurst</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09317">
                                    <div class="article-summary-box-inner">
                                        <span>Due to the advancements in cellular technologies and the dense deployment of
cellular infrastructure, integrating unmanned aerial vehicles (UAVs) into the
fifth-generation (5G) and beyond cellular networks is a promising solution to
achieve safe UAV operation as well as enabling diversified applications with
mission-specific payload data delivery. In particular, 5G networks need to
support three typical usage scenarios, namely, enhanced mobile broadband
(eMBB), ultra-reliable low-latency communications (URLLC), and massive
machine-type communications (mMTC). On the one hand, UAVs can be leveraged as
cost-effective aerial platforms to provide ground users with enhanced
communication services by exploiting their high cruising altitude and
controllable maneuverability in three-dimensional (3D) space. On the other
hand, providing such communication services simultaneously for both UAV and
ground users poses new challenges due to the need for ubiquitous 3D signal
coverage as well as the strong air-ground network interference. Besides the
requirement of high-performance wireless communications, the ability to support
effective and efficient sensing as well as network intelligence is also
essential for 5G-and-beyond 3D heterogeneous wireless networks with coexisting
aerial and ground users. In this paper, we provide a comprehensive overview of
the latest research efforts on integrating UAVs into cellular networks, with an
emphasis on how to exploit advanced techniques (e.g., intelligent reflecting
surface, short packet transmission, energy harvesting, joint communication and
radar sensing, and edge intelligence) to meet the diversified service
requirements of next-generation wireless systems. Moreover, we highlight
important directions for further investigation in future work.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bellman-consistent Pessimism for Offline Reinforcement Learning. (arXiv:2106.06926v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tengyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Ching-An Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1">Nan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1">Paul Mineiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1">Alekh Agarwal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06926">
                                    <div class="article-summary-box-inner">
                                        <span>The use of pessimism, when reasoning about datasets lacking exhaustive
exploration has recently gained prominence in offline reinforcement learning.
Despite the robustness it adds to the algorithm, overly pessimistic reasoning
can be equally damaging in precluding the discovery of good policies, which is
an issue for the popular bonus-based pessimism. In this paper, we introduce the
notion of Bellman-consistent pessimism for general function approximation:
instead of calculating a point-wise lower bound for the value function, we
implement pessimism at the initial state over the set of functions consistent
with the Bellman equations. Our theoretical guarantees only require Bellman
closedness as standard in the exploratory setting, in which case bonus-based
pessimism fails to provide guarantees. Even in the special case of linear MDPs
where stronger function-approximation assumptions hold, our result improves
upon a recent bonus-based approach by $\mathcal{O}(d)$ in its sample complexity
when the action space is finite. Remarkably, our algorithms automatically adapt
to the best bias-variance tradeoff in the hindsight, whereas most prior
approaches require tuning extra hyperparameters a priori.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rojas_Gomez_R/0/1/0/all/0/1">Renan A. Rojas-Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1">Raymond A. Yeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1">Minh N. Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Nguyen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06927">
                                    <div class="article-summary-box-inner">
                                        <span>Recent research in adversarially robust classifiers suggests their
representations tend to be aligned with human perception, which makes them
attractive for image synthesis and restoration applications. Despite favorable
empirical results on a few downstream tasks, their advantages are limited to
slow and sensitive optimization-based techniques. Moreover, their use on
generative models remains unexplored. This work proposes the use of robust
representations as a perceptual primitive for feature inversion models, and
show its benefits with respect to standard non-robust image features. We
empirically show that adopting robust representations as an image prior
significantly improves the reconstruction accuracy of CNN-based feature
inversion models. Furthermore, it allows reconstructing images at multiple
scales out-of-the-box. Following these findings, we propose an
encoding-decoding network based on robust representations and show its
advantages for applications such as anomaly detection, style transfer and image
denoising.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DeepShift: Towards Multiplication-Less Neural Networks. (arXiv:1905.13298v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1">Mostafa Elhoushi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zihao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafiq_F/0/1/0/all/0/1">Farhan Shafiq</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Ye Henry Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Joey Yiwei Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.13298">
                                    <div class="article-summary-box-inner">
                                        <span>The high computation, memory, and power budgets of inferring convolutional
neural networks (CNNs) are major bottlenecks of model deployment to edge
computing platforms, e.g., mobile devices and IoT. Moreover, training CNNs is
time and energy-intensive even on high-grade servers. Convolution layers and
fully connected layers, because of their intense use of multiplications, are
the dominant contributor to this computation budget.

We propose to alleviate this problem by introducing two new operations:
convolutional shifts and fully-connected shifts which replace multiplications
with bitwise shift and sign flipping during both training and inference. During
inference, both approaches require only 5 bits (or less) to represent the
weights. This family of neural network architectures (that use convolutional
shifts and fully connected shifts) is referred to as DeepShift models. We
propose two methods to train DeepShift models: DeepShift-Q which trains regular
weights constrained to powers of 2, and DeepShift-PS that trains the values of
the shifts and sign flips directly.

Very close accuracy, and in some cases higher accuracy, to baselines are
achieved. Converting pre-trained 32-bit floating-point baseline models of
ResNet18, ResNet50, VGG16, and GoogleNet to DeepShift and training them for 15
to 30 epochs, resulted in Top-1/Top-5 accuracies higher than that of the
original model.

Last but not least, we implemented the convolutional shifts and fully
connected shift GPU kernels and showed a reduction in latency time of 25% when
inferring ResNet18 compared to unoptimized multiplication-based GPU kernels.
The code can be found at https://github.com/mostafaelhoushi/DeepShift.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online Learning with Optimism and Delay. (arXiv:2106.06885v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Flaspohler_G/0/1/0/all/0/1">Genevieve Flaspohler</a>, <a href="http://arxiv.org/find/cs/1/au:+Orabona_F/0/1/0/all/0/1">Francesco Orabona</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1">Judah Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mouatadid_S/0/1/0/all/0/1">Soukayna Mouatadid</a>, <a href="http://arxiv.org/find/cs/1/au:+Oprescu_M/0/1/0/all/0/1">Miruna Oprescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Orenstein_P/0/1/0/all/0/1">Paulo Orenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1">Lester Mackey</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06885">
                                    <div class="article-summary-box-inner">
                                        <span>Inspired by the demands of real-time climate and weather forecasting, we
develop optimistic online learning algorithms that require no parameter tuning
and have optimal regret guarantees under delayed feedback. Our algorithms --
DORM, DORMP, and AdaHedgeD -- arise from a novel reduction of delayed online
learning to optimistic online learning that reveals how optimistic hints can
mitigate the regret penalty caused by delay. We pair this delay-as-optimism
perspective with a new analysis of optimistic learning that exposes its
robustness to hinting errors and a new meta-algorithm for learning effective
hinting strategies in the presence of delay. We conclude by benchmarking our
algorithms on four subseasonal climate forecasting tasks, demonstrating low
regret relative to state-of-the-art forecasting models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Graph-based Visual-Semantic Entanglement Network for Zero-shot Image Recognition. (arXiv:2006.04648v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_G/0/1/0/all/0/1">Guihua Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chapman_A/0/1/0/all/0/1">Adriane Chapman</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1">Pei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1">Mingnan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yingxue Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1">Dan Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Hall_W/0/1/0/all/0/1">Wendy Hall</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04648">
                                    <div class="article-summary-box-inner">
                                        <span>Zero-shot learning uses semantic attributes to connect the search space of
unseen objects. In recent years, although the deep convolutional network brings
powerful visual modeling capabilities to the ZSL task, its visual features have
severe pattern inertia and lack of representation of semantic relationships,
which leads to severe bias and ambiguity. In response to this, we propose the
Graph-based Visual-Semantic Entanglement Network to conduct graph modeling of
visual features, which is mapped to semantic attributes by using a knowledge
graph, it contains several novel designs: 1. it establishes a multi-path
entangled network with the convolutional neural network (CNN) and the graph
convolutional network (GCN), which input the visual features from CNN to GCN to
model the implicit semantic relations, then GCN feedback the graph modeled
information to CNN features; 2. it uses attribute word vectors as the target
for the graph semantic modeling of GCN, which forms a self-consistent
regression for graph modeling and supervise GCN to learn more personalized
attribute relations; 3. it fuses and supplements the hierarchical
visual-semantic features refined by graph modeling into visual embedding. Our
method outperforms state-of-the-art approaches on multiple representative ZSL
datasets: AwA2, CUB, and SUN by promoting the semantic linkage modelling of
visual features.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion. (arXiv:2104.02194v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1">Duc Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1">Mahaveer Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Keren_G/0/1/0/all/0/1">Gil Keren</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Suyoun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yangyang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1">Jay Mahadeokar</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1">Julian Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1">Yuan Shangguan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fuegen_C/0/1/0/all/0/1">Christian Fuegen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1">Ozlem Kalinli</a>, <a href="http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1">Yatharth Saraf</a>, <a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1">Michael L. Seltzer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02194">
                                    <div class="article-summary-box-inner">
                                        <span>How to leverage dynamic contextual information in end-to-end speech
recognition has remained an active research area. Previous solutions to this
problem were either designed for specialized use cases that did not generalize
well to open-domain scenarios, did not scale to large biasing lists, or
underperformed on rare long-tail words. We address these limitations by
proposing a novel solution that combines shallow fusion, trie-based deep
biasing, and neural network language model contextualization. These techniques
result in significant 19.5% relative Word Error Rate improvement over existing
contextual biasing approaches and 5.4%-9.3% improvement compared to a strong
hybrid baseline on both open-domain and constrained contextualization tasks,
where the targets consist of mostly rare long-tail words. Our final system
remains lightweight and modular, allowing for quick modification without model
re-training.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Harmonization with Flow-based Causal Inference. (arXiv:2106.06845v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rongguang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1">Pratik Chaudhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1">Christos Davatzikos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06845">
                                    <div class="article-summary-box-inner">
                                        <span>Heterogeneity in medical data, e.g., from data collected at different sites
and with different protocols in a clinical study, is a fundamental hurdle for
accurate prediction using machine learning models, as such models often fail to
generalize well. This paper presents a normalizing-flow-based method to perform
counterfactual inference upon a structural causal model (SCM) to harmonize such
data. We formulate a causal model for observed effects (brain magnetic
resonance imaging data) that result from known confounders (site, gender and
age) and exogenous noise variables. Our method exploits the bijection induced
by flow for harmonization. We can infer the posterior of exogenous variables,
intervene on observations, and draw samples from the resultant SCM to obtain
counterfactuals. We evaluate on multiple, large, real-world medical datasets to
observe that this method leads to better cross-domain generalization compared
to state-of-the-art algorithms. Further experiments that evaluate the quality
of confounder-independent data generated by our model using regression and
classification tasks are provided.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Transformation Importance with Applications to Cosmology. (arXiv:2003.01926v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Singh_C/0/1/0/all/0/1">Chandan Singh</a>, <a href="http://arxiv.org/find/stat/1/au:+Ha_W/0/1/0/all/0/1">Wooseok Ha</a>, <a href="http://arxiv.org/find/stat/1/au:+Lanusse_F/0/1/0/all/0/1">Francois Lanusse</a>, <a href="http://arxiv.org/find/stat/1/au:+Boehm_V/0/1/0/all/0/1">Vanessa Boehm</a>, <a href="http://arxiv.org/find/stat/1/au:+Liu_J/0/1/0/all/0/1">Jia Liu</a>, <a href="http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1">Bin Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.01926">
                                    <div class="article-summary-box-inner">
                                        <span>Machine learning lies at the heart of new possibilities for scientific
discovery, knowledge generation, and artificial intelligence. Its potential
benefits to these fields requires going beyond predictive accuracy and focusing
on interpretability. In particular, many scientific problems require
interpretations in a domain-specific interpretable feature space (e.g. the
frequency domain) whereas attributions to the raw features (e.g. the pixel
space) may be unintelligible or even misleading. To address this challenge, we
propose TRIM (TRansformation IMportance), a novel approach which attributes
importances to features in a transformed space and can be applied post-hoc to a
fully trained model. TRIM is motivated by a cosmological parameter estimation
problem using deep neural networks (DNNs) on simulated data, but it is
generally applicable across domains/models and can be combined with any local
interpretation method. In our cosmology example, combining TRIM with contextual
decomposition shows promising results for identifying which frequencies a DNN
uses, helping cosmologists to understand and validate that the model learns
appropriate physical features rather than simulation artifacts.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AutoLoss: Automated Loss Function Search in Recommendations. (arXiv:2106.06713v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiangyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haochen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1">Wenqi Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chong Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06713">
                                    <div class="article-summary-box-inner">
                                        <span>Designing an effective loss function plays a crucial role in training deep
recommender systems. Most existing works often leverage a predefined and fixed
loss function that could lead to suboptimal recommendation quality and training
efficiency. Some recent efforts rely on exhaustively or manually searched
weights to fuse a group of candidate loss functions, which is exceptionally
costly in computation and time. They also neglect the various convergence
behaviors of different data examples. In this work, we propose an AutoLoss
framework that can automatically and adaptively search for the appropriate loss
function from a set of candidates. To be specific, we develop a novel
controller network, which can dynamically adjust the loss probabilities in a
differentiable manner. Unlike existing algorithms, the proposed controller can
adaptively generate the loss probabilities for different data examples
according to their varied convergence behaviors. Such design improves the
model&#x27;s generalizability and transferability between deep recommender systems
and datasets. We evaluate the proposed framework on two benchmark datasets. The
results show that AutoLoss outperforms representative baselines. Further
experiments have been conducted to deepen our understandings of AutoLoss,
including its transferability, components and training efficiency.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Semi-supervised Active Regression. (arXiv:2106.06676v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Devvrit_F/0/1/0/all/0/1">Fnu Devvrit</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajaraman_N/0/1/0/all/0/1">Nived Rajaraman</a>, <a href="http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1">Pranjal Awasthi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06676">
                                    <div class="article-summary-box-inner">
                                        <span>Labelled data often comes at a high cost as it may require recruiting human
labelers or running costly experiments. At the same time, in many practical
scenarios, one already has access to a partially labelled, potentially biased
dataset that can help with the learning task at hand. Motivated by such
settings, we formally initiate a study of $semi-supervised$ $active$ $learning$
through the frame of linear regression. In this setting, the learner has access
to a dataset $X \in \mathbb{R}^{(n_1+n_2) \times d}$ which is composed of $n_1$
unlabelled examples that an algorithm can actively query, and $n_2$ examples
labelled a-priori. Concretely, denoting the true labels by $Y \in
\mathbb{R}^{n_1 + n_2}$, the learner&#x27;s objective is to find $\widehat{\beta}
\in \mathbb{R}^d$ such that, \begin{equation}

\| X \widehat{\beta} - Y \|_2^2 \le (1 + \epsilon) \min_{\beta \in
\mathbb{R}^d} \| X \beta - Y \|_2^2 \end{equation} while making as few
additional label queries as possible. In order to bound the label queries, we
introduce an instance dependent parameter called the reduced rank, denoted by
$R_X$, and propose an efficient algorithm with query complexity
$O(R_X/\epsilon)$. This result directly implies improved upper bounds for two
important special cases: (i) active ridge regression, and (ii) active kernel
ridge regression, where the reduced-rank equates to the statistical dimension,
$sd_\lambda$ and effective dimension, $d_\lambda$ of the problem respectively,
where $\lambda \ge 0$ denotes the regularization parameter. For active ridge
regression we also prove a matching lower bound of $O(sd_\lambda / \epsilon)$
on the query complexity of any algorithm. This subsumes prior work that only
considered the unregularized case, i.e., $\lambda &#x3D; 0$.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-Label Annotation of Chest Abdomen Pelvis Computed Tomography Text Reports Using Deep Learning. (arXiv:2102.02959v4 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+DAnniballe_V/0/1/0/all/0/1">Vincent M. D&#x27;Anniballe</a>, <a href="http://arxiv.org/find/cs/1/au:+Tushar_F/0/1/0/all/0/1">Fakrul I. Tushar</a>, <a href="http://arxiv.org/find/cs/1/au:+Faryna_K/0/1/0/all/0/1">Khrystyna Faryna</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Songyue Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1">Maciej A. Mazurowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubin_G/0/1/0/all/0/1">Geoffrey D. Rubin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_J/0/1/0/all/0/1">Joseph Y. Lo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02959">
                                    <div class="article-summary-box-inner">
                                        <span>Purpose: To develop high throughput multi-label annotators for body (chest,
abdomen, and pelvis) Computed Tomography (CT) reports that can be applied
across a variety of abnormalities, organs, and disease states.

Approach: We used a dictionary approach to develop rule-based algorithms
(RBA) for extraction of disease labels from radiology text reports. We targeted
three organ systems (lungs/pleura, liver/gallbladder, kidneys/ureters) with
four diseases per system based on their prevalence in our dataset. To expand
the algorithms beyond pre-defined keywords, attention-guided recurrent neural
networks (RNN) were trained using the RBA-extracted labels to classify reports
as being positive for one or more diseases or normal for each organ system.
Confounding effects on model performance were evaluated using random
initialization or pre-trained embedding as well as different sizes of training
datasets. Performance was evaluated using the receiver operating characteristic
(ROC) area under the curve (AUC) against 2,158 manually obtained labels.

Results: Our models extracted disease labels from 261,229 radiology reports
of 112,501 unique subjects. Pre-trained models outperformed random
initialization across all diseases. As the training dataset size was reduced,
performance was robust except for a few diseases with relatively small number
of cases. Pre-trained classification AUCs achieved &gt; 0.95 for all five disease
outcomes across all three organ systems.

Conclusions: Our label-extracting pipeline was able to encompass a variety of
cases and diseases by generalizing beyond strict rules with exceptional
accuracy. This method can be easily adapted to enable automated labeling of
hospital-scale medical data sets for training image-based disease classifiers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GANs N&#x27; Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!). (arXiv:2106.06561v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1">Min Jin Chong</a>, <a href="http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1">David Forsyth</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06561">
                                    <div class="article-summary-box-inner">
                                        <span>We show how to learn a map that takes a content code, derived from a face
image, and a randomly chosen style code to an anime image. We derive an
adversarial loss from our simple and effective definitions of style and
content. This adversarial loss guarantees the map is diverse -- a very wide
range of anime can be produced from a single content code. Under plausible
assumptions, the map is not just diverse, but also correctly represents the
probability of an anime, conditioned on an input face. In contrast, current
multimodal generation procedures cannot capture the complex styles that appear
in anime. Extensive quantitative experiments support the idea the map is
correct. Extensive qualitative results show that the method can generate a much
more diverse range of styles than SOTA comparisons. Finally, we show that our
formalization of content and style allows us to perform video to video
translation without ever training on videos.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Reconstruction of turbulent data with deep generative models for semantic inpainting from TURB-Rot database. (arXiv:2006.09179v2 [physics.flu-dyn] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/physics/1/au:+Buzzicotti_M/0/1/0/all/0/1">M. Buzzicotti</a>, <a href="http://arxiv.org/find/physics/1/au:+Bonaccorso_F/0/1/0/all/0/1">F. Bonaccorso</a>, <a href="http://arxiv.org/find/physics/1/au:+Leoni_P/0/1/0/all/0/1">P. Clark Di Leoni</a>, <a href="http://arxiv.org/find/physics/1/au:+Biferale_L/0/1/0/all/0/1">L. Biferale</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.09179">
                                    <div class="article-summary-box-inner">
                                        <span>We study the applicability of tools developed by the computer vision
community for features learning and semantic image inpainting to perform data
reconstruction of fluid turbulence configurations. The aim is twofold. First,
we explore on a quantitative basis, the capability of Convolutional Neural
Networks embedded in a Deep Generative Adversarial Model (Deep-GAN) to generate
missing data in turbulence, a paradigmatic high dimensional chaotic system. In
particular, we investigate their use in reconstructing two-dimensional damaged
snapshots extracted from a large database of numerical configurations of 3d
turbulence in the presence of rotation, a case with multi-scale random features
where both large-scale organised structures and small-scale highly intermittent
and non-Gaussian fluctuations are present. Second, following a reverse
engineering approach, we aim to rank the input flow properties (features) in
terms of their qualitative and quantitative importance to obtain a better set
of reconstructed fields. We present two approaches both based on Context
Encoders. The first one infers the missing data via a minimization of the L2
pixel-wise reconstruction loss, plus a small adversarial penalisation. The
second searches for the closest encoding of the corrupted flow configuration
from a previously trained generator. Finally, we present a comparison with a
different data assimilation tool, based on Nudging, an equation-informed
unbiased protocol, well known in the numerical weather prediction community.
The TURB-Rot database, this http URL, of roughly 300K 2d
turbulent images is released and details on how to download it are given.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Domain-adaptive Fall Detection Using Deep Adversarial Training. (arXiv:2012.10911v2 [eess.SP] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1">Kai-Chun Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Can_M/0/1/0/all/0/1">Michael Can</a>, <a href="http://arxiv.org/find/eess/1/au:+Kuo_H/0/1/0/all/0/1">Heng-Cheng Kuo</a>, <a href="http://arxiv.org/find/eess/1/au:+Hsieh_C/0/1/0/all/0/1">Chia-Yeh Hsieh</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1">Hsiang-Yun Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1">Chia-Tai Chan</a>, <a href="http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1">Yu Tsao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10911">
                                    <div class="article-summary-box-inner">
                                        <span>Fall detection (FD) systems are important assistive technologies for
healthcare that can detect emergency fall events and alert caregivers. However,
it is not easy to obtain large-scale annotated fall events with various
specifications of sensors or sensor positions during the implementation of
accurate FD systems. Moreover, the knowledge obtained through machine learning
has been restricted to tasks in the same domain. The mismatch between different
domains might hinder the performance of FD systems. Cross-domain knowledge
transfer is very beneficial for machine-learning-based FD systems to train a
reliable FD model with well-labeled data in new environments. In this study, we
propose domain-adaptive fall detection (DAFD) using deep adversarial training
(DAT) to tackle cross-domain problems, such as cross-position and
cross-configuration. The proposed DAFD can transfer knowledge from the source
domain to the target domain by minimizing the domain discrepancy to avoid
mismatch problems. The experimental results show that the average F1-score
improvement when using DAFD ranges from 1.5% to 7% in the cross-position
scenario, and from 3.5% to 12% in the cross-configuration scenario, compared to
using the conventional FD model without domain adaptation training. The results
demonstrate that the proposed DAFD successfully helps to deal with cross-domain
problems and to achieve better detection performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Predicting the Ordering of Characters in Japanese Historical Documents. (arXiv:2106.06786v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1">Alex Lamb</a>, <a href="http://arxiv.org/find/cs/1/au:+Clanuwat_T/0/1/0/all/0/1">Tarin Clanuwat</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Siyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Bober_Irizar_M/0/1/0/all/0/1">Mikel Bober-Irizar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kitamoto_A/0/1/0/all/0/1">Asanobu Kitamoto</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06786">
                                    <div class="article-summary-box-inner">
                                        <span>Japan is a unique country with a distinct cultural heritage, which is
reflected in billions of historical documents that have been preserved.
However, the change in Japanese writing system in 1900 made these documents
inaccessible for the general public. A major research project has been to make
these historical documents accessible and understandable. An increasing amount
of research has focused on the character recognition task and the location of
characters on image, yet less research has focused on how to predict the
sequential ordering of the characters. This is because sequence in classical
Japanese is very different from modern Japanese. Ordering characters into a
sequence is important for making the document text easily readable and
searchable. Additionally, it is a necessary step for any kind of natural
language processing on the data (e.g. machine translation, language modeling,
and word embeddings). We explore a few approaches to the task of predicting the
sequential ordering of the characters: one using simple hand-crafted rules,
another using hand-crafted rules with adaptive thresholds, and another using a
deep recurrent sequence model trained with teacher forcing. We provide a
quantitative and qualitative comparison of these techniques as well as their
distinct trade-offs. Our best-performing system has an accuracy of 98.65\% and
has a perfect accuracy on 49\% of the books in our dataset, suggesting that the
technique is able to predict the order of the characters well enough for many
tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bandwidth-Agile Image Transmission with Deep Joint Source-Channel Coding. (arXiv:2009.12480v2 [cs.IT] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kurka_D/0/1/0/all/0/1">David Burth Kurka</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1">Deniz G&#xfc;nd&#xfc;z</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.12480">
                                    <div class="article-summary-box-inner">
                                        <span>We propose deep learning based communication methods for adaptive-bandwidth
transmission of images over wireless channels. We consider the scenario in
which images are transmitted progressively in layers over time or frequency,
and such layers can be aggregated by receivers in order to increase the quality
of their reconstructions. We investigate two scenarios, one in which the layers
are sent sequentially, and incrementally contribute to the refinement of a
reconstruction, and another in which the layers are independent and can be
retrieved in any order. Those scenarios correspond to the well known problems
of \textit{successive refinement} and \textit{multiple descriptions},
respectively, in the context of joint source-channel coding (JSCC). We propose
DeepJSCC-$l$, an innovative solution that uses convolutional autoencoders, and
present three architectures with different complexity trade-offs. To the best
of our knowledge, this is the first practical multiple-description JSCC scheme
developed and tested for practical information sources and channels. Numerical
results show that DeepJSCC-$l$ can learn to transmit the source progressively
with negligible losses in the end-to-end performance compared with a single
transmission. Moreover, DeepJSCC-$l$ has comparable performance with state of
the art digital progressive transmission schemes in the challenging low
signal-to-noise ratio (SNR) and small bandwidth regimes, with the additional
advantage of graceful degradation with channel SNR.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization. (arXiv:2106.06607v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1">Kartik Ahuja</a>, <a href="http://arxiv.org/find/cs/1/au:+Caballero_E/0/1/0/all/0/1">Ethan Caballero</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dinghuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitliagkas_I/0/1/0/all/0/1">Ioannis Mitliagkas</a>, <a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1">Irina Rish</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06607">
                                    <div class="article-summary-box-inner">
                                        <span>The invariance principle from causality is at the heart of notable approaches
such as invariant risk minimization (IRM) that seek to address
out-of-distribution (OOD) generalization failures. Despite the promising
theory, invariance principle-based approaches fail in common classification
tasks, where invariant (causal) features capture all the information about the
label. Are these failures due to the methods failing to capture the invariance?
Or is the invariance principle itself insufficient? To answer these questions,
we revisit the fundamental assumptions in linear regression tasks, where
invariance-based approaches were shown to provably generalize OOD. In contrast
to the linear regression tasks, we show that for linear classification tasks we
need much stronger restrictions on the distribution shifts, or otherwise OOD
generalization is impossible. Furthermore, even with appropriate restrictions
on distribution shifts in place, we show that the invariance principle alone is
insufficient. We prove that a form of the information bottleneck constraint
along with invariance helps address key failures when invariant features
capture all the information about the label and also retains the existing
success when they do not. We propose an approach that incorporates both of
these principles and demonstrate its effectiveness in several experiments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup. (arXiv:2101.06983v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1">Luyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiawei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1">Jamie Callan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06983">
                                    <div class="article-summary-box-inner">
                                        <span>Contrastive learning has been applied successfully to learn vector
representations of text. Previous research demonstrated that learning
high-quality representations benefits from batch-wise contrastive loss with a
large number of negatives. In practice, the technique of in-batch negative is
used, where for each example in a batch, other batch examples&#x27; positives will
be taken as its negatives, avoiding encoding extra negatives. This, however,
still conditions each example&#x27;s loss on all batch examples and requires fitting
the entire large batch into GPU memory. This paper introduces a gradient
caching technique that decouples backpropagation between contrastive loss and
the encoder, removing encoder backward pass data dependency along the batch
dimension. As a result, gradients can be computed for one subset of the batch
at a time, leading to almost constant memory usage.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Guaranteed Fixed-Confidence Best Arm Identification in Multi-Armed Bandit. (arXiv:2106.06848v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1">MohammadJavad Azizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ross_S/0/1/0/all/0/1">Sheldon M Ross</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhengyu Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06848">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of finding, through adaptive sampling, which of n
populations (arms) has the largest mean. Our objective is to determine a rule
which identifies the best population with a fixed minimum confidence using as
few observations as possible, i.e. fixed-confidence (FC) best arm
identification (BAI) in multi-armed bandits. We study such problems under the
Bayesian setting with both Bernoulli and Gaussian populations. We propose to
use the classical vector at a time (VT) rule, which samples each alive
population once in each round. We show how VT can be implemented and analyzed
in our Bayesian setting and be improved by early elimination. We also propose
and analyze a variant of the classical play the winner (PW) algorithm.
Numerical results show that these rules compare favorably with state-of-art
algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">What Can I Do Here? Learning New Skills by Imagining Visual Affordances. (arXiv:2106.00671v2 [cs.RO] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Khazatsky_A/0/1/0/all/0/1">Alexander Khazatsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1">Ashvin Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_D/0/1/0/all/0/1">Daniel Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00671">
                                    <div class="article-summary-box-inner">
                                        <span>A generalist robot equipped with learned skills must be able to perform many
tasks in many different environments. However, zero-shot generalization to new
settings is not always possible. When the robot encounters a new environment or
object, it may need to finetune some of its previously learned skills to
accommodate this change. But crucially, previously learned behaviors and models
should still be suitable to accelerate this relearning. In this paper, we aim
to study how generative models of possible outcomes can allow a robot to learn
visual representations of affordances, so that the robot can sample potentially
possible outcomes in new situations, and then further train its policy to
achieve those outcomes. In effect, prior data is used to learn what kinds of
outcomes may be possible, such that when the robot encounters an unfamiliar
setting, it can sample potential outcomes from its model, attempt to reach
them, and thereby update both its skills and its outcome model. This approach,
visuomotor affordance learning (VAL), can be used to train goal-conditioned
policies that operate on raw image inputs, and can rapidly learn to manipulate
new objects via our proposed affordance-directed exploration scheme. We show
that VAL can utilize prior data to solve real-world tasks such drawer opening,
grasping, and placing objects in new scenes with only five minutes of online
experience in the new scene.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Distributed Saddle-Point Problems: Lower Bounds, Optimal and Robust Algorithms. (arXiv:2010.13112v6 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Beznosikov_A/0/1/0/all/0/1">Aleksandr Beznosikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Samokhin_V/0/1/0/all/0/1">Valentin Samokhin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gasnikov_A/0/1/0/all/0/1">Alexander Gasnikov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13112">
                                    <div class="article-summary-box-inner">
                                        <span>This paper focuses on the distributed optimization of smooth stochastic
saddle-point problems. The first part of the paper is devoted to lower bounds
for the cenralized and decentralized distributed methods for smooth
(strongly-)convex-(strongly-)concave saddle-point problems as well as the
optimal algorithms by which these bounds are achieved. Next, we present a new
federated algorithm for saddle-point problems - Extra Step Local SGD.
Theoretical analysis of the new method is carried out for
(strongly-)convex-(strongly-)concave and non-convex-non-concave problems. In
the experimental part of the paper, we show the effectiveness of our method in
practice. In particular, we train GANs in a distributed manner.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Double/Debiased Machine Learning for Dynamic Treatment Effects via g-Estimation. (arXiv:2002.07285v4 [econ.EM] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/econ/1/au:+Lewis_G/0/1/0/all/0/1">Greg Lewis</a>, <a href="http://arxiv.org/find/econ/1/au:+Syrgkanis_V/0/1/0/all/0/1">Vasilis Syrgkanis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.07285">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the estimation of treatment effects in settings when multiple
treatments are assigned over time and treatments can have a causal effect on
future outcomes or the state of the treated unit. We propose an extension of
the double/debiased machine learning framework to estimate the dynamic effects
of treatments, which can be viewed as a Neyman orthogonal (locally robust)
cross-fitted version of $g$-estimation in the dynamic treatment regime. Our
method applies to a general class of non-linear dynamic treatment models known
as Structural Nested Mean Models and allows the use of machine learning methods
to control for potentially high dimensional state variables, subject to a mean
square error guarantee, while still allowing parametric estimation and
construction of confidence intervals for the structural parameters of interest.
These structural parameters can be used for off-policy evaluation of any target
dynamic policy at parametric rates, subject to semi-parametric restrictions on
the data generating process. Our work is based on a recursive peeling process,
typical in $g$-estimation, and formulates a strongly convex objective at each
stage, which allows us to extend the $g$-estimation framework in multiple
directions: i) to provide finite sample guarantees, ii) to estimate non-linear
effect heterogeneity with respect to fixed unit characteristics, within
arbitrary function spaces, enabling a dynamic analogue of the RLearner
algorithm for heterogeneous effects, iii) to allow for high-dimensional sparse
parameterizations of the target structural functions, enabling automated model
selection via a recursive lasso algorithm. We also provide guarantees for data
stemming from a single treated unit over a long horizon and under stationarity
conditions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Training Deep Architectures Without End-to-End Backpropagation: A Brief Survey. (arXiv:2101.03419v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1">Shiyu Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1">Jose C. Principe</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.03419">
                                    <div class="article-summary-box-inner">
                                        <span>This tutorial paper surveys training alternatives to end-to-end
backpropagation (E2EBP) -- the de facto standard for training deep
architectures. Modular training refers to strictly local training without both
the forward and the backward pass, i.e., dividing a deep architecture into
several nonoverlapping modules and training them separately without any
end-to-end operation. Between the fully global E2EBP and the strictly local
modular training, there are &quot;weakly modular&quot; hybrids performing training
without the backward pass only. These alternatives can match or surpass the
performance of E2EBP on challenging datasets such as ImageNet, and are gaining
increased attention primarily because they offer practical advantages over
E2EBP, which will be enumerated herein. In particular, they allow for greater
modularity and transparency in deep learning workflows, aligning deep learning
with the mainstream computer science engineering that heavily exploits
modularization for scalability. Modular training has also revealed novel
insights about learning and has further implications on other important
research domains. Specifically, it induces natural and effective solutions to
some important practical problems such as data efficiency and transferability
estimation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Understanding Generalization in Adversarial Training via the Bias-Variance Decomposition. (arXiv:2103.09947v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yaodong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zitong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1">Edgar Dobriban</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1">Jacob Steinhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09947">
                                    <div class="article-summary-box-inner">
                                        <span>Adversarially trained models exhibit a large generalization gap: they can
interpolate the training set even for large perturbation radii, but at the cost
of large test error on clean samples. To investigate this gap, we decompose the
test risk into its bias and variance components and study their behavior as a
function of adversarial training perturbation radii ($\varepsilon$). We find
that the bias increases monotonically with $\varepsilon$ and is the dominant
term in the risk. Meanwhile, the variance is unimodal as a function of
$\varepsilon$, peaking near the interpolation threshold for the training set.
This characteristic behavior occurs robustly across different datasets and also
for other robust training procedures such as randomized smoothing. It thus
provides a test for proposed explanations of the generalization gap. We find
that some existing explanations fail this test--for instance, by predicting a
monotonically increasing variance curve. This underscores the power of
bias-variance decompositions in modern settings-by providing two measurements
instead of one, they can rule out more explanations than test accuracy alone.
We also show that bias and variance can provide useful guidance for scalably
reducing the generalization gap, highlighting pre-training and unlabeled data
as promising routes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Distributed Model-Free Ride-Sharing Approach for Joint Matching, Pricing, and Dispatching using Deep Reinforcement Learning. (arXiv:2010.01755v2 [cs.MA] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Haliem_M/0/1/0/all/0/1">Marina Haliem</a>, <a href="http://arxiv.org/find/cs/1/au:+Mani_G/0/1/0/all/0/1">Ganapathy Mani</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1">Vaneet Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhargava_B/0/1/0/all/0/1">Bharat Bhargava</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01755">
                                    <div class="article-summary-box-inner">
                                        <span>Significant development of ride-sharing services presents a plethora of
opportunities to transform urban mobility by providing personalized and
convenient transportation while ensuring efficiency of large-scale ride
pooling. However, a core problem for such services is route planning for each
driver to fulfill the dynamically arriving requests while satisfying given
constraints. Current models are mostly limited to static routes with only two
rides per vehicle (optimally) or three (with heuristics). In this paper, we
present a dynamic, demand aware, and pricing-based vehicle-passenger matching
and route planning framework that (1) dynamically generates optimal routes for
each vehicle based on online demand, pricing associated with each ride, vehicle
capacities and locations. This matching algorithm starts greedily and optimizes
over time using an insertion operation, (2) involves drivers in the
decision-making process by allowing them to propose a different price based on
the expected reward for a particular ride as well as the destination locations
for future rides, which is influenced by supply-and demand computed by the Deep
Q-network, (3) allows customers to accept or reject rides based on their set of
preferences with respect to pricing and delay windows, vehicle type and
carpooling preferences, and (4) based on demand prediction, our approach
re-balances idle vehicles by dispatching them to the areas of anticipated high
demand using deep Reinforcement Learning (RL). Our framework is validated using
the New York City Taxi public dataset; however, we consider different vehicle
types and designed customer utility functions to validate the setup and study
different settings. Experimental results show the effectiveness of our approach
in real-time and large scale settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Graph Neural Networks with Local Graph Parameters. (arXiv:2106.06707v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Barcelo_P/0/1/0/all/0/1">Pablo Barcel&#xf3;</a>, <a href="http://arxiv.org/find/cs/1/au:+Geerts_F/0/1/0/all/0/1">Floris Geerts</a>, <a href="http://arxiv.org/find/cs/1/au:+Reutter_J/0/1/0/all/0/1">Juan Reutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryschkov_M/0/1/0/all/0/1">Maksimilian Ryschkov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06707">
                                    <div class="article-summary-box-inner">
                                        <span>Various recent proposals increase the distinguishing power of Graph Neural
Networks GNNs by propagating features between $k$-tuples of vertices. The
distinguishing power of these &quot;higher-order&#x27;&#x27; GNNs is known to be bounded by
the $k$-dimensional Weisfeiler-Leman (WL) test, yet their $\mathcal O(n^k)$
memory requirements limit their applicability. Other proposals infuse GNNs with
local higher-order graph structural information from the start, hereby
inheriting the desirable $\mathcal O(n)$ memory requirement from GNNs at the
cost of a one-time, possibly non-linear, preprocessing step. We propose local
graph parameter enabled GNNs as a framework for studying the latter kind of
approaches and precisely characterize their distinguishing power, in terms of a
variant of the WL test, and in terms of the graph structural properties that
they can take into account. Local graph parameters can be added to any GNN
architecture, and are cheap to compute. In terms of expressive power, our
proposal lies in the middle of GNNs and their higher-order counterparts.
Further, we propose several techniques to aide in choosing the right local
graph parameters. Our results connect GNNs with deep results in finite model
theory and finite variable logics. Our experimental evaluation shows that
adding local graph parameters often has a positive effect for a variety of
GNNs, datasets and graph learning tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">What can linearized neural networks actually say about generalization?. (arXiv:2106.06770v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ortiz_Jimenez_G/0/1/0/all/0/1">Guillermo Ortiz-Jim&#xe9;nez</a>, <a href="http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1">Seyed-Mohsen Moosavi-Dezfooli</a>, <a href="http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1">Pascal Frossard</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06770">
                                    <div class="article-summary-box-inner">
                                        <span>For certain infinitely-wide neural networks, the neural tangent kernel (NTK)
theory fully characterizes generalization. However, for the networks used in
practice, the empirical NTK represents only a rough first-order approximation
of these architectures. Still, a growing body of work keeps leveraging this
approximation to successfully analyze important deep learning phenomena and
derive algorithms for new applications. In our work, we provide strong
empirical evidence to determine the practical validity of such approximation by
conducting a systematic comparison of the behaviour of different neural
networks and their linear approximations on different tasks. We show that the
linear approximations can indeed rank the learning complexity of certain tasks
for neural networks, albeit with important nuances. Specifically, we discover
that, in contrast to what was previously observed, neural networks do not
always perform better than their kernel approximations, and reveal that their
performance gap heavily depends on architecture, number of samples and training
task. In fact, we show that during training, deep networks increase the
alignment of their empirical NTK with the target task, which explains why
linear approximations at the end of training can better explain the dynamics of
deep networks. Overall, our work provides concrete examples of novel deep
learning phenomena which can inspire future theoretical research, as well as
provides a new perspective on the use of the NTK approximation in deep
learning.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Auto-NBA: Efficient and Effective Search Over the Joint Space of Networks, Bitwidths, and Accelerators. (arXiv:2106.06575v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yonggan Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1">David Cox</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yingyan Lin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06575">
                                    <div class="article-summary-box-inner">
                                        <span>While maximizing deep neural networks&#x27; (DNNs&#x27;) acceleration efficiency
requires a joint search/design of three different yet highly coupled aspects,
including the networks, bitwidths, and accelerators, the challenges associated
with such a joint search have not yet been fully understood and addressed. The
key challenges include (1) the dilemma of whether to explode the memory
consumption due to the huge joint space or achieve sub-optimal designs, (2) the
discrete nature of the accelerator design space that is coupled yet different
from that of the networks and bitwidths, and (3) the chicken and egg problem
associated with network-accelerator co-search, i.e., co-search requires
operation-wise hardware cost, which is lacking during search as the optimal
accelerator depending on the whole network is still unknown during search. To
tackle these daunting challenges towards optimal and fast development of DNN
accelerators, we propose a framework dubbed Auto-NBA to enable jointly
searching for the Networks, Bitwidths, and Accelerators, by efficiently
localizing the optimal design within the huge joint design space for each
target dataset and acceleration specification. Our Auto-NBA integrates a
heterogeneous sampling strategy to achieve unbiased search with constant memory
consumption, and a novel joint-search pipeline equipped with a generic
differentiable accelerator search engine. Extensive experiments and ablation
studies validate that both Auto-NBA generated networks and accelerators
consistently outperform state-of-the-art designs (including
co-search/exploration techniques, hardware-aware NAS methods, and DNN
accelerators), in terms of search time, task accuracy, and accelerator
efficiency. Our codes are available at: https://github.com/RICE-EIC/Auto-NBA.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Relaxing Local Robustness. (arXiv:2106.06624v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Leino_K/0/1/0/all/0/1">Klas Leino</a>, <a href="http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1">Matt Fredrikson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06624">
                                    <div class="article-summary-box-inner">
                                        <span>Certifiable local robustness, which rigorously precludes small-norm
adversarial examples, has received significant attention as a means of
addressing security concerns in deep learning. However, for some classification
problems, local robustness is not a natural objective, even in the presence of
adversaries; for example, if an image contains two classes of subjects, the
correct label for the image may be considered arbitrary between the two, and
thus enforcing strict separation between them is unnecessary. In this work, we
introduce two relaxed safety properties for classifiers that address this
observation: (1) relaxed top-k robustness, which serves as the analogue of
top-k accuracy; and (2) affinity robustness, which specifies which sets of
labels must be separated by a robustness margin, and which can be
$\epsilon$-close in $\ell_p$ space. We show how to construct models that can be
efficiently certified against each relaxed robustness property, and trained
with very little overhead relative to standard gradient descent. Finally, we
demonstrate experimentally that these relaxed variants of robustness are
well-suited to several significant classification problems, leading to lower
rejection rates and higher certified accuracies than can be obtained when
certifying &quot;standard&quot; local robustness.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Low Budget Active Learning via Wasserstein Distance: An Integer Programming Approach. (arXiv:2106.02968v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1">Rafid Mahmood</a>, <a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1">Sanja Fidler</a>, <a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1">Marc T. Law</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02968">
                                    <div class="article-summary-box-inner">
                                        <span>Given restrictions on the availability of data, active learning is the
process of training a model with limited labeled data by selecting a core
subset of an unlabeled data pool to label. Although selecting the most useful
points for training is an optimization problem, the scale of deep learning data
sets forces most selection strategies to employ efficient heuristics. Instead,
we propose a new integer optimization problem for selecting a core set that
minimizes the discrete Wasserstein distance from the unlabeled pool. We
demonstrate that this problem can be tractably solved with a Generalized
Benders Decomposition algorithm. Our strategy requires high-quality latent
features which we obtain by unsupervised learning on the unlabeled pool.
Numerical results on several data sets show that our optimization approach is
competitive with baselines and particularly outperforms them in the low budget
regime where less than one percent of the data set is labeled.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation. (arXiv:2106.03153v2 [eess.AS] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Min_D/0/1/0/all/0/1">Dongchan Min</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1">Dong Bok Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_E/0/1/0/all/0/1">Eunho Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03153">
                                    <div class="article-summary-box-inner">
                                        <span>With rapid progress in neural text-to-speech (TTS) models, personalized
speech generation is now in high demand for many applications. For practical
applicability, a TTS model should generate high-quality speech with only a few
audio samples from the given speaker, that are also short in length. However,
existing methods either require to fine-tune the model or achieve low
adaptation quality without fine-tuning. In this work, we propose StyleSpeech, a
new TTS model which not only synthesizes high-quality speech but also
effectively adapts to new speakers. Specifically, we propose Style-Adaptive
Layer Normalization (SALN) which aligns gain and bias of the text input
according to the style extracted from a reference speech audio. With SALN, our
model effectively synthesizes speech in the style of the target speaker even
from single speech audio. Furthermore, to enhance StyleSpeech&#x27;s adaptation to
speech from new speakers, we extend it to Meta-StyleSpeech by introducing two
discriminators trained with style prototypes, and performing episodic training.
The experimental results show that our models generate high-quality speech
which accurately follows the speaker&#x27;s voice with single short-duration (1-3
sec) speech audio, significantly outperforming baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Ensemble Squared: A Meta AutoML System. (arXiv:2012.05390v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1">Jason Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Joseph_T/0/1/0/all/0/1">Tony Joseph</a>, <a href="http://arxiv.org/find/cs/1/au:+Yung_D/0/1/0/all/0/1">Dylan Yung</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasseri_S/0/1/0/all/0/1">S. Ali Nasseri</a>, <a href="http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1">Frank Wood</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05390">
                                    <div class="article-summary-box-inner">
                                        <span>There are currently many barriers that prevent non-experts from exploiting
machine learning solutions ranging from the lack of intuition on statistical
learning techniques to the trickiness of hyperparameter tuning. Such barriers
have led to an explosion of interest in automated machine learning (AutoML),
whereby an off-the-shelf system can take care of many of the steps for
end-users without the need for expertise in machine learning. This paper
presents Ensemble Squared (Ensemble$^2$), an AutoML system that ensembles the
results of state-of-the-art open-source AutoML systems. Ensemble$^2$ exploits
the diversity of existing AutoML systems by leveraging the differences in their
model search space and heuristics. Empirically, we show that diversity of each
AutoML system is sufficient to justify ensembling at the AutoML system level.
In demonstrating this, we also establish new state-of-the-art AutoML results on
the OpenML tabular classification benchmark.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Randomized Entity-wise Factorization for Multi-Agent Reinforcement Learning. (arXiv:2006.04222v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Iqbal_S/0/1/0/all/0/1">Shariq Iqbal</a>, <a href="http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1">Christian A. Schroeder de Witt</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Bei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohmer_W/0/1/0/all/0/1">Wendelin B&#xf6;hmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1">Shimon Whiteson</a>, <a href="http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1">Fei Sha</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04222">
                                    <div class="article-summary-box-inner">
                                        <span>Multi-agent settings in the real world often involve tasks with varying types
and quantities of agents and non-agent entities; however, common patterns of
behavior often emerge among these agents/entities. Our method aims to leverage
these commonalities by asking the question: &#x60;&#x60;What is the expected utility of
each agent when only considering a randomly selected sub-group of its observed
entities?&#x27;&#x27; By posing this counterfactual question, we can recognize
state-action trajectories within sub-groups of entities that we may have
encountered in another task and use what we learned in that task to inform our
prediction in the current one. We then reconstruct a prediction of the full
returns as a combination of factors considering these disjoint groups of
entities and train this &#x60;&#x60;randomly factorized&quot; value function as an auxiliary
objective for value-based multi-agent reinforcement learning. By doing so, our
model can recognize and leverage similarities across tasks to improve learning
efficiency in a multi-task setting. Our approach, Randomized Entity-wise
Factorization for Imagined Learning (REFIL), outperforms all strong baselines
by a significant margin in challenging multi-task StarCraft micromanagement
settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AutoScore-Survival: Developing interpretable machine learning-based time-to-event scores with right-censored survival data. (arXiv:2106.06957v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1">Feng Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1">Yilin Ning</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1">Han Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_B/0/1/0/all/0/1">Benjamin Alan Goldstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1">Marcus Eng Hock Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Nan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1">Bibhas Chakraborty</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06957">
                                    <div class="article-summary-box-inner">
                                        <span>Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician&#x27;s knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.

AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.

The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.

Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">User Label Leakage from Gradients in Federated Learning. (arXiv:2105.09369v3 [cs.CR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wainakh_A/0/1/0/all/0/1">Aidmar Wainakh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1">Fabrizio Ventola</a>, <a href="http://arxiv.org/find/cs/1/au:+Mussig_T/0/1/0/all/0/1">Till M&#xfc;&#xdf;ig</a>, <a href="http://arxiv.org/find/cs/1/au:+Keim_J/0/1/0/all/0/1">Jens Keim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cordero_C/0/1/0/all/0/1">Carlos Garcia Cordero</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmer_E/0/1/0/all/0/1">Ephraim Zimmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Grube_T/0/1/0/all/0/1">Tim Grube</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a>, <a href="http://arxiv.org/find/cs/1/au:+Muhlhauser_M/0/1/0/all/0/1">Max M&#xfc;hlh&#xe4;user</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09369">
                                    <div class="article-summary-box-inner">
                                        <span>Federated learning enables multiple users to build a joint model by sharing
their model updates (gradients), while their raw data remains local on their
devices. In contrast to the common belief that this provides privacy benefits,
we here add to the very recent results on privacy risks when sharing gradients.
Specifically, we propose Label Leakage from Gradients (LLG), a novel attack to
extract the labels of the users&#x27; training data from their shared gradients. The
attack exploits the direction and magnitude of gradients to determine the
presence or absence of any label. LLG is simple yet effective, capable of
leaking potential sensitive information represented by labels, and scales well
to arbitrary batch sizes and multiple classes. We empirically and
mathematically demonstrate the validity of our attack under different settings.
Moreover, empirical results show that LLG successfully extracts labels with
high accuracy at the early stages of model training. We also discuss different
defense mechanisms against such leakage. Our findings suggest that gradient
compression is a practical technique to prevent our attack.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Shape of Elephant: Study of Macro Properties of Word Embeddings Spaces. (arXiv:2106.06964v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1">Alexey Tikhonov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06964">
                                    <div class="article-summary-box-inner">
                                        <span>Pre-trained word representations became a key component in many NLP tasks.
However, the global geometry of the word embeddings remains poorly understood.
In this paper, we demonstrate that a typical word embeddings cloud is shaped as
a high-dimensional simplex with interpretable vertices and propose a simple yet
effective method for enumeration of these vertices. We show that the proposed
method can detect and describe vertices of the simplex for GloVe and fasttext
spaces.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust Gaussian Process Regression Based on Iterative Trimming. (arXiv:2011.11057v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhao-Zhou Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhengyi Shao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11057">
                                    <div class="article-summary-box-inner">
                                        <span>The Gaussian process (GP) regression can be severely biased when the data are
contaminated by outliers. This paper presents a new robust GP regression
algorithm that iteratively trims the most extreme data points. While the new
algorithm retains the attractive properties of the standard GP as a
nonparametric and flexible regression method, it can greatly improve the model
accuracy for contaminated data even in the presence of extreme or abundant
outliers. It is also easier to implement compared with previous robust GP
variants that rely on approximate inference. Applied to a wide range of
experiments with different contamination levels, the proposed method
significantly outperforms the standard GP and the popular robust GP variant
with the Student-t likelihood in most test cases. In addition, as a practical
example in the astrophysical study, we show that this method can precisely
determine the main-sequence ridge line in the color-magnitude diagram of star
clusters.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Equivariant Networks for Pixelized Spheres. (arXiv:2106.06662v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shakerinava_M/0/1/0/all/0/1">Mehran Shakerinava</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravanbakhsh_S/0/1/0/all/0/1">Siamak Ravanbakhsh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06662">
                                    <div class="article-summary-box-inner">
                                        <span>Pixelizations of Platonic solids such as the cube and icosahedron have been
widely used to represent spherical data, from climate records to Cosmic
Microwave Background maps. Platonic solids have well-known global symmetries.
Once we pixelize each face of the solid, each face also possesses its own local
symmetries in the form of Euclidean isometries. One way to combine these
symmetries is through a hierarchy. However, this approach does not adequately
model the interplay between the two levels of symmetry transformations. We show
how to model this interplay using ideas from group theory, identify the
equivariant linear maps, and introduce equivariant padding that respects these
symmetries. Deep networks that use these maps as their building blocks
generalize gauge equivariant CNNs on pixelized spheres. These deep networks
achieve state-of-the-art results on semantic segmentation for climate data and
omnidirectional image processing. Code is available at https://git.io/JGiZA.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Model-free Reinforcement Learning for Branching Markov Decision Processes. (arXiv:2106.06777v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hahn_E/0/1/0/all/0/1">Ernst Moritz Hahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1">Mateo Perez</a>, <a href="http://arxiv.org/find/cs/1/au:+Schewe_S/0/1/0/all/0/1">Sven Schewe</a>, <a href="http://arxiv.org/find/cs/1/au:+Somenzi_F/0/1/0/all/0/1">Fabio Somenzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1">Ashutosh Trivedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wojtczak_D/0/1/0/all/0/1">Dominik Wojtczak</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06777">
                                    <div class="article-summary-box-inner">
                                        <span>We study reinforcement learning for the optimal control of Branching Markov
Decision Processes (BMDPs), a natural extension of (multitype) Branching Markov
Chains (BMCs). The state of a (discrete-time) BMCs is a collection of entities
of various types that, while spawning other entities, generate a payoff. In
comparison with BMCs, where the evolution of a each entity of the same type
follows the same probabilistic pattern, BMDPs allow an external controller to
pick from a range of options. This permits us to study the best/worst behaviour
of the system. We generalise model-free reinforcement learning techniques to
compute an optimal control strategy of an unknown BMDP in the limit. We present
results of an implementation that demonstrate the practicality of the approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep manifold learning reveals hidden dynamics of proteasome autoregulation. (arXiv:2012.12854v2 [q-bio.QM] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/q-bio/1/au:+Wu_Z/0/1/0/all/0/1">Zhaolong Wu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhang_S/0/1/0/all/0/1">Shuwen Zhang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_W/0/1/0/all/0/1">Wei Li Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ma_Y/0/1/0/all/0/1">Yinping Ma</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Dong_Y/0/1/0/all/0/1">Yuanchen Dong</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Mao_Y/0/1/0/all/0/1">Youdong Mao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.12854">
                                    <div class="article-summary-box-inner">
                                        <span>The 2.5-MDa 26S proteasome maintains proteostasis and regulates myriad
cellular processes. How polyubiquitylated substrate interactions regulate
proteasome activity is not understood. Here we introduce a deep manifold
learning framework, named AlphaCryo4D, which enables atomic-level cryogenic
electron microscopy (cryo-EM) reconstructions of nonequilibrium conformational
continuum and reconstitutes hidden dynamics of proteasome autoregulation in the
act of substrate degradation. AlphaCryo4D integrates 3D deep residual learning
with manifold embedding of free-energy landscapes, which directs 3D clustering
via an energy-based particle-voting algorithm. In blind assessments using
simulated heterogeneous cryo-EM datasets, AlphaCryo4D achieved 3D
classification accuracy three times that of conventional method and
reconstructed continuous conformational changes of a 130-kDa protein at
sub-3-angstrom resolution. By using AlphaCryo4D to analyze a single
experimental cryo-EM dataset, we identified 64 conformers of the
substrate-bound human 26S proteasome, revealing conformational entanglement of
two regulatory particles in the doubly capped holoenzymes and their energetic
differences with singly capped ones. Novel ubiquitin-binding sites are
discovered on the RPN2, RPN10 and Alpha5 subunits to remodel polyubiquitin
chains for deubiquitylation and recycle. Importantly, AlphaCryo4D choreographs
single-nucleotide-exchange dynamics of proteasomal AAA-ATPase motor during
translocation initiation, which upregulates proteolytic activity by
allosterically promoting nucleophilic attack. Our systemic analysis illuminates
a grand hierarchical allostery for proteasome autoregulation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Locally Persistent Exploration in Continuous Control Tasks with Sparse Rewards. (arXiv:2012.13658v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Amin_S/0/1/0/all/0/1">Susan Amin</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Gomrokchi_M/0/1/0/all/0/1">Maziar Gomrokchi</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Aboutalebi_H/0/1/0/all/0/1">Hossein Aboutalebi</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Satija_H/0/1/0/all/0/1">Harsh Satija</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a> (1 and 2) ((1) McGill University, (2) Mila- Quebec Artificial Intelligence Institute, (3) University of Waterloo)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13658">
                                    <div class="article-summary-box-inner">
                                        <span>A major challenge in reinforcement learning is the design of exploration
strategies, especially for environments with sparse reward structures and
continuous state and action spaces. Intuitively, if the reinforcement signal is
very scarce, the agent should rely on some form of short-term memory in order
to cover its environment efficiently. We propose a new exploration method,
based on two intuitions: (1) the choice of the next exploratory action should
depend not only on the (Markovian) state of the environment, but also on the
agent&#x27;s trajectory so far, and (2) the agent should utilize a measure of spread
in the state space to avoid getting stuck in a small region. Our method
leverages concepts often used in statistical physics to provide explanations
for the behavior of simplified (polymer) chains in order to generate persistent
(locally self-avoiding) trajectories in state space. We discuss the theoretical
properties of locally self-avoiding walks and their ability to provide a kind
of short-term memory through a decaying temporal correlation within the
trajectory. We provide empirical evaluations of our approach in a simulated 2D
navigation task, as well as higher-dimensional MuJoCo continuous control
locomotion tasks with sparse rewards.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Matrix games with bandit feedback. (arXiv:2006.05145v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+ODonoghue_B/0/1/0/all/0/1">Brendan O&#x27;Donoghue</a>, <a href="http://arxiv.org/find/cs/1/au:+Lattimore_T/0/1/0/all/0/1">Tor Lattimore</a>, <a href="http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1">Ian Osband</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.05145">
                                    <div class="article-summary-box-inner">
                                        <span>We study a version of the classical zero-sum matrix game with unknown payoff
matrix and bandit feedback, where the players only observe each others actions
and a noisy payoff. This generalizes the usual matrix game, where the payoff
matrix is known to the players. Despite numerous applications, this problem has
received relatively little attention. Although adversarial bandit algorithms
achieve low regret, they do not exploit the matrix structure and perform poorly
relative to the new algorithms. The main contributions are regret analyses of
variants of UCB and K-learning that hold for any opponent, e.g., even when the
opponent adversarially plays the best-response to the learner&#x27;s mixed strategy.
Along the way, we show that Thompson fails catastrophically in this setting and
provide empirical comparison to existing algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack. (arXiv:2106.06895v1 [cs.CR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1">Tolulope Odetola</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalid_F/0/1/0/all/0/1">Faiq Khalid</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandefur_T/0/1/0/all/0/1">Travis Sandefur</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammed_H/0/1/0/all/0/1">Hawzhin Mohammed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1">Syed Rafay Hasan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06895">
                                    <div class="article-summary-box-inner">
                                        <span>Convolutional Neural Networks (CNN) have shown impressive performance in
computer vision, natural language processing, and many other applications, but
they exhibit high computations and substantial memory requirements. To address
these limitations, especially in resource-constrained devices, the use of cloud
computing for CNNs is becoming more popular. This comes with privacy and
latency concerns that have motivated the designers to develop embedded hardware
accelerators for CNNs. However, designing a specialized accelerator increases
the time-to-market and cost of production. Therefore, to reduce the
time-to-market and access to state-of-the-art techniques, CNN hardware mapping
and deployment on embedded accelerators are often outsourced to untrusted third
parties, which is going to be more prevalent in futuristic artificial
intelligence of things (AIoT) systems. These AIoT systems anticipate horizontal
collaboration among different resource-constrained AIoT node devices, where CNN
layers are partitioned and these devices collaboratively compute complex CNN
tasks Therefore, there is a dire need to explore this attack surface for
designing secure embedded hardware accelerators for CNNs. Towards this goal, in
this paper, we exploited this attack surface to propose an HT-based attack
called FeSHI. This attack exploits the statistical distribution i.e., Gaussian
distribution, of the layer-by-layer feature maps of the CNN to design two
triggers for stealthy HT with a very low probability of triggering. To
illustrate the effectiveness of the proposed attack, we deployed the LeNet and
LeNet-3D on PYNQ to classify the MNIST and CIFAR-10 datasets, respectively, and
tested FeSHI. The experimental results show that FeSHI utilizes up to 2% extra
LUTs, and the overall resource overhead is less than 1% compared to the
original designs</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Disentangling the Roles of Curation, Data-Augmentation and the Prior in the Cold Posterior Effect. (arXiv:2106.06596v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Noci_L/0/1/0/all/0/1">Lorenzo Noci</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1">Kevin Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachmann_G/0/1/0/all/0/1">Gregor Bachmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowozin_S/0/1/0/all/0/1">Sebastian Nowozin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1">Thomas Hofmann</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06596">
                                    <div class="article-summary-box-inner">
                                        <span>The &quot;cold posterior effect&quot; (CPE) in Bayesian deep learning describes the
uncomforting observation that the predictive performance of Bayesian neural
networks can be significantly improved if the Bayes posterior is artificially
sharpened using a temperature parameter T&lt;1. The CPE is problematic in theory
and practice and since the effect was identified many researchers have proposed
hypotheses to explain the phenomenon. However, despite this intensive research
effort the effect remains poorly understood. In this work we provide novel and
nuanced evidence relevant to existing explanations for the cold posterior
effect, disentangling three hypotheses: 1. The dataset curation hypothesis of
Aitchison (2020): we show empirically that the CPE does not arise in a real
curated data set but can be produced in a controlled experiment with varying
curation strength. 2. The data augmentation hypothesis of Izmailov et al.
(2021) and Fortuin et al. (2021): we show empirically that data augmentation is
sufficient but not necessary for the CPE to be present. 3. The bad prior
hypothesis of Wenzel et al. (2020): we use a simple experiment evaluating the
relative importance of the prior and the likelihood, strongly linking the CPE
to the prior. Our results demonstrate how the CPE can arise in isolation from
synthetic curation, data augmentation, and bad priors. Cold posteriors observed
&quot;in the wild&quot; are therefore unlikely to arise from a single simple cause; as a
result, we do not expect a simple &quot;fix&quot; for cold posteriors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Single-Timescale Actor-Critic Provably Finds Globally Optimal Policy. (arXiv:2008.00483v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1">Zuyue Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhuoran Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoran Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.00483">
                                    <div class="article-summary-box-inner">
                                        <span>We study the global convergence and global optimality of actor-critic, one of
the most popular families of reinforcement learning algorithms. While most
existing works on actor-critic employ bi-level or two-timescale updates, we
focus on the more practical single-timescale setting, where the actor and
critic are updated simultaneously. Specifically, in each iteration, the critic
update is obtained by applying the Bellman evaluation operator only once while
the actor is updated in the policy gradient direction computed using the
critic. Moreover, we consider two function approximation settings where both
the actor and critic are represented by linear or deep neural networks. For
both cases, we prove that the actor sequence converges to a globally optimal
policy at a sublinear $O(K^{-1/2})$ rate, where $K$ is the number of
iterations. To the best of our knowledge, we establish the rate of convergence
and global optimality of single-timescale actor-critic with linear function
approximation for the first time. Moreover, under the broader scope of policy
optimization with nonlinear function approximation, we prove that actor-critic
with deep neural network finds the globally optimal policy at a sublinear rate
for the first time.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust Optimization for Multilingual Translation with Imbalanced Data. (arXiv:2104.07639v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1">Hongyu Gong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07639">
                                    <div class="article-summary-box-inner">
                                        <span>Multilingual models are parameter-efficient with the prospect improving
low-resource languages by leveraging crosslingual transfer. Despite recent
advance in massive multilingual translation with ever-growing model and data,
how to effectively train multilingual models has not been well understood. In
this paper, we show that a common situation in multilingual training, data
imbalance among languages, poses optimization tension between high resource and
low resource languages where the found multilingual solution is often
sub-optimal for low resources. We show that common training method which
upsamples low resources can not robustly optimize population loss with risks of
either underfitting high resource languages or overfitting low resource ones.
Drawing on recent findings on the geometry of loss landscape and its effect on
generalization, we propose a principled optimization algorithm, Curvature Aware
Task Scaling (CATS), which adaptively rescales gradients from different tasks
with a meta objective of guiding multilingual training to low-curvature
neighborhoods with uniformly low loss for all languages. We ran experiments on
common benchmarks (TED, WMT and OPUS-100) with varying degrees of data
imbalance. CATS effectively improved multilingual optimization and as a result
demonstrated consistent gains on low resources ( to BLEU) without hurting high
resources. In addition, CATS is robust to overparameterization and large batch
size training, making it a promising training method for massive multilingual
models that truly improve low resource languages.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Graph Neural Network-Based Anomaly Detection in Multivariate Time Series. (arXiv:2106.06947v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Deng_A/0/1/0/all/0/1">Ailin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1">Bryan Hooi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06947">
                                    <div class="article-summary-box-inner">
                                        <span>Given high-dimensional time series data (e.g., sensor data), how can we
detect anomalous events, such as system faults and attacks? More challengingly,
how can we do this in a way that captures complex inter-sensor relationships,
and detects and explains anomalies which deviate from these relationships?
Recently, deep learning approaches have enabled improvements in anomaly
detection in high-dimensional datasets; however, existing methods do not
explicitly learn the structure of existing relationships between variables, or
use them to predict the expected behavior of time series. Our approach combines
a structure learning approach with graph neural networks, additionally using
attention weights to provide explainability for the detected anomalies.
Experiments on two real-world sensor datasets with ground truth anomalies show
that our method detects anomalies more accurately than baseline approaches,
accurately captures correlations between sensors, and allows users to deduce
the root cause of a detected anomaly.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FeatureNorm: L2 Feature Normalization for Dynamic Graph Embedding. (arXiv:2103.00164v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Menglin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1">Ziqiao Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1">Irwin King</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00164">
                                    <div class="article-summary-box-inner">
                                        <span>Dynamic graphs arise in a plethora of practical scenarios such as social
networks, communication networks, and financial transaction networks. Given a
dynamic graph, it is fundamental and essential to learn a graph representation
that is expected not only to preserve structural proximity but also jointly
capture the time-evolving patterns. Recently, graph convolutional network (GCN)
has been widely explored and used in non-Euclidean application domains. The
main success of GCN, especially in handling dependencies and passing messages
within nodes, lies in its approximation to Laplacian smoothing. As a matter of
fact, this smoothing technique can not only encourage must-link node pairs to
get closer but also push cannot-link pairs to shrink together, which
potentially cause serious feature shrink or oversmoothing problem, especially
when stacking graph convolution in multiple layers or steps. For learning
time-evolving patterns, a natural solution is to preserve historical state and
combine it with the current interactions to obtain the most recent
representation. Then the serious feature shrink or oversmoothing problem could
happen when stacking graph convolution explicitly or implicitly according to
current prevalent methods, which would make nodes too similar to distinguish
each other. To solve this problem in dynamic graph embedding, we analyze the
shrinking properties in the node embedding space at first, and then design a
simple yet versatile method, which exploits L2 feature normalization constraint
to rescale all nodes to hypersphere of a unit ball so that nodes would not
shrink together, and yet similar nodes can still get closer. Extensive
experiments on four real-world dynamic graph datasets compared with competitive
baseline models demonstrate the effectiveness of the proposed method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Tutorial on Sparse Gaussian Processes and Variational Inference. (arXiv:2012.13962v10 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Leibfried_F/0/1/0/all/0/1">Felix Leibfried</a>, <a href="http://arxiv.org/find/cs/1/au:+Dutordoir_V/0/1/0/all/0/1">Vincent Dutordoir</a>, <a href="http://arxiv.org/find/cs/1/au:+John_S/0/1/0/all/0/1">ST John</a>, <a href="http://arxiv.org/find/cs/1/au:+Durrande_N/0/1/0/all/0/1">Nicolas Durrande</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13962">
                                    <div class="article-summary-box-inner">
                                        <span>Gaussian processes (GPs) provide a framework for Bayesian inference that can
offer principled uncertainty estimates for a large range of problems. For
example, if we consider regression problems with Gaussian likelihoods, a GP
model enjoys a posterior in closed form. However, identifying the posterior GP
scales cubically with the number of training examples and requires to store all
examples in memory. In order to overcome these obstacles, sparse GPs have been
proposed that approximate the true posterior GP with pseudo-training examples.
Importantly, the number of pseudo-training examples is user-defined and enables
control over computational and memory complexity. In the general case, sparse
GPs do not enjoy closed-form solutions and one has to resort to approximate
inference. In this context, a convenient choice for approximate inference is
variational inference (VI), where the problem of Bayesian inference is cast as
an optimization problem -- namely, to maximize a lower bound of the log
marginal likelihood. This paves the way for a powerful and versatile framework,
where pseudo-training examples are treated as optimization arguments of the
approximate posterior that are jointly identified together with hyperparameters
of the generative model (i.e. prior and likelihood). The framework can
naturally handle a wide scope of supervised learning problems, ranging from
regression with heteroscedastic and non-Gaussian likelihoods to classification
problems with discrete labels, but also multilabel problems. The purpose of
this tutorial is to provide access to the basic matter for readers without
prior knowledge in both GPs and VI. A proper exposition to the subject enables
also access to more recent advances (like importance-weighted VI as well as
interdomain, multioutput and deep GPs) that can serve as an inspiration for new
research ideas.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">LaProp: Separating Momentum and Adaptivity in Adam. (arXiv:2002.04839v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1">Liu Ziyin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhikang T.Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1">Masahito Ueda</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.04839">
                                    <div class="article-summary-box-inner">
                                        <span>We identity a by-far-unrecognized problem of Adam-style optimizers which
results from unnecessary coupling between momentum and adaptivity. The coupling
leads to instability and divergence when the momentum and adaptivity parameters
are mismatched. In this work, we propose a method, Laprop, which decouples
momentum and adaptivity in the Adam-style methods. We show that the decoupling
leads to greater flexibility in the hyperparameters and allows for a
straightforward interpolation between the signed gradient methods and the
adaptive gradient methods. We experimentally show that Laprop has consistently
improved speed and stability over Adam on a variety of tasks. We also bound the
regret of Laprop on a convex problem and show that our bound differs from that
of Adam by a key factor, which demonstrates its advantage.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learngene: From Open-World to Your Learning Task. (arXiv:2106.06788v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qiufeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xin Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Shuxia Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shiyu Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1">Lei Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Ning Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06788">
                                    <div class="article-summary-box-inner">
                                        <span>Although deep learning has made significant progress on fixed large-scale
datasets, it typically encounters challenges regarding improperly detecting
new/unseen classes in the open-world classification, over-parametrized, and
overfitting small samples. In contrast, biological systems can overcome the
above difficulties very well. Individuals inherit an innate gene from
collective creatures that have evolved over hundreds of millions of years, and
can learn new skills through a few examples. Inspired by this, we propose a
practical collective-individual paradigm where open-world tasks are trained in
sequence using an evolution (expandable) network. To be specific, we
innovatively introduce learngene that inherits the meta-knowledge from the
collective model and reconstructs a new lightweight individual model for the
target task, to realize the collective-individual paradigm. Particularly, we
present a novel criterion that can discover the learngene in the collective
model, according to the gradient information. Finally, the individual model is
trained only with a few samples in the absence of the source data. We
demonstrate the effectiveness of our approach in an extensive empirical study
and theoretical analysis.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning. (arXiv:2101.07393v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1">Austin W. Hanjie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1">Victor Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1">Karthik Narasimhan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07393">
                                    <div class="article-summary-box-inner">
                                        <span>We investigate the use of natural language to drive the generalization of
control policies and introduce the new multi-task environment Messenger with
free-form text manuals describing the environment dynamics. Unlike previous
work, Messenger does not assume prior knowledge connecting text and state
observations $-$ the control policy must simultaneously ground the game manual
to entity symbols and dynamics in the environment. We develop a new model, EMMA
(Entity Mapper with Multi-modal Attention) which uses an entity-conditioned
attention module that allows for selective focus over relevant descriptions in
the manual for each entity in the environment. EMMA is end-to-end
differentiable and learns a latent grounding of entities and dynamics from text
to observations using only environment rewards. EMMA achieves successful
zero-shot generalization to unseen games with new dynamics, obtaining a 40%
higher win rate compared to multiple baselines. However, win rate on the
hardest stage of Messenger remains low (10%), demonstrating the need for
additional work in this direction.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Meta-Learning Bidirectional Update Rules. (arXiv:2104.04657v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1">Mark Sandler</a>, <a href="http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1">Max Vladymyrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1">Andrey Zhmoginov</a>, <a href="http://arxiv.org/find/cs/1/au:+Miller_N/0/1/0/all/0/1">Nolan Miller</a>, <a href="http://arxiv.org/find/cs/1/au:+Jackson_A/0/1/0/all/0/1">Andrew Jackson</a>, <a href="http://arxiv.org/find/cs/1/au:+Madams_T/0/1/0/all/0/1">Tom Madams</a>, <a href="http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1">Blaise Aguera y Arcas</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04657">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we introduce a new type of generalized neural network where
neurons and synapses maintain multiple states. We show that classical
gradient-based backpropagation in neural networks can be seen as a special case
of a two-state network where one state is used for activations and another for
gradients, with update rules derived from the chain rule. In our generalized
framework, networks have neither explicit notion of nor ever receive gradients.
The synapses and neurons are updated using a bidirectional Hebb-style update
rule parameterized by a shared low-dimensional &quot;genome&quot;. We show that such
genomes can be meta-learned from scratch, using either conventional
optimization techniques, or evolutionary strategies, such as CMA-ES. Resulting
update rules generalize to unseen tasks and train faster than gradient descent
based optimizers for several standard computer vision and synthetic tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Markov Decision Processes with Long-Term Average Constraints. (arXiv:2106.06680v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1">Mridul Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1">Qinbo Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1">Vaneet Aggarwal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06680">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of constrained Markov Decision Process (CMDP) where
an agent interacts with a unichain Markov Decision Process. At every
interaction, the agent obtains a reward. Further, there are $K$ cost functions.
The agent aims to maximize the long-term average reward while simultaneously
keeping the $K$ long-term average costs lower than a certain threshold. In this
paper, we propose CMDP-PSRL, a posterior sampling based algorithm using which
the agent can learn optimal policies to interact with the CMDP. Further, for
MDP with $S$ states, $A$ actions, and diameter $D$, we prove that following
CMDP-PSRL algorithm, the agent can bound the regret of not accumulating rewards
from optimal policy by $\Tilde{O}(poly(DSA)\sqrt{T})$. Further, we show that
the violations for any of the $K$ constraints is also bounded by
$\Tilde{O}(poly(DSA)\sqrt{T})$. To the best of our knowledge, this is the first
work which obtains a $\Tilde{O}(\sqrt{T})$ regret bounds for ergodic MDPs with
long-term average constraints.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Simple Unified Framework for High Dimensional Bandit Problems. (arXiv:2102.09626v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wenjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Barik_A/0/1/0/all/0/1">Adarsh Barik</a>, <a href="http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1">Jean Honorio</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09626">
                                    <div class="article-summary-box-inner">
                                        <span>Stochastic high dimensional bandit problems with low dimensional structures
are useful in different applications such as online advertising and drug
discovery. In this work, we propose a simple unified algorithm for such
problems and present a general analysis framework for the regret upper bound of
our algorithm. We show that under some mild unified assumptions, our algorithm
can be applied to different high dimensional bandit problems. Our framework
utilizes the low dimensional structure to guide the parameter estimation in the
problem, therefore our algorithm achieves the best regret bounds in the LASSO
bandit, as well as novel bounds in the low-rank matrix bandit, the group sparse
matrix bandit, and in a new problem: the multi-agent LASSO bandit.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Recomposing the Reinforcement Learning Building Blocks with Hypernetworks. (arXiv:2106.06842v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Keynan_S/0/1/0/all/0/1">Shai Keynan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarafian_E/0/1/0/all/0/1">Elad Sarafian</a>, <a href="http://arxiv.org/find/cs/1/au:+Kraus_S/0/1/0/all/0/1">Sarit Kraus</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06842">
                                    <div class="article-summary-box-inner">
                                        <span>The Reinforcement Learning (RL) building blocks, i.e. Q-functions and policy
networks, usually take elements from the cartesian product of two domains as
input. In particular, the input of the Q-function is both the state and the
action, and in multi-task problems (Meta-RL) the policy can take a state and a
context. Standard architectures tend to ignore these variables&#x27; underlying
interpretations and simply concatenate their features into a single vector. In
this work, we argue that this choice may lead to poor gradient estimation in
actor-critic algorithms and high variance learning steps in Meta-RL algorithms.
To consider the interaction between the input variables, we suggest using a
Hypernetwork architecture where a primary network determines the weights of a
conditional dynamic network. We show that this approach improves the gradient
approximation and reduces the learning step variance, which both accelerates
learning and improves the final performance. We demonstrate a consistent
improvement across different locomotion tasks and different algorithms both in
RL (TD3 and SAC) and in Meta-RL (MAML and PEARL).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Federated Learning on Non-IID Data: A Survey. (arXiv:2106.06843v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hangyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jinjin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shiqing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yaochu Jin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06843">
                                    <div class="article-summary-box-inner">
                                        <span>Federated learning is an emerging distributed machine learning framework for
privacy preservation. However, models trained in federated learning usually
have worse performance than those trained in the standard centralized learning
mode, especially when the training data are not independent and identically
distributed (Non-IID) on the local devices. In this survey, we pro-vide a
detailed analysis of the influence of Non-IID data on both parametric and
non-parametric machine learning models in both horizontal and vertical
federated learning. In addition, cur-rent research work on handling challenges
of Non-IID data in federated learning are reviewed, and both advantages and
disadvantages of these approaches are discussed. Finally, we suggest several
future research directions before concluding the paper.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Estimating Treatment Effects with Observed Confounders and Mediators. (arXiv:2003.11991v3 [stat.ME] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1">Shantanu Gupta</a>, <a href="http://arxiv.org/find/stat/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary C. Lipton</a>, <a href="http://arxiv.org/find/stat/1/au:+Childers_D/0/1/0/all/0/1">David Childers</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.11991">
                                    <div class="article-summary-box-inner">
                                        <span>Given a causal graph, the do-calculus can express treatment effects as
functionals of the observational joint distribution that can be estimated
empirically. Sometimes the do-calculus identifies multiple valid formulae,
prompting us to compare the statistical properties of the corresponding
estimators. For example, the backdoor formula applies when all confounders are
observed and the frontdoor formula applies when an observed mediator transmits
the causal effect. In this paper, we investigate the over-identified scenario
where both confounders and mediators are observed, rendering both estimators
valid. Addressing the linear Gaussian causal model, we demonstrate that either
estimator can dominate the other by an unbounded constant factor. Next, we
derive an optimal estimator, which leverages all observed variables, and bound
its finite-sample variance. We show that it strictly outperforms the backdoor
and frontdoor estimators and that this improvement can be unbounded. We also
present a procedure for combining two datasets, one with observed confounders
and another with observed mediators. Finally, we evaluate our methods on both
simulated data and the IHDP and JTPA datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Meta-Learning of Neural Architectures for Few-Shot Learning. (arXiv:1911.11090v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Elsken_T/0/1/0/all/0/1">Thomas Elsken</a>, <a href="http://arxiv.org/find/cs/1/au:+Staffler_B/0/1/0/all/0/1">Benedikt Staffler</a>, <a href="http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1">Jan Hendrik Metzen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.11090">
                                    <div class="article-summary-box-inner">
                                        <span>The recent progress in neural architecture search (NAS) has allowed scaling
the automated design of neural architectures to real-world domains, such as
object detection and semantic segmentation. However, one prerequisite for the
application of NAS are large amounts of labeled data and compute resources.
This renders its application challenging in few-shot learning scenarios, where
many related tasks need to be learned, each with limited amounts of data and
compute time. Thus, few-shot learning is typically done with a fixed neural
architecture. To improve upon this, we propose MetaNAS, the first method which
fully integrates NAS with gradient-based meta-learning. MetaNAS optimizes a
meta-architecture along with the meta-weights during meta-training. During
meta-testing, architectures can be adapted to a novel task with a few steps of
the task optimizer, that is: task adaptation becomes computationally cheap and
requires only little data per task. Moreover, MetaNAS is agnostic in that it
can be used with arbitrary model-agnostic meta-learning algorithms and
arbitrary gradient-based NAS methods. %We present encouraging results for
MetaNAS with a combination of DARTS and REPTILE on few-shot classification
benchmarks. Empirical results on standard few-shot classification benchmarks
show that MetaNAS with a combination of DARTS and REPTILE yields
state-of-the-art results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Boosting Randomized Smoothing with Variance Reduced Classifiers. (arXiv:2106.06946v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Horvath_M/0/1/0/all/0/1">Mikl&#xf3;s Z. Horv&#xe1;th</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1">Mark Niklas M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1">Marc Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1">Martin Vechev</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06946">
                                    <div class="article-summary-box-inner">
                                        <span>Randomized Smoothing (RS) is a promising method for obtaining robustness
certificates by evaluating a base model under noise. In this work we: (i)
theoretically motivate why ensembles are a particularly suitable choice as base
models for RS, and (ii) empirically confirm this choice, obtaining state of the
art results in multiple settings. The key insight of our work is that the
reduced variance of ensembles over the perturbations introduced in RS leads to
significantly more consistent classifications for a given input, in turn
leading to substantially increased certifiable radii for difficult samples. We
also introduce key optimizations which enable an up to 50-fold decrease in
sample complexity of RS, thus drastically reducing its computational overhead.
Experimentally, we show that ensembles of only 3 to 10 classifiers consistently
improve on the strongest single model with respect to their average certified
radius (ACR) by 5% to 21% on both CIFAR-10 and ImageNet. On the latter, we
achieve a state-of-the-art ACR of 1.11. We release all code and models required
to reproduce our results upon publication.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online learning in MDPs with linear function approximation and bandit feedback. (arXiv:2007.01612v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Neu_G/0/1/0/all/0/1">Gergely Neu</a>, <a href="http://arxiv.org/find/cs/1/au:+Olkhovskaya_J/0/1/0/all/0/1">Julia Olkhovskaya</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01612">
                                    <div class="article-summary-box-inner">
                                        <span>We consider an online learning problem where the learner interacts with a
Markov decision process in a sequence of episodes, where the reward function is
allowed to change between episodes in an adversarial manner and the learner
only gets to observe the rewards associated with its actions. We allow the
state space to be arbitrarily large, but we assume that all action-value
functions can be represented as linear functions in terms of a known
low-dimensional feature map, and that the learner has access to a simulator of
the environment that allows generating trajectories from the true MDP dynamics.
Our main contribution is developing a computationally efficient algorithm that
we call MDP-LinExp3, and prove that its regret is bounded by
$\widetilde{\mathcal{O}}\big(H^2 T^{2/3} (dK)^{1/3}\big)$, where $T$ is the
number of episodes, $H$ is the number of steps in each episode, $K$ is the
number of actions, and $d$ is the dimension of the feature map. We also show
that the regret can be improved to $\widetilde{\mathcal{O}}\big(H^2
\sqrt{TdK}\big)$ under much stronger assumptions on the MDP dynamics. To our
knowledge, MDP-LinExp3 is the first provably efficient algorithm for this
problem setting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hippocampus segmentation in magnetic resonance images of Alzheimer&#x27;s patients using Deep machine learning. (arXiv:2106.06743v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Varmazyar_H/0/1/0/all/0/1">Hadi Varmazyar</a>, <a href="http://arxiv.org/find/eess/1/au:+Yousefi_Banaem_H/0/1/0/all/0/1">Hossein Yousefi-Banaem</a>, <a href="http://arxiv.org/find/eess/1/au:+Malekzadeh_S/0/1/0/all/0/1">Saber Malekzadeh</a>, <a href="http://arxiv.org/find/eess/1/au:+Gharehaghaji_N/0/1/0/all/0/1">Nahideh Gharehaghaji</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06743">
                                    <div class="article-summary-box-inner">
                                        <span>Background: Alzheimers disease is a progressive neurodegenerative disorder
and the main cause of dementia in aging. Hippocampus is prone to changes in the
early stages of Alzheimers disease. Detection and observation of the
hippocampus changes using magnetic resonance imaging (MRI) before the onset of
Alzheimers disease leads to the faster preventive and therapeutic measures.
Objective: The aim of this study was the segmentation of the hippocampus in
magnetic resonance (MR) images of Alzheimers patients using deep machine
learning method. Methods: U-Net architecture of convolutional neural network
was proposed to segment the hippocampus in the real MRI data. The MR images of
the 100 and 35 patients available in Alzheimers disease Neuroimaging Initiative
(ADNI) dataset, was used for the train and test of the model, respectively. The
performance of the proposed method was compared with manual segmentation by
measuring the similarity metrics. Results: The desired segmentation achieved
after 10 iterations. A Dice similarity coefficient (DSC) &#x3D; 92.3%, sensitivity &#x3D;
96.5%, positive predicted value (PPV) &#x3D; 90.4%, and Intersection over Union
(IoU) value for the train 92.94 and test 92.93 sets were obtained which are
acceptable. Conclusion: The proposed approach is promising and can be extended
in the prognosis of Alzheimers disease by the prediction of the hippocampus
volume changes in the early stage of the disease.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Not All Memories are Created Equal: Learning to Forget by Expiring. (arXiv:2105.06548v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1">Sainbayar Sukhbaatar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_D/0/1/0/all/0/1">Da Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Poff_S/0/1/0/all/0/1">Spencer Poff</a>, <a href="http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1">Stephen Roller</a>, <a href="http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1">Arthur Szlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1">Jason Weston</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1">Angela Fan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06548">
                                    <div class="article-summary-box-inner">
                                        <span>Attention mechanisms have shown promising results in sequence modeling tasks
that require long-term memory. Recent work investigated mechanisms to reduce
the computational cost of preserving and storing memories. However, not all
content in the past is equally important to remember. We propose Expire-Span, a
method that learns to retain the most important information and expire the
irrelevant information. This forgetting of memories enables Transformers to
scale to attend over tens of thousands of previous timesteps efficiently, as
not all states from previous timesteps are preserved. We demonstrate that
Expire-Span can help models identify and retain critical information and show
it can achieve strong performance on reinforcement learning tasks specifically
designed to challenge this functionality. Next, we show that Expire-Span can
scale to memories that are tens of thousands in size, setting a new state of
the art on incredibly long context tasks such as character-level language
modeling and a frame-by-frame moving objects task. Finally, we analyze the
efficiency of Expire-Span compared to existing approaches and demonstrate that
it trains faster and uses less memory.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning. (arXiv:2105.15134v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1">Zixin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhi Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15134">
                                    <div class="article-summary-box-inner">
                                        <span>How can neural networks trained by contrastive learning extract features from
the unlabeled data? Why does contrastive learning usually need much stronger
data augmentations than supervised learning to ensure good representations?
These questions involve both the optimization and statistical aspects of deep
learning, but can hardly be answered by analyzing supervised learning, where
the target functions are the highest pursuit. Indeed, in self-supervised
learning, it is inevitable to relate to the optimization/generalization of
neural networks to how they can encode the latent structures in the data, which
we refer to as the feature learning process.

In this work, we formally study how contrastive learning learns the feature
representations for neural networks by analyzing its feature learning process.
We consider the case where our data are comprised of two types of features: the
more semantically aligned sparse features which we want to learn from, and the
other dense features we want to avoid. Theoretically, we prove that contrastive
learning using $\mathbf{ReLU}$ networks provably learns the desired sparse
features if proper augmentations are adopted. We present an underlying
principle called $\textbf{feature decoupling}$ to explain the effects of
augmentations, where we theoretically characterize how augmentations can reduce
the correlations of dense features between positive samples while keeping the
correlations of sparse features intact, thereby forcing the neural networks to
learn from the self-supervision of sparse features. Empirically, we verified
that the feature decoupling principle matches the underlying mechanism of
contrastive learning in practice.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Federated Learning with Sparsification-Amplified Privacy and Adaptive Optimization. (arXiv:2008.01558v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1">Rui Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yanmin Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yuanxiong Guo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01558">
                                    <div class="article-summary-box-inner">
                                        <span>Federated learning (FL) enables distributed agents to collaboratively learn a
centralized model without sharing their raw data with each other. However, data
locality does not provide sufficient privacy protection, and it is desirable to
facilitate FL with rigorous differential privacy (DP) guarantee. Existing DP
mechanisms would introduce random noise with magnitude proportional to the
model size, which can be quite large in deep neural networks. In this paper, we
propose a new FL framework with sparsification-amplified privacy. Our approach
integrates random sparsification with gradient perturbation on each agent to
amplify privacy guarantee. Since sparsification would increase the number of
communication rounds required to achieve a certain target accuracy, which is
unfavorable for DP guarantee, we further introduce acceleration techniques to
help reduce the privacy cost. We rigorously analyze the convergence of our
approach and utilize Renyi DP to tightly account the end-to-end DP guarantee.
Extensive experiments on benchmark datasets validate that our approach
outperforms previous differentially-private FL approaches in both privacy
guarantee and communication efficiency.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fictitious play in zero-sum stochastic games. (arXiv:2010.04223v4 [cs.GT] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sayin_M/0/1/0/all/0/1">Muhammed O. Sayin</a>, <a href="http://arxiv.org/find/cs/1/au:+Parise_F/0/1/0/all/0/1">Francesca Parise</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozdaglar_A/0/1/0/all/0/1">Asuman Ozdaglar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04223">
                                    <div class="article-summary-box-inner">
                                        <span>We present fictitious play dynamics for stochastic games and analyze its
convergence properties in zero-sum stochastic games. Our dynamics involves
players forming beliefs on opponent strategy and their own continuation payoff
(Q-function), and playing a greedy best response using estimated continuation
payoffs. Players update their beliefs from observations of opponent actions. A
key property of the learning dynamics is that update of the beliefs on
Q-functions occurs at a slower timescale than update of the beliefs on
strategies. We show both in the model-based and model-free cases (without
knowledge of player payoff functions and state transition probabilities), the
beliefs on strategies converge to a stationary mixed Nash equilibrium of the
zero-sum stochastic game.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Randomly Perturbed Structured Predictors for Direct Loss Minimization. (arXiv:2007.05724v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Indelman_H/0/1/0/all/0/1">Hedda Cohen Indelman</a>, <a href="http://arxiv.org/find/stat/1/au:+Hazan_T/0/1/0/all/0/1">Tamir Hazan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.05724">
                                    <div class="article-summary-box-inner">
                                        <span>Direct loss minimization is a popular approach for learning predictors over
structured label spaces. This approach is computationally appealing as it
replaces integration with optimization and allows to propagate gradients in a
deep net using loss-perturbed prediction. Recently, this technique was extended
to generative models, while introducing a randomized predictor that samples a
structure from a randomly perturbed score function. In this work, we learn the
variance of these randomized structured predictors and show that it balances
better between the learned score function and the randomized noise in
structured prediction. We demonstrate empirically the effectiveness of learning
the balance between the signal and the random noise in structured discrete
spaces.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Human Apprenticeship Learning via Kernel-based Inverse Reinforcement Learning. (arXiv:2002.10904v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rucker_M/0/1/0/all/0/1">Mark A. Rucker</a>, <a href="http://arxiv.org/find/cs/1/au:+Watson_L/0/1/0/all/0/1">Layne T. Watson</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnes_L/0/1/0/all/0/1">Laura E. Barnes</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerber_M/0/1/0/all/0/1">Matthew S. Gerber</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.10904">
                                    <div class="article-summary-box-inner">
                                        <span>It has been well demonstrated that inverse reinforcement learning (IRL) is an
effective technique for teaching machines to perform tasks at human skill
levels given human demonstrations (i.e., human to machine apprenticeship
learning). This paper seeks to show that a similar application can be
demonstrated with human learners. That is, given demonstrations from human
experts inverse reinforcement learning techniques can be used to teach other
humans to perform at higher skill levels (i.e., human to human apprenticeship
learning). To show this two experiments were conducted using a simple,
real-time web game where players were asked to touch targets in order to earn
as many points as possible. For the experiment player performance was defined
as the number of targets a player touched, irrespective of the points that a
player actually earned. This allowed for in-game points to be modified and the
effect of these alterations on performance measured. At no time were
participants told the true performance metric. To determine the point
modifications IRL was applied on demonstrations of human experts playing the
game. The results of the experiment show with significance that performance
improved over the control for select treatment groups. Finally, in addition to
the experiment, we also detail the algorithmic challenges we faced when
conducting the experiment and the techniques we used to overcome them.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Root-finding Approaches for Computing Conformal Prediction Set. (arXiv:2104.06648v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Ndiaye_E/0/1/0/all/0/1">Eugene Ndiaye</a>, <a href="http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1">Ichiro Takeuchi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06648">
                                    <div class="article-summary-box-inner">
                                        <span>Conformal prediction constructs a confidence set for an unobserved response
of a feature vector based on previous identically distributed and exchangeable
observations of responses and features. It has a coverage guarantee at any
nominal level without additional assumptions on their distribution. Its
computation deplorably requires a refitting procedure for all replacement
candidates of the target response. In regression settings, this corresponds to
an infinite number of model fit. Apart from relatively simple estimators that
can be written as pieces of linear function of the response, efficiently
computing such sets is difficult and is still considered as an open problem. We
exploit the fact that, \emph{often}, conformal prediction sets are intervals
whose boundaries can be efficiently approximated by classical root-finding
algorithm. We investigate how this approach can overcome many limitations of
formerly used strategies and we discuss its complexity and drawbacks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yunhao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengmeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianbu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Weiwei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1">Ran Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1">Qian Du</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06896">
                                    <div class="article-summary-box-inner">
                                        <span>The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Markov Neural Operators for Learning Chaotic Systems. (arXiv:2106.06898v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kovachki_N/0/1/0/all/0/1">Nikola Kovachki</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1">Kamyar Azizzadenesheli</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Burigede Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1">Kaushik Bhattacharya</a>, <a href="http://arxiv.org/find/cs/1/au:+Stuart_A/0/1/0/all/0/1">Andrew Stuart</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06898">
                                    <div class="article-summary-box-inner">
                                        <span>Chaotic systems are notoriously challenging to predict because of their
instability. Small errors accumulate in the simulation of each time step,
resulting in completely different trajectories. However, the trajectories of
many prominent chaotic systems live in a low-dimensional subspace (attractor).
If the system is Markovian, the attractor is uniquely determined by the Markov
operator that maps the evolution of infinitesimal time steps. This makes it
possible to predict the behavior of the chaotic system by learning the Markov
operator even if we cannot predict the exact trajectory. Recently, a new
framework for learning resolution-invariant solution operators for PDEs was
proposed, known as neural operators. In this work, we train a Markov neural
operator (MNO) with only the local one-step evolution information. We then
compose the learned operator to obtain the global attractor and invariant
measure. Such a Markov neural operator forms a discrete semigroup and we
empirically observe that does not collapse or blow up. Experiments show neural
operators are more accurate and stable compared to previous methods on chaotic
systems such as the Kuramoto-Sivashinsky and Navier-Stokes equations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Event Outlier Detection in Continuous Time. (arXiv:1912.09522v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Siqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hauskrecht_M/0/1/0/all/0/1">Milos Hauskrecht</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.09522">
                                    <div class="article-summary-box-inner">
                                        <span>Continuous-time event sequences represent discrete events occurring in
continuous time. Such sequences arise frequently in real-life. Usually we
expect the sequences to follow some regular pattern over time. However,
sometimes these patterns may be interrupted by unexpected absence or
occurrences of events. Identification of these unexpected cases can be very
important as they may point to abnormal situations that need human attention.
In this work, we study and develop methods for detecting outliers in
continuous-time event sequences, including unexpected absence and unexpected
occurrences of events. Since the patterns that event sequences tend to follow
may change in different contexts, we develop outlier detection methods based on
point processes that can take context information into account. Our methods are
based on Bayesian decision theory and hypothesis testing with theoretical
guarantees. To test the performance of the methods, we conduct experiments on
both synthetic data and real-world clinical data and show the effectiveness of
the proposed methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Functorial Manifold Learning. (arXiv:2011.07435v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shiebler_D/0/1/0/all/0/1">Dan Shiebler</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07435">
                                    <div class="article-summary-box-inner">
                                        <span>We adapt previous research on category theory and topological unsupervised
learning to develop a functorial perspective on manifold learning. We first
characterize manifold learning algorithms as functors that map pseudometric
spaces to optimization objectives and factor through hierachical clustering
functors. We then use this characterization to prove refinement bounds on
manifold learning loss functions and construct a hierarchy of manifold learning
algorithms based on their invariants. We express several popular manifold
learning algorithms as functors at different levels of this hierarchy,
including Metric Multidimensional Scaling, IsoMap, and UMAP. Next, we use
interleaving distance to study the stability of a broad class of manifold
learning algorithms. We present bounds on how closely the embeddings these
algorithms produce from noisy data approximate the embeddings they would learn
from noiseless data. Finally, we use our framework to derive a set of novel
manifold learning algorithms, which we experimentally demonstrate are
competitive with the state of the art.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Understanding Iterative Magnitude Pruning: Why Lottery Tickets Win. (arXiv:2106.06955v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Maene_J/0/1/0/all/0/1">Jaron Maene</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingxiao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1">Marie-Francine Moens</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06955">
                                    <div class="article-summary-box-inner">
                                        <span>The lottery ticket hypothesis states that sparse subnetworks exist in
randomly initialized dense networks that can be trained to the same accuracy as
the dense network they reside in. However, the subsequent work has failed to
replicate this on large-scale models and required rewinding to an early stable
state instead of initialization. We show that by using a training method that
is stable with respect to linear mode connectivity, large networks can also be
entirely rewound to initialization. Our subsequent experiments on common vision
tasks give strong credence to the hypothesis in Evci et al. (2020b) that
lottery tickets simply retrain to the same regions (although not necessarily to
the same basin). These results imply that existing lottery tickets could not
have been found without the preceding dense training by iterative magnitude
pruning, raising doubts about the use of the lottery ticket hypothesis.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Confidential Machine Learning on Untrusted Platforms: A Survey. (arXiv:2012.08156v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1">Sagar Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Keke Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08156">
                                    <div class="article-summary-box-inner">
                                        <span>With the ever-growing data and the need for developing powerful machine
learning models, data owners increasingly depend on various untrusted platforms
(e.g., public clouds, edges, and machine learning service providers) for
scalable processing or collaborative learning. Thus, sensitive data and models
are in danger of unauthorized access, misuse, and privacy compromises. A
relatively new body of research confidentially trains machine learning models
on protected data to address these concerns. In this survey, we summarize
notable studies in this emerging area of research. With a unified framework, we
highlight the critical challenges and innovations in outsourcing machine
learning confidentially. We focus on the cryptographic approaches for
confidential machine learning (CML), primarily on model training, while also
covering other directions such as perturbation-based approaches and CML in the
hardware-assisted computing environment. The discussion will take a holistic
way to consider a rich context of the related threat models, security
assumptions, design principles, and associated trade-offs amongst data utility,
cost, and confidentiality.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning. (arXiv:2011.00382v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dong-Ki Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Miao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1">Matthew Riemer</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chuangchuang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdulhai_M/0/1/0/all/0/1">Marwa Abdulhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Habibi_G/0/1/0/all/0/1">Golnaz Habibi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_Cot_S/0/1/0/all/0/1">Sebastian Lopez-Cot</a>, <a href="http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1">Gerald Tesauro</a>, <a href="http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1">Jonathan P. How</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00382">
                                    <div class="article-summary-box-inner">
                                        <span>A fundamental challenge in multiagent reinforcement learning is to learn
beneficial behaviors in a shared environment with other simultaneously learning
agents. In particular, each agent perceives the environment as effectively
non-stationary due to the changing policies of other agents. Moreover, each
agent is itself constantly learning, leading to natural non-stationarity in the
distribution of experiences encountered. In this paper, we propose a novel
meta-multiagent policy gradient theorem that directly accounts for the
non-stationary policy dynamics inherent to multiagent learning settings. This
is achieved by modeling our gradient updates to consider both an agent&#x27;s own
non-stationary policy dynamics and the non-stationary policy dynamics of other
agents in the environment. We show that our theoretically grounded approach
provides a general solution to the multiagent learning problem, which
inherently comprises all key aspects of previous state of the art approaches on
this topic. We test our method on a diverse suite of multiagent benchmarks and
demonstrate a more efficient ability to adapt to new agents as they learn than
baseline methods across the full spectrum of mixed incentive, competitive, and
cooperative domains.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-Disease Classification of 13,667 Body CT Scans Using Weakly Supervised Deep Learning. (arXiv:2008.01158v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tushar_F/0/1/0/all/0/1">Fakrul Islam Tushar</a>, <a href="http://arxiv.org/find/cs/1/au:+DAnniballe_V/0/1/0/all/0/1">Vincent M. D&#x27;Anniballe</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1">Rui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1">Maciej A. Mazurowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1">Wanyi Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Samei_E/0/1/0/all/0/1">Ehsan Samei</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubin_G/0/1/0/all/0/1">Geoffrey D. Rubin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lo_J/0/1/0/all/0/1">Joseph Y. Lo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01158">
                                    <div class="article-summary-box-inner">
                                        <span>Background: Training deep learning classifiers typically requires massive
amounts of manual annotation. Weak supervision may leverage existing medical
data to classify multiple diseases and organ systems. Purpose: To design
multi-disease classifiers for body computed tomography (CT) scans using
automatically extracted labels from radiology text reports. Materials &amp;
Methods: This retrospective study deployed rule-based algorithms to extract
19,255 disease labels from reports of 13,667 body CT scans of 12,092 subjects
for training. Using a 3D DenseVNet, three organ systems were segmented:
lungs/pleura, liver/gallbladder, and kidneys/ureters. For each organ, a 3D
convolutional neural network classified normality versus four common diseases.
Testing was performed on an additional 2,158 CT volumes relative to 2,875
manually derived reference labels. Results: Manual validation of the extracted
labels confirmed 91 to 99% accuracy. Performance using the receiver operating
characteristic area under the curve (AUC) for lungs/pleura labels were as
follows: atelectasis 0.77 (95% CI: 0.74 to 0.81), nodule 0.65 (0.61 to 0.69),
emphysema 0.89 (0.86 to 0.92), effusion 0.97 (0.96 to 0.98), and normal 0.89
(0.87 to 0.91). For liver/gallbladder: stone 0.62 (0.56 to 0.67), lesion 0.73
(0.69 to 0.77), dilation 0.87 (0.84 to 0.90), fatty 0.89 (0.86 to 0.92), and
normal 0.82 (0.78 to 0.85). For kidneys/ureters: stone 0.83 (0.79 to 0.87),
atrophy 0.92 (0.89 to 0.94), lesion 0.68 (0.64 to 0.72), cyst 0.70 (0.66 to
0.73), and normal 0.79 (0.75 to 0.83). Conclusion: Weakly supervised deep
learning classifiers leveraged massive amounts of unannotated body CT data to
classify multiple organ systems and diverse diseases.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Game-Theoretic Approach to Multi-Agent Trust Region Optimization. (arXiv:2106.06828v1 [cs.MA])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Ying Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zheng Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Minne Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06828">
                                    <div class="article-summary-box-inner">
                                        <span>Trust region methods are widely applied in single-agent reinforcement
learning problems due to their monotonic performance-improvement guarantee at
every iteration. Nonetheless, when applied in multi-agent settings, the
guarantee of trust region methods no longer holds because an agent&#x27;s payoff is
also affected by other agents&#x27; adaptive behaviors. To tackle this problem, we
conduct a game-theoretical analysis in the policy space, and propose a
multi-agent trust region learning method (MATRL), which enables trust region
optimization for multi-agent learning. Specifically, MATRL finds a stable
improvement direction that is guided by the solution concept of Nash
equilibrium at the meta-game level. We derive the monotonic improvement
guarantee in multi-agent settings and empirically show the local convergence of
MATRL to stable fixed points in the two-player rotational differential game. To
test our method, we evaluate MATRL in both discrete and continuous multiplayer
general-sum games including checker and switch grid worlds, multi-agent MuJoCo,
and Atari games. Results suggest that MATRL significantly outperforms strong
multi-agent reinforcement learning baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Piecewise-constant Neural ODEs. (arXiv:2106.06621v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Greydanus_S/0/1/0/all/0/1">Sam Greydanus</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Stefan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1">Alan Fern</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06621">
                                    <div class="article-summary-box-inner">
                                        <span>Neural networks are a popular tool for modeling sequential data but they
generally do not treat time as a continuous variable. Neural ODEs represent an
important exception: they parameterize the time derivative of a hidden state
with a neural network and then integrate over arbitrary amounts of time. But
these parameterizations, which have arbitrary curvature, can be hard to
integrate and thus train and evaluate. In this paper, we propose making a
piecewise-constant approximation to Neural ODEs to mitigate these issues. Our
model can be integrated exactly via Euler integration and can generate
autoregressive samples in 3-20 times fewer steps than comparable RNN and
ODE-RNN models. We evaluate our model on several synthetic physics tasks and a
planning task inspired by the game of billiards. We find that it matches the
performance of baseline approaches while requiring less time to train and
evaluate.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Relearning ensemble selection based on new generated features. (arXiv:2106.06761v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Burduk_R/0/1/0/all/0/1">Robert Burduk</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06761">
                                    <div class="article-summary-box-inner">
                                        <span>The ensemble methods are meta-algorithms that combine several base machine
learning techniques to increase the effectiveness of the classification. Many
existing committees of classifiers use the classifier selection process to
determine the optimal set of base classifiers. In this article, we propose the
classifiers selection framework with relearning base classifiers. Additionally,
we use in the proposed framework the new generated feature, which can be
obtained after the relearning process. The proposed technique was compared with
state-of-the-art ensemble methods using three benchmark datasets and one
synthetic dataset. Four classification performance measures are used to
evaluate the proposed method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Machine Unlearning for Random Forests. (arXiv:2009.05567v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Brophy_J/0/1/0/all/0/1">Jonathan Brophy</a>, <a href="http://arxiv.org/find/cs/1/au:+Lowd_D/0/1/0/all/0/1">Daniel Lowd</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.05567">
                                    <div class="article-summary-box-inner">
                                        <span>Responding to user data deletion requests, removing noisy examples, or
deleting corrupted training data are just a few reasons for wanting to delete
instances from a machine learning (ML) model. However, efficiently removing
this data from an ML model is generally difficult. In this paper, we introduce
data removal-enabled (DaRE) forests, a variant of random forests that enables
the removal of training data with minimal retraining. Model updates for each
DaRE tree in the forest are exact, meaning that removing instances from a DaRE
model yields exactly the same model as retraining from scratch on updated data.

DaRE trees use randomness and caching to make data deletion efficient. The
upper levels of DaRE trees use random nodes, which choose split attributes and
thresholds uniformly at random. These nodes rarely require updates because they
only minimally depend on the data. At the lower levels, splits are chosen to
greedily optimize a split criterion such as Gini index or mutual information.
DaRE trees cache statistics at each node and training data at each leaf, so
that only the necessary subtrees are updated as data is removed. For numerical
attributes, greedy nodes optimize over a random subset of thresholds, so that
they can maintain statistics while approximating the optimal threshold. By
adjusting the number of thresholds considered for greedy nodes, and the number
of random nodes, DaRE trees can trade off between more accurate predictions and
more efficient updates.

In experiments on 13 real-world datasets and one synthetic dataset, we find
DaRE forests delete data orders of magnitude faster than retraining from
scratch while sacrificing little to no predictive power.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online Learning and Distributed Control for Residential Demand Response. (arXiv:2010.05153v2 [eess.SY] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xin Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Yingying Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Shimada_J/0/1/0/all/0/1">Jun Shimada</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1">Na Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05153">
                                    <div class="article-summary-box-inner">
                                        <span>This paper studies the automated control method for regulating air
conditioner (AC) loads in incentive-based residential demand response (DR). The
critical challenge is that the customer responses to load adjustment are
uncertain and unknown in practice. In this paper, we formulate the AC control
problem in a DR event as a multi-period stochastic optimization that integrates
the indoor thermal dynamics and customer opt-out status transition.
Specifically, machine learning techniques including Gaussian process and
logistic regression are employed to learn the unknown thermal dynamics model
and customer opt-out behavior model, respectively. We consider two typical DR
objectives for AC load control: 1) minimizing the total demand, 2) closely
tracking a regulated power trajectory. Based on the Thompson sampling
framework, we propose an online DR control algorithm to learn customer
behaviors and make real-time AC control schemes. This algorithm considers the
influence of various environmental factors on customer behaviors and is
implemented in a distributed fashion to preserve the privacy of customers.
Numerical simulations demonstrate the control optimality and learning
efficiency of the proposed algorithm.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Distribution-free uncertainty quantification for classification under label shift. (arXiv:2103.03323v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1">Aleksandr Podkopaev</a>, <a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1">Aaditya Ramdas</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03323">
                                    <div class="article-summary-box-inner">
                                        <span>Trustworthy deployment of ML models requires a proper measure of uncertainty,
especially in safety-critical applications. We focus on uncertainty
quantification (UQ) for classification problems via two avenues -- prediction
sets using conformal prediction and calibration of probabilistic predictors by
post-hoc binning -- since these possess distribution-free guarantees for i.i.d.
data. Two common ways of generalizing beyond the i.i.d. setting include
handling covariate and label shift. Within the context of distribution-free UQ,
the former has already received attention, but not the latter. It is known that
label shift hurts prediction, and we first argue that it also hurts UQ, by
showing degradation in coverage and calibration. Piggybacking on recent
progress in addressing label shift (for better prediction), we examine the
right way to achieve UQ by reweighting the aforementioned conformal and
calibration procedures whenever some unlabeled data from the target
distribution is available. We examine these techniques theoretically in a
distribution-free framework and demonstrate their excellent practical
performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adaptive Dynamic Pruning for Non-IID Federated Learning. (arXiv:2106.06921v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Sixing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1">Phuong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1">Ali Anwar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1">Ali Jannesari</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06921">
                                    <div class="article-summary-box-inner">
                                        <span>Federated Learning~(FL) has emerged as a new paradigm of training machine
learning models without sacrificing data security and privacy. Learning models
at edge devices such as cell phones is one of the most common use case of FL.
However, the limited computing power and energy constraints of edge devices
hinder the adoption of FL for both model training and deployment, especially
for the resource-hungry Deep Neural Networks~(DNNs). To this end, many model
compression methods have been proposed and network pruning is among the most
well-known. However, a pruning policy for a given model is highly
dataset-dependent, which is not suitable for non-Independent and Identically
Distributed~(Non-IID) FL edge devices. In this paper, we present an adaptive
pruning scheme for edge devices in an FL system, which applies dataset-aware
dynamic pruning for inference acceleration on Non-IID datasets. Our evaluation
shows that the proposed method accelerates inference by $2\times$~($50\%$ FLOPs
reduction) while maintaining the model&#x27;s quality on edge devices.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Case Study on Detecting COVID-19 Health-Related Misinformation in Social Media. (arXiv:2106.06811v1 [cs.SI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pritom_M/0/1/0/all/0/1">Mir Mehedi A. Pritom</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_R/0/1/0/all/0/1">Rosana Montanez Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Asad Ali Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nugroho_S/0/1/0/all/0/1">Sebastian A. Nugroho</a>, <a href="http://arxiv.org/find/cs/1/au:+Alrashydah_E/0/1/0/all/0/1">Esra&#x27;a Alrashydah</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruiz_B/0/1/0/all/0/1">Beatrice N. Ruiz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1">Anthony Rios</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06811">
                                    <div class="article-summary-box-inner">
                                        <span>COVID-19 pandemic has generated what public health officials called an
infodemic of misinformation. As social distancing and stay-at-home orders came
into effect, many turned to social media for socializing. This increase in
social media usage has made it a prime vehicle for the spreading of
misinformation. This paper presents a mechanism to detect COVID-19
health-related misinformation in social media following an interdisciplinary
approach. Leveraging social psychology as a foundation and existing
misinformation frameworks, we defined misinformation themes and associated
keywords incorporated into the misinformation detection mechanism using applied
machine learning techniques. Next, using the Twitter dataset, we explored the
performance of the proposed methodology using multiple state-of-the-art machine
learning classifiers. Our method shows promising results with at most 78%
accuracy in classifying health-related misinformation versus true information
using uni-gram-based NLP feature generations from tweets and the Decision Tree
classifier. We also provide suggestions on alternatives for countering
misinformation and ethical consideration for the study.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Variational Auto-Regressive Gaussian Processes for Continual Learning. (arXiv:2006.05468v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Kapoor_S/0/1/0/all/0/1">Sanyam Kapoor</a>, <a href="http://arxiv.org/find/stat/1/au:+Karaletsos_T/0/1/0/all/0/1">Theofanis Karaletsos</a>, <a href="http://arxiv.org/find/stat/1/au:+Bui_T/0/1/0/all/0/1">Thang D. Bui</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.05468">
                                    <div class="article-summary-box-inner">
                                        <span>Through sequential construction of posteriors on observing data online,
Bayes&#x27; theorem provides a natural framework for continual learning. We develop
Variational Auto-Regressive Gaussian Processes (VAR-GPs), a principled
posterior updating mechanism to solve sequential tasks in continual learning.
By relying on sparse inducing point approximations for scalable posteriors, we
propose a novel auto-regressive variational distribution which reveals two
fruitful connections to existing results in Bayesian inference, expectation
propagation and orthogonal inducing points. Mean predictive entropy estimates
show VAR-GPs prevent catastrophic forgetting, which is empirically supported by
strong performance on modern continual learning benchmarks against competitive
baselines. A thorough ablation study demonstrates the efficacy of our modeling
choices.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">COHORTNEY: Non-Parametric Clustering of Event Sequences. (arXiv:2104.01440v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhuzhel_V/0/1/0/all/0/1">Vladislav Zhuzhel</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivera_Castro_R/0/1/0/all/0/1">Rodrigo Rivera-Castro</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaploukhaya_N/0/1/0/all/0/1">Nina Kaploukhaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Mironova_L/0/1/0/all/0/1">Liliya Mironova</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1">Alexey Zaytsev</a>, <a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1">Evgeny Burnaev</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01440">
                                    <div class="article-summary-box-inner">
                                        <span>Cohort analysis is a pervasive activity in web analytics. One divides users
into groups according to specific criteria and tracks their behavior over time.
Despite its extensive use, academic circles do not discuss cohort analysis to
evaluate user behavior online. This work introduces an unsupervised
non-parametric approach to group Internet users based on their activities. In
comparison, canonical methods in marketing and engineering-based techniques
underperform. COHORTNEY is the first machine learning-based cohort analysis
algorithm with a robust theoretical explanation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning from Crowds by Modeling Common Confusions. (arXiv:2012.13052v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1">Zhendong Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jing Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongning Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13052">
                                    <div class="article-summary-box-inner">
                                        <span>Crowdsourcing provides a practical way to obtain large amounts of labeled
data at a low cost. However, the annotation quality of annotators varies
considerably, which imposes new challenges in learning a high-quality model
from the crowdsourced annotations. In this work, we provide a new perspective
to decompose annotation noise into common noise and individual noise and
differentiate the source of confusion based on instance difficulty and
annotator expertise on a per-instance-annotator basis. We realize this new
crowdsourcing model by an end-to-end learning solution with two types of noise
adaptation layers: one is shared across annotators to capture their commonly
shared confusions, and the other one is pertaining to each annotator to realize
individual confusion. To recognize the source of noise in each annotation, we
use an auxiliary network to choose the two noise adaptation layers with respect
to both instances and annotators. Extensive experiments on both synthesized and
real-world benchmarks demonstrate the effectiveness of our proposed common
noise adaptation solution.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial Robustness via Fisher-Rao Regularization. (arXiv:2106.06685v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Picot_M/0/1/0/all/0/1">Marine Picot</a>, <a href="http://arxiv.org/find/cs/1/au:+Messina_F/0/1/0/all/0/1">Francisco Messina</a>, <a href="http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1">Malik Boudiaf</a>, <a href="http://arxiv.org/find/cs/1/au:+Labeau_F/0/1/0/all/0/1">Fabrice Labeau</a>, <a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1">Ismail Ben Ayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1">Pablo Piantanida</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06685">
                                    <div class="article-summary-box-inner">
                                        <span>Adversarial robustness has become a topic of growing interest in machine
learning since it was observed that neural networks tend to be brittle. We
propose an information-geometric formulation of adversarial defense and
introduce FIRE, a new Fisher-Rao regularization for the categorical
cross-entropy loss, which is based on the geodesic distance between natural and
perturbed input features. Based on the information-geometric properties of the
class of softmax distributions, we derive an explicit characterization of the
Fisher-Rao Distance (FRD) for the binary and multiclass cases, and draw some
interesting properties as well as connections with standard regularization
metrics. Furthermore, for a simple linear and Gaussian model, we show that all
Pareto-optimal points in the accuracy-robustness region can be reached by FIRE
while other state-of-the-art methods fail. Empirically, we evaluate the
performance of various classifiers trained with the proposed loss on standard
datasets, showing up to 2\% of improvements in terms of robustness while
reducing the training time by 20\% over the best-performing methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Federated Learning with Spiking Neural Networks. (arXiv:2106.06579v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Venkatesha_Y/0/1/0/all/0/1">Yeshwanth Venkatesha</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Youngeun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Tassiulas_L/0/1/0/all/0/1">Leandros Tassiulas</a>, <a href="http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1">Priyadarshini Panda</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06579">
                                    <div class="article-summary-box-inner">
                                        <span>As neural networks get widespread adoption in resource-constrained embedded
devices, there is a growing need for low-power neural systems. Spiking Neural
Networks (SNNs)are emerging to be an energy-efficient alternative to the
traditional Artificial Neural Networks (ANNs) which are known to be
computationally intensive. From an application perspective, as federated
learning involves multiple energy-constrained devices, there is a huge scope to
leverage energy efficiency provided by SNNs. Despite its importance, there has
been little attention on training SNNs on a large-scale distributed system like
federated learning. In this paper, we bring SNNs to a more realistic federated
learning scenario. Specifically, we propose a federated learning framework for
decentralized and privacy-preserving training of SNNs. To validate the proposed
federated learning framework, we experimentally evaluate the advantages of SNNs
on various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks.
We observe that SNNs outperform ANNs in terms of overall accuracy by over 15%
when the data is distributed across a large number of clients in the federation
while providing up to5.3x energy efficiency. In addition to efficiency, we also
analyze the sensitivity of the proposed federated SNN framework to data
distribution among the clients, stragglers, and gradient noise and perform a
comprehensive comparison with ANNs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning disentangled representations via product manifold projection. (arXiv:2103.01638v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fumero_M/0/1/0/all/0/1">Marco Fumero</a>, <a href="http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1">Luca Cosmo</a>, <a href="http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1">Simone Melzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1">Emanuele Rodol&#xe0;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01638">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a novel approach to disentangle the generative factors of
variation underlying a given set of observations. Our method builds upon the
idea that the (unknown) low-dimensional manifold underlying the data space can
be explicitly modeled as a product of submanifolds. This definition of
disentanglement gives rise to a novel weakly-supervised algorithm for
recovering the unknown explanatory factors behind the data. At training time,
our algorithm only requires pairs of non i.i.d. data samples whose elements
share at least one, possibly multidimensional, generative factor of variation.
We require no knowledge on the nature of these transformations, and do not make
any limiting assumption on the properties of each subspace. Our approach is
easy to implement, and can be successfully applied to different kinds of data
(from images to 3D surfaces) undergoing arbitrary transformations. In addition
to standard synthetic benchmarks, we showcase our method in challenging
real-world applications, where we compare favorably with the state of the art.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A fast randomized incremental gradient method for decentralized non-convex optimization. (arXiv:2011.03853v2 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Xin_R/0/1/0/all/0/1">Ran Xin</a>, <a href="http://arxiv.org/find/math/1/au:+Khan_U/0/1/0/all/0/1">Usman A. Khan</a>, <a href="http://arxiv.org/find/math/1/au:+Kar_S/0/1/0/all/0/1">Soummya Kar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.03853">
                                    <div class="article-summary-box-inner">
                                        <span>We study decentralized non-convex finite-sum minimization problems described
over a network of nodes, where each node possesses a local batch of data
samples. In this context, we analyze a single-timescale randomized incremental
gradient method, called GT-SAGA. GT-SAGA is computationally efficient as it
evaluates one component gradient per node per iteration and achieves provably
fast and robust performance by leveraging node-level variance reduction and
network-level gradient tracking. For general smooth non-convex problems, we
show the almost sure and mean-squared convergence of GT-SAGA to a first-order
stationary point and further describe regimes of practical significance where
it outperforms the existing approaches and achieves a network
topology-independent iteration complexity respectively. When the global
function satisfies the Polyak-Lojaciewisz condition, we show that GT-SAGA
exhibits linear convergence to an optimal solution in expectation and describe
regimes of practical interest where the performance is network
topology-independent and improves upon the existing methods. Numerical
experiments are included to highlight the main convergence aspects of GT-SAGA
in non-convex settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1">Pietro Barbiero</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1">Gabriele Ciravegna</a>, <a href="http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1">Francesco Giannini</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf3;</a>, <a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1">Marco Gori</a>, <a href="http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1">Stefano Melacci</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06804">
                                    <div class="article-summary-box-inner">
                                        <span>Explainable artificial intelligence has rapidly emerged since lawmakers have
started requiring interpretable models for safety-critical domains.
Concept-based neural networks have arisen as explainable-by-design methods as
they leverage human-understandable symbols (i.e. concepts) to predict class
memberships. However, most of these approaches focus on the identification of
the most relevant concepts but do not provide concise, formal explanations of
how such concepts are leveraged by the classifier to make predictions. In this
paper, we propose a novel end-to-end differentiable approach enabling the
extraction of logic explanations from neural networks using the formalism of
First-Order Logic. The method relies on an entropy-based criterion which
automatically identifies the most relevant concepts. We consider four different
case studies to demonstrate that: (i) this entropy-based criterion enables the
distillation of concise logic explanations in safety-critical domains from
clinical data to computer vision; (ii) the proposed approach outperforms
state-of-the-art white-box models in terms of classification accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">BRAIN2DEPTH: Lightweight CNN Model for Classification of Cognitive States from EEG Recordings. (arXiv:2106.06688v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1">Pankaj Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Miyapuram_K/0/1/0/all/0/1">Krishna Prasad Miyapuram</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06688">
                                    <div class="article-summary-box-inner">
                                        <span>Several Convolutional Deep Learning models have been proposed to classify the
cognitive states utilizing several neuro-imaging domains. These models have
achieved significant results, but they are heavily designed with millions of
parameters, which increases train and test time, making the model complex and
less suitable for real-time analysis. This paper proposes a simple, lightweight
CNN model to classify cognitive states from Electroencephalograph (EEG)
recordings. We develop a novel pipeline to learn distinct cognitive
representation consisting of two stages. The first stage is to generate the 2D
spectral images from neural time series signals in a particular frequency band.
Images are generated to preserve the relationship between the neighboring
electrodes and the spectral property of the cognitive events. The second is to
develop a time-efficient, computationally less loaded, and high-performing
model. We design a network containing 4 blocks and major components include
standard and depth-wise convolution for increasing the performance and followed
by separable convolution to decrease the number of parameters which maintains
the tradeoff between time and performance. We experiment on open access EEG
meditation dataset comprising expert, nonexpert meditative, and control states.
We compare performance with six commonly used machine learning classifiers and
four state of the art deep learning models. We attain comparable performance
utilizing less than 4\% of the parameters of other models. This model can be
employed in a real-time computation environment such as neurofeedback.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Memory-efficient Transformers via Top-$k$ Attention. (arXiv:2106.06899v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Ankit Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1">Guy Dar</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_S/0/1/0/all/0/1">Shaya Goodman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciprut_D/0/1/0/all/0/1">David Ciprut</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06899">
                                    <div class="article-summary-box-inner">
                                        <span>Following the success of dot-product attention in Transformers, numerous
approximations have been recently proposed to address its quadratic complexity
with respect to the input length. While these variants are memory and compute
efficient, it is not possible to directly use them with popular pre-trained
language models trained using vanilla attention, without an expensive
corrective pre-training stage. In this work, we propose a simple yet highly
accurate approximation for vanilla attention. We process the queries in chunks,
and for each query, compute the top-$k$ scores with respect to the keys. Our
approach offers several advantages: (a) its memory usage is linear in the input
size, similar to linear attention variants, such as Performer and RFA (b) it is
a drop-in replacement for vanilla attention that does not require any
corrective pre-training, and (c) it can also lead to significant memory savings
in the feed-forward layers after casting them into the familiar query-key-value
framework. We evaluate the quality of top-$k$ approximation for multi-head
attention layers on the Long Range Arena Benchmark, and for feed-forward layers
of T5 and UnifiedQA on multiple QA datasets. We show our approach leads to
accuracy that is nearly-identical to vanilla attention in multiple setups
including training from scratch, fine-tuning, and zero-shot inference.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Speak or Chat with Me: End-to-End Spoken Language Understanding System with Flexible Inputs. (arXiv:2104.05752v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1">Sujeong Cha</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1">Wangrui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1">Hyun Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Phung_M/0/1/0/all/0/1">My Phung</a>, <a href="http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1">Michael Picheny</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1">Hong-Kwang Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1">Samuel Thomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Morais_E/0/1/0/all/0/1">Edmilson Morais</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05752">
                                    <div class="article-summary-box-inner">
                                        <span>A major focus of recent research in spoken language understanding (SLU) has
been on the end-to-end approach where a single model can predict intents
directly from speech inputs without intermediate transcripts. However, this
approach presents some challenges. First, since speech can be considered as
personally identifiable information, in some cases only automatic speech
recognition (ASR) transcripts are accessible. Second, intent-labeled speech
data is scarce. To address the first challenge, we propose a novel system that
can predict intents from flexible types of inputs: speech, ASR transcripts, or
both. We demonstrate strong performance for either modality separately, and
when both speech and ASR transcripts are available, through system combination,
we achieve better results than using a single input modality. To address the
second challenge, we leverage a semantically robust pre-trained BERT model and
adopt a cross-modal system that co-trains text embeddings and acoustic
embeddings in a shared latent space. We further enhance this system by
utilizing an acoustic module pre-trained on LibriSpeech and domain-adapting the
text module on our target datasets. Our experiments show significant advantages
for these pre-training and fine-tuning strategies, resulting in a system that
achieves competitive intent-classification performance on Snips SLU and Fluent
Speech Commands datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1">Seyed Saeed Changiz Rezaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1">Fred X. Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1">Di Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1">Mohammad Salameh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1">Keith Mills</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1">Shuo Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1">Wei Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1">Shangling Jui</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09356">
                                    <div class="article-summary-box-inner">
                                        <span>Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data-driven Prediction of General Hamiltonian Dynamics via Learning Exactly-Symplectic Maps. (arXiv:2103.05632v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Renyi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1">Molei Tao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05632">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the learning and prediction of nonlinear time series generated by
a latent symplectic map. A special case is (not necessarily separable)
Hamiltonian systems, whose solution flows give such symplectic maps. For this
special case, both generic approaches based on learning the vector field of the
latent ODE and specialized approaches based on learning the Hamiltonian that
generates the vector field exist. Our method, however, is different as it does
not rely on the vector field nor assume its existence; instead, it directly
learns the symplectic evolution map in discrete time. Moreover, we do so by
representing the symplectic map via a generating function, which we approximate
by a neural network (hence the name GFNN). This way, our approximation of the
evolution map is always \emph{exactly} symplectic. This additional geometric
structure allows the local prediction error at each step to accumulate in a
controlled fashion, and we will prove, under reasonable assumptions, that the
global prediction error grows at most \emph{linearly} with long prediction
time, which significantly improves an otherwise exponential growth. In
addition, as a map-based and thus purely data-driven method, GFNN avoids two
additional sources of inaccuracies common in vector-field based approaches,
namely the error in approximating the vector field by finite difference of the
data, and the error in numerical integration of the vector field for making
predictions. Numerical experiments further demonstrate our claims.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CoPE: Conditional image generation using Polynomial Expansions. (arXiv:2104.05077v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1">Grigorios G Chrysos</a>, <a href="http://arxiv.org/find/cs/1/au:+Georgopoulos_M/0/1/0/all/0/1">Markos Georgopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1">Yannis Panagakis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05077">
                                    <div class="article-summary-box-inner">
                                        <span>Generative modeling has evolved to a notable field of machine learning. Deep
polynomial neural networks (PNNs) have demonstrated impressive results in
unsupervised image generation, where the task is to map an input vector (i.e.,
noise) to a synthesized image. However, the success of PNNs has not been
replicated in conditional generation tasks, such as super-resolution. Existing
PNNs focus on single-variable polynomial expansions which do not fare well to
two-variable inputs, i.e., the noise variable and the conditional variable. In
this work, we introduce a general framework, called CoPE, that enables a
polynomial expansion of two input variables and captures their auto- and
cross-correlations. We exhibit how CoPE can be trivially augmented to accept an
arbitrary number of input variables. CoPE is evaluated in five tasks
(class-conditional generation, inverse problems, edges-to-image translation,
image-to-image translation, attribute-guided generation) involving eight
datasets. The thorough evaluation suggests that CoPE can be useful for tackling
diverse conditional generation tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning-based Robust Motion Planning with Guaranteed Stability: A Contraction Theory Approach. (arXiv:2102.12668v3 [cs.RO] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tsukamoto_H/0/1/0/all/0/1">Hiroyasu Tsukamoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1">Soon-Jo Chung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12668">
                                    <div class="article-summary-box-inner">
                                        <span>This paper presents Learning-based Autonomous Guidance with RObustness and
Stability guarantees (LAG-ROS), which provides machine learning-based nonlinear
motion planners with formal robustness and stability guarantees, by designing a
differential Lyapunov function using contraction theory. LAG-ROS utilizes a
neural network to model a robust tracking controller independently of a target
trajectory, for which we show that the Euclidean distance between the target
and controlled trajectories is exponentially bounded linearly in the learning
error, even under the existence of bounded external disturbances. We also
present a convex optimization approach that minimizes the steady-state bound of
the tracking error to construct the robust control law for neural network
training. In numerical simulations, it is demonstrated that the proposed method
indeed possesses superior properties of robustness and nonlinear stability
resulting from contraction theory, whilst retaining the computational
efficiency of existing learning-based motion planners.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Strategic Classification Made Practical. (arXiv:2103.01826v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Levanon_S/0/1/0/all/0/1">Sagi Levanon</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenfeld_N/0/1/0/all/0/1">Nir Rosenfeld</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01826">
                                    <div class="article-summary-box-inner">
                                        <span>Strategic classification regards the problem of learning in settings where
users can strategically modify their features to improve outcomes. This setting
applies broadly and has received much recent attention. But despite its
practical significance, work in this space has so far been predominantly
theoretical. In this paper we present a learning framework for strategic
classification that is practical. Our approach directly minimizes the
&quot;strategic&quot; empirical risk, achieved by differentiating through the strategic
response of users. This provides flexibility that allows us to extend beyond
the original problem formulation and towards more realistic learning scenarios.
A series of experiments demonstrates the effectiveness of our approach on
various learning settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fre-GAN: Adversarial Frequency-consistent Audio Synthesis. (arXiv:2106.02297v2 [eess.AS] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1">Ji-Hoon Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1">Sang-Hoon Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1">Ji-Hyun Lee</a>, <a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02297">
                                    <div class="article-summary-box-inner">
                                        <span>Although recent works on neural vocoder have improved the quality of
synthesized audio, there still exists a gap between generated and ground-truth
audio in frequency space. This difference leads to spectral artifacts such as
hissing noise or reverberation, and thus degrades the sample quality. In this
paper, we propose Fre-GAN which achieves frequency-consistent audio synthesis
with highly improved generation quality. Specifically, we first present
resolution-connected generator and resolution-wise discriminators, which help
learn various scales of spectral distributions over multiple frequency bands.
Additionally, to reproduce high-frequency components accurately, we leverage
discrete wavelet transform in the discriminators. From our experiments, Fre-GAN
achieves high-fidelity waveform generation with a gap of only 0.03 MOS compared
to ground-truth audio while outperforming standard models in quality.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Complexity of Linear Minimization and Projection on Some Sets. (arXiv:2101.10040v2 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Combettes_C/0/1/0/all/0/1">Cyrille W. Combettes</a>, <a href="http://arxiv.org/find/math/1/au:+Pokutta_S/0/1/0/all/0/1">Sebastian Pokutta</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10040">
                                    <div class="article-summary-box-inner">
                                        <span>The Frank-Wolfe algorithm is a method for constrained optimization that
relies on linear minimizations, as opposed to projections. Therefore, a
motivation put forward in a large body of work on the Frank-Wolfe algorithm is
the computational advantage of solving linear minimizations instead of
projections. However, the discussions supporting this advantage are often too
succinct or incomplete. In this paper, we review the complexity bounds for both
tasks on several sets commonly used in optimization. Projection methods onto
the $\ell_p$-ball, $p\in\left]1,2\right[\cup\left]2,+\infty\right[$, and the
Birkhoff polytope are also proposed.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CATE: Computation-aware Neural Architecture Encoding with Transformers. (arXiv:2102.07108v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shen Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kaiqiang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mi Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07108">
                                    <div class="article-summary-box-inner">
                                        <span>Recent works (White et al., 2020a; Yan et al., 2020) demonstrate the
importance of architecture encodings in Neural Architecture Search (NAS). These
encodings encode either structure or computation information of the neural
architectures. Compared to structure-aware encodings, computation-aware
encodings map architectures with similar accuracies to the same region, which
improves the downstream architecture search performance (Zhang et al., 2019;
White et al., 2020a). In this work, we introduce a Computation-Aware
Transformer-based Encoding method called CATE. Different from existing
computation-aware encodings based on fixed transformation (e.g. path encoding),
CATE employs a pairwise pre-training scheme to learn computation-aware
encodings using Transformers with cross-attention. Such learned encodings
contain dense and contextualized computation information of neural
architectures. We compare CATE with eleven encodings under three major
encoding-dependent NAS subroutines in both small and large search spaces. Our
experiments show that CATE is beneficial to the downstream search, especially
in the large search space. Moreover, the outside search space experiment
demonstrates its superior generalization ability beyond the search space on
which it was trained. Our code is available at:
https://github.com/MSU-MLSys-Lab/CATE.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Noise Transition Matrix from Only Noisy Labels via Total Variation Regularization. (arXiv:2102.02414v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1">Yivan Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Niu_G/0/1/0/all/0/1">Gang Niu</a>, <a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1">Masashi Sugiyama</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02414">
                                    <div class="article-summary-box-inner">
                                        <span>Many weakly supervised classification methods employ a noise transition
matrix to capture the class-conditional label corruption. To estimate the
transition matrix from noisy data, existing methods often need to estimate the
noisy class-posterior, which could be unreliable due to the overconfidence of
neural networks. In this work, we propose a theoretically grounded method that
can estimate the noise transition matrix and learn a classifier simultaneously,
without relying on the error-prone noisy class-posterior estimation.
Concretely, inspired by the characteristics of the stochastic label corruption
process, we propose total variation regularization, which encourages the
predicted probabilities to be more distinguishable from each other. Under mild
assumptions, the proposed method yields a consistent estimator of the
transition matrix. We show the effectiveness of the proposed method through
experiments on benchmark and real-world datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scaling Multi-Agent Reinforcement Learning with Selective Parameter Sharing. (arXiv:2102.07475v2 [cs.MA] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1">Filippos Christianos</a>, <a href="http://arxiv.org/find/cs/1/au:+Papoudakis_G/0/1/0/all/0/1">Georgios Papoudakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1">Arrasy Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1">Stefano V. Albrecht</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07475">
                                    <div class="article-summary-box-inner">
                                        <span>Sharing parameters in multi-agent deep reinforcement learning has played an
essential role in allowing algorithms to scale to a large number of agents.
Parameter sharing between agents significantly decreases the number of
trainable parameters, shortening training times to tractable levels, and has
been linked to more efficient learning. However, having all agents share the
same parameters can also have a detrimental effect on learning. We demonstrate
the impact of parameter sharing methods on training speed and converged
returns, establishing that when applied indiscriminately, their effectiveness
is highly dependent on the environment. We propose a novel method to
automatically identify agents which may benefit from sharing parameters by
partitioning them based on their abilities and goals. Our approach combines the
increased sample efficiency of parameter sharing with the representational
capacity of multiple independent networks to reduce training time and increase
final returns.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue. (arXiv:2101.00151v2 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Hung Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1">Chinnadhurai Sankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1">Seungwhan Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1">Ahmad Beirami</a>, <a href="http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1">Alborz Geramifard</a>, <a href="http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1">Satwik Kottur</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00151">
                                    <div class="article-summary-box-inner">
                                        <span>A video-grounded dialogue system is required to understand both dialogue,
which contains semantic dependencies from turn to turn, and video, which
contains visual cues of spatial and temporal scene variations. Building such
dialogue systems is a challenging problem, involving various reasoning types on
both visual and language inputs. Existing benchmarks do not have enough
annotations to thoroughly analyze dialogue systems and understand their
capabilities and limitations in isolation. These benchmarks are also not
explicitly designed to minimise biases that models can exploit without actual
reasoning. To address these limitations, in this paper, we present DVD, a
Diagnostic Dataset for Video-grounded Dialogues. The dataset is designed to
contain minimal biases and has detailed annotations for the different types of
reasoning over the spatio-temporal space of video. Dialogues are synthesized
over multiple question turns, each of which is injected with a set of
cross-turn semantic relationships. We use DVD to analyze existing approaches,
providing interesting insights into their abilities and limitations. In total,
DVD is built from $11k$ CATER synthetic videos and contains $10$ instances of
$10$-round dialogues for each video, resulting in more than $100k$ dialogues
and $1M$ question-answer pairs. Our code and dataset are publicly available at
https://github.com/facebookresearch/DVDialogues.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Precise characterization of the prior predictive distribution of deep ReLU networks. (arXiv:2106.06615v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Noci_L/0/1/0/all/0/1">Lorenzo Noci</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachmann_G/0/1/0/all/0/1">Gregor Bachmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1">Kevin Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowozin_S/0/1/0/all/0/1">Sebastian Nowozin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1">Thomas Hofmann</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06615">
                                    <div class="article-summary-box-inner">
                                        <span>Recent works on Bayesian neural networks (BNNs) have highlighted the need to
better understand the implications of using Gaussian priors in combination with
the compositional structure of the network architecture. Similar in spirit to
the kind of analysis that has been developed to devise better initialization
schemes for neural networks (cf. He- or Xavier initialization), we derive a
precise characterization of the prior predictive distribution of finite-width
ReLU networks with Gaussian weights. While theoretical results have been
obtained for their heavy-tailedness, the full characterization of the prior
predictive distribution (i.e. its density, CDF and moments), remained unknown
prior to this work. Our analysis, based on the Meijer-G function, allows us to
quantify the influence of architectural choices such as the width or depth of
the network on the resulting shape of the prior predictive distribution. We
also formally connect our results to previous work in the infinite width
setting, demonstrating that the moments of the distribution converge to those
of a normal log-normal mixture in the infinite depth limit. Finally, our
results provide valuable guidance on prior design: for instance, controlling
the predictive variance with depth- and width-informed priors on the weights of
the network.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online Learning of Competitive Equilibria in Exchange Economies. (arXiv:2106.06616v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1">Wenshuo Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kandasamy_K/0/1/0/all/0/1">Kirthevasan Kandasamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1">Joseph E Gonzalez</a>, <a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1">Michael I. Jordan</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1">Ion Stoica</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06616">
                                    <div class="article-summary-box-inner">
                                        <span>The sharing of scarce resources among multiple rational agents is one of the
classical problems in economics. In exchange economies, which are used to model
such situations, agents begin with an initial endowment of resources and
exchange them in a way that is mutually beneficial until they reach a
competitive equilibrium (CE). CE allocations are Pareto efficient and fair.
Consequently, they are used widely in designing mechanisms for fair division.
However, computing CEs requires the knowledge of agent preferences which are
unknown in several applications of interest. In this work, we explore a new
online learning mechanism, which, on each round, allocates resources to the
agents and collects stochastic feedback on their experience in using that
allocation. Its goal is to learn the agent utilities via this feedback and
imitate the allocations at a CE in the long run. We quantify CE behavior via
two losses and propose a randomized algorithm which achieves
$\bigOtilde(\sqrt{T})$ loss after $T$ rounds under both criteria. Empirically,
we demonstrate the effectiveness of this mechanism through numerical
simulations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Game of GANs: Game Theoretical Models for Generative Adversarial Networks. (arXiv:2106.06976v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Moghadam_M/0/1/0/all/0/1">Monireh Mohebbi Moghadam</a>, <a href="http://arxiv.org/find/cs/1/au:+Boroumand_B/0/1/0/all/0/1">Bahar Boroumand</a>, <a href="http://arxiv.org/find/cs/1/au:+Jalali_M/0/1/0/all/0/1">Mohammad Jalali</a>, <a href="http://arxiv.org/find/cs/1/au:+Zareian_A/0/1/0/all/0/1">Arman Zareian</a>, <a href="http://arxiv.org/find/cs/1/au:+Javad_A/0/1/0/all/0/1">Alireza Daei Javad</a>, <a href="http://arxiv.org/find/cs/1/au:+Manshaei_M/0/1/0/all/0/1">Mohammad Hossein Manshaei</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06976">
                                    <div class="article-summary-box-inner">
                                        <span>Generative Adversarial Network, as a promising research direction in the AI
community, recently attracts considerable attention due to its ability to
generating high-quality realistic data. GANs are a competing game between two
neural networks trained in an adversarial manner to reach a Nash equilibrium.
Despite the improvement accomplished in GANs in the last years, there remain
several issues to solve. In this way, how to tackle these issues and make
advances leads to rising research interests. This paper reviews literature that
leverages the game theory in GANs and addresses how game models can relieve
specific generative models&#x27; challenges and improve the GAN&#x27;s performance. In
particular, we firstly review some preliminaries, including the basic GAN model
and some game theory backgrounds. After that, we present our taxonomy to
summarize the state-of-the-art solutions into three significant categories:
modified game model, modified architecture, and modified learning method. The
classification is based on the modifications made in the basic model by the
proposed approaches from the game-theoretic perspective. We further classify
each category into several subcategories. Following the proposed taxonomy, we
explore the main objective of each class and review the recent work in each
group. Finally, we discuss the remaining challenges in this field and present
the potential future research topics.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Evidence Lower Bound of Variational Autoencoders Converges to a Sum of Three Entropies. (arXiv:2010.14860v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Lucke_J/0/1/0/all/0/1">J&#xf6;rg L&#xfc;cke</a>, <a href="http://arxiv.org/find/stat/1/au:+Forster_D/0/1/0/all/0/1">Dennis Forster</a>, <a href="http://arxiv.org/find/stat/1/au:+Dai_Z/0/1/0/all/0/1">Zhenwen Dai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.14860">
                                    <div class="article-summary-box-inner">
                                        <span>The central objective function of a variational autoencoder (VAE) is its
variational lower bound. Here we show that for standard VAEs the variational
bound converges to a value given by the sum of three entropies: the (negative)
entropy of the latent distribution, the expected (negative) entropy of the
observable distribution, and the average entropy of the variational
distributions. Our derived analytical results are exact and apply for small as
well as complex neural networks for decoder and encoder. Furthermore, they
apply for finitely and infinitely many data points and at any stationary point
(including local and global maxima). As a consequence, we show that the
variance parameters of encoder and decoder play the key role in determining the
values of variational bounds at stationary points. Furthermore, the obtained
results can allow for closed-form analytical expressions at points of
convergence, which may be unexpected as neither variational lower bounds of
VAEs nor log-likelihoods of VAEs are closed-form during learning. As our main
contribution, we provide the proofs for convergence of standard VAEs to sums of
entropies. Furthermore, we numerically verify our analytical results and
discuss some potential applications. The obtained equality to entropy sums
provides novel information on those points in parameter space that variational
learning converges to. As such, we believe, they can contribute to our
understanding of established as well as novel VAE approaches.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-modal Scene-compliant User Intention Estimation for Navigation. (arXiv:2106.06920v1 [cs.RO])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Katuwandeniya_K/0/1/0/all/0/1">Kavindie Katuwandeniya</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiss_S/0/1/0/all/0/1">Stefan H. Kiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1">Lei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Miro_J/0/1/0/all/0/1">Jaime Valls Miro</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06920">
                                    <div class="article-summary-box-inner">
                                        <span>A multi-modal framework to generated user intention distributions when
operating a mobile vehicle is proposed in this work. The model learns from past
observed trajectories and leverages traversability information derived from the
visual surroundings to produce a set of future trajectories, suitable to be
directly embedded into a perception-action shared control strategy on a mobile
agent, or as a safety layer to supervise the prudent operation of the vehicle.
We base our solution on a conditional Generative Adversarial Network with
Long-Short Term Memory cells to capture trajectory distributions conditioned on
past trajectories, further fused with traversability probabilities derived from
visual segmentation with a Convolutional Neural Network. The proposed
data-driven framework results in a significant reduction in error of the
predicted trajectories (versus the ground truth) from comparable strategies in
the literature (e.g. Social-GAN) that fail to account for information other
than the agent&#x27;s past history. Experiments were conducted on a dataset
collected with a custom wheelchair model built onto the open-source urban
driving simulator CARLA, proving also that the proposed framework can be used
with a small, un-annotated dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Distributionally Robust Optimization with Markovian Data. (arXiv:2106.06741v1 [math.OC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Li_M/0/1/0/all/0/1">Mengmeng Li</a>, <a href="http://arxiv.org/find/math/1/au:+Sutter_T/0/1/0/all/0/1">Tobias Sutter</a>, <a href="http://arxiv.org/find/math/1/au:+Kuhn_D/0/1/0/all/0/1">Daniel Kuhn</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06741">
                                    <div class="article-summary-box-inner">
                                        <span>We study a stochastic program where the probability distribution of the
uncertain problem parameters is unknown and only indirectly observed via
finitely many correlated samples generated by an unknown Markov chain with $d$
states. We propose a data-driven distributionally robust optimization model to
estimate the problem&#x27;s objective function and optimal solution. By leveraging
results from large deviations theory, we derive statistical guarantees on the
quality of these estimators. The underlying worst-case expectation problem is
nonconvex and involves $\mathcal O(d^2)$ decision variables. Thus, it cannot be
solved efficiently for large $d$. By exploiting the structure of this problem,
we devise a customized Frank-Wolfe algorithm with convex direction-finding
subproblems of size $\mathcal O(d)$. We prove that this algorithm finds a
stationary point efficiently under mild conditions. The efficiency of the
method is predicated on a dimensionality reduction enabled by a dual
reformulation. Numerical experiments indicate that our approach has better
computational and statistical properties than the state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural Networks Calibration. (arXiv:2106.06984v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shikuang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1">Ruihao Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1">Shi Gu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06984">
                                    <div class="article-summary-box-inner">
                                        <span>Spiking Neural Network (SNN) has been recognized as one of the next
generation of neural networks. Conventionally, SNN can be converted from a
pre-trained ANN by only replacing the ReLU activation to spike activation while
keeping the parameters intact. Perhaps surprisingly, in this work we show that
a proper way to calibrate the parameters during the conversion of ANN to SNN
can bring significant improvements. We introduce SNN Calibration, a cheap but
extraordinarily effective method by leveraging the knowledge within a
pre-trained Artificial Neural Network (ANN). Starting by analyzing the
conversion error and its propagation through layers theoretically, we propose
the calibration algorithm that can correct the error layer-by-layer. The
calibration only takes a handful number of training data and several minutes to
finish. Moreover, our calibration algorithm can produce SNN with
state-of-the-art architecture on the large-scale ImageNet dataset, including
MobileNet and RegNet. Extensive experiments demonstrate the effectiveness and
efficiency of our algorithm. For example, our advanced pipeline can increase up
to 69% top-1 accuracy when converting MobileNet on ImageNet compared to
baselines. Codes are released at https://github.com/yhhhli/SNN_Calibration.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">EL-Attention: Memory Efficient Lossless Attention for Generation. (arXiv:2105.04779v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiusheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1">Weizhen Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1">Nikhil Bhendawade</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yeyun Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1">Nan Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruofei Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04779">
                                    <div class="article-summary-box-inner">
                                        <span>Transformer model with multi-head attention requires caching intermediate
results for efficient inference in generation tasks. However, cache brings new
memory-related costs and prevents leveraging larger batch size for faster
speed. We propose memory-efficient lossless attention (called EL-attention) to
address this issue. It avoids heavy operations for building multi-head keys and
values, cache for them is not needed. EL-attention constructs an ensemble of
attention results by expanding query while keeping key and value shared. It
produces the same result as multi-head attention with less GPU memory and
faster inference speed. We conduct extensive experiments on Transformer, BART,
and GPT-2 for summarization and question generation tasks. The results show
EL-attention speeds up existing models by 1.6x to 5.3x without accuracy loss.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Solving PDEs on Unknown Manifolds with Machine Learning. (arXiv:2106.06682v1 [math.NA])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Liang_S/0/1/0/all/0/1">Senwei Liang</a>, <a href="http://arxiv.org/find/math/1/au:+Jiang_S/0/1/0/all/0/1">Shixiao W. Jiang</a>, <a href="http://arxiv.org/find/math/1/au:+Harlim_J/0/1/0/all/0/1">John Harlim</a>, <a href="http://arxiv.org/find/math/1/au:+Yang_H/0/1/0/all/0/1">Haizhao Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06682">
                                    <div class="article-summary-box-inner">
                                        <span>This paper proposes a mesh-free computational framework and machine learning
theory for solving elliptic PDEs on unknown manifolds, identified with point
clouds, based on diffusion maps (DM) and deep learning. The PDE solver is
formulated as a supervised learning task to solve a least-squares regression
problem that imposes an algebraic equation approximating a PDE (and boundary
conditions if applicable). This algebraic equation involves a graph-Laplacian
type matrix obtained via DM asymptotic expansion, which is a consistent
estimator of second-order elliptic differential operators. The resulting
numerical method is to solve a highly non-convex empirical risk minimization
problem subjected to a solution from a hypothesis space of neural-network type
functions. In a well-posed elliptic PDE setting, when the hypothesis space
consists of feedforward neural networks with either infinite width or depth, we
show that the global minimizer of the empirical loss function is a consistent
solution in the limit of large training data. When the hypothesis space is a
two-layer neural network, we show that for a sufficiently large width, the
gradient descent method can identify a global minimizer of the empirical loss
function. Supporting numerical examples demonstrate the convergence of the
solutions and the effectiveness of the proposed solver in avoiding numerical
issues that hampers the traditional approach when a large data set becomes
available, e.g., large matrix inversion.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Joint Client Scheduling and Resource Allocation under Channel Uncertainty in Federated Learning. (arXiv:2106.06796v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wadu_M/0/1/0/all/0/1">Madhusanka Manimel Wadu</a>, <a href="http://arxiv.org/find/cs/1/au:+Samarakoon_S/0/1/0/all/0/1">Sumudu Samarakoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1">Mehdi Bennis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06796">
                                    <div class="article-summary-box-inner">
                                        <span>The performance of federated learning (FL) over wireless networks depend on
the reliability of the client-server connectivity and clients&#x27; local
computation capabilities. In this article we investigate the problem of client
scheduling and resource block (RB) allocation to enhance the performance of
model training using FL, over a pre-defined training duration under imperfect
channel state information (CSI) and limited local computing resources. First,
we analytically derive the gap between the training losses of FL with clients
scheduling and a centralized training method for a given training duration.
Then, we formulate the gap of the training loss minimization over client
scheduling and RB allocation as a stochastic optimization problem and solve it
using Lyapunov optimization. A Gaussian process regression-based channel
prediction method is leveraged to learn and track the wireless channel, in
which, the clients&#x27; CSI predictions and computing power are incorporated into
the scheduling decision. Using an extensive set of simulations, we validate the
robustness of the proposed method under both perfect and imperfect CSI over an
array of diverse data distributions. Results show that the proposed method
reduces the gap of the training accuracy loss by up to 40.7% compared to
state-of-theart client scheduling and RB allocation methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective. (arXiv:2003.00330v7 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1">Luis C. Lamb</a>, <a href="http://arxiv.org/find/cs/1/au:+Garcez_A/0/1/0/all/0/1">Artur Garcez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1">Marco Gori</a>, <a href="http://arxiv.org/find/cs/1/au:+Prates_M/0/1/0/all/0/1">Marcelo Prates</a>, <a href="http://arxiv.org/find/cs/1/au:+Avelar_P/0/1/0/all/0/1">Pedro Avelar</a>, <a href="http://arxiv.org/find/cs/1/au:+Vardi_M/0/1/0/all/0/1">Moshe Vardi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.00330">
                                    <div class="article-summary-box-inner">
                                        <span>Neural-symbolic computing has now become the subject of interest of both
academic and industry research laboratories. Graph Neural Networks (GNN) have
been widely used in relational and symbolic domains, with widespread
application of GNNs in combinatorial optimization, constraint satisfaction,
relational reasoning and other scientific domains. The need for improved
explainability, interpretability and trust of AI systems in general demands
principled methodologies, as suggested by neural-symbolic computing. In this
paper, we review the state-of-the-art on the use of GNNs as a model of
neural-symbolic computing. This includes the application of GNNs in several
domains as well as its relationship to current developments in neural-symbolic
computing.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Residual Networks based Distortion Classification and Ranking for Laparoscopic Image Quality Assessment. (arXiv:2106.06784v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Khan_Z/0/1/0/all/0/1">Zohaib Amjad Khan</a>, <a href="http://arxiv.org/find/eess/1/au:+Beghdadi_A/0/1/0/all/0/1">Azeddine Beghdadi</a>, <a href="http://arxiv.org/find/eess/1/au:+Kaaniche_M/0/1/0/all/0/1">Mounir Kaaniche</a>, <a href="http://arxiv.org/find/eess/1/au:+Cheikh_F/0/1/0/all/0/1">Faouzi Alaya Cheikh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06784">
                                    <div class="article-summary-box-inner">
                                        <span>Laparoscopic images and videos are often affected by different types of
distortion like noise, smoke, blur and nonuniform illumination. Automatic
detection of these distortions, followed generally by application of
appropriate image quality enhancement methods, is critical to avoid errors
during surgery. In this context, a crucial step involves an objective
assessment of the image quality, which is a two-fold problem requiring both the
classification of the distortion type affecting the image and the estimation of
the severity level of that distortion. Unlike existing image quality measures
which focus mainly on estimating a quality score, we propose in this paper to
formulate the image quality assessment task as a multi-label classification
problem taking into account both the type as well as the severity level (or
rank) of distortions. Here, this problem is then solved by resorting to a deep
neural networks based approach. The obtained results on a laparoscopic image
dataset show the efficiency of the proposed approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SPADE: A Spectral Method for Black-Box Adversarial Robustness Evaluation. (arXiv:2102.03716v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wuxinlin Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1">Chenhui Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhiqiang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yaohui Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiru Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhuo Feng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03716">
                                    <div class="article-summary-box-inner">
                                        <span>A black-box spectral method is introduced for evaluating the adversarial
robustness of a given machine learning (ML) model. Our approach, named SPADE,
exploits bijective distance mapping between the input/output graphs constructed
for approximating the manifolds corresponding to the input/output data. By
leveraging the generalized Courant-Fischer theorem, we propose a SPADE score
for evaluating the adversarial robustness of a given model, which is proved to
be an upper bound of the best Lipschitz constant under the manifold setting. To
reveal the most non-robust data samples highly vulnerable to adversarial
attacks, we develop a spectral graph embedding procedure leveraging dominant
generalized eigenvectors. This embedding step allows assigning each data sample
a robustness score that can be further harnessed for more effective adversarial
training. Our experiments show the proposed SPADE method leads to promising
empirical results for neural network models that are adversarially trained with
the MNIST and CIFAR-10 data sets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Accelerating Feedforward Computation via Parallel Nonlinear Equation Solving. (arXiv:2002.03629v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Chenlin Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1">Renjie Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.03629">
                                    <div class="article-summary-box-inner">
                                        <span>Feedforward computation, such as evaluating a neural network or sampling from
an autoregressive model, is ubiquitous in machine learning. The sequential
nature of feedforward computation, however, requires a strict order of
execution and cannot be easily accelerated with parallel computing. To enable
parallelization, we frame the task of feedforward computation as solving a
system of nonlinear equations. We then propose to find the solution using a
Jacobi or Gauss-Seidel fixed-point iteration method, as well as hybrid methods
of both. Crucially, Jacobi updates operate independently on each equation and
can be executed in parallel. Our method is guaranteed to give exactly the same
values as the original feedforward computation with a reduced (or equal) number
of parallelizable iterations, and hence reduced time given sufficient parallel
computing power. Experimentally, we demonstrate the effectiveness of our
approach in accelerating (i) backpropagation of RNNs, (ii) evaluation of
DenseNets, and (iii) autoregressive sampling of MADE and PixelCNN++, with
speedup factors between 2.1 and 26 under various settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Understanding self-supervised Learning Dynamics without Contrastive Pairs. (arXiv:2102.06810v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yuandong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinlei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1">Surya Ganguli</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06810">
                                    <div class="article-summary-box-inner">
                                        <span>While contrastive approaches of self-supervised learning (SSL) learn
representations by minimizing the distance between two augmented views of the
same data point (positive pairs) and maximizing views from different data
points (negative pairs), recent \emph{non-contrastive} SSL (e.g., BYOL and
SimSiam) show remarkable performance {\it without} negative pairs, with an
extra learnable predictor and a stop-gradient operation. A fundamental question
arises: why do these methods not collapse into trivial representations? We
answer this question via a simple theoretical study and propose a novel
approach, DirectPred, that \emph{directly} sets the linear predictor based on
the statistics of its inputs, without gradient training. On ImageNet, it
performs comparably with more complex two-layer non-linear predictors that
employ BatchNorm and outperforms a linear predictor by $2.5\%$ in 300-epoch
training (and $5\%$ in 60-epoch). DirectPred is motivated by our theoretical
study of the nonlinear learning dynamics of non-contrastive SSL in simple
linear networks. Our study yields conceptual insights into how non-contrastive
SSL methods learn, how they avoid representational collapse, and how multiple
factors, like predictor networks, stop-gradients, exponential moving averages,
and weight decay all come into play. Our simple theory recapitulates the
results of real-world ablation studies in both STL-10 and ImageNet. Code is
released\footnote{\url{https://github.com/facebookresearch/luckmatters/tree/master/ssl}}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">About exchanging expectation and supremum for conditional Wasserstein GANs. (arXiv:2103.13906v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1">J&#xf6;rg Martin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13906">
                                    <div class="article-summary-box-inner">
                                        <span>In cases where a Wasserstein GAN depends on a condition the latter is usually
handled via an expectation within the loss function. Depending on the way this
is motivated, the discriminator is either required to be Lipschitz-1 in both or
in only one of its arguments. For the weaker requirement to become usable one
needs to exchange a supremum and an expectation. This is a mathematically
perilous operation, which is, so far, only partially justified in the
literature. This short mathematical note intends to fill this gap and provides
the mathematical rationale for discriminators that are only partially
Lipschitz-1 for cases where this approach is more appropriate or successful.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning. (arXiv:2012.10630v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1">Krishnateja Killamsetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1">Durga Sivasubramanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1">Ganesh Ramakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1">Rishabh Iyer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10630">
                                    <div class="article-summary-box-inner">
                                        <span>Large scale machine learning and deep models are extremely data-hungry.
Unfortunately, obtaining large amounts of labeled data is expensive, and
training state-of-the-art models (with hyperparameter tuning) requires
significant computing resources and time. Secondly, real-world data is noisy
and imbalanced. As a result, several recent papers try to make the training
process more efficient and robust. However, most existing work either focuses
on robustness or efficiency, but not both. In this work, we introduce Glister,
a GeneraLIzation based data Subset selecTion for Efficient and Robust learning
framework. We formulate Glister as a mixed discrete-continuous bi-level
optimization problem to select a subset of the training data, which maximizes
the log-likelihood on a held-out validation set. Next, we propose an iterative
online algorithm Glister-Online, which performs data selection iteratively
along with the parameter updates and can be applied to any loss-based learning
algorithm. We then show that for a rich class of loss functions including
cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete
data selection is an instance of (weakly) submodular optimization, and we
analyze conditions for which Glister-Online reduces the validation loss and
converges. Finally, we propose Glister-Active, an extension to batch active
learning, and we empirically demonstrate the performance of Glister on a wide
range of tasks including, (a) data selection to reduce training time, (b)
robust learning under label noise and imbalance settings, and (c) batch-active
learning with several deep and shallow models. We show that our framework
improves upon state of the art both in efficiency and accuracy (in cases (a)
and (c)) and is more efficient compared to other state-of-the-art robust
learning algorithms in case (b).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-facet Contextual Bandits: A Neural Network Perspective. (arXiv:2106.03039v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1">Yikun Ban</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jingrui He</a>, <a href="http://arxiv.org/find/cs/1/au:+Cook_C/0/1/0/all/0/1">Curtiss B. Cook</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03039">
                                    <div class="article-summary-box-inner">
                                        <span>Contextual multi-armed bandit has shown to be an effective tool in
recommender systems. In this paper, we study a novel problem of multi-facet
bandits involving a group of bandits, each characterizing the users&#x27; needs from
one unique aspect. In each round, for the given user, we need to select one arm
from each bandit, such that the combination of all arms maximizes the final
reward. This problem can find immediate applications in E-commerce, healthcare,
etc. To address this problem, we propose a novel algorithm, named MuFasa, which
utilizes an assembled neural network to jointly learn the underlying reward
functions of multiple bandits. It estimates an Upper Confidence Bound (UCB)
linked with the expected reward to balance between exploitation and
exploration. Under mild assumptions, we provide the regret analysis of MuFasa.
It can achieve the near-optimal $\widetilde{ \mathcal{O}}((K+1)\sqrt{T})$
regret bound where $K$ is the number of bandits and $T$ is the number of played
rounds. Furthermore, we conduct extensive experiments to show that MuFasa
outperforms strong baselines on real-world data sets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Heterogeneous Risk Minimization. (arXiv:2105.03818v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiashuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zheyuan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1">Peng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zheyan Shen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03818">
                                    <div class="article-summary-box-inner">
                                        <span>Machine learning algorithms with empirical risk minimization usually suffer
from poor generalization performance due to the greedy exploitation of
correlations among the training data, which are not stable under distributional
shifts. Recently, some invariant learning methods for out-of-distribution (OOD)
generalization have been proposed by leveraging multiple training environments
to find invariant relationships. However, modern datasets are frequently
assembled by merging data from multiple sources without explicit source labels.
The resultant unobserved heterogeneity renders many invariant learning methods
inapplicable. In this paper, we propose Heterogeneous Risk Minimization (HRM)
framework to achieve joint learning of latent heterogeneity among the data and
invariant relationship, which leads to stable prediction despite distributional
shifts. We theoretically characterize the roles of the environment labels in
invariant learning and justify our newly proposed HRM framework. Extensive
experimental results validate the effectiveness of our HRM framework.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Feasible Actor-Critic: Constrained Reinforcement Learning for Ensuring Statewise Safety. (arXiv:2105.10682v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haitong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1">Yang Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shegnbo Eben Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangteng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Sifa Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianyu Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10682">
                                    <div class="article-summary-box-inner">
                                        <span>The safety constraints commonly used by existing safe reinforcement learning
(RL) methods are defined only on expectation of initial states, but allow each
certain state to be unsafe, which is unsatisfying for real-world
safety-critical tasks. In this paper, we introduce the feasible actor-critic
(FAC) algorithm, which is the first model-free constrained RL method that
considers statewise safety, e.g, safety for each initial state. We claim that
some states are inherently unsafe no matter what policy we choose, while for
other states there exist policies ensuring safety, where we say such states and
policies are feasible. By constructing a statewise Lagrange function available
on RL sampling and adopting an additional neural network to approximate the
statewise Lagrange multiplier, we manage to obtain the optimal feasible policy
which ensures safety for each feasible state and the safest possible policy for
infeasible states. Furthermore, the trained multiplier net can indicate whether
a given state is feasible or not through the statewise complementary slackness
condition. We provide theoretical guarantees that FAC outperforms previous
expectation-based constrained RL methods in terms of both constraint
satisfaction and reward optimization. Experimental results on both robot
locomotive tasks and safe exploration tasks verify the safety enhancement and
feasibility interpretation of the proposed method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cluster-to-Conquer: A Framework for End-to-End Multi-Instance Learning for Whole Slide Image Classification. (arXiv:2103.10626v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Sharma_Y/0/1/0/all/0/1">Yash Sharma</a>, <a href="http://arxiv.org/find/eess/1/au:+Shrivastava_A/0/1/0/all/0/1">Aman Shrivastava</a>, <a href="http://arxiv.org/find/eess/1/au:+Ehsan_L/0/1/0/all/0/1">Lubaina Ehsan</a>, <a href="http://arxiv.org/find/eess/1/au:+Moskaluk_C/0/1/0/all/0/1">Christopher A. Moskaluk</a>, <a href="http://arxiv.org/find/eess/1/au:+Syed_S/0/1/0/all/0/1">Sana Syed</a>, <a href="http://arxiv.org/find/eess/1/au:+Brown_D/0/1/0/all/0/1">Donald E. Brown</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10626">
                                    <div class="article-summary-box-inner">
                                        <span>In recent years, the availability of digitized Whole Slide Images (WSIs) has
enabled the use of deep learning-based computer vision techniques for automated
disease diagnosis. However, WSIs present unique computational and algorithmic
challenges. WSIs are gigapixel-sized ($\sim$100K pixels), making them
infeasible to be used directly for training deep neural networks. Also, often
only slide-level labels are available for training as detailed annotations are
tedious and can be time-consuming for experts. Approaches using
multiple-instance learning (MIL) frameworks have been shown to overcome these
challenges. Current state-of-the-art approaches divide the learning framework
into two decoupled parts: a convolutional neural network (CNN) for encoding the
patches followed by an independent aggregation approach for slide-level
prediction. In this approach, the aggregation step has no bearing on the
representations learned by the CNN encoder. We have proposed an end-to-end
framework that clusters the patches from a WSI into ${k}$-groups, samples
${k}&#x27;$ patches from each group for training, and uses an adaptive attention
mechanism for slide level prediction; Cluster-to-Conquer (C2C). We have
demonstrated that dividing a WSI into clusters can improve the model training
by exposing it to diverse discriminative features extracted from the patches.
We regularized the clustering mechanism by introducing a KL-divergence loss
between the attention weights of patches in a cluster and the uniform
distribution. The framework is optimized end-to-end on slide-level
cross-entropy, patch-level cross-entropy, and KL-divergence loss
(Implementation: https://github.com/YashSharma/C2C).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Federated Learning with Buffered Asynchronous Aggregation. (arXiv:2106.06639v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1">John Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1">Kshitiz Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1">Hongyuan Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yousefpour_A/0/1/0/all/0/1">Ashkan Yousefpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1">Michael Rabbat</a>, <a href="http://arxiv.org/find/cs/1/au:+Esmaeili_M/0/1/0/all/0/1">Mani Malek Esmaeili</a>, <a href="http://arxiv.org/find/cs/1/au:+Huba_D/0/1/0/all/0/1">Dzmitry Huba</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06639">
                                    <div class="article-summary-box-inner">
                                        <span>Federated Learning (FL) trains a shared model across distributed devices
while keeping the training data on the devices. Most FL schemes are
synchronous: they perform a synchronized aggregation of model updates from
individual devices. Synchronous training can be slow because of late-arriving
devices (stragglers). On the other hand, completely asynchronous training makes
FL less private because of incompatibility with secure aggregation. In this
work, we propose a model aggregation scheme, FedBuff, that combines the best
properties of synchronous and asynchronous FL. Similar to synchronous FL,
FedBuff is compatible with secure aggregation. Similar to asynchronous FL,
FedBuff is robust to stragglers. In FedBuff, clients trains asynchronously and
send updates to the server. The server aggregates client updates in a private
buffer until updates have been received, at which point a server model update
is immediately performed. We provide theoretical convergence guarantees for
FedBuff in a non-convex setting. Empirically, FedBuff converges up to 3.8x
faster than previous proposals for synchronous FL (e.g., FedAvgM), and up to
2.5x faster than previous proposals for asynchronous FL (e.g., FedAsync). We
show that FedBuff is robust to different staleness distributions and is more
scalable than synchronous FL techniques.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Power Modeling for Effective Datacenter Planning and Compute Management. (arXiv:2103.13308v2 [cs.DC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Radovanovic_A/0/1/0/all/0/1">Ana Radovanovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bokan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Talukdar_S/0/1/0/all/0/1">Saurav Talukdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1">Binz Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Duarte_A/0/1/0/all/0/1">Alexandre Duarte</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahbazi_M/0/1/0/all/0/1">Mahya Shahbazi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13308">
                                    <div class="article-summary-box-inner">
                                        <span>Datacenter power demand has been continuously growing and is the key driver
of its cost. An accurate mapping of compute resources (CPU, RAM, etc.) and
hardware types (servers, accelerators, etc.) to power consumption has emerged
as a critical requirement for major Web and cloud service providers. With the
global growth in datacenter capacity and associated power consumption, such
models are essential for important decisions around datacenter design and
operation. In this paper, we discuss two classes of statistical power models
designed and validated to be accurate, simple, interpretable and applicable to
all hardware configurations and workloads across hyperscale datacenters of
Google fleet. To the best of our knowledge, this is the largest scale power
modeling study of this kind, in both the scope of diverse datacenter planning
and real-time management use cases, as well as the variety of hardware
configurations and workload types used for modeling and validation. We
demonstrate that the proposed statistical modeling techniques, while simple and
scalable, predict power with less than 5% Mean Absolute Percent Error (MAPE)
for more than 95% diverse Power Distribution Units (more than 2000) using only
4 features. This performance matches the reported accuracy of the previous
started-of-the-art methods, while using significantly less features and
covering a wider range of use cases.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bias-Variance Reduced Local SGD for Less Heterogeneous Federated Learning. (arXiv:2102.03198v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Murata_T/0/1/0/all/0/1">Tomoya Murata</a>, <a href="http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1">Taiji Suzuki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03198">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, local SGD has got much attention and been extensively studied in
the distributed learning community to overcome the communication bottleneck
problem. However, the superiority of local SGD to minibatch SGD only holds in
quite limited situations. In this paper, we study a new local algorithm called
Bias-Variance Reduced Local SGD (BVR-L-SGD) for nonconvex distributed
optimization. Algorithmically, our proposed bias and variance reduced local
gradient estimator fully utilizes small second-order heterogeneity of local
objectives and suggests randomly picking up one of the local models instead of
taking the average of them when workers are synchronized. Theoretically, under
small heterogeneity of local objectives, we show that BVR-L-SGD achieves better
communication complexity than both the previous non-local and local methods
under mild conditions, and particularly BVR-L-SGD is the first method that
breaks the barrier of communication complexity $\Theta(1/\varepsilon)$ for
general nonconvex smooth objectives when the heterogeneity is small and the
local computation budget is large. Numerical results are given to verify the
theoretical findings and give empirical evidence of the superiority of our
method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Finding the Stochastic Shortest Path with Low Regret: The Adversarial Cost and Unknown Transition Case. (arXiv:2102.05284v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Haipeng Luo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05284">
                                    <div class="article-summary-box-inner">
                                        <span>We make significant progress toward the stochastic shortest path problem with
adversarial costs and unknown transition. Specifically, we develop algorithms
that achieve $\widetilde{O}(\sqrt{S^2ADT_\star K})$ regret for the
full-information setting and $\widetilde{O}(\sqrt{S^3A^2DT_\star K})$ regret
for the bandit feedback setting, where $D$ is the diameter, $T_\star$ is the
expected hitting time of the optimal policy, $S$ is the number of states, $A$
is the number of actions, and $K$ is the number of episodes. Our work strictly
improves (Rosenberg and Mansour, 2020) in the full information setting, extends
(Chen et al., 2020) from known transition to unknown transition, and is also
the first to consider the most challenging combination: bandit feedback with
adversarial costs and unknown transition. To remedy the gap between our upper
bounds and the current best lower bounds constructed via a stochastically
oblivious adversary, we also propose algorithms with near-optimal regret for
this special case.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Achieving Efficiency in Black Box Simulation of Distribution Tails with Self-structuring Importance Samplers. (arXiv:2102.07060v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Deo_A/0/1/0/all/0/1">Anand Deo</a>, <a href="http://arxiv.org/find/stat/1/au:+Murthy_K/0/1/0/all/0/1">Karthyek Murthy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07060">
                                    <div class="article-summary-box-inner">
                                        <span>Motivated by the increasing adoption of models which facilitate greater
automation in risk management and decision-making, this paper presents a novel
Importance Sampling (IS) scheme for measuring distribution tails of objectives
modelled with enabling tools such as feature-based decision rules, mixed
integer linear programs, deep neural networks, etc. Conventional efficient IS
approaches suffer from feasibility and scalability concerns due to the need to
intricately tailor the sampler to the underlying probability distribution and
the objective. This challenge is overcome in the proposed black-box scheme by
automating the selection of an effective IS distribution with a transformation
that implicitly learns and replicates the concentration properties observed in
less rare samples. This novel approach is guided by a large deviations
principle that brings out the phenomenon of self-similarity of optimal IS
distributions. The proposed sampler is the first to attain asymptotically
optimal variance reduction across a spectrum of multivariate distributions
despite being oblivious to the underlying structure. The large deviations
principle additionally results in new distribution tail asymptotics capable of
yielding operational insights. The applicability is illustrated by considering
product distribution networks and portfolio credit risk models informed by
neural networks as examples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Binary Decision Trees by Argmin Differentiation. (arXiv:2010.04627v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zantedeschi_V/0/1/0/all/0/1">Valentina Zantedeschi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1">Matt J. Kusner</a>, <a href="http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1">Vlad Niculae</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04627">
                                    <div class="article-summary-box-inner">
                                        <span>We address the problem of learning binary decision trees that partition data
for some downstream task. We propose to learn discrete parameters (i.e., for
tree traversals and node pruning) and continuous parameters (i.e., for tree
split functions and prediction functions) simultaneously using argmin
differentiation. We do so by sparsely relaxing a mixed-integer program for the
discrete parameters, to allow gradients to pass through the program to
continuous parameters. We derive customized algorithms to efficiently compute
the forward and backward passes. This means that our tree learning procedure
can be used as an (implicit) layer in arbitrary deep networks, and can be
optimized with arbitrary loss functions. We demonstrate that our approach
produces binary trees that are competitive with existing single tree and
ensemble approaches, in both supervised and unsupervised settings. Further,
apart from greedy approaches (which do not have competitive accuracies), our
method is faster to train than all other tree-learning baselines we compare
with. The code for reproducing the results is available at
https://github.com/vzantedeschi/LatentTrees.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Kwame: A Bilingual AI Teaching Assistant for Online SuaCode Courses. (arXiv:2010.11387v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1">George Boateng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11387">
                                    <div class="article-summary-box-inner">
                                        <span>Introductory hands-on courses such as our smartphone-based coding course,
SuaCode require a lot of support for students to accomplish learning goals.
Online environments make it even more difficult to get assistance especially
more recently because of COVID-19. Given the multilingual context of SuaCode
students - learners across 42 African countries that are mostly Anglophone or
Francophone - in this work, we developed a bilingual Artificial Intelligence
(AI) Teaching Assistant (TA) - Kwame - that provides answers to students&#x27;
coding questions from SuaCode courses in English and French. Kwame is a
Sentence-BERT (SBERT)-based question-answering (QA) system that we trained and
evaluated offline using question-answer pairs created from the course&#x27;s
quizzes, lesson notes and students&#x27; questions in past cohorts. Kwame finds the
paragraph most semantically similar to the question via cosine similarity. We
compared the system with TF-IDF and Universal Sentence Encoder. Our results
showed that fine-tuning on the course data and returning the top 3 and 5
answers improved the accuracy results. Kwame will make it easy for students to
get quick and accurate answers to questions in SuaCode courses.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Quantized Neural Nets by Coarse Gradient Method for Non-linear Classification. (arXiv:2011.11256v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1">Ziang Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1">Penghang Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1">Jack Xin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11256">
                                    <div class="article-summary-box-inner">
                                        <span>Quantized or low-bit neural networks are attractive due to their inference
efficiency. However, training deep neural networks with quantized activations
involves minimizing a discontinuous and piecewise constant loss function. Such
a loss function has zero gradients almost everywhere (a.e.), which makes the
conventional gradient-based algorithms inapplicable. To this end, we study a
novel class of \emph{biased} first-order oracle, termed coarse gradient, for
overcoming the vanished gradient issue. A coarse gradient is generated by
replacing the a.e. zero derivatives of quantized (i.e., stair-case) ReLU
activation composited in the chain rule with some heuristic proxy derivative
called straight-through estimator (STE). Although having been widely used in
training quantized networks empirically, fundamental questions like when and
why the ad-hoc STE trick works, still lacks theoretical understanding. In this
paper, we propose a class of STEs with certain monotonicity, and consider their
applications to the training of a two-linear-layer network with quantized
activation functions for non-linear multi-category classification. We establish
performance guarantees for the proposed STEs by showing that the corresponding
coarse gradient methods converge to the global minimum, which leads to a
perfect classification. Lastly, we present experimental results on synthetic
data as well as MNIST dataset to verify our theoretical findings and
demonstrate the effectiveness of our proposed STEs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Shape-Preserving Dimensionality Reduction : An Algorithm and Measures of Topological Equivalence. (arXiv:2106.02096v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1">Byeongsu Yu</a>, <a href="http://arxiv.org/find/stat/1/au:+You_K/0/1/0/all/0/1">Kisung You</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02096">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a linear dimensionality reduction technique preserving
topological features via persistent homology. The method is designed to find
linear projection $L$ which preserves the persistent diagram of a point cloud
$\mathbb{X}$ via simulated annealing. The projection $L$ induces a set of
canonical simplicial maps from the Rips (or \v{C}ech) filtration of
$\mathbb{X}$ to that of $L\mathbb{X}$. In addition to the distance between
persistent diagrams, the projection induces a map between filtrations, called
filtration homomorphism. Using the filtration homomorphism, one can measure the
difference between shapes of two filtrations directly comparing simplicial
complexes with respect to quasi-isomorphism $\mu_{\operatorname{quasi-iso}}$ or
strong homotopy equivalence $\mu_{\operatorname{equiv}}$. These
$\mu_{\operatorname{quasi-iso}}$ and $\mu_{\operatorname{equiv}}$ measures how
much portion of corresponding simplicial complexes is quasi-isomorphic or
homotopy equivalence respectively. We validate the effectiveness of our
framework with simple examples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SKIing on Simplices: Kernel Interpolation on the Permutohedral Lattice for Scalable Gaussian Processes. (arXiv:2106.06695v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1">Sanyam Kapoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Finzi_M/0/1/0/all/0/1">Marc Finzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Alexander Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1">Andrew Gordon Wilson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06695">
                                    <div class="article-summary-box-inner">
                                        <span>State-of-the-art methods for scalable Gaussian processes use iterative
algorithms, requiring fast matrix vector multiplies (MVMs) with the covariance
kernel. The Structured Kernel Interpolation (SKI) framework accelerates these
MVMs by performing efficient MVMs on a grid and interpolating back to the
original space. In this work, we develop a connection between SKI and the
permutohedral lattice used for high-dimensional fast bilateral filtering. Using
a sparse simplicial grid instead of a dense rectangular one, we can perform GP
inference exponentially faster in the dimension than SKI. Our approach,
Simplex-GP, enables scaling SKI to high dimensions, while maintaining strong
predictive performance. We additionally provide a CUDA implementation of
Simplex-GP, which enables significant GPU acceleration of MVM based inference.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization. (arXiv:2008.10898v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhize Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1">Hongyan Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1">Peter Richt&#xe1;rik</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.10898">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose a novel stochastic gradient estimator --
ProbAbilistic Gradient Estimator (PAGE) -- for nonconvex optimization. PAGE is
easy to implement as it is designed via a small adjustment to vanilla SGD: in
each iteration, PAGE uses the vanilla minibatch SGD update with probability
$p_t$ or reuses the previous gradient with a small adjustment, at a much lower
computational cost, with probability $1-p_t$. We give a simple formula for the
optimal choice of $p_t$. Moreover, we prove the first tight lower bound
$\Omega(n+\frac{\sqrt{n}}{\epsilon^2})$ for nonconvex finite-sum problems,
which also leads to a tight lower bound $\Omega(b+\frac{\sqrt{b}}{\epsilon^2})$
for nonconvex online problems, where $b:&#x3D; \min\{\frac{\sigma^2}{\epsilon^2},
n\}$. Then, we show that PAGE obtains the optimal convergence results
$O(n+\frac{\sqrt{n}}{\epsilon^2})$ (finite-sum) and
$O(b+\frac{\sqrt{b}}{\epsilon^2})$ (online) matching our lower bounds for both
nonconvex finite-sum and online problems. Besides, we also show that for
nonconvex functions satisfying the Polyak-\L{}ojasiewicz (PL) condition, PAGE
can automatically switch to a faster linear convergence rate $O(\cdot\log
\frac{1}{\epsilon})$. Finally, we conduct several deep learning experiments
(e.g., LeNet, VGG, ResNet) on real datasets in PyTorch showing that PAGE not
only converges much faster than SGD in training but also achieves the higher
test accuracy, validating the optimal theoretical results and confirming the
practical superiority of PAGE.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Prioritized Level Replay. (arXiv:2010.03934v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1">Minqi Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1">Edward Grefenstette</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1">Tim Rockt&#xe4;schel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03934">
                                    <div class="article-summary-box-inner">
                                        <span>Environments with procedurally generated content serve as important
benchmarks for testing systematic generalization in deep reinforcement
learning. In this setting, each level is an algorithmically created environment
instance with a unique configuration of its factors of variation. Training on a
prespecified subset of levels allows for testing generalization to unseen
levels. What can be learned from a level depends on the current policy, yet
prior work defaults to uniform sampling of training levels independently of the
policy. We introduce Prioritized Level Replay (PLR), a general framework for
selectively sampling the next training level by prioritizing those with higher
estimated learning potential when revisited in the future. We show TD-errors
effectively estimate a level&#x27;s future learning potential and, when used to
guide the sampling procedure, induce an emergent curriculum of increasingly
difficult levels. By adapting the sampling of training levels, PLR
significantly improves sample efficiency and generalization on Procgen
Benchmark--matching the previous state-of-the-art in test return--and readily
combines with other methods. Combined with the previous leading method, PLR
raises the state-of-the-art to over 76% improvement in test return relative to
standard RL baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Capsule Attention for Multimodal EEG-EOG Representation Learning with Application to Driver Vigilance Estimation. (arXiv:1912.07812v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guangyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1">Ali Etemad</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.07812">
                                    <div class="article-summary-box-inner">
                                        <span>Driver vigilance estimation is an important task for transportation safety.
Wearable and portable brain-computer interface devices provide a powerful means
for real-time monitoring of the vigilance level of drivers to help with
avoiding distracted or impaired driving. In this paper, we propose a novel
multimodal architecture for in-vehicle vigilance estimation from
Electroencephalogram and Electrooculogram. To enable the system to focus on the
most salient parts of the learned multimodal representations, we propose an
architecture composed of a capsule attention mechanism following a deep Long
Short-Term Memory (LSTM) network. Our model learns hierarchical dependencies in
the data through the LSTM and capsule feature representation layers. To better
explore the discriminative ability of the learned representations, we study the
effect of the proposed capsule attention mechanism including the number of
dynamic routing iterations as well as other parameters. Experiments show the
robustness of our method by outperforming other solutions and baseline
techniques, setting a new state-of-the-art. We then provide an analysis on
different frequency bands and brain regions to evaluate their suitability for
driver vigilance estimation. Lastly, an analysis on the role of capsule
attention, multimodality, and robustness to noise is performed, highlighting
the advantages of our approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1">Michihiro Yasunaga</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Percy Liang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06600">
                                    <div class="article-summary-box-inner">
                                        <span>We consider repair tasks: given a critic (e.g., compiler) that assesses the
quality of an input, the goal is to train a fixer that converts a bad example
(e.g., code with syntax errors) into a good one (e.g., code with no errors).
Existing works create training data consisting of (bad, good) pairs by
corrupting good examples using heuristics (e.g., dropping tokens). However,
fixers trained on this synthetically-generated data do not extrapolate well to
the real distribution of bad inputs. To bridge this gap, we propose a new
training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use
the critic to check a fixer&#x27;s output on real bad inputs and add good (fixed)
outputs to the training data, and (ii) we train a breaker to generate realistic
bad code from good code. Based on these ideas, we iteratively update the
breaker and the fixer while using them in conjunction to generate more paired
data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new
dataset we introduce where the goal is to repair Python code with AST parse
errors; and DeepFix, where the goal is to repair C code with compiler errors.
BIFI outperforms existing methods, obtaining 90.5% repair accuracy on
GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not
require any labeled data; we hope it will be a strong starting point for
unsupervised learning of various repair tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fortify Machine Learning Production Systems: Detect and Classify Adversarial Attacks. (arXiv:2102.09695v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1">Matthew Ciolino</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalin_J/0/1/0/all/0/1">Josh Kalin</a>, <a href="http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1">David Noever</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09695">
                                    <div class="article-summary-box-inner">
                                        <span>Production machine learning systems are consistently under attack by
adversarial actors. Various deep learning models must be capable of accurately
detecting fake or adversarial input while maintaining speed. In this work, we
propose one piece of the production protection system: detecting an incoming
adversarial attack and its characteristics. Detecting types of adversarial
attacks has two primary effects: the underlying model can be trained in a
structured manner to be robust from those attacks and the attacks can be
potentially filtered out in real-time before causing any downstream damage. The
adversarial image classification space is explored for models commonly used in
transfer learning.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Maximum n-times Coverage for Vaccine Design. (arXiv:2101.10902v2 [q-bio.QM] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/q-bio/1/au:+Liu_G/0/1/0/all/0/1">Ge Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Dimitrakakis_A/0/1/0/all/0/1">Alexander Dimitrakakis</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Carter_B/0/1/0/all/0/1">Brandon Carter</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gifford_D/0/1/0/all/0/1">David Gifford</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10902">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce the maximum $n$-times coverage problem that selects $k$ overlays
to maximize the summed coverage of weighted elements, where each element must
be covered at least $n$ times. We also define the min-cost $n$-times coverage
problem where the objective is to select the minimum set of overlays such that
the sum of the weights of elements that are covered at least $n$ times is at
least $\tau$. Maximum $n$-times coverage is a generalization of the multi-set
multi-cover problem, is NP-complete, and is not submodular. We introduce two
new practical solutions for $n$-times coverage based on integer linear
programming and sequential greedy optimization. We show that maximum $n$-times
coverage is a natural way to frame peptide vaccine design, and find that it
produces a pan-strain COVID-19 vaccine design that is superior to 29 other
published designs in predicted population coverage and the expected number of
peptides displayed by each individual&#x27;s HLA molecules.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unifying Cardiovascular Modelling with Deep Reinforcement Learning for Uncertainty Aware Control of Sepsis Treatment. (arXiv:2101.08477v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nanayakkara_T/0/1/0/all/0/1">Thesath Nanayakkara</a>, <a href="http://arxiv.org/find/cs/1/au:+Clermont_G/0/1/0/all/0/1">Gilles Clermont</a>, <a href="http://arxiv.org/find/cs/1/au:+Langmead_C/0/1/0/all/0/1">Christopher James Langmead</a>, <a href="http://arxiv.org/find/cs/1/au:+Swigon_D/0/1/0/all/0/1">David Swigon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08477">
                                    <div class="article-summary-box-inner">
                                        <span>Sepsis is a potentially life threatening inflammatory response to infection
or severe tissue damage. It has a highly variable clinical course, requiring
constant monitoring of the patient&#x27;s state to guide the management of
intravenous fluids and vasopressors, among other interventions. Despite decades
of research, there&#x27;s still debate among experts on optimal treatment. Here, we
combine for the first time, distributional deep reinforcement learning with
mechanistic physiological models to find personalized sepsis treatment
strategies. Our method handles partial observability by leveraging known
cardiovascular physiology, introducing a novel physiology-driven recurrent
autoencoder, and quantifies the uncertainty of its own results. Moreover, we
introduce a framework for uncertainty aware decision support with humans in the
loop. We show that our method learns physiologically explainable, robust
policies that are consistent with clinical knowledge. Further our method
consistently identifies high risk states that lead to death, which could
potentially benefit from more frequent vasopressor administration, providing
valuable guidance for future research</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">TDGIA:Effective Injection Attacks on Graph Neural Networks. (arXiv:2106.06663v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xu Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1">Qinkai Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuxiao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1">Xinyu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1">Evgeny Kharlamov</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jialiang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06663">
                                    <div class="article-summary-box-inner">
                                        <span>Graph Neural Networks (GNNs) have achieved promising performance in various
real-world applications. However, recent studies have shown that GNNs are
vulnerable to adversarial attacks. In this paper, we study a
recently-introduced realistic attack scenario on graphs -- graph injection
attack (GIA). In the GIA scenario, the adversary is not able to modify the
existing link structure and node attributes of the input graph, instead the
attack is performed by injecting adversarial nodes into it. We present an
analysis on the topological vulnerability of GNNs under GIA setting, based on
which we propose the Topological Defective Graph Injection Attack (TDGIA) for
effective injection attacks. TDGIA first introduces the topological defective
edge selection strategy to choose the original nodes for connecting with the
injected ones. It then designs the smooth feature optimization objective to
generate the features for the injected nodes. Extensive experiments on
large-scale datasets show that TDGIA can consistently and significantly
outperform various attack baselines in attacking dozens of defense GNN models.
Notably, the performance drop on target GNNs resultant from TDGIA is more than
double the damage brought by the best attack solution among hundreds of
submissions on KDD-CUP 2020.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving the Adversarial Robustness for Speaker Verification by Self-Supervised Learning. (arXiv:2106.00273v2 [cs.SD] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haibin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Andy T. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1">Helen Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hung-yi Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00273">
                                    <div class="article-summary-box-inner">
                                        <span>Previous works have shown that automatic speaker verification (ASV) is
seriously vulnerable to malicious spoofing attacks, such as replay, synthetic
speech, and recently emerged adversarial attacks. Great efforts have been
dedicated to defending ASV against replay and synthetic speech; however, only a
few approaches have been explored to deal with adversarial attacks. All the
existing approaches to tackle adversarial attacks for ASV require the knowledge
for adversarial samples generation, but it is impractical for defenders to know
the exact attack algorithms that are applied by the in-the-wild attackers. This
work is among the first to perform adversarial defense for ASV without knowing
the specific attack algorithms. Inspired by self-supervised learning models
(SSLMs) that possess the merits of alleviating the superficial noise in the
inputs and reconstructing clean samples from the interrupted ones, this work
regards adversarial perturbations as one kind of noise and conducts adversarial
defense for ASV by SSLMs. Specifically, we propose to perform adversarial
defense from two perspectives: 1) adversarial perturbation purification and 2)
adversarial perturbation detection. Experimental results show that our
detection module effectively shields the ASV by detecting adversarial samples
with an accuracy of around 80%. Moreover, since there is no common metric for
evaluating the adversarial defense performance for ASV, this work also
formalizes evaluation metrics for adversarial defense considering both
purification and detection based approaches into account. We sincerely
encourage future works to benchmark their approaches based on the proposed
evaluation framework.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Strategic Classification in the Dark. (arXiv:2102.11592v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ghalme_G/0/1/0/all/0/1">Ganesh Ghalme</a>, <a href="http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1">Vineet Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Eilat_I/0/1/0/all/0/1">Itay Eilat</a>, <a href="http://arxiv.org/find/cs/1/au:+Talgam_Cohen_I/0/1/0/all/0/1">Inbal Talgam-Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenfeld_N/0/1/0/all/0/1">Nir Rosenfeld</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11592">
                                    <div class="article-summary-box-inner">
                                        <span>Strategic classification studies the interaction between a classification
rule and the strategic agents it governs. Under the assumption that the
classifier is known, rational agents respond to it by manipulating their
features. However, in many real-life scenarios of high-stake classification
(e.g., credit scoring), the classifier is not revealed to the agents, which
leads agents to attempt to learn the classifier and game it too. In this paper
we generalize the strategic classification model to such scenarios. We define
the price of opacity as the difference in prediction error between opaque and
transparent strategy-robust classifiers, characterize it, and give a sufficient
condition for this price to be strictly positive, in which case transparency is
the recommended policy. Our experiments show how Hardt et al.&#x27;s robust
classifier is affected by keeping agents in the dark.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Understanding Deflation Process in Over-parametrized Tensor Decomposition. (arXiv:2106.06573v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Ge_R/0/1/0/all/0/1">Rong Ge</a>, <a href="http://arxiv.org/find/stat/1/au:+Ren_Y/0/1/0/all/0/1">Yunwei Ren</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1">Xiang Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1">Mo Zhou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06573">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper we study the training dynamics for gradient flow on
over-parametrized tensor decomposition problems. Empirically, such training
process often first fits larger components and then discovers smaller
components, which is similar to a tensor deflation process that is commonly
used in tensor decomposition algorithms. We prove that for orthogonally
decomposable tensor, a slightly modified version of gradient flow would follow
a tensor deflation process and recover all the tensor components. Our proof
suggests that for orthogonal tensors, gradient flow dynamics works similarly as
greedy low-rank learning in the matrix setting, which is a first step towards
understanding the implicit regularization effect of over-parametrized models
for low-rank tensors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Few-Shot Learning with Class Imbalance. (arXiv:2101.02523v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ochal_M/0/1/0/all/0/1">Mateusz Ochal</a>, <a href="http://arxiv.org/find/cs/1/au:+Patacchiola_M/0/1/0/all/0/1">Massimiliano Patacchiola</a>, <a href="http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1">Amos Storkey</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazquez_J/0/1/0/all/0/1">Jose Vazquez</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sen Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02523">
                                    <div class="article-summary-box-inner">
                                        <span>Few-Shot Learning (FSL) algorithms are commonly trained through Meta-Learning
(ML), which exposes models to batches of tasks sampled from a meta-dataset to
mimic tasks seen during evaluation. However, the standard training procedures
overlook the real-world dynamics where classes commonly occur at different
frequencies. While it is generally understood that class imbalance harms the
performance of supervised methods, limited research examines the impact of
imbalance on the FSL evaluation task. Our analysis compares 10 state-of-the-art
meta-learning and FSL methods on different imbalance distributions and
rebalancing techniques. Our results reveal that 1) some FSL methods display a
natural disposition against imbalance while most other approaches produce a
performance drop by up to 17\% compared to the balanced task without the
appropriate mitigation; 2) contrary to popular belief, many meta-learning
algorithms will not automatically learn to balance from exposure to imbalanced
training tasks; 3) classical rebalancing strategies, such as random
oversampling, can still be very effective, leading to state-of-the-art
performances and should not be overlooked; 4) FSL methods are more robust
against meta-dataset imbalance than imbalance at the task-level with a similar
imbalance ratio ($\rho&lt;20$), with the effect holding even in long-tail datasets
under a larger imbalance ($\rho&#x3D;65$).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Reinforcement Learning for Electric Vehicle Routing Problem with Time Windows. (arXiv:2010.02068v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Bo Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghaddar_B/0/1/0/all/0/1">Bissan Ghaddar</a>, <a href="http://arxiv.org/find/cs/1/au:+Nathwani_J/0/1/0/all/0/1">Jatin Nathwani</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.02068">
                                    <div class="article-summary-box-inner">
                                        <span>The past decade has seen a rapid penetration of electric vehicles (EV) in the
market, more and more logistics and transportation companies start to deploy
EVs for service provision. In order to model the operations of a commercial EV
fleet, we utilize the EV routing problem with time windows (EVRPTW). In this
research, we propose an end-to-end deep reinforcement learning framework to
solve the EVRPTW. In particular, we develop an attention model incorporating
the pointer network and a graph embedding technique to parameterize a
stochastic policy for solving the EVRPTW. The model is then trained using
policy gradient with rollout baseline. Our numerical studies show that the
proposed model is able to efficiently solve EVRPTW instances of large sizes
that are not solvable with any existing approaches.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning from History for Byzantine Robust Optimization. (arXiv:2012.10333v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1">Sai Praneeth Karimireddy</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lie He</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1">Martin Jaggi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10333">
                                    <div class="article-summary-box-inner">
                                        <span>Byzantine robustness has received significant attention recently given its
importance for distributed and federated learning. In spite of this, we
identify severe flaws in existing algorithms even when the data across the
participants is identically distributed. First, we show realistic examples
where current state of the art robust aggregation rules fail to converge even
in the absence of any Byzantine attackers. Secondly, we prove that even if the
aggregation rules may succeed in limiting the influence of the attackers in a
single round, the attackers can couple their attacks across time eventually
leading to divergence. To address these issues, we present two surprisingly
simple strategies: a new robust iterative clipping procedure, and incorporating
worker momentum to overcome time-coupled attacks. This is the first provably
robust method for the standard stochastic optimization setting. Our code is
open sourced at https://github.com/epfml/byzantine-robust-optimizer.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On Continuous Local BDD-Based Search for Hybrid SAT Solving. (arXiv:2012.07983v2 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1">Anastasios Kyrillidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Vardi_M/0/1/0/all/0/1">Moshe Y. Vardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiwei Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07983">
                                    <div class="article-summary-box-inner">
                                        <span>We explore the potential of continuous local search (CLS) in SAT solving by
proposing a novel approach for finding a solution of a hybrid system of Boolean
constraints. The algorithm is based on CLS combined with belief propagation on
binary decision diagrams (BDDs). Our framework accepts all Boolean constraints
that admit compact BDDs, including symmetric Boolean constraints and
small-coefficient pseudo-Boolean constraints as interesting families. We
propose a novel algorithm for efficiently computing the gradient needed by CLS.
We study the capabilities and limitations of our versatile CLS solver, GradSAT,
by applying it on many benchmark instances. The experimental results indicate
that GradSAT can be a useful addition to the portfolio of existing SAT and
MaxSAT solvers for solving Boolean satisfiability and optimization problems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Lifelong Neural Predictive Coding: Learning Cumulatively Online without Forgetting. (arXiv:1905.10696v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1">Alexander Ororbia</a>, <a href="http://arxiv.org/find/cs/1/au:+Mali_A/0/1/0/all/0/1">Ankur Mali</a>, <a href="http://arxiv.org/find/cs/1/au:+Kifer_D/0/1/0/all/0/1">Daniel Kifer</a>, <a href="http://arxiv.org/find/cs/1/au:+Giles_C/0/1/0/all/0/1">C. Lee Giles</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.10696">
                                    <div class="article-summary-box-inner">
                                        <span>In lifelong learning systems, especially those based on artificial neural
networks, one of the biggest obstacles is the severe inability to retain old
knowledge as new information is encountered. This phenomenon is known as
catastrophic forgetting. In this article, we propose a new kind of
connectionist architecture, the Sequential Neural Coding Network, that is
robust to forgetting when learning from streams of data points and, unlike
networks of today, does not learn via the immensely popular back-propagation of
errors. Grounded in the neurocognitive theory of predictive processing, our
model adapts its synapses in a biologically-plausible fashion, while another,
complementary neural system rapidly learns to direct and control this
cortex-like structure by mimicking the task-executive control functionality of
the basal ganglia. In our experiments, we demonstrate that our self-organizing
system experiences significantly less forgetting as compared to standard neural
models and outperforms a wide swath of previously proposed methods even though
it is trained across task datasets in a stream-like fashion. The promising
performance of our complementary system on benchmarks, e.g., SplitMNIST, Split
Fashion MNIST, and Split NotMNIST, offers evidence that by incorporating
mechanisms prominent in real neuronal systems, such as competition, sparse
activation patterns, and iterative input processing, a new possibility for
tackling the grand challenge of lifelong machine learning opens up.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scalable Low-Rank Tensor Learning for Spatiotemporal Traffic Data Imputation. (arXiv:2008.03194v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1">Xinyu Chen</a>, <a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1">Yixian Chen</a>, <a href="http://arxiv.org/find/stat/1/au:+Saunier_N/0/1/0/all/0/1">Nicolas Saunier</a>, <a href="http://arxiv.org/find/stat/1/au:+Sun_L/0/1/0/all/0/1">Lijun Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.03194">
                                    <div class="article-summary-box-inner">
                                        <span>Missing value problem in spatiotemporal traffic data has long been a
challenging topic, in particular for large-scale and high-dimensional data with
complex missing mechanisms and diverse degrees of missingness. Recent studies
based on tensor nuclear norm have demonstrated the superiority of tensor
learning in imputation tasks by effectively characterizing the complex
correlations/dependencies in spatiotemporal data. However, despite the
promising results, these approaches do not scale well to large data tensors. In
this paper, we focus on addressing the missing data imputation problem for
large-scale spatiotemporal traffic data. To achieve both high accuracy and
efficiency, we develop a scalable tensor learning model -- Low-Tubal-Rank
Smoothing Tensor Completion (LSTC-Tubal) -- based on the existing framework of
Low-Rank Tensor Completion, which is well-suited for spatiotemporal traffic
data that is characterized by multidimensional structure of location$\times$
time of day $\times$ day. In particular, the proposed LSTC-Tubal model involves
a scalable tensor nuclear norm minimization scheme by integrating linear
unitary transformation. Therefore, tensor nuclear norm minimization can be
solved by singular value thresholding on the transformed matrix of each day
while the day-to-day correlation can be effectively preserved by the unitary
transform matrix. We compare LSTC-Tubal with state-of-the-art baseline models,
and find that LSTC-Tubal can achieve competitive accuracy with a significantly
lower computational cost. In addition, the LSTC-Tubal will also benefit other
tasks in modeling large-scale spatiotemporal traffic data, such as
network-level traffic forecasting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Solving Graph-based Public Good Games with Tree Search and Imitation Learning. (arXiv:2106.06762v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Darvariu_V/0/1/0/all/0/1">Victor-Alexandru Darvariu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hailes_S/0/1/0/all/0/1">Stephen Hailes</a>, <a href="http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1">Mirco Musolesi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06762">
                                    <div class="article-summary-box-inner">
                                        <span>Public goods games represent insightful settings for studying incentives for
individual agents to make contributions that, while costly for each of them,
benefit the wider society. In this work, we adopt the perspective of a central
planner with a global view of a network of self-interested agents and the goal
of maximizing some desired property in the context of a best-shot public goods
game. Existing algorithms for this known NP-complete problem find solutions
that are sub-optimal and cannot optimize for criteria other than social
welfare.

In order to efficiently solve public goods games, our proposed method
directly exploits the correspondence between equilibria and the Maximal
Independent Set (mIS) structural property of graphs. In particular, we define a
Markov Decision Process, which incrementally generates an mIS, and adopt a
planning method to search for equilibria, outperforming existing methods.
Furthermore, we devise an imitation learning technique that uses demonstrations
of the search to obtain a graph neural network parametrized policy which
quickly generalizes to unseen game instances. Our evaluation results show that
this policy is able to reach 99.5% of the performance of the planning method
while being approximately three orders of magnitude faster to evaluate on the
largest graphs tested. The methods presented in this work can be applied to a
large class of public goods games of potentially high societal impact.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Information Obfuscation of Graph Neural Networks. (arXiv:2009.13504v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liao_P/0/1/0/all/0/1">Peiyuan Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Han Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Keyulu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1">Tommi Jaakkola</a>, <a href="http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1">Geoffrey Gordon</a>, <a href="http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1">Stefanie Jegelka</a>, <a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1">Ruslan Salakhutdinov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13504">
                                    <div class="article-summary-box-inner">
                                        <span>While the advent of Graph Neural Networks (GNNs) has greatly improved node
and graph representation learning in many applications, the neighborhood
aggregation scheme exposes additional vulnerabilities to adversaries seeking to
extract node-level information about sensitive attributes. In this paper, we
study the problem of protecting sensitive attributes by information obfuscation
when learning with graph structured data. We propose a framework to locally
filter out pre-determined sensitive attributes via adversarial training with
the total variation and the Wasserstein distance. Our method creates a strong
defense against inference attacks, while only suffering small loss in task
performance. Theoretically, we analyze the effectiveness of our framework
against a worst-case adversary, and characterize an inherent trade-off between
maximizing predictive accuracy and minimizing information leakage. Experiments
across multiple datasets from recommender systems, knowledge graphs and quantum
chemistry demonstrate that the proposed approach provides a robust defense
across various graph structures and tasks, while producing competitive GNN
encoders for downstream tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">NDPNet: A novel non-linear data projection network for few-shot fine-gained image classification. (arXiv:2106.06988v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhangy_W/0/1/0/all/0/1">Weichuan Zhangy</a>, <a href="http://arxiv.org/find/cs/1/au:+Liuy_X/0/1/0/all/0/1">Xuefang Liuy</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1">Zhe Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yongsheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Changming Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06988">
                                    <div class="article-summary-box-inner">
                                        <span>Metric-based few-shot fine-grained image classification (FSFGIC) aims to
learn a transferable feature embedding network by estimating the similarities
between query images and support classes from very few examples. In this work,
we propose, for the first time, to introduce the non-linear data projection
concept into the design of FSFGIC architecture in order to address the limited
sample problem in few-shot learning and at the same time to increase the
discriminability of the model for fine-grained image classification.
Specifically, we first design a feature re-abstraction embedding network that
has the ability to not only obtain the required semantic features for effective
metric learning but also re-enhance such features with finer details from input
images. Then the descriptors of the query images and the support classes are
projected into different non-linear spaces in our proposed similarity metric
learning network to learn discriminative projection factors. This design can
effectively operate in the challenging and restricted condition of a FSFGIC
task for making the distance between the samples within the same class smaller
and the distance between samples from different classes larger and for reducing
the coupling relationship between samples from different categories.
Furthermore, a novel similarity measure based on the proposed non-linear data
project is presented for evaluating the relationships of feature information
between a query image and a support set. It is worth to note that our proposed
architecture can be easily embedded into any episodic training mechanisms for
end-to-end training from scratch. Extensive experiments on FSFGIC tasks
demonstrate the superiority of the proposed methods over the state-of-the-art
benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FPT Approximation for Socially Fair Clustering. (arXiv:2106.06755v1 [cs.DS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Goyal_D/0/1/0/all/0/1">Dishant Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaiswal_R/0/1/0/all/0/1">Ragesh Jaiswal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06755">
                                    <div class="article-summary-box-inner">
                                        <span>In this work, we study the socially fair $k$-median/$k$-means problem. We are
given a set of points $P$ in a metric space $\mathcal{X}$ with a distance
function $d(.,.)$. There are $\ell$ groups: $P_1,\dotsc,P_{\ell} \subseteq P$.
We are also given a set $F$ of feasible centers in $\mathcal{X}$. The goal of
the socially fair $k$-median problem is to find a set $C \subseteq F$ of $k$
centers that minimizes the maximum average cost over all the groups. That is,
find $C$ that minimizes the objective function $\Phi(C,P) \equiv \max_{j}
\sum_{x \in P_j} d(C,x)/|P_j|$, where $d(C,x)$ is the distance of $x$ to the
closest center in $C$. The socially fair $k$-means problem is defined similarly
by using squared distances, i.e., $d^{2}(.,.)$ instead of $d(.,.)$. In this
work, we design $(5+\varepsilon)$ and $(33 + \varepsilon)$ approximation
algorithms for the socially fair $k$-median and $k$-means problems,
respectively. For the parameters: $k$ and $\ell$, the algorithms have an FPT
(fixed parameter tractable) running time of $f(k,\ell,\varepsilon) \cdot n$ for
$f(k,\ell,\varepsilon) &#x3D; 2^{{O}(k \, \ell/\varepsilon)}$ and $n &#x3D; |P \cup F|$.
We also study a special case of the problem where the centers are allowed to be
chosen from the point set $P$, i.e., $P \subseteq F$. For this special case,
our algorithms give better approximation guarantees of $(4+\varepsilon)$ and
$(18+\varepsilon)$ for the socially fair $k$-median and $k$-means problems,
respectively. Furthermore, we convert these algorithms to constant pass
log-space streaming algorithms. Lastly, we show FPT hardness of approximation
results for the problem with a small gap between our upper and lower bounds.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Path Features and Neural Path Kernel : Understanding the role of gates in deep learning. (arXiv:2006.10529v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_C/0/1/0/all/0/1">Chandrashekar Lakshminarayanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Amit Vikram Singh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10529">
                                    <div class="article-summary-box-inner">
                                        <span>Rectified linear unit (ReLU) activations can also be thought of as &#x27;gates&#x27;,
which, either pass or stop their pre-activation input when they are &#x27;on&#x27; (when
the pre-activation input is positive) or &#x27;off&#x27; (when the pre-activation input
is negative) respectively. A deep neural network (DNN) with ReLU activations
has many gates, and the on/off status of each gate changes across input
examples as well as network weights. For a given input example, only a subset
of gates are &#x27;active&#x27;, i.e., on, and the sub-network of weights connected to
these active gates is responsible for producing the output. At randomised
initialisation, the active sub-network corresponding to a given input example
is random. During training, as the weights are learnt, the active sub-networks
are also learnt, and potentially hold very valuable information. In this paper,
we analytically characterise the role of active sub-networks in deep learning.
To this end, we encode the on/off state of the gates of a given input in a
novel &#x27;neural path feature&#x27; (NPF), and the weights of the DNN are encoded in a
novel &#x27;neural path value&#x27; (NPV). Further, we show that the output of network is
indeed the inner product of NPF and NPV. The main result of the paper shows
that the &#x27;neural path kernel&#x27; associated with the NPF is a fundamental quantity
that characterises the information stored in the gates of a DNN. We show via
experiments (on MNIST and CIFAR-10) that in standard DNNs with ReLU activations
NPFs are learnt during training and such learning is key for generalisation.
Furthermore, NPFs and NPVs can be learnt in two separate networks and such
learning also generalises well in experiments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability. (arXiv:2006.14512v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1">Kaizhao Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jacky Y. Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1">Oluwasanmi Koyejo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14512">
                                    <div class="article-summary-box-inner">
                                        <span>Knowledge transferability, or transfer learning, has been widely adopted to
allow a pre-trained model in the source domain to be effectively adapted to
downstream tasks in the target domain. It is thus important to explore and
understand the factors affecting knowledge transferability. In this paper, as
the first work, we analyze and demonstrate the connections between knowledge
transferability and another important phenomenon--adversarial transferability,
\emph{i.e.}, adversarial examples generated against one model can be
transferred to attack other models. Our theoretical studies show that
adversarial transferability indicates knowledge transferability and vice versa.
Moreover, based on the theoretical insights, we propose two practical
adversarial transferability metrics to characterize this process, serving as
bidirectional indicators between adversarial and knowledge transferability. We
conduct extensive experiments for different scenarios on diverse datasets,
showing a positive correlation between adversarial transferability and
knowledge transferability. Our findings will shed light on future research
about effective knowledge transfer learning and adversarial transferability
analyses.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Prediction with Unpredictable Feature Evolution. (arXiv:1904.12171v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1">Bo-Jian Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lijun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhi-Hua Zhou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.12171">
                                    <div class="article-summary-box-inner">
                                        <span>Learning with feature evolution studies the scenario where the features of
the data streams can evolve, i.e., old features vanish and new features emerge.
Its goal is to keep the model always performing well even when the features
happen to evolve. To tackle this problem, canonical methods assume that the old
features will vanish simultaneously and the new features themselves will emerge
simultaneously as well. They also assume there is an overlapping period where
old and new features both exist when the feature space starts to change.
However, in reality, the feature evolution could be unpredictable, which means
the features can vanish or emerge arbitrarily, causing the overlapping period
incomplete. In this paper, we propose a novel paradigm: Prediction with
Unpredictable Feature Evolution (PUFE) where the feature evolution is
unpredictable. To address this problem, we fill the incomplete overlapping
period and formulate it as a new matrix completion problem. We give a
theoretical bound on the least number of observed entries to make the
overlapping period intact. With this intact overlapping period, we leverage an
ensemble method to take the advantage of both the old and new feature spaces
without manually deciding which base models should be incorporated. Theoretical
and experimental results validate that our method can always follow the best
base models and thus realize the goal of learning with feature evolution.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Transferability of Spectral Graph Convolutional Neural Networks. (arXiv:1907.12972v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Levie_R/0/1/0/all/0/1">Ron Levie</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bucci_L/0/1/0/all/0/1">Lorenzo Bucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1">Michael M. Bronstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1">Gitta Kutyniok</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.12972">
                                    <div class="article-summary-box-inner">
                                        <span>This paper focuses on spectral graph convolutional neural networks
(ConvNets), where filters are defined as elementwise multiplication in the
frequency domain of a graph. In machine learning settings where the dataset
consists of signals defined on many different graphs, the trained ConvNet
should generalize to signals on graphs unseen in the training set. It is thus
important to transfer ConvNets between graphs. Transferability, which is a
certain type of generalization capability, can be loosely defined as follows:
if two graphs describe the same phenomenon, then a single filter or ConvNet
should have similar repercussions on both graphs. This paper aims at debunking
the common misconception that spectral filters are not transferable. We show
that if two graphs discretize the same &quot;continuous&quot; space, then a spectral
filter or ConvNet has approximately the same repercussion on both graphs. Our
analysis is more permissive than the standard analysis. Transferability is
typically described as the robustness of the filter to small graph
perturbations and re-indexing of the vertices. Our analysis accounts also for
large graph perturbations. We prove transferability between graphs that can
have completely different dimensions and topologies, only requiring that both
graphs discretize the same underlying space in some generic sense.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Disrupting Model Training with Adversarial Shortcuts. (arXiv:2106.06654v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1">Ivan Evtimov</a>, <a href="http://arxiv.org/find/cs/1/au:+Covert_I/0/1/0/all/0/1">Ian Covert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1">Aditya Kusupati</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohno_T/0/1/0/all/0/1">Tadayoshi Kohno</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06654">
                                    <div class="article-summary-box-inner">
                                        <span>When data is publicly released for human consumption, it is unclear how to
prevent its unauthorized usage for machine learning purposes. Successful model
training may be preventable with carefully designed dataset modifications, and
we present a proof-of-concept approach for the image classification setting. We
propose methods based on the notion of adversarial shortcuts, which encourage
models to rely on non-robust signals rather than semantic features, and our
experiments demonstrate that these measures successfully prevent deep learning
models from achieving high accuracy on real, unmodified data examples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Shuffling Framework for Local Differential Privacy. (arXiv:2106.06603v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Meehan_C/0/1/0/all/0/1">Casey Meehan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1">Amrita Roy Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1">Kamalika Chaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1">Somesh Jha</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06603">
                                    <div class="article-summary-box-inner">
                                        <span>ldp deployments are vulnerable to inference attacks as an adversary can link
the noisy responses to their identity and subsequently, auxiliary information
using the order of the data. An alternative model, shuffle DP, prevents this by
shuffling the noisy responses uniformly at random. However, this limits the
data learnability -- only symmetric functions (input order agnostic) can be
learned. In this paper, we strike a balance and propose a generalized shuffling
framework that interpolates between the two deployment models. We show that
systematic shuffling of the noisy responses can thwart specific inference
attacks while retaining some meaningful data learnability. To this end, we
propose a novel privacy guarantee, d-sigma privacy, that captures the privacy
of the order of a data sequence. d-sigma privacy allows tuning the granularity
at which the ordinal information is maintained, which formalizes the degree the
resistance to inference attacks trading it off with data learnability.
Additionally, we propose a novel shuffling mechanism that can achieve d-sigma
privacy and demonstrate the practicality of our mechanism via evaluation on
real-world datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On The Radon-Nikodym Spectral Approach With Optimal Clustering. (arXiv:1906.00460v16 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Malyshkin_V/0/1/0/all/0/1">Vladislav Gennadievich Malyshkin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.00460">
                                    <div class="article-summary-box-inner">
                                        <span>Problems of interpolation, classification, and clustering are considered. In
the tenets of Radon--Nikodym approach $\langle f(\mathbf{x})\psi^2 \rangle /
\langle\psi^2\rangle$, where the $\psi(\mathbf{x})$ is a linear function on
input attributes, all the answers are obtained from a generalized eigenproblem
$|f|\psi^{[i]}\rangle &#x3D; \lambda^{[i]} |\psi^{[i]}\rangle$. The solution to the
interpolation problem is a regular Radon-Nikodym derivative. The solution to
the classification problem requires prior and posterior probabilities that are
obtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian
approach new observations change only outcome probabilities, in the
Radon-Nikodym approach not only outcome probabilities but also the probability
space $|\psi^{[i]}\rangle$ change with new observations. This is a remarkable
feature of the approach: both the probabilities and the probability space are
constructed from the data. The Lebesgue quadrature technique can be also
applied to the optimal clustering problem. The problem is solved by
constructing a Gaussian quadrature on the Lebesgue measure. A distinguishing
feature of the Radon-Nikodym approach is the knowledge of the invariant group:
all the answers are invariant relatively any non-degenerated linear transform
of input vector $\mathbf{x}$ components. A software product implementing the
algorithms of interpolation, classification, and optimal clustering is
available from the authors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dynamic Clone Transformer for Efficient Convolutional Neural Netwoks. (arXiv:2106.06778v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1">Longqing Ye</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06778">
                                    <div class="article-summary-box-inner">
                                        <span>Convolutional networks (ConvNets) have shown impressive capability to solve
various vision tasks. Nevertheless, the trade-off between performance and
efficiency is still a challenge for a feasible model deployment on
resource-constrained platforms. In this paper, we introduce a novel concept
termed multi-path fully connected pattern (MPFC) to rethink the
interdependencies of topology pattern, accuracy and efficiency for ConvNets.
Inspired by MPFC, we further propose a dual-branch module named dynamic clone
transformer (DCT) where one branch generates multiple replicas from inputs and
another branch reforms those clones through a series of difference vectors
conditional on inputs itself to produce more variants. This operation allows
the self-expansion of channel-wise information in a data-driven way with little
computational cost while providing sufficient learning capacity, which is a
potential unit to replace computationally expensive pointwise convolution as an
expansion layer in the bottleneck structure.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Expected Tight Bounds for Robust Training. (arXiv:1905.12418v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Alsubaihi_S/0/1/0/all/0/1">Salman Alsubaihi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1">Adel Bibi</a>, <a href="http://arxiv.org/find/cs/1/au:+Alfadly_M/0/1/0/all/0/1">Modar Alfadly</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1">Abdullah Hamdi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1">Bernard Ghanem</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.12418">
                                    <div class="article-summary-box-inner">
                                        <span>Training Deep Neural Networks that are robust to norm bounded adversarial
attacks remains an elusive problem. While exact and inexact verification-based
methods are generally too expensive to train large networks, it was
demonstrated that bounded input intervals can be inexpensively propagated from
a layer to another through deep networks. This interval bound propagation
approach (IBP) not only has improved both robustness and certified accuracy but
was the first to be employed on large/deep networks. However, due to the very
loose nature of the IBP bounds, the required training procedure is complex and
involved. In this paper, we closely examine the bounds of a block of layers
composed in the form of Affine-ReLU-Affine. To this end, we propose expected
tight bounds (true bounds in expectation), referred to as ETB, which are
provably tighter than IBP bounds in expectation. We then extend this result to
deeper networks through blockwise propagation and show that we can achieve
orders of magnitudes tighter bounds compared to IBP. Furthermore, using a
simple standard training procedure, we can achieve impressive
robustness-accuracy trade-off on both MNIST and CIFAR10.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Weakly-supervised Graph Meta-learning for Few-shot Node Classification. (arXiv:2106.06873v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1">Kaize Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianling Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jundong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1">James Caverlee</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Huan Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06873">
                                    <div class="article-summary-box-inner">
                                        <span>Graphs are widely used to model the relational structure of data, and the
research of graph machine learning (ML) has a wide spectrum of applications
ranging from drug design in molecular graphs to friendship recommendation in
social networks. Prevailing approaches for graph ML typically require abundant
labeled instances in achieving satisfactory results, which is commonly
infeasible in real-world scenarios since labeled data for newly emerged
concepts (e.g., new categorizations of nodes) on graphs is limited. Though
meta-learning has been applied to different few-shot graph learning problems,
most existing efforts predominately assume that all the data from those seen
classes is gold-labeled, while those methods may lose their efficacy when the
seen data is weakly-labeled with severe label noise. As such, we aim to
investigate a novel problem of weakly-supervised graph meta-learning for
improving the model robustness in terms of knowledge transfer. To achieve this
goal, we propose a new graph meta-learning framework -- Graph Hallucination
Networks (Meta-GHN) in this paper. Based on a new robustness-enhanced episodic
training, Meta-GHN is meta-learned to hallucinate clean node representations
from weakly-labeled data and extracts highly transferable meta-knowledge, which
enables the model to quickly adapt to unseen tasks with few labeled instances.
Extensive experiments demonstrate the superiority of Meta-GHN over existing
graph meta-learning studies on the task of weakly-supervised few-shot node
classification.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Optimal Counterfactual Explanations in Tree Ensembles. (arXiv:2106.06631v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Parmentier_A/0/1/0/all/0/1">Axel Parmentier</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidal_T/0/1/0/all/0/1">Thibaut Vidal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06631">
                                    <div class="article-summary-box-inner">
                                        <span>Counterfactual explanations are usually generated through heuristics that are
sensitive to the search&#x27;s initial conditions. The absence of guarantees of
performance and robustness hinders trustworthiness. In this paper, we take a
disciplined approach towards counterfactual explanations for tree ensembles. We
advocate for a model-based search aiming at &quot;optimal&quot; explanations and propose
efficient mixed-integer programming approaches. We show that isolation forests
can be modeled within our framework to focus the search on plausible
explanations with a low outlier score. We provide comprehensive coverage of
additional constraints that model important objectives, heterogeneous data
types, structural constraints on the feature space, along with resource and
actionability restrictions. Our experimental analyses demonstrate that the
proposed search approach requires a computational effort that is orders of
magnitude smaller than previous mathematical programming algorithms. It scales
up to large data sets and tree ensembles, where it provides, within seconds,
systematic explanations grounded on well-defined models solved to optimality.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scalars are universal: Gauge-equivariant machine learning, structured like classical physics. (arXiv:2106.06610v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Villar_S/0/1/0/all/0/1">Soledad Villar</a> (JHU), <a href="http://arxiv.org/find/cs/1/au:+Hogg_D/0/1/0/all/0/1">David W.Hogg</a> (Flatiron, NYU), <a href="http://arxiv.org/find/cs/1/au:+Storey_Fisher_K/0/1/0/all/0/1">Kate Storey-Fisher</a> (NYU), <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Weichi Yao</a> (NYU), <a href="http://arxiv.org/find/cs/1/au:+Blum_Smith_B/0/1/0/all/0/1">Ben Blum-Smith</a> (NYU)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06610">
                                    <div class="article-summary-box-inner">
                                        <span>There has been enormous progress in the last few years in designing
conceivable (though not always practical) neural networks that respect the
gauge symmetries -- or coordinate freedom -- of physical law. Some of these
frameworks make use of irreducible representations, some make use of higher
order tensor objects, and some apply symmetry-enforcing constraints. Different
physical laws obey different combinations of fundamental symmetries, but a
large fraction (possibly all) of classical physics is equivariant to
translation, rotation, reflection (parity), boost (relativity), and
permutations. Here we show that it is simple to parameterize universally
approximating polynomial functions that are equivariant under these symmetries,
or under the Euclidean, Lorentz, and Poincar\&#x27;e groups, at any dimensionality
$d$. The key observation is that nonlinear O($d$)-equivariant (and
related-group-equivariant) functions can be expressed in terms of a lightweight
collection of scalars -- scalar products and scalar contractions of the scalar,
vector, and tensor inputs. These results demonstrate theoretically that
gauge-invariant deep learning models for classical physics with good scaling
for large problems are feasible right now.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ATRAS: Adversarially Trained Robust Architecture Search. (arXiv:2106.06917v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Alparslan_Y/0/1/0/all/0/1">Yigit Alparslan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1">Edward Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06917">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we explore the effect of architecture completeness on
adversarial robustness. We train models with different architectures on
CIFAR-10 and MNIST dataset. For each model, we vary different number of layers
and different number of nodes in the layer. For every architecture candidate,
we use Fast Gradient Sign Method (FGSM) to generate untargeted adversarial
attacks and use adversarial training to defend against those attacks. For each
architecture candidate, we report pre-attack, post-attack and post-defense
accuracy for the model as well as the architecture parameters and the impact of
completeness to the model accuracies.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Doubly Non-Central Beta Matrix Factorization for DNA Methylation Data. (arXiv:2106.06691v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Schein_A/0/1/0/all/0/1">Aaron Schein</a>, <a href="http://arxiv.org/find/stat/1/au:+Nagulpally_A/0/1/0/all/0/1">Anjali Nagulpally</a>, <a href="http://arxiv.org/find/stat/1/au:+Wallach_H/0/1/0/all/0/1">Hanna Wallach</a>, <a href="http://arxiv.org/find/stat/1/au:+Flaherty_P/0/1/0/all/0/1">Patrick Flaherty</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06691">
                                    <div class="article-summary-box-inner">
                                        <span>We present a new non-negative matrix factorization model for $(0,1)$
bounded-support data based on the doubly non-central beta (DNCB) distribution,
a generalization of the beta distribution. The expressiveness of the DNCB
distribution is particularly useful for modeling DNA methylation datasets,
which are typically highly dispersed and multi-modal; however, the model
structure is sufficiently general that it can be adapted to many other domains
where latent representations of $(0,1)$ bounded-support data are of interest.
Although the DNCB distribution lacks a closed-form conjugate prior, several
augmentations let us derive an efficient posterior inference algorithm composed
entirely of analytic updates. Our model improves out-of-sample predictive
performance on both real and synthetic DNA methylation datasets over
state-of-the-art methods in bioinformatics. In addition, our model yields
meaningful latent representations that accord with existing biological
knowledge.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Random Shuffling Beats SGD Only After Many Epochs on Ill-Conditioned Problems. (arXiv:2106.06880v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Safran_I/0/1/0/all/0/1">Itay Safran</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1">Ohad Shamir</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06880">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, there has been much interest in studying the convergence rates of
without-replacement SGD, and proving that it is faster than with-replacement
SGD in the worst case. However, these works ignore or do not provide tight
bounds in terms of the problem&#x27;s geometry, including its condition number.
Perhaps surprisingly, we prove that when the condition number is taken into
account, without-replacement SGD \emph{does not} significantly improve on
with-replacement SGD in terms of worst-case bounds, unless the number of epochs
(passes over the data) is larger than the condition number. Since many problems
in machine learning and other areas are both ill-conditioned and involve large
datasets, this indicates that without-replacement does not necessarily improve
over with-replacement sampling for realistic iteration budgets. We show this by
providing new lower and upper bounds which are tight (up to log factors), for
quadratic problems with commuting quadratic terms, precisely quantifying the
dependence on the problem parameters.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">D2C: Diffusion-Denoising Models for Few-shot Conditional Generation. (arXiv:2106.06819v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">Abhishek Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jiaming Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Chenlin Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1">Stefano Ermon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06819">
                                    <div class="article-summary-box-inner">
                                        <span>Conditional generative models of high-dimensional images have many
applications, but supervision signals from conditions to images can be
expensive to acquire. This paper describes Diffusion-Decoding models with
Contrastive representations (D2C), a paradigm for training unconditional
variational autoencoders (VAEs) for few-shot conditional image generation. D2C
uses a learned diffusion-based prior over the latent representations to improve
generation and contrastive self-supervised learning to improve representation
quality. D2C can adapt to novel generation tasks conditioned on labels or
manipulation constraints, by learning from as few as 100 labeled examples. On
conditional generation from new labels, D2C achieves superior performance over
state-of-the-art VAEs and diffusion models. On conditional image manipulation,
D2C generations are two orders of magnitude faster to produce over StyleGAN2
ones and are preferred by 50% - 60% of the human evaluators in a double-blind
study.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bayesian Inference Gaussian Process Multiproxy Alignment of Continuous Signals (BIGMACS): Applications for Paleoceanography. (arXiv:1907.08738v4 [stat.AP] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Lee_T/0/1/0/all/0/1">Taehee Lee</a>, <a href="http://arxiv.org/find/stat/1/au:+Lisiecki_L/0/1/0/all/0/1">Lorraine E. Lisiecki</a>, <a href="http://arxiv.org/find/stat/1/au:+Rand_D/0/1/0/all/0/1">Devin Rand</a>, <a href="http://arxiv.org/find/stat/1/au:+Gebbie_G/0/1/0/all/0/1">Geoffrey Gebbie</a>, <a href="http://arxiv.org/find/stat/1/au:+Lawrence_C/0/1/0/all/0/1">Charles E. Lawrence</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.08738">
                                    <div class="article-summary-box-inner">
                                        <span>We first introduce a novel profile-based alignment algorithm, the multiple
continuous Signal Alignment algorithm with Gaussian Process Regression profiles
(SA-GPR). SA-GPR addresses the limitations of currently available signal
alignment methods by adopting a hybrid of the particle smoothing and
Markov-chain Monte Carlo (MCMC) algorithms to align signals, and by applying
the Gaussian process regression to construct profiles to be aligned
continuously. SA-GPR shares all the strengths of the existing alignment
algorithms that depend on profiles but is more exact in the sense that profiles
do not need to be discretized as sequential bins. The uncertainty of
performance over the resolution of such bins is thereby eliminated. This
methodology produces alignments that are consistent, that regularize extreme
cases, and that properly reflect the inherent uncertainty.

Then we extend SA-GPR to a specific problem in the field of paleoceanography
with a method called Bayesian Inference Gaussian Process Multiproxy Alignment
of Continuous Signals (BIGMACS). The goal of BIGMACS is to infer continuous
ages for ocean sediment cores using two classes of age proxies: proxies that
explicitly return calendar ages (e.g., radiocarbon) and those used to
synchronize ages in multiple marine records (e.g., an oxygen isotope based
marine proxy known as benthic ${\delta}^{18}{\rm O}$). BIGMACS integrates these
two proxies by iteratively performing two steps: profile construction from
benthic ${\delta}^{18}{\rm O}$ age models and alignment of each core to the
profile also reflecting radiocarbon dates. We use BIGMACS to construct a new
Deep Northeastern Atlantic stack (i.e., a profile from a particular benthic
${\delta}^{18}{\rm O}$ records) of five ocean sediment cores. We conclude by
constructing multiproxy age models for two additional cores from the same
region by aligning them to the stack.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CARTL: Cooperative Adversarially-Robust Transfer Learning. (arXiv:2106.06667v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hongxin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yinli Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qi Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06667">
                                    <div class="article-summary-box-inner">
                                        <span>Transfer learning eases the burden of training a well-performed model from
scratch, especially when training data is scarce and computation power is
limited. In deep learning, a typical strategy for transfer learning is to
freeze the early layers of a pre-trained model and fine-tune the rest of its
layers on the target domain. Previous work focuses on the accuracy of the
transferred model but neglects the transfer of adversarial robustness. In this
work, we first show that transfer learning improves the accuracy on the target
domain but degrades the inherited robustness of the target model. To address
such a problem, we propose a novel cooperative adversarially-robust transfer
learning (CARTL) by pre-training the model via feature distance minimization
and fine-tuning the pre-trained model with non-expansive fine-tuning for target
domain tasks. Empirical results show that CARTL improves the inherited
robustness by about 28% at most compared with the baseline with the same degree
of accuracy. Furthermore, we study the relationship between the batch
normalization (BN) layers and the robustness in the context of transfer
learning, and we reveal that freezing BN layers can further boost the
robustness transfer.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Two-way Spectrum Pursuit for CUR Decomposition and Its Application in Joint Column/Row Subset Selection. (arXiv:2106.06983v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Esmaeili_A/0/1/0/all/0/1">Ashkan Esmaeili</a>, <a href="http://arxiv.org/find/cs/1/au:+Joneidi_M/0/1/0/all/0/1">Mohsen Joneidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Salimitari_M/0/1/0/all/0/1">Mehrdad Salimitari</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1">Umar Khalid</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1">Nazanin Rahnavard</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06983">
                                    <div class="article-summary-box-inner">
                                        <span>The problem of simultaneous column and row subset selection is addressed in
this paper. The column space and row space of a matrix are spanned by its left
and right singular vectors, respectively. However, the singular vectors are not
within actual columns/rows of the matrix. In this paper, an iterative approach
is proposed to capture the most structural information of columns/rows via
selecting a subset of actual columns/rows. This algorithm is referred to as
two-way spectrum pursuit (TWSP) which provides us with an accurate solution for
the CUR matrix decomposition. TWSP is applicable in a wide range of
applications since it enjoys a linear complexity w.r.t. number of original
columns/rows. We demonstrated the application of TWSP for joint channel and
sensor selection in cognitive radio networks, informative users and contents
detection, and efficient supervised data reduction.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Stochastic Alternating Direction Method of Multipliers for Byzantine-Robust Distributed Learning. (arXiv:2106.06891v1 [math.OC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Lin_F/0/1/0/all/0/1">Feng Lin</a>, <a href="http://arxiv.org/find/math/1/au:+Li_W/0/1/0/all/0/1">Weiyu Li</a>, <a href="http://arxiv.org/find/math/1/au:+Ling_Q/0/1/0/all/0/1">Qing Ling</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06891">
                                    <div class="article-summary-box-inner">
                                        <span>This paper aims to solve a distributed learning problem under Byzantine
attacks. In the underlying distributed system, a number of unknown but
malicious workers (termed as Byzantine workers) can send arbitrary messages to
the master and bias the learning process, due to data corruptions, computation
errors or malicious attacks. Prior work has considered a total variation (TV)
norm-penalized approximation formulation to handle the Byzantine attacks, where
the TV norm penalty forces the regular workers&#x27; local variables to be close,
and meanwhile, tolerates the outliers sent by the Byzantine workers. To solve
the TV norm-penalized approximation formulation, we propose a Byzantine-robust
stochastic alternating direction method of multipliers (ADMM) that fully
utilizes the separable problem structure. Theoretically, we prove that the
proposed method converges to a bounded neighborhood of the optimal solution at
a rate of O(1/k) under mild assumptions, where k is the number of iterations
and the size of neighborhood is determined by the number of Byzantine workers.
Numerical experiments on the MNIST and COVERTYPE datasets demonstrate the
effectiveness of the proposed method to various Byzantine attacks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Knowledge Consolidation based Class Incremental Online Learning with Limited Data. (arXiv:2106.06795v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1">Mohammed Asad Karim</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1">Vinay Kumar Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1">Pravendra Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1">Vinay Namboodiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1">Piyush Rai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06795">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a novel approach for class incremental online learning in a
limited data setting. This problem setting is challenging because of the
following constraints: (1) Classes are given incrementally, which necessitates
a class incremental learning approach; (2) Data for each class is given in an
online fashion, i.e., each training example is seen only once during training;
(3) Each class has very few training examples; and (4) We do not use or assume
access to any replay/memory to store data from previous classes. Therefore, in
this setting, we have to handle twofold problems of catastrophic forgetting and
overfitting. In our approach, we learn robust representations that are
generalizable across tasks without suffering from the problems of catastrophic
forgetting and overfitting to accommodate future classes with limited samples.
Our proposed method leverages the meta-learning framework with knowledge
consolidation. The meta-learning framework helps the model for rapid learning
when samples appear in an online fashion. Simultaneously, knowledge
consolidation helps to learn a robust representation against forgetting under
online updates to facilitate future learning. Our approach significantly
outperforms other methods on several benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Toward Accurate and Realistic Outfits Visualization with Attention to Details. (arXiv:2106.06593v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kedan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1">Min jin Chong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jeffrey Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingen Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06593">
                                    <div class="article-summary-box-inner">
                                        <span>Virtual try-on methods aim to generate images of fashion models wearing
arbitrary combinations of garments. This is a challenging task because the
generated image must appear realistic and accurately display the interaction
between garments. Prior works produce images that are filled with artifacts and
fail to capture important visual details necessary for commercial applications.
We propose Outfit Visualization Net (OVNet) to capture these important details
(e.g. buttons, shading, textures, realistic hemlines, and interactions between
garments) and produce high quality multiple-garment virtual try-on images.
OVNet consists of 1) a semantic layout generator and 2) an image generation
pipeline using multiple coordinated warps. We train the warper to output
multiple warps using a cascade loss, which refines each successive warp to
focus on poorly generated regions of a previous warp and yields consistent
improvements in detail. In addition, we introduce a method for matching outfits
with the most suitable model and produce significant improvements for both our
and other previous try-on methods. Through quantitative and qualitative
analysis, we demonstrate our method generates substantially higher-quality
studio images compared to prior works for multi-garment outfits. An interactive
interface powered by this method has been deployed on fashion e-commerce
websites and received overwhelmingly positive feedback.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Provable Adaptation across Multiway Domains via Representation Learning. (arXiv:2106.06657v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhili Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Shaobo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Simon S. Du</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06657">
                                    <div class="article-summary-box-inner">
                                        <span>This paper studies zero-shot domain adaptation where each domain is indexed
on a multi-dimensional array, and we only have data from a small subset of
domains. Our goal is to produce predictors that perform well on \emph{unseen}
domains. We propose a model which consists of a domain-invariant latent
representation layer and a domain-specific linear prediction layer with a
low-rank tensor structure. Theoretically, we present explicit sample complexity
bounds to characterize the prediction error on unseen domains in terms of the
number of domains with training data and the number of data per domain. To our
knowledge, this is the first finite-sample guarantee for zero-shot domain
adaptation. In addition, we provide experiments on two-way MNIST and four-way
fiber sensing datasets to demonstrate the effectiveness of our proposed model.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Assessing Multilingual Fairness in Pre-trained Multimodal Representations. (arXiv:2106.06683v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jialu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Eric Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06683">
                                    <div class="article-summary-box-inner">
                                        <span>Recently pre-trained multimodal models, such as CLIP, have received a surge
of attention for their exceptional capabilities towards connecting images and
natural language. The textual representations in English can be desirably
transferred to multilingualism and support promising downstream multimodal
tasks for different languages. Nevertheless, previous fairness discourse in
vision-and-language learning mainly focuses on monolingual representational
biases, and rarely scrutinizes the principles of multilingual fairness in this
multimodal setting, where one language is equated to a group of individuals and
images provide the universal grounding for bridging different languages.

In this paper, we provide a nuanced understanding of individual fairness and
group fairness by viewing language as the recipient of fairness notions. We
define new fairness notions within multilingual context and analytically
articulate that, pre-trained vision-and-language representations are
individually fair across languages but not guaranteed to group fairness.
Furthermore, we conduct extensive experiments to explore the prevalent group
disparity across languages and protected groups including race, gender and age.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.MM"">cs.MM updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Learning for Reversible Steganography: Principles and Insights. (arXiv:2106.06924v1 [cs.MM])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Ching-Chun Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sisheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1">Isao Echizen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1">Victor Sanchez</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chang-Tsun Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06924">
                                    <div class="article-summary-box-inner">
                                        <span>Deep-learning\textendash{centric} reversible steganography has emerged as a
promising research paradigm. A direct way of applying deep learning to
reversible steganography is to construct a pair of encoder and decoder, whose
parameters are trained jointly, thereby learning the steganographic system as a
whole. This end-to-end framework, however, falls short of the reversibility
requirement because it is difficult for this kind of monolithic system, as a
black box, to create or duplicate intricate reversible mechanisms. In response
to this issue, a recent approach is to carve up the steganographic system and
work on modules independently. In particular, neural networks are deployed in
an analytics module to learn the data distribution, while an established
mechanism is called upon to handle the remaining tasks. In this paper, we
investigate the modular framework and deploy deep neural networks in a
reversible steganographic scheme referred to as prediction-error modulation, in
which an analytics module serves the purpose of pixel intensity prediction. The
primary focus of this study is on deep-learning\textendash{based} context-aware
pixel intensity prediction. We address the unsolved issues reported in related
literature, including the impact of pixel initialisation on prediction accuracy
and the influence of uncertainty propagation in dual-layer embedding.
Furthermore, we establish a connection between context-aware pixel intensity
prediction and low-level computer vision and analyse the performance of several
advanced neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Lattice-Based Minimum-Distortion Data Hiding. (arXiv:2105.13096v2 [cs.IT] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jieni Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1">Junren Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1">Shanxiang Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1">Bingwen Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiabo Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13096">
                                    <div class="article-summary-box-inner">
                                        <span>Lattices have been conceived as a powerful tool for data hiding. While
conventional studies and applications focus on achieving the optimal robustness
versus distortion tradeoff, in some applications such as data hiding in
medical/physiological signals, the primary concern is to achieve a minimum
amount of distortion to the cover signal. In this paper, we revisit the
celebrated quantization index modulation (QIM) scheme and propose a
minimum-distortion version of it, referred to as MD-QIM. The crux of MD-QIM is
to move the data point to only the boundary of the Voronoi region of the
lattice point indexed by a message, which suffices for subsequent correct
decoding. At any fixed code rate, the scheme achieves the minimum amount of
distortion by sacrificing the robustness to the additive white Gaussian noise
(AWGN) attacks. Simulation results confirm that our scheme significantly
outperforms QIM in terms of mean square error (MSE), peak signal to noise ratio
(PSNR) and percentage residual difference (PRD).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-level Attention Fusion Network for Audio-visual Event Recognition. (arXiv:2106.06736v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Brousmiche_M/0/1/0/all/0/1">Mathilde Brousmiche</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1">Jean Rouat</a>, <a href="http://arxiv.org/find/cs/1/au:+Dupont_S/0/1/0/all/0/1">St&#xe9;phane Dupont</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06736">
                                    <div class="article-summary-box-inner">
                                        <span>Event classification is inherently sequential and multimodal. Therefore, deep
neural models need to dynamically focus on the most relevant time window and/or
modality of a video. In this study, we propose the Multi-level Attention Fusion
network (MAFnet), an architecture that can dynamically fuse visual and audio
information for event recognition. Inspired by prior studies in neuroscience,
we couple both modalities at different levels of visual and audio paths.
Furthermore, the network dynamically highlights a modality at a given time
window relevant to classify events. Experimental results in AVE (Audio-Visual
Event), UCF51, and Kinetics-Sounds datasets show that the approach can
effectively improve the accuracy in audio-visual event classification. Code is
available at: https://github.com/numediart/MAFnet</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
        </ul>
    </section>
    <section class="daily-content">
        <h2 class="daily-heading"><time datatime="2021-06-14">2021-06-14</time></h2>
        <ul class="sources card">
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.CL"">cs.CL updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unsupervised Knowledge Graph Alignment by Probabilistic Reasoning and Semantic Embedding. (arXiv:2105.05596v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1">Zhiyuan Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaoyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1">Yuejia Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05596">
                                    <div class="article-summary-box-inner">
                                        <span>Knowledge Graph (KG) alignment is to discover the mappings (i.e., equivalent
entities, relations, and others) between two KGs. The existing methods can be
divided into the embedding-based models, and the conventional reasoning and
lexical matching based systems. The former compute the similarity of entities
via their cross-KG embeddings, but they usually rely on an ideal supervised
learning setting for good performance and lack appropriate reasoning to avoid
logically wrong mappings; while the latter address the reasoning issue but are
poor at utilizing the KG graph structures and the entity contexts. In this
study, we aim at combining the above two solutions and thus propose an
iterative framework named PRASE which is based on probabilistic reasoning and
semantic embedding. It learns the KG embeddings via entity mappings from a
probabilistic reasoning system named PARIS, and feeds the resultant entity
mappings and embeddings back into PARIS for augmentation. The PRASE framework
is compatible with different embedding-based models, and our experiments on
multiple datasets have demonstrated its state-of-the-art performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">KLUE: Korean Language Understanding Evaluation. (arXiv:2105.09680v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sungjoon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1">Jihyung Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sungdong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1">Won Ik Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jiyoon Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jangwon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1">Chisung Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Junseong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yongsook Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1">Taehwan Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joohong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Juhyun Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1">Sungwon Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1">Younghoon Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1">Inkwon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1">Sangwoo Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dongjun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunwoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Myeonghwa Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1">Seongbo Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_S/0/1/0/all/0/1">Seungwon Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sunkyoung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1">Kyungtae Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jongwon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1">Kyumin Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1">Jamin Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seonghyun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1">Lucy Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1">Alice Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1">Jung-Woo Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1">Kyunghyun Cho</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09680">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE
is a collection of 8 Korean natural language understanding (NLU) tasks,
including Topic Classification, SemanticTextual Similarity, Natural Language
Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing,
Machine Reading Comprehension, and Dialogue State Tracking. We build all of the
tasks from scratch from diverse source corpora while respecting copyrights, to
ensure accessibility for anyone without any restrictions. With ethical
considerations in mind, we carefully design annotation protocols. Along with
the benchmark tasks and data, we provide suitable evaluation metrics and
fine-tuning recipes for pretrained language models for each task. We
furthermore release the pretrained language models (PLM), KLUE-BERT and
KLUE-RoBERTa, to help reproducing baseline models on KLUE and thereby
facilitate future research. We make a few interesting observations from the
preliminary experiments using the proposed KLUE benchmark suite, already
demonstrating the usefulness of this new benchmark suite. First, we find
KLUE-RoBERTa-large outperforms other baselines, including multilingual PLMs and
existing open-source Korean PLMs. Second, we see minimal degradation in
performance even when we replace personally identifiable information from the
pretraining corpus, suggesting that privacy and NLU capability are not at odds
with each other. Lastly, we find that using BPE tokenization in combination
with morpheme-level pre-tokenization is effective in tasks involving
morpheme-level tagging, detection and generation. In addition to accelerating
Korean NLP research, our comprehensive documentation on creating KLUE will
facilitate creating similar resources for other languages in the future. KLUE
is available at https://klue-benchmark.com.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Semi-Supervised and Unsupervised Sense Annotation via Translations. (arXiv:2106.06462v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1">Bradley Hauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1">Grzegorz Kondrak</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1">Yixing Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallik_A/0/1/0/all/0/1">Arnob Mallik</a>, <a href="http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1">Lili Mou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06462">
                                    <div class="article-summary-box-inner">
                                        <span>Acquisition of multilingual training data continues to be a challenge in word
sense disambiguation (WSD). To address this problem, unsupervised approaches
have been developed in recent years that automatically generate sense
annotations suitable for training supervised WSD systems. We present three new
methods to creating sense-annotated corpora, which leverage translations,
parallel corpora, lexical resources, and contextual and synset embeddings. Our
semi-supervised method applies machine translation to transfer existing sense
annotations to other languages. Our two unsupervised methods use a
knowledge-based WSD system to annotate a parallel corpus, and refine the
resulting sense annotations by identifying lexical translations. We obtain
state-of-the-art results on standard WSD benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">WAX-ML: A Python library for machine learning and feedback loops on streaming data. (arXiv:2106.06524v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Serie_E/0/1/0/all/0/1">Emmanuel S&#xe9;ri&#xe9;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06524">
                                    <div class="article-summary-box-inner">
                                        <span>Wax is what you put on a surfboard to avoid slipping. It is an essential tool
to go surfing... We introduce WAX-ML a research-oriented Python library
providing tools to design powerful machine learning algorithms and feedback
loops working on streaming data. It strives to complement JAX with tools
dedicated to time series. WAX-ML makes JAX-based programs easy to use for
end-users working with pandas and xarray for data manipulation. It provides a
simple mechanism for implementing feedback loops, allows the implementation of
online learning and reinforcement learning algorithms with functions, and makes
them easy to integrate by end-users working with the object-oriented
reinforcement learning framework from the Gym library. It is released with an
Apache open-source license on GitHub at https://github.com/eserie/wax-ml.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses. (arXiv:2106.06519v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ganesan_K/0/1/0/all/0/1">Karthik Ganesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bamdev_P/0/1/0/all/0/1">Pakhi Bamdev</a>, <a href="http://arxiv.org/find/cs/1/au:+B_J/0/1/0/all/0/1">Jaivarsan B</a>, <a href="http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1">Amresh Venugopal</a>, <a href="http://arxiv.org/find/cs/1/au:+Tushar_A/0/1/0/all/0/1">Abhinav Tushar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06519">
                                    <div class="article-summary-box-inner">
                                        <span>Spoken Language Understanding (SLU) systems parse speech into semantic
structures like dialog acts and slots. This involves the use of an Automatic
Speech Recognizer (ASR) to transcribe speech into multiple text alternatives
(hypotheses). Transcription errors, common in ASRs, impact downstream SLU
performance negatively. Approaches to mitigate such errors involve using richer
information from the ASR, either in form of N-best hypotheses or word-lattices.
We hypothesize that transformer models learn better with a simpler utterance
representation using the concatenation of the N-best ASR alternatives, where
each alternative is separated by a special delimiter [SEP]. In our work, we
test our hypothesis by using concatenated N-best ASR alternatives as the input
to transformer encoder models, namely BERT and XLM-RoBERTa, and achieve
performance equivalent to the prior state-of-the-art model on DSTC2 dataset. We
also show that our approach significantly outperforms the prior
state-of-the-art when subjected to the low data regime. Additionally, this
methodology is accessible to users of third-party ASR APIs which do not provide
word-lattice information.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">UnNatural Language Inference. (arXiv:2101.00010v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1">Koustuv Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1">Prasanna Parthasarathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1">Joelle Pineau</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1">Adina Williams</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00010">
                                    <div class="article-summary-box-inner">
                                        <span>Recent investigations into the inner-workings of state-of-the-art large-scale
pre-trained Transformer-based Natural Language Understanding (NLU) models
indicate that they appear to know humanlike syntax, at least to some extent. We
provide novel evidence that complicates this claim: we find that
state-of-the-art Natural Language Inference (NLI) models assign the same labels
to permuted examples as they do to the original, i.e. they are largely
invariant to random word-order permutations. This behavior notably differs from
that of humans; we struggle with ungrammatical sentences. To measure the
severity of this issue, we propose a suite of metrics and investigate which
properties of particular permutations lead models to be word-order invariant.
In the MNLI dataset, for example, we find almost all (98.7%) examples contain
at least one permutation which elicits the gold label. Models are sometimes
even able to assign gold labels to permutations that they originally failed to
predict correctly. We provide a comprehensive empirical evaluation of this
phenomenon, and further show that this issue exists for both Transformers and
pre-Transformer RNN / ConvNet based encoders, as well as across multiple
languages (English and Mandarin Chinese). Our code and data are available at
https://github.com/facebookresearch/unlu.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">What is Multimodality?. (arXiv:2103.06304v3 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1">Letitia Parcalabescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Trost_N/0/1/0/all/0/1">Nils Trost</a>, <a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1">Anette Frank</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06304">
                                    <div class="article-summary-box-inner">
                                        <span>The last years have shown rapid developments in the field of multimodal
machine learning, combining e.g., vision, text or speech. In this position
paper we explain how the field uses outdated definitions of multimodality that
prove unfit for the machine learning era. We propose a new task-relative
definition of (multi)modality in the context of multimodal machine learning
that focuses on representations and information that are relevant for a given
machine learning task. With our new definition of multimodality we aim to
provide a missing foundation for multimodal research, an important component of
language grounding and a crucial milestone towards NLU.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">HMM-Free Encoder Pre-Training for Streaming RNN Transducer. (arXiv:2104.10764v2 [eess.AS] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1">Lu Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1">Jingyu Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1">Yufeng Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1">Junfeng Hou</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1">Jinkun Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_Z/0/1/0/all/0/1">Zejun Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10764">
                                    <div class="article-summary-box-inner">
                                        <span>This work describes an encoder pre-training procedure using frame-wise label
to improve the training of streaming recurrent neural network transducer
(RNN-T) model. Streaming RNN-T trained from scratch usually performs worse than
non-streaming RNN-T. Although it is common to address this issue through
pre-training components of RNN-T with other criteria or frame-wise alignment
guidance, the alignment is not easily available in end-to-end manner. In this
work, frame-wise alignment, used to pre-train streaming RNN-T&#x27;s encoder, is
generated without using a HMM-based system. Therefore an all-neural framework
equipping HMM-free encoder pre-training is constructed. This is achieved by
expanding the spikes of CTC model to their left/right blank frames, and two
expanding strategies are proposed. To our best knowledge, this is the first
work to simulate HMM-based frame-wise label using CTC model for pre-training.
Experiments conducted on LibriSpeech and MLS English tasks show the proposed
pre-training procedure, compared with random initialization, reduces the WER by
relatively 5%~11% and the emission latency by 60 ms. Besides, the method is
lexicon-free, so it is friendly to new languages without manually designed
lexicon.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Generalizing Cross-Document Event Coreference Resolution Across Multiple Corpora. (arXiv:2011.12249v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bugert_M/0/1/0/all/0/1">Michael Bugert</a>, <a href="http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1">Nils Reimers</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1">Iryna Gurevych</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12249">
                                    <div class="article-summary-box-inner">
                                        <span>Cross-document event coreference resolution (CDCR) is an NLP task in which
mentions of events need to be identified and clustered throughout a collection
of documents. CDCR aims to benefit downstream multi-document applications, but
despite recent progress on corpora and system development, downstream
improvements from applying CDCR have not been shown yet. We make the
observation that every CDCR system to date was developed, trained, and tested
only on a single respective corpus. This raises strong concerns on their
generalizability -- a must-have for downstream applications where the magnitude
of domains or event mentions is likely to exceed those found in a curated
corpus. To investigate this assumption, we define a uniform evaluation setup
involving three CDCR corpora: ECB+, the Gun Violence Corpus and the Football
Coreference Corpus (which we reannotate on token level to make our analysis
possible). We compare a corpus-independent, feature-based system against a
recent neural system developed for ECB+. Whilst being inferior in absolute
numbers, the feature-based system shows more consistent performance across all
corpora whereas the neural system is hit-and-miss. Via model introspection, we
find that the importance of event actions, event time, etc. for resolving
coreference in practice varies greatly between the corpora. Additional analysis
shows that several systems overfit on the structure of the ECB+ corpus. We
conclude with recommendations on how to achieve generally applicable CDCR
systems in the future -- the most important being that evaluation on multiple
CDCR corpora is strongly necessary. To facilitate future research, we release
our dataset, annotation guidelines, and system implementation to the public.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Discussion on Building Practical NLP Leaderboards: The Case of Machine Translation. (arXiv:2106.06292v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Santy_S/0/1/0/all/0/1">Sebastin Santy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1">Prasanta Bhattacharya</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06292">
                                    <div class="article-summary-box-inner">
                                        <span>Recent advances in AI and ML applications have benefited from rapid progress
in NLP research. Leaderboards have emerged as a popular mechanism to track and
accelerate progress in NLP through competitive model development. While this
has increased interest and participation, the over-reliance on single, and
accuracy-based metrics have shifted focus from other important metrics that
might be equally pertinent to consider in real-world contexts. In this paper,
we offer a preliminary discussion of the risks associated with focusing
exclusively on accuracy metrics and draw on recent discussions to highlight
prescriptive suggestions on how to develop more practical and effective
leaderboards that can better reflect the real-world utility of models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dynamic Language Models for Continuously Evolving Content. (arXiv:2106.06297v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hombaiah_S/0/1/0/all/0/1">Spurthi Amba Hombaiah</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mingyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1">Michael Bendersky</a>, <a href="http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1">Marc Najork</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06297">
                                    <div class="article-summary-box-inner">
                                        <span>The content on the web is in a constant state of flux. New entities, issues,
and ideas continuously emerge, while the semantics of the existing conversation
topics gradually shift. In recent years, pre-trained language models like BERT
greatly improved the state-of-the-art for a large spectrum of content
understanding tasks. Therefore, in this paper, we aim to study how these
language models can be adapted to better handle continuously evolving web
content. In our study, we first analyze the evolution of 2013 - 2019 Twitter
data, and unequivocally confirm that a BERT model trained on past tweets would
heavily deteriorate when directly applied to data from later years. Then, we
investigate two possible sources of the deterioration: the semantic shift of
existing tokens and the sub-optimal or failed understanding of new tokens. To
this end, we both explore two different vocabulary composition methods, as well
as propose three sampling methods which help in efficient incremental training
for BERT-like models. Compared to a new model trained from scratch offline, our
incremental training (a) reduces the training costs, (b) achieves better
performance on evolving content, and (c) is suitable for online deployment. The
superiority of our methods is validated using two downstream tasks. We
demonstrate significant improvements when incrementally evolving the model from
a particular base year, on the task of Country Hashtag Prediction, as well as
on the OffensEval 2019 task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards unsupervised phone and word segmentation using self-supervised vector-quantized neural networks. (arXiv:2012.07551v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1">Herman Kamper</a>, <a href="http://arxiv.org/find/cs/1/au:+Niekerk_B/0/1/0/all/0/1">Benjamin van Niekerk</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07551">
                                    <div class="article-summary-box-inner">
                                        <span>We investigate segmenting and clustering speech into low-bitrate phone-like
sequences without supervision. We specifically constrain pretrained
self-supervised vector-quantized (VQ) neural networks so that blocks of
contiguous feature vectors are assigned to the same code, thereby giving a
variable-rate segmentation of the speech into discrete units. Two segmentation
methods are considered. In the first, features are greedily merged until a
prespecified number of segments are reached. The second uses dynamic
programming to optimize a squared error with a penalty term to encourage fewer
but longer segments. We show that these VQ segmentation methods can be used
without alteration across a wide range of tasks: unsupervised phone
segmentation, ABX phone discrimination, same-different word discrimination, and
as inputs to a symbolic word segmentation algorithm. The penalized dynamic
programming method generally performs best. While performance on individual
tasks is only comparable to the state-of-the-art in some cases, in all tasks a
reasonable competing approach is outperformed at a substantially lower bitrate.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Continual Learning for Text Classification with Information Disentanglement Based Regularization. (arXiv:2104.05489v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yufan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanzhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuezhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Diyi Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05489">
                                    <div class="article-summary-box-inner">
                                        <span>Continual learning has become increasingly important as it enables NLP models
to constantly learn and gain knowledge over time. Previous continual learning
methods are mainly designed to preserve knowledge from previous tasks, without
much emphasis on how to well generalize models to new tasks. In this work, we
propose an information disentanglement based regularization method for
continual learning on text classification. Our proposed method first
disentangles text hidden spaces into representations that are generic to all
tasks and representations specific to each individual task, and further
regularizes these representations differently to better constrain the knowledge
required to generalize. We also introduce two simple auxiliary tasks: next
sentence prediction and task-id prediction, for learning better generic and
specific representation spaces. Experiments conducted on large-scale benchmarks
demonstrate the effectiveness of our method in continual text classification
tasks with various sequences and lengths over state-of-the-art baselines. We
have publicly released our code at https://github.com/GT-SALT/IDBR.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs. (arXiv:2106.06363v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1">Thomas Scialom</a>, <a href="http://arxiv.org/find/cs/1/au:+Dray_P/0/1/0/all/0/1">Paul-Alexis Dray</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1">Sylvain Lamprier</a>, <a href="http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1">Benjamin Piwowarski</a>, <a href="http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1">Jacopo Staiano</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06363">
                                    <div class="article-summary-box-inner">
                                        <span>Due to the discrete nature of words, language GANs require to be optimized
from rewards provided by discriminator networks, via reinforcement learning
methods. This is a much harder setting than for continuous tasks, which enjoy
gradient flows from discriminators to generators, usually leading to dramatic
learning instabilities. However, we claim that this can be solved by making
discriminator and generator networks cooperate to produce output sequences
during training. These cooperative outputs, inherently built to obtain higher
discrimination scores, not only provide denser rewards for training, but also
form a more compact artificial set for discriminator training, hence improving
its accuracy and stability. In this paper, we show that our SelfGAN framework,
built on this cooperative principle, outperforms Teacher Forcing and obtains
state-of-the-art results on two challenging tasks, Summarization and Question
Generation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment. (arXiv:2106.06381v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1">Zewen Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1">Li Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1">Bo Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shaohan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1">Xian-Ling Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Heyan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06381">
                                    <div class="article-summary-box-inner">
                                        <span>The cross-lingual language models are typically pretrained with masked
language modeling on multilingual text or parallel sentences. In this paper, we
introduce denoising word alignment as a new cross-lingual pre-training task.
Specifically, the model first self-labels word alignments for parallel
sentences. Then we randomly mask tokens in a bitext pair. Given a masked token,
the model uses a pointer network to predict the aligned token in the other
language. We alternately perform the above two steps in an
expectation-maximization manner. Experimental results show that our method
improves cross-lingual transferability on various datasets, especially on the
token-level tasks, such as question answering, and structured prediction.
Moreover, the model can serve as a pretrained word aligner, which achieves
reasonably low error rates on the alignment benchmarks. The code and pretrained
parameters are available at https://github.com/CZWin32768/XLM-Align.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation. (arXiv:2106.06471v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xingyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1">Muchao Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1">Quanzeng You</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1">Fenglong Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06471">
                                    <div class="article-summary-box-inner">
                                        <span>Medical report generation is one of the most challenging tasks in medical
image analysis. Although existing approaches have achieved promising results,
they either require a predefined template database in order to retrieve
sentences or ignore the hierarchical nature of medical report generation. To
address these issues, we propose MedWriter that incorporates a novel
hierarchical retrieval mechanism to automatically extract both report and
sentence-level templates for clinically accurate report generation. MedWriter
first employs the Visual-Language Retrieval~(VLR) module to retrieve the most
relevant reports for the given images. To guarantee the logical coherence
between sentences, the Language-Language Retrieval~(LLR) module is introduced
to retrieve relevant sentences based on the previous generated description. At
last, a language decoder fuses image features and features from retrieved
reports and sentences to generate meaningful medical reports. We verified the
effectiveness of our model by automatic evaluation and human evaluation on two
datasets, i.e., Open-I and MIMIC-CXR.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Zero-Shot Controlled Generation with Encoder-Decoder Transformers. (arXiv:2106.06411v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1">Devamanyu Hazarika</a>, <a href="http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1">Mahdi Namazifar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1">Dilek Hakkani-T&#xfc;r</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06411">
                                    <div class="article-summary-box-inner">
                                        <span>Controlling neural network-based models for natural language generation (NLG)
has broad applications in numerous areas such as machine translation, document
summarization, and dialog systems. Approaches that enable such control in a
zero-shot manner would be of great importance as, among other reasons, they
remove the need for additional annotated data and training. In this work, we
propose novel approaches for controlling encoder-decoder transformer-based NLG
models in a zero-shot manner. This is done by introducing three control knobs;
namely, attention biasing, decoder mixing, and context augmentation, that are
applied to these models at generation time. These knobs control the generation
process by directly manipulating trained NLG models (e.g., biasing
cross-attention layers) to realize the desired attributes in the generated
outputs. We show that not only are these NLG models robust to such
manipulations, but also their behavior could be controlled without an impact on
their generation performance. These results, to the best of our knowledge, are
the first of their kind. Through these control knobs, we also investigate the
role of transformer decoder&#x27;s self-attention module and show strong evidence
that its primary role is maintaining fluency of sentences generated by these
models. Based on this hypothesis, we show that alternative architectures for
transformer decoders could be viable options. We also study how this hypothesis
could lead to more efficient ways for training encoder-decoder transformer
models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Local Explanation of Dialogue Response Generation. (arXiv:2106.06528v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1">Yi-Lin Tuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pryor_C/0/1/0/all/0/1">Connor Pryor</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wenhu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1">Lise Getoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06528">
                                    <div class="article-summary-box-inner">
                                        <span>In comparison to the interpretation of classification models, the explanation
of sequence generation models is also an important problem, however it has seen
little attention. In this work, we study model-agnostic explanations of a
representative text generation task -- dialogue response generation. Dialog
response generation is challenging with its open-ended sentences and multiple
acceptable responses. To gain insights into the reasoning process of a
generation model, we propose anew method, local explanation of response
generation (LERG) that regards the explanations as the mutual interaction of
segments in input and output sentences. LERG views the sequence prediction as
uncertainty estimation of a human response and then creates explanations by
perturbing the input and calculating the certainty change over the human
response. We show that LERG adheres to desired properties of explanations for
text generation including unbiased approximation, consistency and cause
identification. Empirically, our results show that our method consistently
improves other widely used methods on proposed automatic- and human- evaluation
metrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can
extract both explicit and implicit relations between input and output segments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">How Should Agents Ask Questions For Situated Learning? An Annotated Dialogue Corpus. (arXiv:2106.06504v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gervits_F/0/1/0/all/0/1">Felix Gervits</a>, <a href="http://arxiv.org/find/cs/1/au:+Roque_A/0/1/0/all/0/1">Antonio Roque</a>, <a href="http://arxiv.org/find/cs/1/au:+Briggs_G/0/1/0/all/0/1">Gordon Briggs</a>, <a href="http://arxiv.org/find/cs/1/au:+Scheutz_M/0/1/0/all/0/1">Matthias Scheutz</a>, <a href="http://arxiv.org/find/cs/1/au:+Marge_M/0/1/0/all/0/1">Matthew Marge</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06504">
                                    <div class="article-summary-box-inner">
                                        <span>Intelligent agents that are confronted with novel concepts in situated
environments will need to ask their human teammates questions to learn about
the physical world. To better understand this problem, we need data about
asking questions in situated task-based interactions. To this end, we present
the Human-Robot Dialogue Learning (HuRDL) Corpus - a novel dialogue corpus
collected in an online interactive virtual environment in which human
participants play the role of a robot performing a collaborative
tool-organization task. We describe the corpus data and a corresponding
annotation scheme to offer insight into the form and content of questions that
humans ask to facilitate learning in a situated environment. We provide the
corpus as an empirically-grounded resource for improving question generation in
situated intelligent agents.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. (arXiv:2102.05918v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1">Chao Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yinfei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Ye Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi-Ting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1">Zarana Parekh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1">Hieu Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1">Quoc V. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1">Yunhsuan Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Duerig_T/0/1/0/all/0/1">Tom Duerig</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05918">
                                    <div class="article-summary-box-inner">
                                        <span>Pre-trained representations are becoming crucial for many NLP and perception
tasks. While representation learning in NLP has transitioned to training on raw
text without human annotations, visual and vision-language representations
still rely heavily on curated training datasets that are expensive or require
expert knowledge. For vision applications, representations are mostly learned
using datasets with explicit class labels such as ImageNet or OpenImages. For
vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all
involve a non-trivial data collection (and cleaning) process. This costly
curation process limits the size of datasets and hence hinders the scaling of
trained models. In this paper, we leverage a noisy dataset of over one billion
image alt-text pairs, obtained without expensive filtering or post-processing
steps in the Conceptual Captions dataset. A simple dual-encoder architecture
learns to align visual and language representations of the image and text pairs
using a contrastive loss. We show that the scale of our corpus can make up for
its noise and leads to state-of-the-art representations even with such a simple
learning scheme. Our visual representation achieves strong performance when
transferred to classification tasks such as ImageNet and VTAB. The aligned
visual and language representations enables zero-shot image classification and
also set new state-of-the-art results on Flickr30K and MSCOCO image-text
retrieval benchmarks, even when compared with more sophisticated
cross-attention models. The representations also enable cross-modality search
with complex text and text + image queries.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Spoken Style Learning with Multi-modal Hierarchical Context Encoding for Conversational Text-to-Speech Synthesis. (arXiv:2106.06233v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jingbei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1">Yi Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiyong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1">Helen Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1">Chao Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1">Dan Su</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06233">
                                    <div class="article-summary-box-inner">
                                        <span>For conversational text-to-speech (TTS) systems, it is vital that the systems
can adjust the spoken styles of synthesized speech according to different
content and spoken styles in historical conversations. However, the study about
learning spoken styles from historical conversations is still in its infancy.
Only the transcripts of the historical conversations are considered, which
neglects the spoken styles in historical speeches. Moreover, only the
interactions of the global aspect between speakers are modeled, missing the
party aspect self interactions inside each speaker. In this paper, to achieve
better spoken style learning for conversational TTS, we propose a spoken style
learning approach with multi-modal hierarchical context encoding. The textual
information and spoken styles in the historical conversations are processed
through multiple hierarchical recurrent neural networks to learn the spoken
style related features in global and party aspects. The attention mechanism is
further employed to summarize these features into a conversational context
encoding. Experimental results demonstrate the effectiveness of our proposed
approach, which outperform a baseline method using context encoding learnt only
from the transcripts in global aspects, with MOS score on the naturalness of
synthesized speech increasing from 3.138 to 3.408 and ABX preference rate
exceeding the baseline method by 36.45%.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Calibrate Before Use: Improving Few-Shot Performance of Language Models. (arXiv:2102.09690v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tony Z. Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1">Eric Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1">Shi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sameer Singh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09690">
                                    <div class="article-summary-box-inner">
                                        <span>GPT-3 can perform numerous tasks when provided a natural language prompt that
contains a few training examples. We show that this type of few-shot learning
can be unstable: the choice of prompt format, training examples, and even the
order of the training examples can cause accuracy to vary from near chance to
near state-of-the-art. We demonstrate that this instability arises from the
bias of language models towards predicting certain answers, e.g., those that
are placed near the end of the prompt or are common in the pre-training data.
To mitigate this, we first estimate the model&#x27;s bias towards each answer by
asking for its prediction when given the training prompt and a content-free
test input such as &quot;N/A&quot;. We then fit calibration parameters that cause the
prediction for this input to be uniform across answers. On a diverse set of
tasks, this contextual calibration procedure substantially improves GPT-3 and
GPT-2&#x27;s average accuracy (up to 30.0% absolute) and reduces variance across
different choices of the prompt.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sprachsynthese -- State-of-the-Art in englischer und deutscher Sprache. (arXiv:2106.06230v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Peinl_R/0/1/0/all/0/1">Ren&#xe9; Peinl</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06230">
                                    <div class="article-summary-box-inner">
                                        <span>Reading text aloud is an important feature for modern computer applications.
It not only facilitates access to information for visually impaired people, but
is also a pleasant convenience for non-impaired users. In this article, the
state of the art of speech synthesis is presented separately for
mel-spectrogram generation and vocoders. It concludes with an overview of
available data sets for English and German with a discussion of the
transferability of the good speech synthesis results from English to German
language.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution. (arXiv:2106.06361v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1">Fanchao Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuan Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Sophia Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06361">
                                    <div class="article-summary-box-inner">
                                        <span>Recent studies show that neural natural language processing (NLP) models are
vulnerable to backdoor attacks. Injected with backdoors, models perform
normally on benign examples but produce attacker-specified predictions when the
backdoor is activated, presenting serious security threats to real-world
applications. Since existing textual backdoor attacks pay little attention to
the invisibility of backdoors, they can be easily detected and blocked. In this
work, we present invisible backdoors that are activated by a learnable
combination of word substitution. We show that NLP models can be injected with
backdoors that lead to a nearly 100% attack success rate, whereas being highly
invisible to existing defense strategies and even human inspections. The
results raise a serious alarm to the security of NLP models, which requires
further research to be resolved. All the data and code of this paper are
released at https://github.com/thunlp/BkdAtk-LWS.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sentence Extraction-Based Machine Reading Comprehension for Vietnamese. (arXiv:2105.09043v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Do_P/0/1/0/all/0/1">Phong Nguyen-Thuan Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nhat Duy Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1">Tin Van Huynh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1">Kiet Van Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">Anh Gia-Tuan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Ngan Luu-Thuy Nguyen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09043">
                                    <div class="article-summary-box-inner">
                                        <span>The development of natural language processing (NLP) in general and machine
reading comprehension in particular has attracted the great attention of the
research community. In recent years, there are a few datasets for machine
reading comprehension tasks in Vietnamese with large sizes, such as UIT-ViQuAD
and UIT-ViNewsQA. However, the datasets are not diverse in answers to serve the
research. In this paper, we introduce UIT-ViWikiQA, the first dataset for
evaluating sentence extraction-based machine reading comprehension in the
Vietnamese language. The UIT-ViWikiQA dataset is converted from the UIT-ViQuAD
dataset, consisting of comprises 23.074 question-answers based on 5.109
passages of 174 Wikipedia Vietnamese articles. We propose a conversion
algorithm to create the dataset for sentence extraction-based machine reading
comprehension and three types of approaches for sentence extraction-based
machine reading comprehension in Vietnamese. Our experiments show that the best
machine model is XLM-R_Large, which achieves an exact match (EM) of 85.97% and
an F1-score of 88.77% on our dataset. Besides, we analyze experimental results
in terms of the question type in Vietnamese and the effect of context on the
performance of the MRC models, thereby showing the challenges from the
UIT-ViWikiQA dataset that we propose to the language processing community.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FedNLP: An interpretable NLP System to Decode Federal Reserve Communications. (arXiv:2106.06247v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jean Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Youn_H/0/1/0/all/0/1">Hoyoul Luis Youn</a>, <a href="http://arxiv.org/find/cs/1/au:+Stevens_N/0/1/0/all/0/1">Nicholas Stevens</a>, <a href="http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1">Josiah Poon</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Soyeon Caren Han</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06247">
                                    <div class="article-summary-box-inner">
                                        <span>The Federal Reserve System (the Fed) plays a significant role in affecting
monetary policy and financial conditions worldwide. Although it is important to
analyse the Fed&#x27;s communications to extract useful information, it is generally
long-form and complex due to the ambiguous and esoteric nature of content. In
this paper, we present FedNLP, an interpretable multi-component Natural
Language Processing system to decode Federal Reserve communications. This
system is designed for end-users to explore how NLP techniques can assist their
holistic understanding of the Fed&#x27;s communications with NO coding. Behind the
scenes, FedNLP uses multiple NLP models from traditional machine learning
algorithms to deep neural network architectures in each downstream task. The
demonstration shows multiple results at once including sentiment analysis,
summary of the document, prediction of the Federal Funds Rate movement and
visualization for interpreting the prediction model&#x27;s result.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">HUI-Audio-Corpus-German: A high quality TTS dataset. (arXiv:2106.06309v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Puchtler_P/0/1/0/all/0/1">Pascal Puchtler</a>, <a href="http://arxiv.org/find/cs/1/au:+Wirth_J/0/1/0/all/0/1">Johannes Wirth</a>, <a href="http://arxiv.org/find/cs/1/au:+Peinl_R/0/1/0/all/0/1">Ren&#xe9; Peinl</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06309">
                                    <div class="article-summary-box-inner">
                                        <span>The increasing availability of audio data on the internet lead to a multitude
of datasets for development and training of text to speech applications, based
on neural networks. Highly differing quality of voice, low sampling rates, lack
of text normalization and disadvantageous alignment of audio samples to
corresponding transcript sentences still limit the performance of deep neural
networks trained on this task. Additionally, data resources in languages like
German are still very limited. We introduce the &quot;HUI-Audio-Corpus-German&quot;, a
large, open-source dataset for TTS engines, created with a processing pipeline,
which produces high quality audio to transcription alignments and decreases
manual effort needed for creation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving RNN-T ASR Performance with Date-Time and Location Awareness. (arXiv:2106.06183v1 [eess.AS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Ray_S/0/1/0/all/0/1">Swayambhu Nath Ray</a>, <a href="http://arxiv.org/find/eess/1/au:+Mitra_S/0/1/0/all/0/1">Soumyajit Mitra</a>, <a href="http://arxiv.org/find/eess/1/au:+Bilgi_R/0/1/0/all/0/1">Raghavendra Bilgi</a>, <a href="http://arxiv.org/find/eess/1/au:+Garimella_S/0/1/0/all/0/1">Sri Garimella</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06183">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we explore the benefits of incorporating context into a
Recurrent Neural Network (RNN-T) based Automatic Speech Recognition (ASR) model
to improve the speech recognition for virtual assistants. Specifically, we use
meta information extracted from the time at which the utterance is spoken and
the approximate location information to make ASR context aware. We show that
these contextual information, when used individually, improves overall
performance by as much as 3.48% relative to the baseline and when the contexts
are combined, the model learns complementary features and the recognition
improves by 4.62%. On specific domains, these contextual signals show
improvements as high as 11.5%, without any significant degradation on others.
We ran experiments with models trained on data of sizes 30K hours and 10K
hours. We show that the scale of improvement with the 10K hours dataset is much
higher than the one obtained with 30K hours dataset. Our results indicate that
with limited data to train the ASR model, contextual signals can improve the
performance significantly.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Nested and Balanced Entity Recognition using Multi-Task Learning. (arXiv:2106.06216v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Waldis_A/0/1/0/all/0/1">Andreas Waldis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazzola_L/0/1/0/all/0/1">Luca Mazzola</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06216">
                                    <div class="article-summary-box-inner">
                                        <span>Entity Recognition (ER) within a text is a fundamental exercise in Natural
Language Processing, enabling further depending tasks such as Knowledge
Extraction, Text Summarisation, or Keyphrase Extraction. An entity consists of
single words or of a consecutive sequence of terms, constituting the basic
building blocks for communication. Mainstream ER approaches are mainly limited
to flat structures, concentrating on the outermost entities while ignoring the
inner ones. This paper introduces a partly-layered network architecture that
deals with the complexity of overlapping and nested cases. The proposed
architecture consists of two parts: (1) a shared Sequence Layer and (2) a
stacked component with multiple Tagging Layers. The adoption of such an
architecture has the advantage of preventing overfit to a specific word-length,
thus maintaining performance for longer entities despite their lower frequency.
To verify the proposed architecture&#x27;s effectiveness, we train and evaluate this
architecture to recognise two kinds of entities - Concepts (CR) and Named
Entities (NER). Our approach achieves state-of-the-art NER performances, while
it outperforms previous CR approaches. Considering these promising results, we
see the possibility to evolve the architecture for other cases such as the
extraction of events or the detection of argumentative components.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards User-Driven Neural Machine Translation. (arXiv:2106.06200v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Huan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1">Liang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Baosong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dayiheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haibo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Weihua Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Degen Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Jinsong Su</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06200">
                                    <div class="article-summary-box-inner">
                                        <span>A good translation should not only translate the original content
semantically, but also incarnate personal traits of the original text. For a
real-world neural machine translation (NMT) system, these user traits (e.g.,
topic preference, stylistic characteristics and expression habits) can be
preserved in user behavior (e.g., historical inputs). However, current NMT
systems marginally consider the user behavior due to: 1) the difficulty of
modeling user portraits in zero-shot scenarios, and 2) the lack of
user-behavior annotated parallel dataset. To fill this gap, we introduce a
novel framework called user-driven NMT. Specifically, a cache-based module and
a user-driven contrastive learning method are proposed to offer NMT the ability
to capture potential user traits from their historical inputs under a zero-shot
learning fashion. Furthermore, we contribute the first Chinese-English parallel
corpus annotated with user behavior called UDT-Corpus. Experimental results
confirm that the proposed user-driven NMT can generate user-specific
translations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection. (arXiv:2106.06213v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Weld_H/0/1/0/all/0/1">Henry Weld</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Guanghao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jean Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tongshu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kunze Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xinghong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1">Siqu Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1">Josiah Poon</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Soyeon Caren Han</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06213">
                                    <div class="article-summary-box-inner">
                                        <span>Traditional toxicity detection models have focused on the single utterance
level without deeper understanding of context. We introduce CONDA, a new
dataset for in-game toxic language detection enabling joint intent
classification and slot filling analysis, which is the core task of Natural
Language Understanding (NLU). The dataset consists of 45K utterances from 12K
conversations from the chat logs of 1.9K completed Dota 2 matches. We propose a
robust dual semantic-level toxicity framework, which handles utterance and
token-level patterns, and rich contextual chatting history. Accompanying the
dataset is a thorough in-game toxicity analysis, which provides comprehensive
understanding of context at utterance, token, and dual levels. Inspired by NLU,
we also apply its metrics to the toxicity detection tasks for assessing
toxicity and game-specific aspects. We evaluate strong NLU models on CONDA,
providing fine-grained results for different intent classes and slot classes.
Furthermore, we examine the coverage of toxicity nature in our dataset by
comparing it with other toxicity datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding. (arXiv:2106.06228v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xin_C/0/1/0/all/0/1">Chunlei Xin</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xianpei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Le Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weipeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiansong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1">Xunliang Cai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06228">
                                    <div class="article-summary-box-inner">
                                        <span>Semantic parsing is challenging due to the structure gap and the semantic gap
between utterances and logical forms. In this paper, we propose an unsupervised
semantic parsing method - Synchronous Semantic Decoding (SSD), which can
simultaneously resolve the semantic gap and the structure gap by jointly
leveraging paraphrasing and grammar constrained decoding. Specifically, we
reformulate semantic parsing as a constrained paraphrasing problem: given an
utterance, our model synchronously generates its canonical utterance and
meaning representation. During synchronous decoding: the utterance paraphrasing
is constrained by the structure of the logical form, therefore the canonical
utterance can be paraphrased controlledly; the semantic decoding is guided by
the semantics of the canonical utterance, therefore its logical form can be
generated unsupervisedly. Experimental results show that SSD is a promising
approach and can achieve competitive unsupervised semantic parsing performance
on multiple datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data. (arXiv:2106.06169v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Haoyu Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kaiyan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei-Nan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Ting Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06169">
                                    <div class="article-summary-box-inner">
                                        <span>Maintaining consistent personas is essential for dialogue agents. Although
tremendous advancements have been brought, the limited-scale of annotated
persona-dense data are still barriers towards training robust and consistent
persona-based dialogue models. In this work, we show how the challenges can be
addressed by disentangling persona-based dialogue generation into two sub-tasks
with a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a
BERT-based encoder and two BERT-based decoders, where one decoder is for
response generation, and another is for consistency understanding. In
particular, to learn the ability of consistency understanding from large-scale
non-dialogue inference data, we train the second decoder in an unlikelihood
manner. Under different limited data settings, both automatic and human
evaluations demonstrate that the proposed model outperforms strong baselines in
response quality and persona consistency.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">TellMeWhy: A Dataset for Answering Why-Questions in Narratives. (arXiv:2106.06132v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1">Yash Kumar Lal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1">Nathanael Chambers</a>, <a href="http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1">Raymond Mooney</a>, <a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1">Niranjan Balasubramanian</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06132">
                                    <div class="article-summary-box-inner">
                                        <span>Answering questions about why characters perform certain actions is central
to understanding and reasoning about narratives. Despite recent progress in QA,
it is not clear if existing models have the ability to answer &quot;why&quot; questions
that may require commonsense knowledge external to the input narrative. In this
work, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more
than 30k questions and free-form answers concerning why characters in short
narratives perform the actions described. For a third of this dataset, the
answers are not present within the narrative. Given the limitations of
automated evaluation for this task, we also present a systematized human
evaluation interface for this dataset. Our evaluation of state-of-the-art
models show that they are far below human performance on answering such
questions. They are especially worse on questions whose answers are external to
the narrative, thus providing a challenge for future QA and narrative
understanding research.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Spoken Term Detection Methods for Sparse Transcription in Very Low-resource Settings. (arXiv:2106.06160v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ferrand_E/0/1/0/all/0/1">&#xc9;ric Le Ferrand</a>, <a href="http://arxiv.org/find/cs/1/au:+Bird_S/0/1/0/all/0/1">Steven Bird</a>, <a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1">Laurent Besacier</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06160">
                                    <div class="article-summary-box-inner">
                                        <span>We investigate the efficiency of two very different spoken term detection
approaches for transcription when the available data is insufficient to train a
robust ASR system. This work is grounded in very low-resource language
documentation scenario where only few minutes of recording have been
transcribed for a given language so far.Experiments on two oral languages show
that a pretrained universal phone recognizer, fine-tuned with only a few
minutes of target language speech, can be used for spoken term detection with a
better overall performance than a dynamic time warping approach. In addition,
we show that representing phoneme recognition ambiguity in a graph structure
can further boost the recall while maintaining high precision in the low
resource spoken term detection task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CodemixedNLP: An Extensible and Open NLP Toolkit for Code-Mixing. (arXiv:2106.06004v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jayanthi_S/0/1/0/all/0/1">Sai Muralidhar Jayanthi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nerella_K/0/1/0/all/0/1">Kavya Nerella</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1">Khyathi Raghavi Chandu</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1">Alan W Black</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06004">
                                    <div class="article-summary-box-inner">
                                        <span>The NLP community has witnessed steep progress in a variety of tasks across
the realms of monolingual and multilingual language processing recently. These
successes, in conjunction with the proliferating mixed language interactions on
social media have boosted interest in modeling code-mixed texts. In this work,
we present CodemixedNLP, an open-source library with the goals of bringing
together the advances in code-mixed NLP and opening it up to a wider machine
learning community. The library consists of tools to develop and benchmark
versatile model architectures that are tailored for mixed texts, methods to
expand training sets, techniques to quantify mixing styles, and fine-tuned
state-of-the-art models for 7 tasks in Hinglish. We believe this work has a
potential to foster a distributed yet collaborative and sustainable ecosystem
in an otherwise dispersed space of code-mixing research. The toolkit is
designed to be simple, easily extensible, and resourceful to both researchers
as well as practitioners.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Assessing Political Prudence of Open-domain Chatbots. (arXiv:2106.06157v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1">Yejin Bang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1">Nayeon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1">Etsuko Ishii</a>, <a href="http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1">Andrea Madotto</a>, <a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1">Pascale Fung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06157">
                                    <div class="article-summary-box-inner">
                                        <span>Politically sensitive topics are still a challenge for open-domain chatbots.
However, dealing with politically sensitive content in a responsible,
non-partisan, and safe behavior way is integral for these chatbots. Currently,
the main approach to handling political sensitivity is by simply changing such
a topic when it is detected. This is safe but evasive and results in a chatbot
that is less engaging. In this work, as a first step towards a politically safe
chatbot, we propose a group of metrics for assessing their political prudence.
We then conduct political prudence analysis of various chatbots and discuss
their behavior from multiple angles through our automatic metric and human
evaluation metrics. The testsets and codebase are released to promote research
in this area.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking. (arXiv:2106.06052v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhiyi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1">Kawin Ethayarajh</a>, <a href="http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1">Tristan Thrush</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Somya Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Ledell Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Robin Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1">Christopher Potts</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1">Adina Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1">Douwe Kiela</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06052">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce Dynaboard, an evaluation-as-a-service framework for hosting
benchmarks and conducting holistic model comparison, integrated with the
Dynabench platform. Our platform evaluates NLP models directly instead of
relying on self-reported metrics or predictions on a single dataset. Under this
paradigm, models are submitted to be evaluated in the cloud, circumventing the
issues of reproducibility, accessibility, and backwards compatibility that
often hinder benchmarking in NLP. This allows users to interact with uploaded
models in real time to assess their quality, and permits the collection of
additional metrics such as memory use, throughput, and robustness, which --
despite their importance to practitioners -- have traditionally been absent
from leaderboards. On each task, models are ranked according to the Dynascore,
a novel utility-based aggregation of these statistics, which users can
customize to better reflect their preferences, placing more/less weight on a
particular axis of evaluation or dataset. As state-of-the-art NLP models push
the limits of traditional benchmarks, Dynaboard offers a standardized solution
for a more diverse and comprehensive evaluation of model quality.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A comprehensive solution to retrieval-based chatbot construction. (arXiv:2106.06139v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Moore_K/0/1/0/all/0/1">Kristen Moore</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1">Shenjun Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhen He</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudolf_T/0/1/0/all/0/1">Torsten Rudolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Fisher_N/0/1/0/all/0/1">Nils Fisher</a>, <a href="http://arxiv.org/find/cs/1/au:+Victor_B/0/1/0/all/0/1">Brandon Victor</a>, <a href="http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1">Neha Jindal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06139">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper we present the results of our experiments in training and
deploying a self-supervised retrieval-based chatbot trained with contrastive
learning for assisting customer support agents. In contrast to most existing
research papers in this area where the focus is on solving just one component
of a deployable chatbot, we present an end-to-end set of solutions to take the
reader from an unlabelled chatlogs to a deployed chatbot. This set of solutions
includes creating a self-supervised dataset and a weakly labelled dataset from
chatlogs, as well as a systematic approach to selecting a fixed list of canned
responses. We present a hierarchical-based RNN architecture for the response
selection model, chosen for its ability to cache intermediate utterance
embeddings, which helped to meet deployment inference speed requirements. We
compare the performance of this architecture across 3 different learning
objectives: self-supervised contrastive learning, binary classification, and
multi-class classification. We find that using a self-supervised contrastive
learning model outperforms training the binary and multi-class classification
models on a weakly labelled dataset. Our results validate that the
self-supervised contrastive learning approach can be effectively used for a
real-world chatbot scenario.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">NAAQA: A Neural Architecture for Acoustic Question Answering. (arXiv:2106.06147v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Abdelnour_J/0/1/0/all/0/1">Jerome Abdelnour</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1">Jean Rouat</a>, <a href="http://arxiv.org/find/cs/1/au:+Salvi_G/0/1/0/all/0/1">Giampiero Salvi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06147">
                                    <div class="article-summary-box-inner">
                                        <span>The goal of the Acoustic Question Answering (AQA) task is to answer a
free-form text question about the content of an acoustic scene. It was inspired
by the Visual Question Answering (VQA) task. In this paper, based on the
previously introduced CLEAR dataset, we propose a new benchmark for AQA that
emphasizes the specific challenges of acoustic inputs, e.g. variable duration
scenes. We also introduce NAAQA, a neural architecture that leverages specific
properties of acoustic inputs. The usage of time and frequency 1D convolutions
to process 2D spectro-temporal representations of acoustic content shows
promising results and enables reductions in model complexity. NAAQA achieves
91.6% of accuracy on the AQA task with about 7 times fewer parameters than the
previously explored VQA model. We provide a detailed analysis of the results
for the different question types. The effectiveness of coordinate maps in this
acoustic context was also studied and we show that time coordinate maps augment
temporal localization capabilities which enhance performance of the network by
about 17 percentage points.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation. (arXiv:2106.06125v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Baosong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dayiheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haibo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Weihua Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haiying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Jinsong Su</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06125">
                                    <div class="article-summary-box-inner">
                                        <span>A well-known limitation in pretrain-finetune paradigm lies in its
inflexibility caused by the one-size-fits-all vocabulary. This potentially
weakens the effect when applying pretrained models into natural language
generation (NLG) tasks, especially for the subword distributions between
upstream and downstream tasks with significant discrepancy. Towards approaching
this problem, we extend the vanilla pretrain-finetune pipeline with an extra
embedding transfer step. Specifically, a plug-and-play embedding generator is
introduced to produce the representation of any input token, according to
pre-trained embeddings of its morphologically similar ones. Thus, embeddings of
mismatch tokens in downstream tasks can also be efficiently initialized. We
conduct experiments on a variety of NLG tasks under the pretrain-finetune
fashion. Experimental results and extensive analyses show that the proposed
strategy offers us opportunities to feel free to transfer the vocabulary,
leading to more efficient and better performed downstream NLG models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Graph Neural Networks for Natural Language Processing: A Survey. (arXiv:2106.06090v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lingfei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1">Kai Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiaojie Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Hanning Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shucheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1">Jian Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1">Bo Long</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06090">
                                    <div class="article-summary-box-inner">
                                        <span>Deep learning has become the dominant approach in coping with various tasks
in Natural LanguageProcessing (NLP). Although text inputs are typically
represented as a sequence of tokens, there isa rich variety of NLP problems
that can be best expressed with a graph structure. As a result, thereis a surge
of interests in developing new deep learning techniques on graphs for a large
numberof NLP tasks. In this survey, we present a comprehensive overview onGraph
Neural Networks(GNNs) for Natural Language Processing. We propose a new
taxonomy of GNNs for NLP, whichsystematically organizes existing research of
GNNs for NLP along three axes: graph construction,graph representation
learning, and graph based encoder-decoder models. We further introducea large
number of NLP applications that are exploiting the power of GNNs and summarize
thecorresponding benchmark datasets, evaluation metrics, and open-source codes.
Finally, we discussvarious outstanding challenges for making the full use of
GNNs for NLP as well as future researchdirections. To the best of our
knowledge, this is the first comprehensive overview of Graph NeuralNetworks for
Natural Language Processing.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models. (arXiv:2106.06087v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1">Matthew Finlayson</a>, <a href="http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1">Aaron Mueller</a>, <a href="http://arxiv.org/find/cs/1/au:+Shieber_S/0/1/0/all/0/1">Stuart Shieber</a>, <a href="http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1">Sebastian Gehrmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1">Tal Linzen</a>, <a href="http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1">Yonatan Belinkov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06087">
                                    <div class="article-summary-box-inner">
                                        <span>Targeted syntactic evaluations have demonstrated the ability of language
models to perform subject-verb agreement given difficult contexts. To elucidate
the mechanisms by which the models accomplish this behavior, this study applies
causal mediation analysis to pre-trained neural language models. We investigate
the magnitude of models&#x27; preferences for grammatical inflections, as well as
whether neurons process subject-verb agreement similarly across sentences with
different syntactic structures. We uncover similarities and differences across
architectures and model sizes -- notably, that larger models do not necessarily
learn stronger preferences. We also observe two distinct mechanisms for
producing subject-verb agreement depending on the syntactic structure of the
input sentence. Finally, we find that language models rely on similar sets of
neurons when given sentences with similar syntactic structure.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-lingual Emotion Detection. (arXiv:2106.06017v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1">Sabit Hassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1">Shaden Shaar</a>, <a href="http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1">Kareem Darwish</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06017">
                                    <div class="article-summary-box-inner">
                                        <span>Emotion detection is of great importance for understanding humans.
Constructing annotated datasets to train automated models can be expensive. We
explore the efficacy of cross-lingual approaches that would use data from a
source language to build models for emotion detection in a target language. We
compare three approaches, namely: i) using inherently multilingual models; ii)
translating training data into the target language; and iii) using an
automatically tagged parallel corpus. In our study, we consider English as the
source language with Arabic and Spanish as target languages. We study the
effectiveness of different classification models such as BERT and SVMs trained
with different features. Our BERT-based monolingual models that are trained on
target language data surpass state-of-the-art (SOTA) by 4% and 5% absolute
Jaccard score for Arabic and Spanish respectively. Next, we show that using
cross-lingual approaches with English data alone, we can achieve more than 90%
and 80% relative effectiveness of the Arabic and Spanish BERT models
respectively. Lastly, we use LIME to interpret the differences between models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Modeling Hierarchical Structures with Continuous Recursive Neural Networks. (arXiv:2106.06038v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1">Jishnu Ray Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1">Cornelia Caragea</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06038">
                                    <div class="article-summary-box-inner">
                                        <span>Recursive Neural Networks (RvNNs), which compose sequences according to their
underlying hierarchical syntactic structure, have performed well in several
natural language processing tasks compared to similar models without structural
biases. However, traditional RvNNs are incapable of inducing the latent
structure in a plain text sequence on their own. Several extensions have been
proposed to overcome this limitation. Nevertheless, these extensions tend to
rely on surrogate gradients or reinforcement learning at the cost of higher
bias or variance. In this work, we propose Continuous Recursive Neural Network
(CRvNN) as a backpropagation-friendly alternative to address the aforementioned
limitations. This is done by incorporating a continuous relaxation to the
induced structure. We demonstrate that CRvNN achieves strong performance in
challenging synthetic tasks such as logical inference and ListOps. We also show
that CRvNN performs comparably or better than prior latent structure models on
real-world tasks such as sentiment analysis and natural language inference.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">One Sense Per Translation. (arXiv:2106.06082v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1">Bradley Hauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1">Grzegorz Kondrak</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06082">
                                    <div class="article-summary-box-inner">
                                        <span>The idea of using lexical translations to define sense inventories has a long
history in lexical semantics. We propose a theoretical framework which allows
us to answer the question of why this apparently reasonable idea failed to
produce useful results. We formally prove several propositions on how the
translations of a word relate to its senses, as well as on the relationship
between synonymy and polysemy. We empirically validate our theoretical findings
on BabelNet, and demonstrate how they could be used to perform unsupervised
word sense disambiguation of a substantial fraction of the lexicon.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Probabilistic, Structure-Aware Algorithms for Improved Variety, Accuracy, and Coverage of AMR Alignments. (arXiv:2106.06002v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Blodgett_A/0/1/0/all/0/1">Austin Blodgett</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1">Nathan Schneider</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06002">
                                    <div class="article-summary-box-inner">
                                        <span>We present algorithms for aligning components of Abstract Meaning
Representation (AMR) graphs to spans in English sentences. We leverage
unsupervised learning in combination with heuristics, taking the best of both
worlds from previous AMR aligners. Our unsupervised models, however, are more
sensitive to graph substructures, without requiring a separate syntactic parse.
Our approach covers a wider variety of AMR substructures than previously
considered, achieves higher coverage of nodes and edges, and does so with
higher accuracy. We will release our LEAMR datasets and aligner for use in
research on AMR parsing, generation, and evaluation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.CV"">cs.CV updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis. (arXiv:2010.05690v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+B_L/0/1/0/all/0/1">Lalith Bharadwaj B</a>, <a href="http://arxiv.org/find/cs/1/au:+Boddeda_R/0/1/0/all/0/1">Rohit Boddeda</a>, <a href="http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1">Sai Vardhan K</a>, <a href="http://arxiv.org/find/cs/1/au:+G_M/0/1/0/all/0/1">Madhu G</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05690">
                                    <div class="article-summary-box-inner">
                                        <span>The issue of COVID-19, increasing with a massive mortality rate. This led to
the WHO declaring it as a pandemic. In this situation, it is crucial to perform
efficient and fast diagnosis. The reverse transcript polymerase chain reaction
(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is
time-consuming and instead chest CT (or Chest X-ray) can be used for a fast and
accurate diagnosis. Automated diagnosis is considered to be important as it
reduces human effort and provides accurate and low-cost tests. The
contributions of our research are three-fold. First, it is aimed to analyse the
behaviour and performance of variant vision models ranging from Inception to
NAS networks with the appropriate fine-tuning procedure. Second, the behaviour
of these models is visually analysed by plotting CAMs for individual networks
and determining classification performance with AUCROC curves. Thirdly, stacked
ensembles techniques are imparted to provide higher generalisation on combining
the fine-tuned models, in which six ensemble neural networks are designed by
combining the existing fine-tuned networks. Implying these stacked ensembles
provides a great generalization to the models. The ensemble model designed by
combining all the fine-tuned networks obtained a state-of-the-art accuracy
score of 99.17%. The precision and recall for the COVID-19 class are 99.99% and
89.79% respectively, which resembles the robustness of the stacked ensembles.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fast Weakly Supervised Action Segmentation Using Mutual Consistency. (arXiv:1904.03116v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Souri_Y/0/1/0/all/0/1">Yaser Souri</a>, <a href="http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1">Mohsen Fayyaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Minciullo_L/0/1/0/all/0/1">Luca Minciullo</a>, <a href="http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1">Gianpiero Francesca</a>, <a href="http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1">Juergen Gall</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.03116">
                                    <div class="article-summary-box-inner">
                                        <span>Action segmentation is the task of predicting the actions for each frame of a
video. As obtaining the full annotation of videos for action segmentation is
expensive, weakly supervised approaches that can learn only from transcripts
are appealing. In this paper, we propose a novel end-to-end approach for weakly
supervised action segmentation based on a two-branch neural network. The two
branches of our network predict two redundant but different representations for
action segmentation and we propose a novel mutual consistency (MuCon) loss that
enforces the consistency of the two redundant representations. Using the MuCon
loss together with a loss for transcript prediction, our proposed approach
achieves the accuracy of state-of-the-art approaches while being $14$ times
faster to train and $20$ times faster during inference. The MuCon loss proves
beneficial even in the fully supervised setting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Pedestrian Attribute Recognition in Video Surveillance Scenarios Based on View-attribute Attention Localization. (arXiv:2106.06485v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weichen Chen</a> (1) <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xinyi Yu</a> (1) <a href="http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1">Linlin Ou</a> (1) ((1) Collage of Information Engineering, Zhejiang University of Technology, Hangzhou, China)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06485">
                                    <div class="article-summary-box-inner">
                                        <span>Pedestrian attribute recognition in surveillance scenarios is still a
challenging task due to inaccurate localization of specific attributes. In this
paper, we propose a novel view-attribute localization method based on attention
(VALA), which relies on the strong relevance between attributes and views to
capture specific view-attributes and to localize attribute-corresponding areas
by attention mechanism. A specific view-attribute is composed by the extracted
attribute feature and four view scores which are predicted by view predictor as
the confidences for attribute from different views. View-attribute is then
delivered back to shallow network layers for supervising deep feature
extraction. To explore the location of a view-attribute, regional attention is
introduced to aggregate spatial information of the input attribute feature in
height and width direction for constraining the image into a narrow range.
Moreover, the inter-channel dependency of view-feature is embedded in the above
two spatial directions. An attention attribute-specific region is gained after
fining the narrow range by balancing the ratio of channel dependencies between
height and width branches. The final view-attribute recognition outcome is
obtained by combining the output of regional attention with the view scores
from view predictor. Experiments on three wide datasets (RAP, RAPv2, PETA, and
PA-100K) demonstrate the effectiveness of our approach compared with
state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dynamic Neural Networks: A Survey. (arXiv:2102.04906v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yizeng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Gao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shiji Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Le Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Honghui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yulin Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04906">
                                    <div class="article-summary-box-inner">
                                        <span>Dynamic neural network is an emerging research topic in deep learning.
Compared to static models which have fixed computational graphs and parameters
at the inference stage, dynamic networks can adapt their structures or
parameters to different inputs, leading to notable advantages in terms of
accuracy, computational efficiency, adaptiveness, etc. In this survey, we
comprehensively review this rapidly developing area by dividing dynamic
networks into three main categories: 1) instance-wise dynamic models that
process each instance with data-dependent architectures or parameters; 2)
spatial-wise dynamic networks that conduct adaptive computation with respect to
different spatial locations of image data and 3) temporal-wise dynamic models
that perform adaptive inference along the temporal dimension for sequential
data such as videos and texts. The important research problems of dynamic
networks, e.g., architecture design, decision making scheme, optimization
technique and applications, are reviewed systematically. Finally, we discuss
the open problems in this field together with interesting future research
directions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ViT-Inception-GAN for Image Colourising. (arXiv:2106.06321v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bana_T/0/1/0/all/0/1">Tejas Bana</a>, <a href="http://arxiv.org/find/cs/1/au:+Loya_J/0/1/0/all/0/1">Jatan Loya</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1">Siddhant Kulkarni</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06321">
                                    <div class="article-summary-box-inner">
                                        <span>Studies involving colourising images has been garnering researchers&#x27; keen
attention over time, assisted by significant advances in various Machine
Learning techniques and compute power availability. Traditionally, colourising
images have been an intricate task that gave a substantial degree of freedom
during the assignment of chromatic information. In our proposed method, we
attempt to colourise images using Vision Transformer - Inception - Generative
Adversarial Network (ViT-I-GAN), which has an Inception-v3 fusion embedding in
the generator. For a stable and robust network, we have used Vision Transformer
(ViT) as the discriminator. We trained the model on the Unsplash and the COCO
dataset for demonstrating the improvement made by the Inception-v3 embedding.
We have compared the results between ViT-GANs with and without Inception-v3
embedding.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Genetic U-Net: Automatically Designed Deep Networks for Retinal Vessel Segmentation Using a Genetic Algorithm. (arXiv:2010.15560v4 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1">Jiahong Wei</a>, <a href="http://arxiv.org/find/eess/1/au:+Fan_Z/0/1/0/all/0/1">Zhun Fan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.15560">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, many methods based on hand-designed convolutional neural networks
(CNNs) have achieved promising results in automatic retinal vessel
segmentation. However, these CNNs remain constrained in capturing retinal
vessels in complex fundus images. To improve their segmentation performance,
these CNNs tend to have many parameters, which may lead to overfitting and high
computational complexity. Moreover, the manual design of competitive CNNs is
time-consuming and requires extensive empirical knowledge. Herein, a novel
automated design method, called Genetic U-Net, is proposed to generate a
U-shaped CNN that can achieve better retinal vessel segmentation but with fewer
architecture-based parameters, thereby addressing the above issues. First, we
devised a condensed but flexible search space based on a U-shaped
encoder-decoder. Then, we used an improved genetic algorithm to identify
better-performing architectures in the search space and investigated the
possibility of finding a superior network architecture with fewer parameters.
The experimental results show that the architecture obtained using the proposed
method offered a superior performance with less than 1% of the number of the
original U-Net parameters in particular and with significantly fewer parameters
than other state-of-the-art models. Furthermore, through in-depth investigation
of the experimental results, several effective operations and patterns of
networks to generate superior retinal vessel segmentations were identified.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust Learning via Persistency of Excitation. (arXiv:2106.02078v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1">Kaustubh Sridhar</a>, <a href="http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1">Oleg Sokolsky</a>, <a href="http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1">Insup Lee</a>, <a href="http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1">James Weimer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02078">
                                    <div class="article-summary-box-inner">
                                        <span>Improving adversarial robustness of neural networks remains a major
challenge. Fundamentally, training a network is a parameter estimation problem.
In adaptive control theory, maintaining persistency of excitation (PoE) is
integral to ensuring convergence of parameter estimates in dynamical systems to
their robust optima. In this work, we show that network training using gradient
descent is equivalent to a dynamical system parameter estimation problem.
Leveraging this relationship, we prove a sufficient condition for PoE of
gradient descent is achieved when the learning rate is less than the inverse of
the Lipschitz constant of the gradient of loss function. We provide an
efficient technique for estimating the corresponding Lipschitz constant using
extreme value theory and demonstrate that by only scaling the learning rate
schedule we can increase adversarial accuracy by up to 15% points on benchmark
datasets. Our approach also universally increases the adversarial accuracy by
0.1% to 0.3% points in various state-of-the-art adversarially trained models on
the AutoAttack benchmark, where every small margin of improvement is
significant.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Refining Pseudo Labels with Clustering Consensus over Generations for Unsupervised Object Re-identification. (arXiv:2106.06133v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yixiao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06133">
                                    <div class="article-summary-box-inner">
                                        <span>Unsupervised object re-identification targets at learning discriminative
representations for object retrieval without any annotations. Clustering-based
methods conduct training with the generated pseudo labels and currently
dominate this research direction. However, they still suffer from the issue of
pseudo label noise. To tackle the challenge, we propose to properly estimate
pseudo label similarities between consecutive training generations with
clustering consensus and refine pseudo labels with temporally propagated and
ensembled pseudo labels. To the best of our knowledge, this is the first
attempt to leverage the spirit of temporal ensembling to improve classification
with dynamically changing classes over generations. The proposed pseudo label
refinery strategy is simple yet effective and can be seamlessly integrated into
existing clustering-based unsupervised re-identification methods. With our
proposed approach, state-of-the-art method can be further boosted with up to
8.8% mAP improvements on the challenging MSMT17 dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Object Segmentation Without Labels with Large-Scale Generative Models. (arXiv:2006.04988v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1">Andrey Voynov</a>, <a href="http://arxiv.org/find/cs/1/au:+Morozov_S/0/1/0/all/0/1">Stanislav Morozov</a>, <a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1">Artem Babenko</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04988">
                                    <div class="article-summary-box-inner">
                                        <span>The recent rise of unsupervised and self-supervised learning has dramatically
reduced the dependency on labeled data, providing effective image
representations for transfer to downstream vision tasks. Furthermore, recent
works employed these representations in a fully unsupervised setup for image
classification, reducing the need for human labels on the fine-tuning stage as
well. This work demonstrates that large-scale unsupervised models can also
perform a more challenging object segmentation task, requiring neither
pixel-level nor image-level labeling. Namely, we show that recent unsupervised
GANs allow to differentiate between foreground/background pixels, providing
high-quality saliency masks. By extensive comparison on standard benchmarks, we
outperform existing unsupervised alternatives for object segmentation,
achieving new state-of-the-art.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Can we have it all? On the Trade-off between Spatial and Adversarial Robustness of Neural Networks. (arXiv:2002.11318v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kamath_S/0/1/0/all/0/1">Sandesh Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1">Amit Deshpande</a>, <a href="http://arxiv.org/find/cs/1/au:+Subrahmanyam_K/0/1/0/all/0/1">K V Subrahmanyam</a>, <a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1">Vineeth N Balasubramanian</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.11318">
                                    <div class="article-summary-box-inner">
                                        <span>(Non-)robustness of neural networks to small, adversarial pixel-wise
perturbations, and as more recently shown, to even random spatial
transformations (e.g., translations, rotations) entreats both theoretical and
empirical understanding. Spatial robustness to random translations and
rotations is commonly attained via equivariant models (e.g., StdCNNs, GCNNs)
and training augmentation, whereas adversarial robustness is typically achieved
by adversarial training. In this paper, we prove a quantitative trade-off
between spatial and adversarial robustness in a simple statistical setting. We
complement this empirically by showing that: (a) as the spatial robustness of
equivariant models improves by training augmentation with progressively larger
transformations, their adversarial robustness worsens progressively, and (b) as
the state-of-the-art robust models are adversarially trained with progressively
larger pixel-wise perturbations, their spatial robustness drops progressively.
Towards achieving pareto-optimality in this trade-off, we propose a method
based on curriculum learning that trains gradually on more difficult
perturbations (both spatial and adversarial) to improve spatial and adversarial
robustness simultaneously.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">K-shot NAS: Learnable Weight-Sharing for NAS with K-shot Supernets. (arXiv:2106.06442v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1">Xiu Su</a>, <a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1">Shan You</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Mingkai Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Changshui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chang Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06442">
                                    <div class="article-summary-box-inner">
                                        <span>In one-shot weight sharing for NAS, the weights of each operation (at each
layer) are supposed to be identical for all architectures (paths) in the
supernet. However, this rules out the possibility of adjusting operation
weights to cater for different paths, which limits the reliability of the
evaluation results. In this paper, instead of counting on a single supernet, we
introduce $K$-shot supernets and take their weights for each operation as a
dictionary. The operation weight for each path is represented as a convex
combination of items in a dictionary with a simplex code. This enables a matrix
approximation of the stand-alone weight matrix with a higher rank ($K&gt;1$). A
\textit{simplex-net} is introduced to produce architecture-customized code for
each path. As a result, all paths can adaptively learn how to share weights in
the $K$-shot supernets and acquire corresponding weights for better evaluation.
$K$-shot supernets and simplex-net can be iteratively trained, and we further
extend the search to the channel dimension. Extensive experiments on benchmark
datasets validate that K-shot NAS significantly improves the evaluation
accuracy of paths and thus brings in impressive performance improvements.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Making EfficientNet More Efficient: Exploring Batch-Independent Normalization, Group Convolutions and Reduced Resolution Training. (arXiv:2106.03640v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1">Dominic Masters</a>, <a href="http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1">Antoine Labatie</a>, <a href="http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1">Zach Eaton-Rosen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1">Carlo Luschi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03640">
                                    <div class="article-summary-box-inner">
                                        <span>Much recent research has been dedicated to improving the efficiency of
training and inference for image classification. This effort has commonly
focused on explicitly improving theoretical efficiency, often measured as
ImageNet validation accuracy per FLOP. These theoretical savings have, however,
proven challenging to achieve in practice, particularly on high-performance
training accelerators.

In this work, we focus on improving the practical efficiency of the
state-of-the-art EfficientNet models on a new class of accelerator, the
Graphcore IPU. We do this by extending this family of models in the following
ways: (i) generalising depthwise convolutions to group convolutions; (ii)
adding proxy-normalized activations to match batch normalization performance
with batch-independent statistics; (iii) reducing compute by lowering the
training resolution and inexpensively fine-tuning at higher resolution. We find
that these three methods improve the practical efficiency for both training and
inference. Our code will be made available online.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Calibration and Auto-Refinement for Light Field Cameras. (arXiv:2106.06181v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Anisimov_Y/0/1/0/all/0/1">Yuriy Anisimov</a>, <a href="http://arxiv.org/find/cs/1/au:+Reis_G/0/1/0/all/0/1">Gerd Reis</a>, <a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1">Didier Stricker</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06181">
                                    <div class="article-summary-box-inner">
                                        <span>The ability to create an accurate three-dimensional reconstruction of a
captured scene draws attention to the principles of light fields. This paper
presents an approach for light field camera calibration and rectification,
based on pairwise pattern-based parameters extraction. It is followed by a
correspondence-based algorithm for camera parameters refinement from arbitrary
scenes using the triangulation filter and nonlinear optimization. The
effectiveness of our approach is validated on both real and synthetic data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online Multi-Object Tracking and Segmentation with GMPHD Filter and Mask-based Affinity Fusion. (arXiv:2009.00100v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Young-min Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1">Young-chul Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1">Kwangjin Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1">Moongu Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1">Witold Pedrycz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.00100">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose a highly practical fully online multi-object
tracking and segmentation (MOTS) method that uses instance segmentation results
as an input. The proposed method is based on the Gaussian mixture probability
hypothesis density (GMPHD) filter, a hierarchical data association (HDA), and a
mask-based affinity fusion (MAF) model to achieve high-performance online
tracking. The HDA consists of two associations: segment-to-track and
track-to-track associations. One affinity, for position and motion, is computed
by using the GMPHD filter, and the other affinity, for appearance is computed
by using the responses from a single object tracker such as a kernalized
correlation filter. These two affinities are simply fused by using a
score-level fusion method such as min-max normalization referred to as MAF. In
addition, to reduce the number of false positive segments, we adopt mask
IoU-based merging (mask merging). The proposed MOTS framework with the key
modules: HDA, MAF, and mask merging, is easily extensible to simultaneously
track multiple types of objects with CPU only execution in parallel processing.
In addition, the developed framework only requires simple parameter tuning
unlike many existing MOTS methods that need intensive hyperparameter
optimization. In the experiments on the two popular MOTS datasets, the key
modules show some improvements. For instance, ID-switch decreases by more than
half compared to a baseline method in the training sets. In conclusion, our
tracker achieves state-of-the-art MOTS performance in the test sets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AugNet: End-to-End Unsupervised Visual Representation Learning with Image Augmentation. (arXiv:2106.06250v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingxiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1">Zhanguo Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1">Haonan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bitao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1">Liufang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhecheng Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06250">
                                    <div class="article-summary-box-inner">
                                        <span>Most of the achievements in artificial intelligence so far were accomplished
by supervised learning which requires numerous annotated training data and thus
costs innumerable manpower for labeling. Unsupervised learning is one of the
effective solutions to overcome such difficulties. In our work, we propose
AugNet, a new deep learning training paradigm to learn image features from a
collection of unlabeled pictures. We develop a method to construct the
similarities between pictures as distance metrics in the embedding space by
leveraging the inter-correlation between augmented versions of samples. Our
experiments demonstrate that the method is able to represent the image in low
dimensional space and performs competitively in downstream tasks such as image
classification and image similarity comparison. Specifically, we achieved over
60% and 27% accuracy on the STL10 and CIFAR100 datasets with unsupervised
clustering, respectively. Moreover, unlike many deep-learning-based image
retrieval algorithms, our approach does not require access to external
annotated datasets to train the feature extractor, but still shows comparable
or even better feature representation ability and easy-to-use characteristics.
In our evaluations, the method outperforms all the state-of-the-art image
retrieval algorithms on some out-of-domain image datasets. The code for the
model implementation is available at
https://github.com/chenmingxiang110/AugNet.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An Image Forensic Technique Based on JPEG Ghosts. (arXiv:2106.06439v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1">Divakar Singh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06439">
                                    <div class="article-summary-box-inner">
                                        <span>The unprecedented growth in the easy availability of photo-editing tools has
endangered the power of digital images.An image was supposed to be worth more
than a thousand words,but now this can be said only if it can be authenticated
orthe integrity of the image can be proved to be intact. In thispaper, we
propose a digital image forensic technique for JPEG images. It can detect any
forgery in the image if the forged portion called a ghost image is having a
compression quality different from that of the cover image. It is based on
resaving the JPEG image at different JPEG qualities, and the detection of the
forged portion is maximum when it is saved at the same JPEG quality as the
cover image. Also, we can precisely predictthe JPEG quality of the cover image
by analyzing the similarity using Structural Similarity Index Measure (SSIM) or
the energyof the images. The first maxima in SSIM or the first minima inenergy
correspond to the cover image JPEG quality. We created adataset for varying
JPEG compression qualities of the ghost and the cover images and validated the
scalability of the experimental results.We also, experimented with varied
attack scenarios, e.g. high-quality ghost image embedded in low quality of
cover image,low-quality ghost image embedded in high-quality of cover image,and
ghost image and cover image both at the same quality.The proposed method is
able to localize the tampered portions accurately even for forgeries as small
as 10x10 sized pixel blocks.Our technique is also robust against other attack
scenarios like copy-move forgery, inserting text into image, rescaling
(zoom-out/zoom-in) ghost image and then pasting on cover image.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Shallow Optical Flow Three-Stream CNN for Macro- and Micro-Expression Spotting from Long Videos. (arXiv:2106.06489v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liong_G/0/1/0/all/0/1">Gen-Bing Liong</a>, <a href="http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1">John See</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1">Lai-Kuan Wong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06489">
                                    <div class="article-summary-box-inner">
                                        <span>Facial expressions vary from the visible to the subtle. In recent years, the
analysis of micro-expressions $-$ a natural occurrence resulting from the
suppression of one&#x27;s true emotions, has drawn the attention of researchers with
a broad range of potential applications. However, spotting microexpressions in
long videos becomes increasingly challenging when intertwined with normal or
macro-expressions. In this paper, we propose a shallow optical flow
three-stream CNN (SOFTNet) model to predict a score that captures the
likelihood of a frame being in an expression interval. By fashioning the
spotting task as a regression problem, we introduce pseudo-labeling to
facilitate the learning process. We demonstrate the efficacy and efficiency of
the proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art
performance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM
Long Videos.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Instance-Level Task Parameters: A Robust Multi-task Weighting Framework. (arXiv:2106.06129v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vasu_P/0/1/0/all/0/1">Pavan Kumar Anasosalu Vasu</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1">Shreyas Saxena</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1">Oncel Tuzel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06129">
                                    <div class="article-summary-box-inner">
                                        <span>Recent works have shown that deep neural networks benefit from multi-task
learning by learning a shared representation across several related tasks.
However, performance of such systems depend on relative weighting between
various losses involved during training. Prior works on loss weighting schemes
assume that instances are equally easy or hard for all tasks. In order to break
this assumption, we let the training process dictate the optimal weighting of
tasks for every instance in the dataset. More specifically, we equip every
instance in the dataset with a set of learnable parameters (instance-level task
parameters) where the cardinality is equal to the number of tasks learned by
the model. These parameters model the weighting of each task for an instance.
They are updated by gradient descent and do not require hand-crafted rules. We
conduct extensive experiments on SURREAL and CityScapes datasets, for human
shape and pose estimation, depth estimation and semantic segmentation tasks. In
these tasks, our approach outperforms recent dynamic loss weighting approaches,
e.g. reducing surface estimation errors by 8.97% on SURREAL. When applied to
datasets where one or more tasks can have noisy annotations, the proposed
method learns to prioritize learning from clean labels for a given task, e.g.
reducing surface estimation errors by up to 60%. We also show that we can
reliably detect corrupt labels for a given task as a by-product from learned
instance-level task parameters.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving Anytime Prediction with Parallel Cascaded Networks and a Temporal-Difference Loss. (arXiv:2102.09808v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Iuzzolino_M/0/1/0/all/0/1">Michael L. Iuzzolino</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1">Michael C. Mozer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1">Samy Bengio</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09808">
                                    <div class="article-summary-box-inner">
                                        <span>Although deep feedforward neural networks share some characteristics with the
primate visual system, a key distinction is their dynamics. Deep nets typically
operate in serial stages wherein each layer completes its computation before
processing begins in subsequent layers. In contrast, biological systems have
cascaded dynamics: information propagates from neurons at all layers in
parallel but transmission occurs gradually over time, leading to speed-accuracy
trade offs even in feedforward architectures. We explore the consequences of
biologically inspired parallel hardware by constructing cascaded ResNets in
which each residual block has propagation delays but all blocks update in
parallel in a stateful manner. Because information transmitted through skip
connections avoids delays, the functional depth of the architecture increases
over time, yielding anytime predictions that improve with internal-processing
time. We introduce a temporal-difference training loss that achieves a strictly
superior speed-accuracy profile over standard losses and enables the cascaded
architecture to outperform state-of-the-art anytime-prediction methods. The
cascaded architecture has intriguing properties, including: it classifies
typical instances more rapidly than atypical instances; it is more robust to
both persistent and transient noise than is a conventional ResNet; and its
time-varying output trace provides a signal that can be exploited to improve
information processing and inference.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Compositional Shape Priors for Few-Shot 3D Reconstruction. (arXiv:2106.06440v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Michalkiewicz_M/0/1/0/all/0/1">Mateusz Michalkiewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsogkas_S/0/1/0/all/0/1">Stavros Tsogkas</a>, <a href="http://arxiv.org/find/cs/1/au:+Parisot_S/0/1/0/all/0/1">Sarah Parisot</a>, <a href="http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1">Mahsa Baktashmotlagh</a>, <a href="http://arxiv.org/find/cs/1/au:+Eriksson_A/0/1/0/all/0/1">Anders Eriksson</a>, <a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1">Eugene Belilovsky</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06440">
                                    <div class="article-summary-box-inner">
                                        <span>The impressive performance of deep convolutional neural networks in
single-view 3D reconstruction suggests that these models perform non-trivial
reasoning about the 3D structure of the output space. Recent work has
challenged this belief, showing that, on standard benchmarks, complex
encoder-decoder architectures perform similarly to nearest-neighbor baselines
or simple linear decoder models that exploit large amounts of per-category
data. However, building large collections of 3D shapes for supervised training
is a laborious process; a more realistic and less constraining task is
inferring 3D shapes for categories with few available training examples,
calling for a model that can successfully generalize to novel object classes.
In this work we experimentally demonstrate that naive baselines fail in this
few-shot learning setting, in which the network must learn informative shape
priors for inference of new categories. We propose three ways to learn a
class-specific global shape prior, directly from data. Using these techniques,
we are able to capture multi-scale information about the 3D shape, and account
for intra-class variability by virtue of an implicit compositional structure.
Experiments on the popular ShapeNet dataset show that our method outperforms a
zero-shot baseline by over 40%, and the current state-of-the-art by over 10%,
in terms of relative performance, in the few-shot setting.12</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation. (arXiv:2106.06471v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xingyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1">Muchao Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1">Quanzeng You</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1">Fenglong Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06471">
                                    <div class="article-summary-box-inner">
                                        <span>Medical report generation is one of the most challenging tasks in medical
image analysis. Although existing approaches have achieved promising results,
they either require a predefined template database in order to retrieve
sentences or ignore the hierarchical nature of medical report generation. To
address these issues, we propose MedWriter that incorporates a novel
hierarchical retrieval mechanism to automatically extract both report and
sentence-level templates for clinically accurate report generation. MedWriter
first employs the Visual-Language Retrieval~(VLR) module to retrieve the most
relevant reports for the given images. To guarantee the logical coherence
between sentences, the Language-Language Retrieval~(LLR) module is introduced
to retrieve relevant sentences based on the previous generated description. At
last, a language decoder fuses image features and features from retrieved
reports and sentences to generate meaningful medical reports. We verified the
effectiveness of our model by automatic evaluation and human evaluation on two
datasets, i.e., Open-I and MIMIC-CXR.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">What is Multimodality?. (arXiv:2103.06304v3 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1">Letitia Parcalabescu</a>, <a href="http://arxiv.org/find/cs/1/au:+Trost_N/0/1/0/all/0/1">Nils Trost</a>, <a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1">Anette Frank</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06304">
                                    <div class="article-summary-box-inner">
                                        <span>The last years have shown rapid developments in the field of multimodal
machine learning, combining e.g., vision, text or speech. In this position
paper we explain how the field uses outdated definitions of multimodality that
prove unfit for the machine learning era. We propose a new task-relative
definition of (multi)modality in the context of multimodal machine learning
that focuses on representations and information that are relevant for a given
machine learning task. With our new definition of multimodality we aim to
provide a missing foundation for multimodal research, an important component of
language grounding and a crucial milestone towards NLU.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. (arXiv:2102.05918v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1">Chao Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yinfei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Ye Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi-Ting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1">Zarana Parekh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1">Hieu Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1">Quoc V. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1">Yunhsuan Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Duerig_T/0/1/0/all/0/1">Tom Duerig</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05918">
                                    <div class="article-summary-box-inner">
                                        <span>Pre-trained representations are becoming crucial for many NLP and perception
tasks. While representation learning in NLP has transitioned to training on raw
text without human annotations, visual and vision-language representations
still rely heavily on curated training datasets that are expensive or require
expert knowledge. For vision applications, representations are mostly learned
using datasets with explicit class labels such as ImageNet or OpenImages. For
vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all
involve a non-trivial data collection (and cleaning) process. This costly
curation process limits the size of datasets and hence hinders the scaling of
trained models. In this paper, we leverage a noisy dataset of over one billion
image alt-text pairs, obtained without expensive filtering or post-processing
steps in the Conceptual Captions dataset. A simple dual-encoder architecture
learns to align visual and language representations of the image and text pairs
using a contrastive loss. We show that the scale of our corpus can make up for
its noise and leads to state-of-the-art representations even with such a simple
learning scheme. Our visual representation achieves strong performance when
transferred to classification tasks such as ImageNet and VTAB. The aligned
visual and language representations enables zero-shot image classification and
also set new state-of-the-art results on Flickr30K and MSCOCO image-text
retrieval benchmarks, even when compared with more sophisticated
cross-attention models. The representations also enable cross-modality search
with complex text and text + image queries.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Framework to Enhance Generalization of Deep Metric Learning methods using General Discriminative Feature Learning and Class Adversarial Neural Networks. (arXiv:2106.06420v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Al_Kaabi_K/0/1/0/all/0/1">Karrar Al-Kaabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Monsefi_R/0/1/0/all/0/1">Reza Monsefi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1">Davood Zabihzadeh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06420">
                                    <div class="article-summary-box-inner">
                                        <span>Metric learning algorithms aim to learn a distance function that brings the
semantically similar data items together and keeps dissimilar ones at a
distance. The traditional Mahalanobis distance learning is equivalent to find a
linear projection. In contrast, Deep Metric Learning (DML) methods are proposed
that automatically extract features from data and learn a non-linear
transformation from input space to a semantically embedding space. Recently,
many DML methods are proposed focused to enhance the discrimination power of
the learned metric by providing novel sampling strategies or loss functions.
This approach is very helpful when both the training and test examples are
coming from the same set of categories. However, it is less effective in many
applications of DML such as image retrieval and person-reidentification. Here,
the DML should learn general semantic concepts from observed classes and employ
them to rank or identify objects from unseen categories. Neglecting the
generalization ability of the learned representation and just emphasizing to
learn a more discriminative embedding on the observed classes may lead to the
overfitting problem. To address this limitation, we propose a framework to
enhance the generalization power of existing DML methods in a Zero-Shot
Learning (ZSL) setting by general yet discriminative representation learning
and employing a class adversarial neural network. To learn a more general
representation, we propose to employ feature maps of intermediate layers in a
deep neural network and enhance their discrimination power through an attention
mechanism. Besides, a class adversarial network is utilized to enforce the deep
model to seek class invariant features for the DML task. We evaluate our work
on widely used machine vision datasets in a ZSL setting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning. (arXiv:2106.06047v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Liangqiong Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuyin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yingda Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Feifei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1">Li Fei-Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1">Ehsan Adeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1">Daniel Rubin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06047">
                                    <div class="article-summary-box-inner">
                                        <span>Federated learning is an emerging research paradigm enabling collaborative
training of machine learning models among different organizations while keeping
data private at each institution. Despite recent progress, there remain
fundamental challenges such as lack of convergence and potential for
catastrophic forgetting in federated learning across real-world heterogeneous
devices. In this paper, we demonstrate that attention-based architectures
(e.g., Transformers) are fairly robust to distribution shifts and hence improve
federated learning over heterogeneous data. Concretely, we conduct the first
rigorous empirical investigation of different neural architectures across a
range of federated algorithms, real-world benchmarks, and heterogeneous data
splits. Our experiments show that simply replacing convolutional networks with
Transformers can greatly reduce catastrophic forgetting of previous devices,
accelerate convergence, and reach a better global model, especially when
dealing with heterogeneous data. We will release our code and pretrained models
at https://github.com/Liangqiong/ViT-FL-main to encourage future exploration in
robust architectures as an alternative to current research efforts on the
optimization front.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Black-box Explanation of Object Detectors via Saliency Maps. (arXiv:2006.03204v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Petsiuk_V/0/1/0/all/0/1">Vitali Petsiuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Rajiv Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Manjunatha_V/0/1/0/all/0/1">Varun Manjunatha</a>, <a href="http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1">Vlad I. Morariu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1">Ashutosh Mehra</a>, <a href="http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1">Vicente Ordonez</a>, <a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1">Kate Saenko</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.03204">
                                    <div class="article-summary-box-inner">
                                        <span>We propose D-RISE, a method for generating visual explanations for the
predictions of object detectors. Utilizing the proposed similarity metric that
accounts for both localization and categorization aspects of object detection
allows our method to produce saliency maps that show image areas that most
affect the prediction. D-RISE can be considered &quot;black-box&quot; in the software
testing sense, as it only needs access to the inputs and outputs of an object
detector. Compared to gradient-based methods, D-RISE is more general and
agnostic to the particular type of object detector being tested, and does not
need knowledge of the inner workings of the model. We show that D-RISE can be
easily applied to different object detectors including one-stage detectors such
as YOLOv3 and two-stage detectors such as Faster-RCNN. We present a detailed
analysis of the generated visual explanations to highlight the utilization of
context and possible biases learned by object detectors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Attention-based Partial Face Recognition. (arXiv:2106.06415v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1">Stefan H&#xf6;rmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zeyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1">Martin Knoche</a>, <a href="http://arxiv.org/find/cs/1/au:+Teepe_T/0/1/0/all/0/1">Torben Teepe</a>, <a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1">Gerhard Rigoll</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06415">
                                    <div class="article-summary-box-inner">
                                        <span>Photos of faces captured in unconstrained environments, such as large crowds,
still constitute challenges for current face recognition approaches as often
faces are occluded by objects or people in the foreground. However, few studies
have addressed the task of recognizing partial faces. In this paper, we propose
a novel approach to partial face recognition capable of recognizing faces with
different occluded areas. We achieve this by combining attentional pooling of a
ResNet&#x27;s intermediate feature maps with a separate aggregation module. We
further adapt common losses to partial faces in order to ensure that the
attention maps are diverse and handle occluded parts. Our thorough analysis
demonstrates that we outperform all baselines under multiple benchmark
protocols, including naturally and synthetically occluded partial faces. This
suggests that our method successfully focuses on the relevant parts of the
occluded face.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Twins: Revisiting the Design of Spatial Attention in Vision Transformers. (arXiv:2104.13840v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiangxiang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Haibing Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiaolin Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1">Huaxia Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13840">
                                    <div class="article-summary-box-inner">
                                        <span>Very recently, a variety of vision transformer architectures for dense
prediction tasks have been proposed and they show that the design of spatial
attention is critical to their success in these tasks. In this work, we revisit
the design of the spatial attention and demonstrate that a carefully-devised
yet simple spatial attention mechanism performs favourably against the
state-of-the-art schemes. As a result, we propose two vision transformer
architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures
are highly-efficient and easy to implement, only involving matrix
multiplications that are highly optimized in modern deep learning frameworks.
More importantly, the proposed architectures achieve excellent performance on a
wide range of visual tasks including imagelevel classification as well as dense
detection and segmentation. The simplicity and strong performance suggest that
our proposed architectures may serve as stronger backbones for many vision
tasks. Our code will be released soon at
https://github.com/Meituan-AutoML/Twins .</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Progressive-Scale Boundary Blackbox Attack via Projective Gradient Estimation. (arXiv:2106.06056v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiawei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huichen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaolu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06056">
                                    <div class="article-summary-box-inner">
                                        <span>Boundary based blackbox attack has been recognized as practical and
effective, given that an attacker only needs to access the final model
prediction. However, the query efficiency of it is in general high especially
for high dimensional image data. In this paper, we show that such efficiency
highly depends on the scale at which the attack is applied, and attacking at
the optimal scale significantly improves the efficiency. In particular, we
propose a theoretical framework to analyze and show three key characteristics
to improve the query efficiency. We prove that there exists an optimal scale
for projective gradient estimation. Our framework also explains the
satisfactory performance achieved by existing boundary black-box attacks. Based
on our theoretical framework, we propose Progressive-Scale enabled projective
Boundary Attack (PSBA) to improve the query efficiency via progressive scaling
techniques. In particular, we employ Progressive-GAN to optimize the scale of
projections, which we call PSBA-PGAN. We evaluate our approach on both spatial
and frequency scales. Extensive experiments on MNIST, CIFAR-10, CelebA, and
ImageNet against different models including a real-world face recognition API
show that PSBA-PGAN significantly outperforms existing baseline attacks in
terms of query efficiency and attack success rate. We also observe relatively
stable optimal scales for different models and datasets. The code is publicly
available at https://github.com/AI-secure/PSBA.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Online Monitoring and Data-driven Control: A Study of Segmentation Algorithms for Laser Powder Bed Fusion Processes. (arXiv:2011.09065v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Nettekoven_A/0/1/0/all/0/1">Alexander Nettekoven</a>, <a href="http://arxiv.org/find/eess/1/au:+Fish_S/0/1/0/all/0/1">Scott Fish</a>, <a href="http://arxiv.org/find/eess/1/au:+Beaman_J/0/1/0/all/0/1">Joseph Beaman</a>, <a href="http://arxiv.org/find/eess/1/au:+Topcu_U/0/1/0/all/0/1">Ufuk Topcu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09065">
                                    <div class="article-summary-box-inner">
                                        <span>An increasing number of laser powder bed fusion machines use off-axis
infrared cameras to improve online monitoring and data-driven control
capabilities. However, there is still a severe lack of algorithmic solutions to
properly process the infrared images from these cameras that has led to several
key limitations: a lack of online monitoring capabilities for the laser tracks,
insufficient pre-processing of the infrared images for data-driven methods, and
large memory requirements for storing the infrared images. To address these
limitations, we study over 30 segmentation algorithms that segment each
infrared image into a foreground and background. By evaluating each algorithm
based on its segmentation accuracy, computational speed, and spatter detection
characteristics, we identify promising algorithmic solutions. The identified
algorithms can be readily applied to the laser powder bed fusion machines to
address each of the above limitations and thus, significantly improve process
control.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction. (arXiv:2105.10446v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kwan Ho Ryan Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yaodong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1">Chong You</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1">Haozhi Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1">John Wright</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10446">
                                    <div class="article-summary-box-inner">
                                        <span>This work attempts to provide a plausible theoretical framework that aims to
interpret modern deep (convolutional) networks from the principles of data
compression and discriminative representation. We argue that for
high-dimensional multi-class data, the optimal linear discriminative
representation maximizes the coding rate difference between the whole dataset
and the average of all the subsets. We show that the basic iterative gradient
ascent scheme for optimizing the rate reduction objective naturally leads to a
multi-layer deep network, named ReduNet, which shares common characteristics of
modern deep networks. The deep layered architectures, linear and nonlinear
operators, and even parameters of the network are all explicitly constructed
layer-by-layer via forward propagation, although they are amenable to
fine-tuning via back propagation. All components of so-obtained &#x60;&#x60;white-box&#x27;&#x27;
network have precise optimization, statistical, and geometric interpretation.
Moreover, all linear operators of the so-derived network naturally become
multi-channel convolutions when we enforce classification to be rigorously
shift-invariant. The derivation in the invariant setting suggests a trade-off
between sparsity and invariance, and also indicates that such a deep
convolution network is significantly more efficient to construct and learn in
the spectral domain. Our preliminary simulations and experiments clearly verify
the effectiveness of both the rate reduction objective and the associated
ReduNet. All code and data are available at https://github.com/Ma-Lab-Berkeley.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Network Modeling of Probabilities for Coding the Octree Representation of Point Clouds. (arXiv:2106.06482v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kaya_E/0/1/0/all/0/1">Emre Can Kaya</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabus_I/0/1/0/all/0/1">Ioan Tabus</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06482">
                                    <div class="article-summary-box-inner">
                                        <span>This paper describes a novel lossless point cloud compression algorithm that
uses a neural network for estimating the coding probabilities for the occupancy
status of voxels, depending on wide three dimensional contexts around the voxel
to be encoded. The point cloud is represented as an octree, with each
resolution layer being sequentially encoded and decoded using arithmetic
coding, starting from the lowest resolution, until the final resolution is
reached. The occupancy probability of each voxel of the splitting pattern at
each node of the octree is modeled by a neural network, having at its input the
already encoded occupancy status of several octree nodes (belonging to the past
and current resolutions), corresponding to a 3D context surrounding the node to
be encoded. The algorithm has a fast and a slow version, the fast version
selecting differently several voxels of the context, which allows an increased
parallelization by sending larger batches of templates to be estimated by the
neural network, at both encoder and decoder. The proposed algorithms yield
state-of-the-art results on benchmark datasets. The implementation will be made
available at https://github.com/marmus12/nnctx</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SimSwap: An Efficient Framework For High Fidelity Face Swapping. (arXiv:2106.06340v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1">Renwang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuanhong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1">Bingbing Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yanhao Ge</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06340">
                                    <div class="article-summary-box-inner">
                                        <span>We propose an efficient framework, called Simple Swap (SimSwap), aiming for
generalized and high fidelity face swapping. In contrast to previous approaches
that either lack the ability to generalize to arbitrary identity or fail to
preserve attributes like facial expression and gaze direction, our framework is
capable of transferring the identity of an arbitrary source face into an
arbitrary target face while preserving the attributes of the target face. We
overcome the above defects in the following two ways. First, we present the ID
Injection Module (IIM) which transfers the identity information of the source
face into the target face at feature level. By using this module, we extend the
architecture of an identity-specific face swapping algorithm to a framework for
arbitrary face swapping. Second, we propose the Weak Feature Matching Loss
which efficiently helps our framework to preserve the facial attributes in an
implicit way. Extensive experiments on wild faces demonstrate that our SimSwap
is able to achieve competitive identity performance while preserving attributes
better than previous state-of-the-art methods. The code is already available on
github: https://github.com/neuralchen/SimSwap.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Architecture Search without Training. (arXiv:2006.04647v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mellor_J/0/1/0/all/0/1">Joseph Mellor</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_J/0/1/0/all/0/1">Jack Turner</a>, <a href="http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1">Amos Storkey</a>, <a href="http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1">Elliot J. Crowley</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04647">
                                    <div class="article-summary-box-inner">
                                        <span>The time and effort involved in hand-designing deep neural networks is
immense. This has prompted the development of Neural Architecture Search (NAS)
techniques to automate this design. However, NAS algorithms tend to be slow and
expensive; they need to train vast numbers of candidate networks to inform the
search process. This could be alleviated if we could partially predict a
network&#x27;s trained accuracy from its initial state. In this work, we examine the
overlap of activations between datapoints in untrained networks and motivate
how this can give a measure which is usefully indicative of a network&#x27;s trained
performance. We incorporate this measure into a simple algorithm that allows us
to search for powerful networks without any training in a matter of seconds on
a single GPU, and verify its effectiveness on NAS-Bench-101, NAS-Bench-201,
NATS-Bench, and Network Design Spaces. Our approach can be readily combined
with more expensive search methods; we examine a simple adaptation of
regularised evolutionary search. Code for reproducing our experiments is
available at https://github.com/BayesWatch/nas-without-training.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">KRADA: Known-region-aware Domain Alignment for Open World Semantic Segmentation. (arXiv:2106.06237v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1">Chenhong Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1">Feng Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Gong_C/0/1/0/all/0/1">Chen Gong</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/eess/1/au:+Cheung_W/0/1/0/all/0/1">William Cheung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06237">
                                    <div class="article-summary-box-inner">
                                        <span>In semantic segmentation, we aim to train a pixel-level classifier to assign
category labels to all pixels in an image, where labeled training images and
unlabeled test images are from the same distribution and share the same label
set. However, in an open world, the unlabeled test images probably contain
unknown categories and have different distributions from the labeled images.
Hence, in this paper, we consider a new, more realistic, and more challenging
problem setting where the pixel-level classifier has to be trained with labeled
images and unlabeled open-world images -- we name it open world semantic
segmentation (OSS). In OSS, the trained classifier is expected to identify
unknown-class pixels and classify known-class pixels well. To solve OSS, we
first investigate which distribution that unknown-class pixels obey. Then,
motivated by the goodness-of-fit test, we use statistical measurements to show
how a pixel fits the distribution of an unknown class and select highly-fitted
pixels to form the unknown region in each image. Eventually, we propose an
end-to-end learning framework, known-region-aware domain alignment (KRADA), to
distinguish unknown classes while aligning distributions of known classes in
labeled and unlabeled open-world images. The effectiveness of KRADA has been
verified on two synthetic tasks and one COVID-19 segmentation task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Recovery of Meteorites Using an Autonomous Drone and Machine Learning. (arXiv:2106.06523v1 [astro-ph.EP])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/astro-ph/1/au:+Citron_R/0/1/0/all/0/1">Robert I. Citron</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Jenniskens_P/0/1/0/all/0/1">Peter Jenniskens</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Watkins_C/0/1/0/all/0/1">Christopher Watkins</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Sinha_S/0/1/0/all/0/1">Sravanthi Sinha</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Shah_A/0/1/0/all/0/1">Amar Shah</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Raissi_C/0/1/0/all/0/1">Chedy Raissi</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Devillepoix_H/0/1/0/all/0/1">Hadrien Devillepoix</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Albers_J/0/1/0/all/0/1">Jim Albers</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06523">
                                    <div class="article-summary-box-inner">
                                        <span>The recovery of freshly fallen meteorites from tracked and triangulated
meteors is critical to determining their source asteroid families. However,
locating meteorite fragments in strewn fields remains a challenge with very few
meteorites being recovered from the meteors triangulated in past and ongoing
meteor camera networks. We examined if locating meteorites can be automated
using machine learning and an autonomous drone. Drones can be programmed to fly
a grid search pattern and take systematic pictures of the ground over a large
survey area. Those images can be analyzed using a machine learning classifier
to identify meteorites in the field among many other features. Here, we
describe a proof-of-concept meteorite classifier that deploys off-line a
combination of different convolution neural networks to recognize meteorites
from images taken by drones in the field. The system was implemented in a
conceptual drone setup and tested in the suspected strewn field of a recent
meteorite fall near Walker Lake, Nevada.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A modular framework for object-based saccadic decisions in dynamic scenes. (arXiv:2106.06073v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Roth_N/0/1/0/all/0/1">Nicolas Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Bideau_P/0/1/0/all/0/1">Pia Bideau</a>, <a href="http://arxiv.org/find/cs/1/au:+Hellwich_O/0/1/0/all/0/1">Olaf Hellwich</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolfs_M/0/1/0/all/0/1">Martin Rolfs</a>, <a href="http://arxiv.org/find/cs/1/au:+Obermayer_K/0/1/0/all/0/1">Klaus Obermayer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06073">
                                    <div class="article-summary-box-inner">
                                        <span>Visually exploring the world around us is not a passive process. Instead, we
actively explore the world and acquire visual information over time. Here, we
present a new model for simulating human eye-movement behavior in dynamic
real-world scenes. We model this active scene exploration as a sequential
decision making process. We adapt the popular drift-diffusion model (DDM) for
perceptual decision making and extend it towards multiple options, defined by
objects present in the scene. For each possible choice, the model integrates
evidence over time and a decision (saccadic eye movement) is triggered as soon
as evidence crosses a decision threshold. Drawing this explicit connection
between decision making and object-based scene perception is highly relevant in
the context of active viewing, where decisions are made continuously while
interacting with an external environment. We validate our model with a
carefully designed ablation study and explore influences of our model
parameters. A comparison on the VidCom dataset supports the plausibility of the
proposed approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Gaussian Bounding Boxes and Probabilistic Intersection-over-Union for Object Detection. (arXiv:2106.06072v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Llerena_J/0/1/0/all/0/1">Jeffri M. Llerena</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeni_L/0/1/0/all/0/1">Luis Felipe Zeni</a>, <a href="http://arxiv.org/find/cs/1/au:+Kristen_L/0/1/0/all/0/1">Lucas N. Kristen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1">Claudio Jung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06072">
                                    <div class="article-summary-box-inner">
                                        <span>Most object detection methods use bounding boxes to encode and represent the
object shape and location. In this work, we explore a fuzzy representation of
object regions using Gaussian distributions, which provides an implicit binary
representation as (potentially rotated) ellipses. We also present a similarity
measure for the Gaussian distributions based on the Hellinger Distance, which
can be viewed as a Probabilistic Intersection-over-Union (ProbIoU). Our
experimental results show that the proposed Gaussian representations are closer
to annotated segmentation masks in publicly available datasets, and that loss
functions based on ProbIoU can be successfully used to regress the parameters
of the Gaussian representation. Furthermore, we present a simple mapping scheme
from traditional (or rotated) bounding boxes to Gaussian representations,
allowing the proposed ProbIoU-based losses to be seamlessly integrated into any
object detector.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Overcoming Difficulty in Obtaining Dark-skinned Subjects for Remote-PPG by Synthetic Augmentation. (arXiv:2106.06007v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ba_Y/0/1/0/all/0/1">Yunhao Ba</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Karinca_K/0/1/0/all/0/1">Kerim Doruk Karinca</a>, <a href="http://arxiv.org/find/cs/1/au:+Bozkurt_O/0/1/0/all/0/1">Oyku Deniz Bozkurt</a>, <a href="http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1">Achuta Kadambi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06007">
                                    <div class="article-summary-box-inner">
                                        <span>Camera-based remote photoplethysmography (rPPG) provides a non-contact way to
measure physiological signals (e.g., heart rate) using facial videos. Recent
deep learning architectures have improved the accuracy of such physiological
measurement significantly, yet they are restricted by the diversity of the
annotated videos. The existing datasets MMSE-HR, AFRL, and UBFC-RPPG contain
roughly 10%, 0%, and 5% of dark-skinned subjects respectively. The unbalanced
training sets result in a poor generalization capability to unseen subjects and
lead to unwanted bias toward different demographic groups. In Western academia,
it is regrettably difficult in a university setting to collect data on these
dark-skinned subjects. Here we show a first attempt to overcome the lack of
dark-skinned subjects by synthetic augmentation. A joint optimization framework
is utilized to translate real videos from light-skinned subjects to dark skin
tones while retaining their pulsatile signals. In the experiment, our method
exhibits around 31% reduction in mean absolute error for the dark-skinned group
and 46% improvement on bias mitigation for all the groups, as compared with the
previous work trained with just real samples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Spectral Unsupervised Domain Adaptation for Visual Recognition. (arXiv:2106.06112v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiaxing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1">Shijian Lu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06112">
                                    <div class="article-summary-box-inner">
                                        <span>Unsupervised domain adaptation (UDA) aims to learn a well-performed model in
an unlabeled target domain by leveraging labeled data from one or multiple
related source domains. It remains a great challenge due to 1) the lack of
annotations in the target domain and 2) the rich discrepancy between the
distributions of source and target data. We propose Spectral UDA (SUDA), an
efficient yet effective UDA technique that works in the spectral space and is
generic across different visual recognition tasks in detection, classification
and segmentation. SUDA addresses UDA challenges from two perspectives. First,
it mitigates inter-domain discrepancies by a spectrum transformer (ST) that
maps source and target images into spectral space and learns to enhance
domain-invariant spectra while suppressing domain-variant spectra
simultaneously. To this end, we design novel adversarial multi-head spectrum
attention that leverages contextual information to identify domain-variant and
domain-invariant spectra effectively. Second, it mitigates the lack of
annotations in target domain by introducing multi-view spectral learning which
aims to learn comprehensive yet confident target representations by maximizing
the mutual information among multiple ST augmentations capturing different
spectral views of each target sample. Extensive experiments over different
visual tasks (e.g., detection, classification and segmentation) show that SUDA
achieves superior accuracy and it is also complementary with state-of-the-art
UDA methods with consistent performance boosts but little extra computation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Part-aware Panoptic Segmentation. (arXiv:2106.06351v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Geus_D/0/1/0/all/0/1">Daan de Geus</a>, <a href="http://arxiv.org/find/cs/1/au:+Meletis_P/0/1/0/all/0/1">Panagiotis Meletis</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chenyang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1">Xiaoxiao Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubbelman_G/0/1/0/all/0/1">Gijs Dubbelman</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06351">
                                    <div class="article-summary-box-inner">
                                        <span>In this work, we introduce the new scene understanding task of Part-aware
Panoptic Segmentation (PPS), which aims to understand a scene at multiple
levels of abstraction, and unifies the tasks of scene parsing and part parsing.
For this novel task, we provide consistent annotations on two commonly used
datasets: Cityscapes and Pascal VOC. Moreover, we present a single metric to
evaluate PPS, called Part-aware Panoptic Quality (PartPQ). For this new task,
using the metric and annotations, we set multiple baselines by merging results
of existing state-of-the-art methods for panoptic segmentation and part
segmentation. Finally, we conduct several experiments that evaluate the
importance of the different levels of abstraction in this single task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PyGAD: An Intuitive Genetic Algorithm Python Library. (arXiv:2106.06158v1 [cs.NE])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gad_A/0/1/0/all/0/1">Ahmed Fawzy Gad</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06158">
                                    <div class="article-summary-box-inner">
                                        <span>This paper introduces PyGAD, an open-source easy-to-use Python library for
building the genetic algorithm. PyGAD supports a wide range of parameters to
give the user control over everything in its life cycle. This includes, but is
not limited to, population, gene value range, gene data type, parent selection,
crossover, and mutation. PyGAD is designed as a general-purpose optimization
library that allows the user to customize the fitness function. Its usage
consists of 3 main steps: build the fitness function, create an instance of the
pygad.GA class, and calling the pygad.GA.run() method. The library supports
training deep learning models created either with PyGAD itself or with
frameworks like Keras and PyTorch. Given its stable state, PyGAD is also in
active development to respond to the user&#x27;s requested features and enhancement
received on GitHub https://github.com/ahmedfgad/GeneticAlgorithmPython. PyGAD
comes with documentation https://pygad.readthedocs.io for further details and
examples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Efficient Deep Learning Architectures for Fast Identification of Bacterial Strains in Resource-Constrained Devices. (arXiv:2106.06505v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1">R. Gallardo Garc&#xed;a</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1">S. Jarqu&#xed;n Rodr&#xed;guez</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1">B. Beltr&#xe1;n Mart&#xed;nez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gracidas_C/0/1/0/all/0/1">C. Hern&#xe1;ndez Gracidas</a>, <a href="http://arxiv.org/find/cs/1/au:+Torres_R/0/1/0/all/0/1">R. Mart&#xed;nez Torres</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06505">
                                    <div class="article-summary-box-inner">
                                        <span>This work presents twelve fine-tuned deep learning architectures to solve the
bacterial classification problem over the Digital Image of Bacterial Species
Dataset. The base architectures were mainly published as mobile or efficient
solutions to the ImageNet challenge, and all experiments presented in this work
consisted of making several modifications to the original designs, in order to
make them able to solve the bacterial classification problem by using
fine-tuning and transfer learning techniques. This work also proposes a novel
data augmentation technique for this dataset, which is based on the idea of
artificial zooming, strongly increasing the performance of every tested
architecture, even doubling it in some cases. In order to get robust and
complete evaluations, all experiments were performed with 10-fold
cross-validation and evaluated with five different metrics: top-1 and top-5
accuracy, precision, recall, and F1 score. This paper presents a complete
comparison of the twelve different architectures, cross-validated with the
original and the augmented version of the dataset, the results are also
compared with several literature methods. Overall, eight of the eleven
architectures surpassed the 0.95 scores in top-1 accuracy with our data
augmentation method, being 0.9738 the highest top-1 accuracy. The impact of the
data augmentation technique is reported with relative improvement scores.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Compositional Video Synthesis with Action Graphs. (arXiv:2006.15327v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1">Amir Bar</a>, <a href="http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1">Roei Herzig</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1">Anna Rohrbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1">Gal Chechik</a>, <a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1">Trevor Darrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1">Amir Globerson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.15327">
                                    <div class="article-summary-box-inner">
                                        <span>Videos of actions are complex signals containing rich compositional structure
in space and time. Current video generation methods lack the ability to
condition the generation on multiple coordinated and potentially simultaneous
timed actions. To address this challenge, we propose to represent the actions
in a graph structure called Action Graph and present the new &#x60;&#x60;Action Graph To
Video&#x27;&#x27; synthesis task. Our generative model for this task (AG2Vid)
disentangles motion and appearance features, and by incorporating a scheduling
mechanism for actions facilitates a timely and coordinated video generation. We
train and evaluate AG2Vid on the CATER and Something-Something V2 datasets, and
show that the resulting videos have better visual quality and semantic
consistency compared to baselines. Finally, our model demonstrates zero-shot
abilities by synthesizing novel compositions of the learned actions. For code
and pretrained models, see the project page https://roeiherz.github.io/AG2Video</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">3D Semantic Scene Completion: a Survey. (arXiv:2103.07466v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Roldao_L/0/1/0/all/0/1">Luis Roldao</a>, <a href="http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1">Raoul de Charette</a>, <a href="http://arxiv.org/find/cs/1/au:+Verroust_Blondet_A/0/1/0/all/0/1">Anne Verroust-Blondet</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07466">
                                    <div class="article-summary-box-inner">
                                        <span>Semantic Scene Completion (SSC) aims to jointly estimate the complete
geometry and semantics of a scene, assuming partial sparse input. In the last
years following the multiplication of large-scale 3D datasets, SSC has gained
significant momentum in the research community because it holds unresolved
challenges. Specifically, SSC lies in the ambiguous completion of large
unobserved areas and the weak supervision signal of the ground truth. This led
to a substantially increasing number of papers on the matter. This survey aims
to identify, compare and analyze the techniques providing a critical analysis
of the SSC literature on both methods and datasets. Throughout the paper, we
provide an in-depth analysis of the existing works covering all choices made by
the authors while highlighting the remaining avenues of research. SSC
performance of the SoA on the most popular datasets is also evaluated and
analyzed.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Finding Physical Adversarial Examples for Autonomous Driving with Fast and Differentiable Image Compositing. (arXiv:2010.08844v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinghan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Boloor_A/0/1/0/all/0/1">Adith Boloor</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1">Ayan Chakrabarti</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1">Yevgeniy Vorobeychik</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08844">
                                    <div class="article-summary-box-inner">
                                        <span>There is considerable evidence that deep neural networks are vulnerable to
adversarial perturbations applied directly to their digital inputs. However, it
remains an open question whether this translates to vulnerabilities in real
systems. For example, an attack on self-driving cars would in practice entail
modifying the driving environment, which then impacts the video inputs to the
car&#x27;s controller, thereby indirectly leading to incorrect driving decisions.
Such attacks require accounting for system dynamics and tracking viewpoint
changes. We propose a scalable approach for finding adversarial modifications
of a simulated autonomous driving environment using a differentiable
approximation for the mapping from environmental modifications (rectangles on
the road) to the corresponding video inputs to the controller neural network.
Given the parameters of the rectangles, our proposed differentiable mapping
composites them onto pre-recorded video streams of the original environment,
accounting for geometric and color variations. Moreover, we propose a multiple
trajectory sampling approach that enables our attacks to be robust to a car&#x27;s
self-correcting behavior. When combined with a neural network-based controller,
our approach allows the design of adversarial modifications through end-to-end
gradient-based optimization. Using the Carla autonomous driving simulator, we
show that our approach is significantly more scalable and far more effective at
identifying autonomous vehicle vulnerabilities in simulation experiments than a
state-of-the-art approach based on Bayesian Optimization.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fast, Accurate Barcode Detection in Ultra High-Resolution Images. (arXiv:2102.06868v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Quenum_J/0/1/0/all/0/1">Jerome Quenum</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kehan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zakhor_A/0/1/0/all/0/1">Avideh Zakhor</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06868">
                                    <div class="article-summary-box-inner">
                                        <span>Object detection in Ultra High-Resolution (UHR) images has long been a
challenging problem in computer vision due to the varying scales of the
targeted objects. When it comes to barcode detection, resizing UHR input images
to smaller sizes often leads to the loss of pertinent information, while
processing them directly is highly inefficient and computationally expensive.
In this paper, we propose using semantic segmentation to achieve a fast and
accurate detection of barcodes of various scales in UHR images. Our pipeline
involves a modified Region Proposal Network (RPN) on images of size greater
than 10k$\times$10k and a newly proposed Y-Net segmentation network, followed
by a post-processing workflow for fitting a bounding box around each segmented
barcode mask. The end-to-end system has a latency of 16 milliseconds, which is
$2.5\times$ faster than YOLOv4 and $5.9\times$ faster than Mask R-CNN. In terms
of accuracy, our method outperforms YOLOv4 and Mask R-CNN by a $mAP$ of 5.5%
and 47.1% respectively, on a synthetic dataset. We have made available the
generated synthetic barcode dataset and its code at
this http URL</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Real-World Blind Face Restoration with Generative Facial Prior. (arXiv:2101.04061v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xintao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Honglun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1">Ying Shan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04061">
                                    <div class="article-summary-box-inner">
                                        <span>Blind face restoration usually relies on facial priors, such as facial
geometry prior or reference prior, to restore realistic and faithful details.
However, very low-quality inputs cannot offer accurate geometric prior while
high-quality references are inaccessible, limiting the applicability in
real-world scenarios. In this work, we propose GFP-GAN that leverages rich and
diverse priors encapsulated in a pretrained face GAN for blind face
restoration. This Generative Facial Prior (GFP) is incorporated into the face
restoration process via novel channel-split spatial feature transform layers,
which allow our method to achieve a good balance of realness and fidelity.
Thanks to the powerful generative facial prior and delicate designs, our
GFP-GAN could jointly restore facial details and enhance colors with just a
single forward pass, while GAN inversion methods require expensive
image-specific optimization at inference. Extensive experiments show that our
method achieves superior performance to prior art on both synthetic and
real-world datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bridge the Gap Between Model-based and Model-free Human Reconstruction. (arXiv:2106.06313v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Lixiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jianke Zhu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06313">
                                    <div class="article-summary-box-inner">
                                        <span>It is challenging to directly estimate the geometry of human from a single
image due to the high diversity and complexity of body shapes with the various
clothing styles. Most of model-based approaches are limited to predict the
shape and pose of a minimally clothed body with over-smoothing surface.
Although capturing the fine detailed geometries, the model-free methods are
lack of the fixed mesh topology. To address these issues, we propose a novel
topology-preserved human reconstruction approach by bridging the gap between
model-based and model-free human reconstruction. We present an end-to-end
neural network that simultaneously predicts the pixel-aligned implicit surface
and the explicit mesh model built by graph convolutional neural network.
Moreover, an extra graph convolutional neural network is employed to estimate
the vertex offsets between the implicit surface and parametric mesh model.
Finally, we suggest an efficient implicit registration method to refine the
neural network output in implicit space. Experiments on DeepHuman dataset
showed that our approach is effective.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions. (arXiv:2104.02710v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jennifer J. Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Karigo_T/0/1/0/all/0/1">Tomomi Karigo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1">Dipam Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1">Sharada P. Mohanty</a>, <a href="http://arxiv.org/find/cs/1/au:+Wild_B/0/1/0/all/0/1">Benjamin Wild</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Quan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1">David J. Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1">Pietro Perona</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yisong Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1">Ann Kennedy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02710">
                                    <div class="article-summary-box-inner">
                                        <span>Multi-agent behavior modeling aims to understand the interactions that occur
between agents. We present a multi-agent dataset from behavioral neuroscience,
the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists
of trajectory data of social interactions, recorded from videos of freely
behaving mice in a standard resident-intruder assay. To help accelerate
behavioral studies, the CalMS21 dataset provides benchmarks to evaluate the
performance of automated behavior classification methods in three settings: (1)
for training on large behavioral datasets all annotated by a single annotator,
(2) for style transfer to learn inter-annotator differences in behavior
definitions, and (3) for learning of new behaviors of interest given limited
training data. The dataset consists of 6 million frames of unlabeled tracked
poses of interacting mice, as well as over 1 million frames with tracked poses
and corresponding frame-level behavior annotations. The challenge of our
dataset is to be able to classify behaviors accurately using both labeled and
unlabeled tracking data, as well as being able to generalize to new settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Survey of Image Based Graph Neural Networks. (arXiv:2106.06307v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nazir_U/0/1/0/all/0/1">Usman Nazir</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">He Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Taj_M/0/1/0/all/0/1">Murtaza Taj</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06307">
                                    <div class="article-summary-box-inner">
                                        <span>In this survey paper, we analyze image based graph neural networks and
propose a three-step classification approach. We first convert the image into
superpixels using the Quickshift algorithm so as to reduce 30% of the input
data. The superpixels are subsequently used to generate a region adjacency
graph. Finally, the graph is passed through a state-of-art graph convolutional
neural network to get classification scores. We also analyze the spatial and
spectral convolution filtering techniques in graph neural networks.
Spectral-based models perform better than spatial-based models and classical
CNN with lesser compute cost.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">View Generalization for Single Image Textured 3D Models. (arXiv:2106.06533v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1">Anand Bhattad</a>, <a href="http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1">Aysegul Dundar</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guilin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1">Andrew Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1">Bryan Catanzaro</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06533">
                                    <div class="article-summary-box-inner">
                                        <span>Humans can easily infer the underlying 3D geometry and texture of an object
only from a single 2D image. Current computer vision methods can do this, too,
but suffer from view generalization problems - the models inferred tend to make
poor predictions of appearance in novel views. As for generalization problems
in machine learning, the difficulty is balancing single-view accuracy (cf.
training error; bias) with novel view accuracy (cf. test error; variance). We
describe a class of models whose geometric rigidity is easily controlled to
manage this tradeoff. We describe a cycle consistency loss that improves view
generalization (roughly, a model from a generated view should predict the
original view well). View generalization of textures requires that models share
texture information, so a car seen from the back still has headlights because
other cars have headlights. We describe a cycle consistency loss that
encourages model textures to be aligned, so as to encourage sharing. We compare
our method against the state-of-the-art method and show both qualitative and
quantitative improvements.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scale-invariant scale-channel networks: Deep networks that generalise to previously unseen scales. (arXiv:2106.06418v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jansson_Y/0/1/0/all/0/1">Ylva Jansson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1">Tony Lindeberg</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06418">
                                    <div class="article-summary-box-inner">
                                        <span>The ability to handle large scale variations is crucial for many real world
visual tasks. A straightforward approach for handling scale in a deep network
is to process an image at several scales simultaneously in a set of scale
channels. Scale invariance can then, in principle, be achieved by using weight
sharing between the scale channels together with max or average pooling over
the outputs from the scale channels. The ability of such scale channel networks
to generalise to scales not present in the training set over significant scale
ranges has, however, not previously been explored.

In this paper, we present a systematic study of this methodology by
implementing different types of scale channel networks and evaluating their
ability to generalise to previously unseen scales. We develop a formalism for
analysing the covariance and invariance properties of scale channel networks,
and explore how different design choices, unique to scaling transformations,
affect the overall performance of scale channel networks. We first show that
two previously proposed scale channel network designs do not generalise well to
scales not present in the training set. We explain theoretically and
demonstrate experimentally why generalisation fails in these cases.

We then propose a new type of foveated scale channel architecture}, where the
scale channels process increasingly larger parts of the image with decreasing
resolution. This new type of scale channel network is shown to generalise
extremely well, provided sufficient image resolution and the absence of
boundary effects. Our proposed FovMax and FovAvg networks perform almost
identically over a scale range of 8, also when training on single scale
training data, and do also give improved performance when learning from
datasets with large scale variations in the small sample regime.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Intra-Batch Connections for Deep Metric Learning. (arXiv:2102.07753v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Seidenschwarz_J/0/1/0/all/0/1">Jenny Seidenschwarz</a>, <a href="http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1">Ismail Elezi</a>, <a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1">Laura Leal-Taix&#xe9;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07753">
                                    <div class="article-summary-box-inner">
                                        <span>The goal of metric learning is to learn a function that maps samples to a
lower-dimensional space where similar samples lie closer than dissimilar ones.
Particularly, deep metric learning utilizes neural networks to learn such a
mapping. Most approaches rely on losses that only take the relations between
pairs or triplets of samples into account, which either belong to the same
class or two different classes. However, these methods do not explore the
embedding space in its entirety. To this end, we propose an approach based on
message passing networks that takes all the relations in a mini-batch into
account. We refine embedding vectors by exchanging messages among all samples
in a given batch allowing the training process to be aware of its overall
structure. Since not all samples are equally important to predict a decision
boundary, we use an attention mechanism during message passing to allow samples
to weigh the importance of each neighbor accordingly. We achieve
state-of-the-art results on clustering and image retrieval on the CUB-200-2011,
Cars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate
further research, we make available the code and the models at
https://github.com/dvl-tum/intra_batch_connections.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A deep learning approach to clustering visual arts. (arXiv:2106.06234v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Castellano_G/0/1/0/all/0/1">Giovanna Castellano</a>, <a href="http://arxiv.org/find/cs/1/au:+Vessio_G/0/1/0/all/0/1">Gennaro Vessio</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06234">
                                    <div class="article-summary-box-inner">
                                        <span>Clustering artworks is difficult for several reasons. On the one hand,
recognizing meaningful patterns based on domain knowledge and visual perception
is extremely hard. On the other hand, applying traditional clustering and
feature reduction techniques to the highly dimensional pixel space can be
ineffective. To address these issues, in this paper we propose DELIUS: a DEep
learning approach to cLustering vIsUal artS. The method uses a pre-trained
convolutional network to extract features and then feeds these features into a
deep embedded clustering model, where the task of mapping the raw input data to
a latent space is jointly optimized with the task of finding a set of cluster
centroids in this latent space. Quantitative and qualitative experimental
results show the effectiveness of the proposed method. DELIUS can be useful for
several tasks related to art analysis, in particular visual link retrieval and
historical knowledge discovery in painting datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A self-adapting super-resolution structures framework for automatic design of GAN. (arXiv:2106.06011v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yibo Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haidi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yiming Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shunyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mingliang Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06011">
                                    <div class="article-summary-box-inner">
                                        <span>With the development of deep learning, the single super-resolution image
reconstruction network models are becoming more and more complex. Small changes
in hyperparameters of the models have a greater impact on model performance. In
the existing works, experts have gradually explored a set of optimal model
parameters based on empirical values or performing brute-force search. In this
paper, we introduce a new super-resolution image reconstruction generative
adversarial network framework, and a Bayesian optimization method used to
optimizing the hyperparameters of the generator and discriminator. The
generator is made by self-calibrated convolution, and discriminator is made by
convolution lays. We have defined the hyperparameters such as the number of
network layers and the number of neurons. Our method adopts Bayesian
optimization as a optimization policy of GAN in our model. Not only can find
the optimal hyperparameter solution automatically, but also can construct a
super-resolution image reconstruction network, reducing the manual workload.
Experiments show that Bayesian optimization can search the optimal solution
earlier than the other two optimization algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Antipodal Robotic Grasping using Generative Residual Convolutional Neural Network. (arXiv:1909.04810v4 [cs.RO] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kumra_S/0/1/0/all/0/1">Sulabh Kumra</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1">Shirin Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahin_F/0/1/0/all/0/1">Ferat Sahin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.04810">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we present a modular robotic system to tackle the problem of
generating and performing antipodal robotic grasps for unknown objects from
n-channel image of the scene. We propose a novel Generative Residual
Convolutional Neural Network (GR-ConvNet) model that can generate robust
antipodal grasps from n-channel input at real-time speeds (~20ms). We evaluate
the proposed model architecture on standard datasets and a diverse set of
household objects. We achieved state-of-the-art accuracy of 97.7% and 94.6% on
Cornell and Jacquard grasping datasets respectively. We also demonstrate a
grasp success rate of 95.4% and 93% on household and adversarial objects
respectively using a 7 DoF robotic arm.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">BiPointNet: Binary Neural Network for Point Clouds. (arXiv:2010.05501v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1">Haotong Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zhongang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mingyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yifu Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haiyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1">Shuai Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xianglong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hao Su</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05501">
                                    <div class="article-summary-box-inner">
                                        <span>To alleviate the resource constraint for real-time point cloud applications
that run on edge devices, in this paper we present BiPointNet, the first model
binarization approach for efficient deep learning on point clouds. We discover
that the immense performance drop of binarized models for point clouds mainly
stems from two challenges: aggregation-induced feature homogenization that
leads to a degradation of information entropy, and scale distortion that
hinders optimization and invalidates scale-sensitive structures. With
theoretical justifications and in-depth analysis, our BiPointNet introduces
Entropy-Maximizing Aggregation (EMA) to modulate the distribution before
aggregation for the maximum information entropy, and Layer-wise Scale Recovery
(LSR) to efficiently restore feature representation capacity. Extensive
experiments show that BiPointNet outperforms existing binarization methods by
convincing margins, at the level even comparable with the full precision
counterpart. We highlight that our techniques are generic, guaranteeing
significant improvements on various fundamental tasks and mainstream backbones.
Moreover, BiPointNet gives an impressive 14.7x speedup and 18.9x storage saving
on real-world resource-constrained devices.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Real-Time Global Illumination Decomposition of Videos. (arXiv:1908.01961v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1">Abhimitra Meka</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafiei_M/0/1/0/all/0/1">Mohammad Shafiei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1">Michael Zollhoefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1">Christian Richardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1">Christian Theobalt</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.01961">
                                    <div class="article-summary-box-inner">
                                        <span>We propose the first approach for the decomposition of a monocular color
video into direct and indirect illumination components in real time. We
retrieve, in separate layers, the contribution made to the scene appearance by
the scene reflectance, the light sources and the reflections from various
coherent scene regions to one another. Existing techniques that invert global
light transport require image capture under multiplexed controlled lighting, or
only enable the decomposition of a single image at slow off-line frame rates.
In contrast, our approach works for regular videos and produces temporally
coherent decomposition layers at real-time frame rates. At the core of our
approach are several sparsity priors that enable the estimation of the
per-pixel direct and indirect illumination layers based on a small set of
jointly estimated base reflectance colors. The resulting variational
decomposition problem uses a new formulation based on sparse and dense sets of
non-linear equations that we solve efficiently using a novel alternating
data-parallel optimization strategy. We evaluate our approach qualitatively and
quantitatively, and show improvements over the state of the art in this field,
in both quality and runtime. In addition, we demonstrate various real-time
appearance editing applications for videos with consistent illumination.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Conterfactual Generative Zero-Shot Semantic Segmentation. (arXiv:2106.06360v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1">Feihong Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1">Ping Hu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06360">
                                    <div class="article-summary-box-inner">
                                        <span>zero-shot learning is an essential part of computer vision. As a classical
downstream task, zero-shot semantic segmentation has been studied because of
its applicant value. One of the popular zero-shot semantic segmentation methods
is based on the generative model Most new proposed works added structures on
the same architecture to enhance this model. However, we found that, from the
view of causal inference, the result of the original model has been influenced
by spurious statistical relationships. Thus the performance of the prediction
shows severe bias. In this work, we consider counterfactual methods to avoid
the confounder in the original model. Based on this method, we proposed a new
framework for zero-shot semantic segmentation. Our model is compared with
baseline models on two real-world datasets, Pascal-VOC and Pascal-Context. The
experiment results show proposed models can surpass previous confounded models
and can still make use of additional structures to improve the performance. We
also design a simple structure based on Graph Convolutional Networks (GCN) in
this work.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Generalising Neural Implicit Representations. (arXiv:2101.12690v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Costain_T/0/1/0/all/0/1">Theo W. Costain</a>, <a href="http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1">Victor Adrian Prisacariu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12690">
                                    <div class="article-summary-box-inner">
                                        <span>Neural implicit representations have shown substantial improvements in
efficiently storing 3D data, when compared to conventional formats. However,
the focus of existing work has mainly been on storage and subsequent
reconstruction. In this work, we show that training neural representations for
reconstruction tasks alongside conventional tasks can produce more general
encodings that admit equal quality reconstructions to single task training,
whilst improving results on conventional tasks when compared to single task
encodings. We reformulate the semantic segmentation task, creating a more
representative task for implicit representation contexts, and through
multi-task experiments on reconstruction, classification, and segmentation,
show our approach learns feature rich encodings that admit equal performance
for each task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Representation Disentanglement for Multi-modal brain MR Analysis. (arXiv:2102.11456v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1">Jiahong Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1">Ehsan Adeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1">Kilian M. Pohl</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qingyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaharchuk_G/0/1/0/all/0/1">Greg Zaharchuk</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11456">
                                    <div class="article-summary-box-inner">
                                        <span>Multi-modal MRIs are widely used in neuroimaging applications since different
MR sequences provide complementary information about brain structures. Recent
works have suggested that multi-modal deep learning analysis can benefit from
explicitly disentangling anatomical (shape) and modality (appearance)
information into separate image presentations. In this work, we challenge
mainstream strategies by showing that they do not naturally lead to
representation disentanglement both in theory and in practice. To address this
issue, we propose a margin loss that regularizes the similarity in
relationships of the representations across subjects and modalities. To enable
robust training, we further use a conditional convolution to design a single
model for encoding images of all modalities. Lastly, we propose a fusion
function to combine the disentangled anatomical representations as a set of
modality-invariant features for downstream tasks. We evaluate the proposed
method on three multi-modal neuroimaging datasets. Experiments show that our
proposed method can achieve superior disentangled representations compared to
existing disentanglement strategies. Results also indicate that the fused
anatomical representation has potential in the downstream task of zero-dose PET
reconstruction and brain tumor segmentation. The code is available at
\url{https://github.com/ouyangjiahong/representation-disentanglement}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Predicting Next Local Appearance for Video Anomaly Detection. (arXiv:2106.06059v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1">Pankaj Raj Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1">Guillaume-Alexandre Bilodeau</a>, <a href="http://arxiv.org/find/cs/1/au:+Seoud_L/0/1/0/all/0/1">Lama Seoud</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06059">
                                    <div class="article-summary-box-inner">
                                        <span>We present a local anomaly detection method in videos. As opposed to most
existing methods that are computationally expensive and are not very
generalizable across different video scenes, we propose an adversarial
framework that learns the temporal local appearance variations by predicting
the appearance of a normally behaving object in the next frame of a scene by
only relying on its current and past appearances. In the presence of an
abnormally behaving object, the reconstruction error between the real and the
predicted next appearance of that object indicates the likelihood of an
anomaly. Our method is competitive with the existing state-of-the-art while
being significantly faster for both training and inference and being better at
generalizing to unseen video scenes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Step-Wise Hierarchical Alignment Network for Image-Text Matching. (arXiv:2106.06509v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1">Zhong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kexin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoran Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06509">
                                    <div class="article-summary-box-inner">
                                        <span>Image-text matching plays a central role in bridging the semantic gap between
vision and language. The key point to achieve precise visual-semantic alignment
lies in capturing the fine-grained cross-modal correspondence between image and
text. Most previous methods rely on single-step reasoning to discover the
visual-semantic interactions, which lacks the ability of exploiting the
multi-level information to locate the hierarchical fine-grained relevance.
Different from them, in this work, we propose a step-wise hierarchical
alignment network (SHAN) that decomposes image-text matching into multi-step
cross-modal reasoning process. Specifically, we first achieve local-to-local
alignment at fragment level, following by performing global-to-local and
global-to-global alignment at context level sequentially. This progressive
alignment strategy supplies our model with more complementary and sufficient
semantic clues to understand the hierarchical correlations between image and
text. The experimental results on two benchmark datasets demonstrate the
superiority of our proposed method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Small Object Detection for Near Real-Time Egocentric Perception in a Manual Assembly Scenario. (arXiv:2106.06403v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tavakoli_H/0/1/0/all/0/1">Hooman Tavakoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Walunj_S/0/1/0/all/0/1">Snehal Walunj</a>, <a href="http://arxiv.org/find/cs/1/au:+Pahlevannejad_P/0/1/0/all/0/1">Parsha Pahlevannejad</a>, <a href="http://arxiv.org/find/cs/1/au:+Plociennik_C/0/1/0/all/0/1">Christiane Plociennik</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruskowski_M/0/1/0/all/0/1">Martin Ruskowski</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06403">
                                    <div class="article-summary-box-inner">
                                        <span>Detecting small objects in video streams of head-worn augmented reality
devices in near real-time is a huge challenge: training data is typically
scarce, the input video stream can be of limited quality, and small objects are
notoriously hard to detect. In industrial scenarios, however, it is often
possible to leverage contextual knowledge for the detection of small objects.
Furthermore, CAD data of objects are typically available and can be used to
generate synthetic training data. We describe a near real-time small object
detection pipeline for egocentric perception in a manual assembly scenario: We
generate a training data set based on CAD data and realistic backgrounds in
Unity. We then train a YOLOv4 model for a two-stage detection process: First,
the context is recognized, then the small object of interest is detected. We
evaluate our pipeline on the augmented reality device Microsoft Hololens 2.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MlTr: Multi-label Classification with Transformer. (arXiv:2106.06195v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xing Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Hezheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiangyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1">Dong Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1">Nian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Honglin Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06195">
                                    <div class="article-summary-box-inner">
                                        <span>The task of multi-label image classification is to recognize all the object
labels presented in an image. Though advancing for years, small objects,
similar objects and objects with high conditional probability are still the
main bottlenecks of previous convolutional neural network(CNN) based models,
limited by convolutional kernels&#x27; representational capacity. Recent vision
transformer networks utilize the self-attention mechanism to extract the
feature of pixel granularity, which expresses richer local semantic
information, while is insufficient for mining global spatial dependence. In
this paper, we point out the three crucial problems that CNN-based methods
encounter and explore the possibility of conducting specific transformer
modules to settle them. We put forward a Multi-label Transformer
architecture(MlTr) constructed with windows partitioning, in-window pixel
attention, cross-window attention, particularly improving the performance of
multi-label image classification tasks. The proposed MlTr shows
state-of-the-art results on various prevalent multi-label datasets such as
MS-COCO, Pascal-VOC, and NUS-WIDE with 88.5%, 95.8%, and 65.5% respectively.
The code will be available soon at https://github.com/starmemda/MlTr/</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Team RUC_AIM3 Technical Report at ActivityNet 2021: Entities Object Localization. (arXiv:2106.06138v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ruan_L/0/1/0/all/0/1">Ludan Ruan</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jieting Chen</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yuqing Song</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shizhe Chen</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qin Jin</a> (1) ((1) Renmin University of China, (2) INRIA)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06138">
                                    <div class="article-summary-box-inner">
                                        <span>Entities Object Localization (EOL) aims to evaluate how grounded or faithful
a description is, which consists of caption generation and object grounding.
Previous works tackle this problem by jointly training the two modules in a
framework, which limits the complexity of each module. Therefore, in this work,
we propose to divide these two modules into two stages and improve them
respectively to boost the whole system performance. For the caption generation,
we propose a Unified Multi-modal Pre-training Model (UMPM) to generate event
descriptions with rich objects for better localization. For the object
grounding, we fine-tune the state-of-the-art detection model MDETR and design a
post processing method to make the grounding results more faithful. Our overall
system achieves the state-of-the-art performances on both sub-tasks in Entities
Object Localization challenge at Activitynet 2021, with 72.57 localization
accuracy on the testing set of sub-task I and 0.2477 F1_all_per_sent on the
hidden testing set of sub-task II.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm. (arXiv:2106.06027v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Mingkang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianlong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06027">
                                    <div class="article-summary-box-inner">
                                        <span>Sparse adversarial attacks can fool deep neural networks (DNNs) by only
perturbing a few pixels (regularized by l_0 norm). Recent efforts combine it
with another l_infty imperceptible on the perturbation magnitudes. The
resultant sparse and imperceptible attacks are practically relevant, and
indicate an even higher vulnerability of DNNs that we usually imagined.
However, such attacks are more challenging to generate due to the optimization
difficulty by coupling the l_0 regularizer and box constraints with a
non-convex objective. In this paper, we address this challenge by proposing a
homotopy algorithm, to jointly tackle the sparsity and the perturbation bound
in one unified framework. Each iteration, the main step of our algorithm is to
optimize an l_0-regularized adversarial loss, by leveraging the nonmonotone
Accelerated Proximal Gradient Method (nmAPG) for nonconvex programming; it is
followed by an l_0 change control step, and an optional post-attack step
designed to escape bad local minima. We also extend the algorithm to handling
the structural sparsity regularizer. We extensively examine the effectiveness
of our proposed homotopy attack for both targeted and non-targeted attack
scenarios, on CIFAR-10 and ImageNet datasets. Compared to state-of-the-art
methods, our homotopy attack leads to significantly fewer perturbations, e.g.,
reducing 42.91% on CIFAR-10 and 75.03% on ImageNet (average case, targeted
attack), at similar maximal perturbation magnitudes, when still achieving 100%
attack success rates. Our codes are available at:
https://github.com/VITA-Group/SparseADV_Homotopy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Within-layer Diversity Reduces Generalization Gap. (arXiv:2106.06012v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Laakom_F/0/1/0/all/0/1">Firas Laakom</a>, <a href="http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1">Jenni Raitoharju</a>, <a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1">Alexandros Iosifidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1">Moncef Gabbouj</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06012">
                                    <div class="article-summary-box-inner">
                                        <span>Neural networks are composed of multiple layers arranged in a hierarchical
structure jointly trained with a gradient-based optimization, where the errors
are back-propagated from the last layer back to the first one. At each
optimization step, neurons at a given layer receive feedback from neurons
belonging to higher layers of the hierarchy. In this paper, we propose to
complement this traditional &#x27;between-layer&#x27; feedback with additional
&#x27;within-layer&#x27; feedback to encourage diversity of the activations within the
same layer. To this end, we measure the pairwise similarity between the outputs
of the neurons and use it to model the layer&#x27;s overall diversity. By penalizing
similarities and promoting diversity, we encourage each neuron to learn a
distinctive representation and, thus, to enrich the data representation learned
within the layer and to increase the total capacity of the model. We
theoretically study how the within-layer activation diversity affects the
generalization performance of a neural network and prove that increasing the
diversity of hidden activations reduces the estimation error. In addition to
the theoretical guarantees, we present an empirical study on three datasets
confirming that the proposed approach enhances the performance of
state-of-the-art neural network models and decreases the generalization gap.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning the Precise Feature for Cluster Assignment. (arXiv:2106.06159v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1">Yanhai Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xinghui Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Huiyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1">Feng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Junyu Dong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06159">
                                    <div class="article-summary-box-inner">
                                        <span>Clustering is one of the fundamental tasks in computer vision and pattern
recognition. Recently, deep clustering methods (algorithms based on deep
learning) have attracted wide attention with their impressive performance. Most
of these algorithms combine deep unsupervised representation learning and
standard clustering together. However, the separation of representation
learning and clustering will lead to suboptimal solutions because the two-stage
strategy prevents representation learning from adapting to subsequent tasks
(e.g., clustering according to specific cues). To overcome this issue, efforts
have been made in the dynamic adaption of representation and cluster
assignment, whereas current state-of-the-art methods suffer from heuristically
constructed objectives with representation and cluster assignment alternatively
optimized. To further standardize the clustering problem, we audaciously
formulate the objective of clustering as finding a precise feature as the cue
for cluster assignment. Based on this, we propose a general-purpose deep
clustering framework which radically integrates representation learning and
clustering into a single pipeline for the first time. The proposed framework
exploits the powerful ability of recently developed generative models for
learning intrinsic features, and imposes an entropy minimization on the
distribution of the cluster assignment by a dedicated variational algorithm.
Experimental results show that the performance of the proposed method is
superior, or at least comparable to, the state-of-the-art methods on the
handwritten digit recognition, fashion recognition, face recognition and object
recognition benchmark datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scaling Vision with Sparse Mixture of Experts. (arXiv:2106.05974v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Riquelme_C/0/1/0/all/0/1">Carlos Riquelme</a>, <a href="http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1">Joan Puigcerver</a>, <a href="http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1">Basil Mustafa</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_M/0/1/0/all/0/1">Maxim Neumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1">Rodolphe Jenatton</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinto_A/0/1/0/all/0/1">Andr&#xe9; Susano Pinto</a>, <a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1">Daniel Keysers</a>, <a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1">Neil Houlsby</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05974">
                                    <div class="article-summary-box-inner">
                                        <span>Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent
scalability in Natural Language Processing. In Computer Vision, however, almost
all performant networks are &quot;dense&quot;, that is, every input is processed by every
parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision
Transformer, that is scalable and competitive with the largest dense networks.
When applied to image recognition, V-MoE matches the performance of
state-of-the-art networks, while requiring as little as half of the compute at
inference time. Further, we propose an extension to the routing algorithm that
can prioritize subsets of each input across the entire batch, leading to
adaptive per-image compute. This allows V-MoE to trade-off performance and
compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE
to scale vision models, and train a 15B parameter model that attains 90.35% on
ImageNet.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Coordinate Independent Convolutional Networks -- Isometry and Gauge Equivariant Convolutions on Riemannian Manifolds. (arXiv:2106.06020v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1">Maurice Weiler</a>, <a href="http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1">Patrick Forr&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Verlinde_E/0/1/0/all/0/1">Erik Verlinde</a>, <a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1">Max Welling</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06020">
                                    <div class="article-summary-box-inner">
                                        <span>Motivated by the vast success of deep convolutional networks, there is a
great interest in generalizing convolutions to non-Euclidean manifolds. A major
complication in comparison to flat spaces is that it is unclear in which
alignment a convolution kernel should be applied on a manifold. The underlying
reason for this ambiguity is that general manifolds do not come with a
canonical choice of reference frames (gauge). Kernels and features therefore
have to be expressed relative to arbitrary coordinates. We argue that the
particular choice of coordinatization should not affect a network&#x27;s inference
-- it should be coordinate independent. A simultaneous demand for coordinate
independence and weight sharing is shown to result in a requirement on the
network to be equivariant under local gauge transformations (changes of local
reference frames). The ambiguity of reference frames depends thereby on the
G-structure of the manifold, such that the necessary level of gauge
equivariance is prescribed by the corresponding structure group G. Coordinate
independent convolutions are proven to be equivariant w.r.t. those isometries
that are symmetries of the G-structure. The resulting theory is formulated in a
coordinate free fashion in terms of fiber bundles. To exemplify the design of
coordinate independent convolutions, we implement a convolutional network on
the M\&quot;obius strip. The generality of our differential geometric formulation of
convolutional networks is demonstrated by an extensive literature review which
explains a large number of Euclidean CNNs, spherical CNNs and CNNs on general
surfaces as specific instances of coordinate independent convolutions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.IR"">cs.IR updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Anytime Ranking on Document-Ordered Indexes. (arXiv:2104.08976v2 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mackenzie_J/0/1/0/all/0/1">Joel Mackenzie</a>, <a href="http://arxiv.org/find/cs/1/au:+Petri_M/0/1/0/all/0/1">Matthias Petri</a>, <a href="http://arxiv.org/find/cs/1/au:+Moffat_A/0/1/0/all/0/1">Alistair Moffat</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08976">
                                    <div class="article-summary-box-inner">
                                        <span>Inverted indexes continue to be a mainstay of text search engines, allowing
efficient querying of large document collections. While there are a number of
possible organizations, document-ordered indexes are the most common, since
they are amenable to various query types, support index updates, and allow for
efficient dynamic pruning operations. One disadvantage with document-ordered
indexes is that high-scoring documents can be distributed across the document
identifier space, meaning that index traversal algorithms that terminate early
might put search effectiveness at risk. The alternative is impact-ordered
indexes, which primarily support top-k disjunctions, but also allow for anytime
query processing, where the search can be terminated at any time, with search
quality improving as processing latency increases. Anytime query processing can
be used to effectively reduce high-percentile tail latency which is essential
for operational scenarios in which a service level agreement (SLA) imposes
response time requirements. In this work, we show how document-ordered indexes
can be organized such that they can be queried in an anytime fashion, enabling
strict latency control with effective early termination. Our experiments show
that processing document-ordered topical segments selected by a simple score
estimator outperforms existing anytime algorithms, and allows query runtimes to
be accurately limited in order to comply with SLA requirements.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Predicting Knowledge Gain during Web Search based on Multimedia Resource Consumption. (arXiv:2106.06244v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Otto_C/0/1/0/all/0/1">Christian Otto</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1">Ran Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pardi_G/0/1/0/all/0/1">Georg Pardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoyer_J/0/1/0/all/0/1">Johannes von Hoyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rokicki_M/0/1/0/all/0/1">Markus Rokicki</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoppe_A/0/1/0/all/0/1">Anett Hoppe</a>, <a href="http://arxiv.org/find/cs/1/au:+Holtz_P/0/1/0/all/0/1">Peter Holtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Kammerer_Y/0/1/0/all/0/1">Yvonne Kammerer</a>, <a href="http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1">Stefan Dietze</a>, <a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1">Ralph Ewerth</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06244">
                                    <div class="article-summary-box-inner">
                                        <span>In informal learning scenarios the popularity of multimedia content, such as
video tutorials or lectures, has significantly increased. Yet, the users&#x27;
interactions, navigation behavior, and consequently learning outcome, have not
been researched extensively. Related work in this field, also called search as
learning, has focused on behavioral or text resource features to predict
learning outcome and knowledge gain. In this paper, we investigate whether we
can exploit features representing multimedia resource consumption to predict of
knowledge gain (KG) during Web search from in-session data, that is without
prior knowledge about the learner. For this purpose, we suggest a set of
multimedia features related to image and video consumption. Our feature
extraction is evaluated in a lab study with 113 participants where we collected
data for a given search as learning task on the formation of thunderstorms and
lightning. We automatically analyze the monitored log data and utilize
state-of-the-art computer vision methods to extract features about the seen
multimedia resources. Experimental results demonstrate that multimedia features
can improve KG prediction. Finally, we provide an analysis on feature
importance (text and multimedia) for KG prediction.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation. (arXiv:2106.06471v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xingyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1">Muchao Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1">Quanzeng You</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1">Fenglong Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06471">
                                    <div class="article-summary-box-inner">
                                        <span>Medical report generation is one of the most challenging tasks in medical
image analysis. Although existing approaches have achieved promising results,
they either require a predefined template database in order to retrieve
sentences or ignore the hierarchical nature of medical report generation. To
address these issues, we propose MedWriter that incorporates a novel
hierarchical retrieval mechanism to automatically extract both report and
sentence-level templates for clinically accurate report generation. MedWriter
first employs the Visual-Language Retrieval~(VLR) module to retrieve the most
relevant reports for the given images. To guarantee the logical coherence
between sentences, the Language-Language Retrieval~(LLR) module is introduced
to retrieve relevant sentences based on the previous generated description. At
last, a language decoder fuses image features and features from retrieved
reports and sentences to generate meaningful medical reports. We verified the
effectiveness of our model by automatic evaluation and human evaluation on two
datasets, i.e., Open-I and MIMIC-CXR.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Large-Scale Rich Context Query and Recommendation Dataset in Online Knowledge-Sharing. (arXiv:2106.06467v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hao_B/0/1/0/all/0/1">Bin Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Weizhi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1">Shaoyun Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xinxing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1">Houzhi Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yiqun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1">Shaoping Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06467">
                                    <div class="article-summary-box-inner">
                                        <span>Data plays a vital role in machine learning studies. In the research of
recommendation, both user behaviors and side information are helpful to model
users. So, large-scale real scenario datasets with abundant user behaviors will
contribute a lot. However, it is not easy to get such datasets as most of them
are only hold and protected by companies. In this paper, a new large-scale
dataset collected from a knowledge-sharing platform is presented, which is
composed of around 100M interactions collected within 10 days, 798K users, 165K
questions, 554K answers, 240K authors, 70K topics, and more than 501K user
query keywords. There are also descriptions of users, answers, questions,
authors, and topics, which are anonymous. Note that each user&#x27;s latest query
keywords have not been included in previous open datasets, which reveal users&#x27;
explicit information needs.

We characterize the dataset and demonstrate its potential applications for
recommendation study. Multiple experiments show the dataset can be used to
evaluate algorithms in general top-N recommendation, sequential recommendation,
and context-aware recommendation. This dataset can also be used to integrate
search and recommendation and recommendation with negative feedback. Besides,
tasks beyond recommendation, such as user gender prediction, most valuable
answerer identification, and high-quality answer recognition, can also use this
dataset. To the best of our knowledge, this is the largest real-world
interaction dataset for personalized recommendation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Framework to Enhance Generalization of Deep Metric Learning methods using General Discriminative Feature Learning and Class Adversarial Neural Networks. (arXiv:2106.06420v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Al_Kaabi_K/0/1/0/all/0/1">Karrar Al-Kaabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Monsefi_R/0/1/0/all/0/1">Reza Monsefi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1">Davood Zabihzadeh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06420">
                                    <div class="article-summary-box-inner">
                                        <span>Metric learning algorithms aim to learn a distance function that brings the
semantically similar data items together and keeps dissimilar ones at a
distance. The traditional Mahalanobis distance learning is equivalent to find a
linear projection. In contrast, Deep Metric Learning (DML) methods are proposed
that automatically extract features from data and learn a non-linear
transformation from input space to a semantically embedding space. Recently,
many DML methods are proposed focused to enhance the discrimination power of
the learned metric by providing novel sampling strategies or loss functions.
This approach is very helpful when both the training and test examples are
coming from the same set of categories. However, it is less effective in many
applications of DML such as image retrieval and person-reidentification. Here,
the DML should learn general semantic concepts from observed classes and employ
them to rank or identify objects from unseen categories. Neglecting the
generalization ability of the learned representation and just emphasizing to
learn a more discriminative embedding on the observed classes may lead to the
overfitting problem. To address this limitation, we propose a framework to
enhance the generalization power of existing DML methods in a Zero-Shot
Learning (ZSL) setting by general yet discriminative representation learning
and employing a class adversarial neural network. To learn a more general
representation, we propose to employ feature maps of intermediate layers in a
deep neural network and enhance their discrimination power through an attention
mechanism. Besides, a class adversarial network is utilized to enforce the deep
model to seek class invariant features for the DML task. We evaluate our work
on widely used machine vision datasets in a ZSL setting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Modeling Sequences as Distributions with Uncertainty for Sequential Recommendation. (arXiv:2106.06165v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Ziwei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06165">
                                    <div class="article-summary-box-inner">
                                        <span>The sequential patterns within the user interactions are pivotal for
representing the user&#x27;s preference and capturing latent relationships among
items. The recent advancements of sequence modeling by Transformers advocate
the community to devise more effective encoders for the sequential
recommendation. Most existing sequential methods assume users are
deterministic. However, item-item transitions might fluctuate significantly in
several item aspects and exhibit randomness of user interests. This
\textit{stochastic characteristics} brings up a solid demand to include
uncertainties in representing sequences and items. Additionally, modeling
sequences and items with uncertainties expands users&#x27; and items&#x27; interaction
spaces, thus further alleviating cold-start problems.

In this work, we propose a Distribution-based Transformer for Sequential
Recommendation (DT4SR), which injects uncertainties into sequential modeling.
We use Elliptical Gaussian distributions to describe items and sequences with
uncertainty. We describe the uncertainty in items and sequences as Elliptical
Gaussian distribution. And we adopt Wasserstein distance to measure the
similarity between distributions. We devise two novel Trans-formers for
modeling mean and covariance, which guarantees the positive-definite property
of distributions. The proposed method significantly outperforms the
state-of-the-art methods. The experiments on three benchmark datasets also
demonstrate its effectiveness in alleviating cold-start issues. The code is
available inhttps://github.com/DyGRec/DT4SR.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Nested and Balanced Entity Recognition using Multi-Task Learning. (arXiv:2106.06216v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Waldis_A/0/1/0/all/0/1">Andreas Waldis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazzola_L/0/1/0/all/0/1">Luca Mazzola</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06216">
                                    <div class="article-summary-box-inner">
                                        <span>Entity Recognition (ER) within a text is a fundamental exercise in Natural
Language Processing, enabling further depending tasks such as Knowledge
Extraction, Text Summarisation, or Keyphrase Extraction. An entity consists of
single words or of a consecutive sequence of terms, constituting the basic
building blocks for communication. Mainstream ER approaches are mainly limited
to flat structures, concentrating on the outermost entities while ignoring the
inner ones. This paper introduces a partly-layered network architecture that
deals with the complexity of overlapping and nested cases. The proposed
architecture consists of two parts: (1) a shared Sequence Layer and (2) a
stacked component with multiple Tagging Layers. The adoption of such an
architecture has the advantage of preventing overfit to a specific word-length,
thus maintaining performance for longer entities despite their lower frequency.
To verify the proposed architecture&#x27;s effectiveness, we train and evaluate this
architecture to recognise two kinds of entities - Concepts (CR) and Named
Entities (NER). Our approach achieves state-of-the-art NER performances, while
it outperforms previous CR approaches. Considering these promising results, we
see the possibility to evolve the architecture for other cases such as the
extraction of events or the detection of argumentative components.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DebiasGAN: Eliminating Position Bias in News Recommendation with Adversarial Learning. (arXiv:2106.06258v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chuhan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fangzhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yongfeng Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06258">
                                    <div class="article-summary-box-inner">
                                        <span>News recommendation is important for improving news reading experience of
users. Users&#x27; news click behaviors are widely used for inferring user interests
and predicting future clicks. However, click behaviors are heavily affected by
the biases brought by the positions of news displayed on the webpage. It is
important to eliminate the effect of position biases on the recommendation
model to accurately target user interests. In this paper, we propose a news
recommendation method named DebiasGAN that can effectively eliminate the effect
of position biases via adversarial learning. We use a bias-aware click model to
capture the influence of position bias on click behaviors, and we use a
bias-invariant click model with random candidate news positions to estimate the
ideally unbiased click scores. We apply adversarial learning techniques to the
hidden representations learned by the two models to help the bias-invariant
click model capture the bias-independent interest of users on news.
Experimental results on two real-world datasets show that DebiasGAN can
effectively improve the accuracy of news recommendation by eliminating position
biases.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">IoT Virtualization with ML-based Information Extraction. (arXiv:2106.06022v1 [cs.DC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1">Martin Bauer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06022">
                                    <div class="article-summary-box-inner">
                                        <span>For IoT to reach its full potential, the sharing and reuse of information in
different applications and across verticals is of paramount importance.
However, there are a plethora of IoT platforms using different representations,
protocols and interaction patterns. To address this issue, the Fed4IoT project
has developed an IoT virtualization platform that, on the one hand, integrates
information from many different source platforms and, on the other hand, makes
the information required by the respective users available in the target
platform of choice. To enable this, information is translated into a common,
neutral exchange format. The format of choice is NGSI-LD, which is being
standardized by the ETSI Industry Specification Group on Context Information
Management (ETSI ISG CIM). Thing Visors are the components that translate the
source information to NGSI-LD, which is then delivered to the target platform
and translated into the target format. ThingVisors can be implemented by hand,
but this requires significant human effort, especially considering the
heterogeneity of low level information produced by a multitude of sensors.
Thus, supporting the human developer and, ideally, fully automating the process
of extracting and enriching data and translating it to NGSI-LD is a crucial
step. Machine learning is a promising approach for this, but it typically
requires large amounts of hand-labelled data for training, an effort that makes
it unrealistic in many IoT scenarios. A programmatic labelling approach called
knowledge infusion that encodes expert knowledge is used for matching a schema
or ontology extracted from the data with a target schema or ontology, providing
the basis for annotating the data and facilitating the translation to NGSI-LD.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.LG"">cs.LG updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online Adversarial Attacks. (arXiv:2103.02014v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mladenovic_A/0/1/0/all/0/1">Andjela Mladenovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1">Avishek Joey Bose</a>, <a href="http://arxiv.org/find/cs/1/au:+Berard_H/0/1/0/all/0/1">Hugo Berard</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1">William L. Hamilton</a>, <a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1">Simon Lacoste-Julien</a>, <a href="http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1">Pascal Vincent</a>, <a href="http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1">Gauthier Gidel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02014">
                                    <div class="article-summary-box-inner">
                                        <span>Adversarial attacks expose important vulnerabilities of deep learning models,
yet little attention has been paid to settings where data arrives as a stream.
In this paper, we formalize the online adversarial attack problem, emphasizing
two key elements found in real-world use-cases: attackers must operate under
partial knowledge of the target model, and the decisions made by the attacker
are irrevocable since they operate on a transient data stream. We first
rigorously analyze a deterministic variant of the online threat model by
drawing parallels to the well-studied $k$-secretary problem in theoretical
computer science and propose Virtual+, a simple yet practical online algorithm.
Our main theoretical result show Virtual+ yields provably the best competitive
ratio over all single-threshold algorithms for $k&lt;5$ -- extending previous
analysis of the $k$-secretary problem. We also introduce the \textit{stochastic
$k$-secretary} -- effectively reducing online blackbox transfer attacks to a
$k$-secretary problem under noise -- and prove theoretical bounds on the
performance of \textit{any} online algorithms adapted to this setting. Finally,
we complement our theoretical results by conducting experiments on both MNIST
and CIFAR-10 with both vanilla and robust classifiers, revealing not only the
necessity of online algorithms in achieving near-optimal performance but also
the rich interplay of a given attack strategy towards online attack selection,
enabling simple strategies like FGSM to outperform classically strong whitebox
adversaries.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths. (arXiv:2101.09612v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1">Quynh Nguyen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09612">
                                    <div class="article-summary-box-inner">
                                        <span>We give a simple proof for the global convergence of gradient descent in
training deep ReLU networks with the standard square loss, and show some of its
improvements over the state-of-the-art. In particular, while prior works
require all the hidden layers to be wide with width at least $\Omega(N^8)$ ($N$
being the number of training samples), we require a single wide layer of
linear, quadratic or cubic width depending on the type of initialization.
Unlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof
need not track the evolution of the entire NTK matrix, or more generally, any
quantities related to the changes of activation patterns during training.
Instead, we only need to track the evolution of the output at the last hidden
layer, which can be done much more easily thanks to the Lipschitz property of
ReLU. Some highlights of our setting: (i) all the layers are trained with
standard gradient descent, (ii) the network has standard parameterization as
opposed to the NTK one, and (iii) the network has a single wide layer as
opposed to having all wide hidden layers as in most of NTK-related results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Accelerating Stochastic Simulation with Interactive Neural Processes. (arXiv:2106.02770v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Dongxia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chinazzi_M/0/1/0/all/0/1">Matteo Chinazzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vespignani_A/0/1/0/all/0/1">Alessandro Vespignani</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi-An Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1">Rose Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02770">
                                    <div class="article-summary-box-inner">
                                        <span>Stochastic simulations such as large-scale, spatiotemporal, age-structured
epidemic models are computationally expensive at fine-grained resolution. We
propose Interactive Neural Process (INP), an interactive framework to
continuously learn a deep learning surrogate model and accelerate simulation.
Our framework is based on the novel integration of Bayesian active learning,
stochastic simulation and deep sequence modeling. In particular, we develop a
novel spatiotemporal neural process model to mimic the underlying process
dynamics. Our model automatically infers the latent process which describes the
intrinsic uncertainty of the simulator. This also gives rise to a new
acquisition function that can quantify the uncertainty of deep learning
predictions. We design Bayesian active learning algorithms to iteratively query
the simulator, gather more data, and continuously improve the model. We perform
theoretical analysis and demonstrate that our approach reduces sample
complexity compared with random sampling in high dimension. Empirically, we
demonstrate our framework can faithfully imitate the behavior of a complex
infectious disease simulator with a small number of examples, enabling rapid
simulation and scenario exploration.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Conditional and Adversarial Euler-based Generators For Time Series. (arXiv:2102.05313v4 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Remlinger_C/0/1/0/all/0/1">Carl Remlinger</a>, <a href="http://arxiv.org/find/stat/1/au:+Mikael_J/0/1/0/all/0/1">Joseph Mikael</a>, <a href="http://arxiv.org/find/stat/1/au:+Elie_R/0/1/0/all/0/1">Romuald Elie</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05313">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce three new generative models for time series. Based on Euler
discretization and Wasserstein metrics, they are able to capture time marginal
distributions and temporal dynamics. Two of these methods rely on the
adaptation of generative adversarial networks (GANs) to time series. Both of
them outperform state-of-the-art benchmarks by capturing the underlying
temporal structure on synthetic time series. The third algorithm, called
Conditional Euler Generator (CEGEN), minimizes a dedicated distance between the
transition probability distributions over all time steps. In the context of Ito
processes, we provide theoretical guarantees that minimizing this criterion
implies accurate estimations of the drift and volatility parameters. We
demonstrate empirically that CEGEN outperforms state-of-the-art and GAN
generators on both marginal and temporal dynamics metrics. Besides, it
identifies accurate correlation structures in high dimension. When few data
points are available, we verify the effectiveness of CEGEN, when combined with
transfer learning methods on Monte Carlo simulations. Finally, we illustrate
the robustness of our method on various real-world datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Logic of Machine Learning. (arXiv:2006.09500v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sapir_M/0/1/0/all/0/1">Marina Sapir</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.09500">
                                    <div class="article-summary-box-inner">
                                        <span>I propose a new, logical, foundation for ML. ML is approached as a problem of
maximizing consistency of a hypothesis in a context of a given training set.
Nonjudgmental logic (NjL) with modalities &#x60;&#x60;It appears that&#x27;&#x27;, &#x60;&#x60;Assume that&#x27;&#x27;
is introduced to formalize and quantify the inconsistency. Many popular ML
algorithms (from hierarchical clustering to k-NN and SVM) are shown to
corroborate the conjecture. In addition, it is demonstrated that NjL allows to
formalize and solve several general learning problems which are not considered
as ML usually.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Modular Analysis of Provable Acceleration via Polyak&#x27;s Momentum: Training a Wide ReLU Network and a Deep Linear Network. (arXiv:2010.01618v6 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jun-Kun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chi-Heng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1">Jacob Abernethy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01618">
                                    <div class="article-summary-box-inner">
                                        <span>Incorporating a so-called &quot;momentum&quot; dynamic in gradient descent methods is
widely used in neural net training as it has been broadly observed that, at
least empirically, it often leads to significantly faster convergence. At the
same time, there are very few theoretical guarantees in the literature to
explain this apparent acceleration effect. Even for the classical strongly
convex quadratic problems, several existing results only show Polyak&#x27;s momentum
has an accelerated linear rate asymptotically. In this paper, we first revisit
the quadratic problems and show a non-asymptotic accelerated linear rate of
Polyak&#x27;s momentum. Then, we provably show that Polyak&#x27;s momentum achieves
acceleration for training a one-layer wide ReLU network and a deep linear
network, which are perhaps the two most popular canonical models for studying
optimization and deep learning in the literature. Prior work Du at al. 2019 and
Wu et al. 2019 showed that using vanilla gradient descent, and with an use of
over-parameterization, the error decays as $(1- \Theta(\frac{1}{ \kappa&#x27;}))^t$
after $t$ iterations, where $\kappa&#x27;$ is the condition number of a Gram Matrix.
Our result shows that with the appropriate choice of parameters Polyak&#x27;s
momentum has a rate of $(1-\Theta(\frac{1}{\sqrt{\kappa&#x27;}}))^t$. For the deep
linear network, prior work Hu et al. 2020 showed that vanilla gradient descent
has a rate of $(1-\Theta(\frac{1}{\kappa}))^t$, where $\kappa$ is the condition
number of a data matrix. Our result shows an acceleration rate $(1-
\Theta(\frac{1}{\sqrt{\kappa}}))^t$ is achievable by Polyak&#x27;s momentum. All the
results in this work are obtained from a modular analysis, which can be of
independent interest. This work establishes that momentum does indeed speed up
neural net training.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Asymmetric Heavy Tails and Implicit Bias in Gaussian Noise Injections. (arXiv:2102.07006v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Camuto_A/0/1/0/all/0/1">Alexander Camuto</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1">Xiaoyu Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhu_L/0/1/0/all/0/1">Lingjiong Zhu</a>, <a href="http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1">Chris Holmes</a>, <a href="http://arxiv.org/find/stat/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1">Mert G&#xfc;rb&#xfc;zbalaban</a>, <a href="http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1">Umut &#x15e;im&#x15f;ekli</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07006">
                                    <div class="article-summary-box-inner">
                                        <span>Gaussian noise injections (GNIs) are a family of simple and widely-used
regularisation methods for training neural networks, where one injects additive
or multiplicative Gaussian noise to the network activations at every iteration
of the optimisation algorithm, which is typically chosen as stochastic gradient
descent (SGD). In this paper we focus on the so-called &#x60;implicit effect&#x27; of
GNIs, which is the effect of the injected noise on the dynamics of SGD. We show
that this effect induces an asymmetric heavy-tailed noise on SGD gradient
updates. In order to model this modified dynamics, we first develop a
Langevin-like stochastic differential equation that is driven by a general
family of asymmetric heavy-tailed noise. Using this model we then formally
prove that GNIs induce an &#x60;implicit bias&#x27;, which varies depending on the
heaviness of the tails and the level of asymmetry. Our empirical results
confirm that different types of neural networks trained with GNIs are
well-modelled by the proposed dynamics and that the implicit effect of these
injections induces a bias that degrades the performance of networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Calibrate Before Use: Improving Few-Shot Performance of Language Models. (arXiv:2102.09690v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tony Z. Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1">Eric Wallace</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1">Shi Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1">Dan Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sameer Singh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09690">
                                    <div class="article-summary-box-inner">
                                        <span>GPT-3 can perform numerous tasks when provided a natural language prompt that
contains a few training examples. We show that this type of few-shot learning
can be unstable: the choice of prompt format, training examples, and even the
order of the training examples can cause accuracy to vary from near chance to
near state-of-the-art. We demonstrate that this instability arises from the
bias of language models towards predicting certain answers, e.g., those that
are placed near the end of the prompt or are common in the pre-training data.
To mitigate this, we first estimate the model&#x27;s bias towards each answer by
asking for its prediction when given the training prompt and a content-free
test input such as &quot;N/A&quot;. We then fit calibration parameters that cause the
prediction for this input to be uniform across answers. On a diverse set of
tasks, this contextual calibration procedure substantially improves GPT-3 and
GPT-2&#x27;s average accuracy (up to 30.0% absolute) and reduces variance across
different choices of the prompt.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">View Generalization for Single Image Textured 3D Models. (arXiv:2106.06533v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1">Anand Bhattad</a>, <a href="http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1">Aysegul Dundar</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guilin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1">Andrew Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1">Bryan Catanzaro</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06533">
                                    <div class="article-summary-box-inner">
                                        <span>Humans can easily infer the underlying 3D geometry and texture of an object
only from a single 2D image. Current computer vision methods can do this, too,
but suffer from view generalization problems - the models inferred tend to make
poor predictions of appearance in novel views. As for generalization problems
in machine learning, the difficulty is balancing single-view accuracy (cf.
training error; bias) with novel view accuracy (cf. test error; variance). We
describe a class of models whose geometric rigidity is easily controlled to
manage this tradeoff. We describe a cycle consistency loss that improves view
generalization (roughly, a model from a generated view should predict the
original view well). View generalization of textures requires that models share
texture information, so a car seen from the back still has headlights because
other cars have headlights. We describe a cycle consistency loss that
encourages model textures to be aligned, so as to encourage sharing. We compare
our method against the state-of-the-art method and show both qualitative and
quantitative improvements.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improved Contrastive Divergence Training of Energy Based Models. (arXiv:2012.01316v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yilun Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua Tenenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1">Igor Mordatch</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01316">
                                    <div class="article-summary-box-inner">
                                        <span>Contrastive divergence is a popular method of training energy-based models,
but is known to have difficulties with training stability. We propose an
adaptation to improve contrastive divergence training by scrutinizing a
gradient term that is difficult to calculate and is often left out for
convenience. We show that this gradient term is numerically significant and in
practice is important to avoid training instabilities, while being tractable to
estimate. We further highlight how data augmentation and multi-scale processing
can be used to improve model robustness and generation quality. Finally, we
empirically evaluate stability of model architectures and show improved
performance on a host of benchmarks and use cases,such as image generation, OOD
detection, and compositional generation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Probabilistic bounds on neuron death in deep rectifier networks. (arXiv:2007.06192v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rister_B/0/1/0/all/0/1">Blaine Rister</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1">Daniel L. Rubin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.06192">
                                    <div class="article-summary-box-inner">
                                        <span>Neuron death is a complex phenomenon with implications for model
trainability: the deeper the network, the lower the probability of finding a
valid initialization. In this work, we derive both upper and lower bounds on
the probability that a ReLU network is initialized to a trainable point, as a
function of model hyperparameters. We show that it is possible to increase the
depth of a network indefinitely, so long as the width increases as well.
Furthermore, our bounds are asymptotically tight under reasonable assumptions:
first, the upper bound coincides with the true probability for a single-layer
network with the largest possible input set. Second, the true probability
converges to our lower bound as the input set shrinks to a single point, or as
the network complexity grows under an assumption about the output variance. We
confirm these results by numerical simulation, showing rapid convergence to the
lower bound with increasing network depth. Then, motivated by the theory, we
propose a practical sign flipping scheme which guarantees that the ratio of
living data points in a $k$-layer network is at least $2^{-k}$. Finally, we
show how these issues are mitigated by network design features currently seen
in practice, such as batch normalization, residual connections, dense networks
and skip connections. This suggests that neuron death may provide insight into
the efficacy of various model architectures.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Quantile Bandits for Best Arms Identification. (arXiv:2010.11568v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mengyan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_C/0/1/0/all/0/1">Cheng Soon Ong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11568">
                                    <div class="article-summary-box-inner">
                                        <span>We consider a variant of the best arm identification task in stochastic
multi-armed bandits. Motivated by risk-averse decision-making problems, our
goal is to identify a set of $m$ arms with the highest $\tau$-quantile values
within a fixed budget. We prove asymmetric two-sided concentration inequalities
for order statistics and quantiles of random variables that have non-decreasing
hazard rate, which may be of independent interest. With these inequalities, we
analyse a quantile version of Successive Accepts and Rejects (Q-SAR). We derive
an upper bound for the probability of arm misidentification, the first
justification of a quantile based algorithm for fixed budget multiple best arms
identification. We show illustrative experiments for best arm identification.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Best Arm Identification in Graphical Bilinear Bandits. (arXiv:2012.07641v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rizk_G/0/1/0/all/0/1">Geovani Rizk</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1">Albert Thomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Colin_I/0/1/0/all/0/1">Igor Colin</a>, <a href="http://arxiv.org/find/cs/1/au:+Laraki_R/0/1/0/all/0/1">Rida Laraki</a>, <a href="http://arxiv.org/find/cs/1/au:+Chevaleyre_Y/0/1/0/all/0/1">Yann Chevaleyre</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07641">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a new graphical bilinear bandit problem where a learner (or a
\emph{central entity}) allocates arms to the nodes of a graph and observes for
each edge a noisy bilinear reward representing the interaction between the two
end nodes. We study the best arm identification problem in which the learner
wants to find the graph allocation maximizing the sum of the bilinear rewards.
By efficiently exploiting the geometry of this bandit problem, we propose a
\emph{decentralized} allocation strategy based on random sampling with
theoretical guarantees. In particular, we characterize the influence of the
graph structure (e.g. star, complete or circle) on the convergence rate and
propose empirical experiments that confirm this dependency.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Continuous-Time Model-Based Reinforcement Learning. (arXiv:2102.04764v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yildiz_C/0/1/0/all/0/1">&#xc7;a&#x11f;atay Y&#x131;ld&#x131;z</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1">Markus Heinonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lahdesmaki_H/0/1/0/all/0/1">Harri L&#xe4;hdesm&#xe4;ki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04764">
                                    <div class="article-summary-box-inner">
                                        <span>Model-based reinforcement learning (MBRL) approaches rely on discrete-time
state transition models whereas physical systems and the vast majority of
control tasks operate in continuous-time. To avoid time-discretization
approximation of the underlying process, we propose a continuous-time MBRL
framework based on a novel actor-critic method. Our approach also infers the
unknown state evolution differentials with Bayesian neural ordinary
differential equations (ODE) to account for epistemic uncertainty. We implement
and test our method on a new ODE-RL suite that explicitly solves
continuous-time control systems. Our experiments illustrate that the model is
robust against irregular and noisy data, is sample-efficient, and can solve
control problems which pose challenges to discrete-time MBRL methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Distributed Learning and its Application for Time-Series Prediction. (arXiv:2106.03211v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1">Nhuong V. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Legitime_S/0/1/0/all/0/1">Sybille Legitime</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03211">
                                    <div class="article-summary-box-inner">
                                        <span>Extreme events are occurrences whose magnitude and potential cause extensive
damage on people, infrastructure, and the environment. Motivated by the extreme
nature of the current global health landscape, which is plagued by the
coronavirus pandemic, we seek to better understand and model extreme events.
Modeling extreme events is common in practice and plays an important role in
time-series prediction applications. Our goal is to (i) compare and investigate
the effect of some common extreme events modeling methods to explore which
method can be practical in reality and (ii) accelerate the deep learning
training process, which commonly uses deep recurrent neural network (RNN), by
implementing the asynchronous local Stochastic Gradient Descent (SGD) framework
among multiple compute nodes. In order to verify our distributed extreme events
modeling, we evaluate our proposed framework on a stock data set S\&amp;P500, with
a standard recurrent neural network. Our intuition is to explore the (best)
extreme events modeling method which could work well under the distributed deep
learning setting. Moreover, by using asynchronous distributed learning, we aim
to significantly reduce the communication cost among the compute nodes and
central server, which is the main bottleneck of almost all distributed learning
frameworks.

We implement our proposed work and evaluate its performance on representative
data sets, such as S&amp;P500 stock in $5$-year period. The experimental results
validate the correctness of the design principle and show a significant
training duration reduction upto $8$x, compared to the baseline single compute
node. Our results also show that our proposed work can achieve the same level
of test accuracy, compared to the baseline setting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Analyzing the Travel and Charging Behavior of Electric Vehicles -- A Data-driven Approach. (arXiv:2106.06475v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Baghali_S/0/1/0/all/0/1">Sina Baghali</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1">Samiul Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhaomiao Guo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06475">
                                    <div class="article-summary-box-inner">
                                        <span>The increasing market penetration of electric vehicles (EVs) may pose
significant electricity demand on power systems. This electricity demand is
affected by the inherent uncertainties of EVs&#x27; travel behavior that makes
forecasting the daily charging demand (CD) very challenging. In this project,
we use the National House Hold Survey (NHTS) data to form sequences of trips,
and develop machine learning models to predict the parameters of the next trip
of the drivers, including trip start time, end time, and distance. These
parameters are later used to model the temporal charging behavior of EVs. The
simulation results show that the proposed modeling can effectively estimate the
daily CD pattern based on travel behavior of EVs, and simple machine learning
techniques can forecast the travel parameters with acceptable accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Feature Selection Tutorial with Python Examples. (arXiv:2106.06437v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cunningham_P/0/1/0/all/0/1">Padraig Cunningham</a>, <a href="http://arxiv.org/find/cs/1/au:+Kathirgamanathan_B/0/1/0/all/0/1">Bahavathy Kathirgamanathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Delany_S/0/1/0/all/0/1">Sarah Jane Delany</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06437">
                                    <div class="article-summary-box-inner">
                                        <span>In Machine Learning, feature selection entails selecting a subset of the
available features in a dataset to use for model development. There are many
motivations for feature selection, it may result in better models, it may
provide insight into the data and it may deliver economies in data gathering or
data processing. For these reasons feature selection has received a lot of
attention in data analytics research. In this paper we provide an overview of
the main methods and present practical examples with Python implementations.
While the main focus is on supervised feature selection techniques, we also
cover some feature transformation methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Making EfficientNet More Efficient: Exploring Batch-Independent Normalization, Group Convolutions and Reduced Resolution Training. (arXiv:2106.03640v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1">Dominic Masters</a>, <a href="http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1">Antoine Labatie</a>, <a href="http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1">Zach Eaton-Rosen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1">Carlo Luschi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03640">
                                    <div class="article-summary-box-inner">
                                        <span>Much recent research has been dedicated to improving the efficiency of
training and inference for image classification. This effort has commonly
focused on explicitly improving theoretical efficiency, often measured as
ImageNet validation accuracy per FLOP. These theoretical savings have, however,
proven challenging to achieve in practice, particularly on high-performance
training accelerators.

In this work, we focus on improving the practical efficiency of the
state-of-the-art EfficientNet models on a new class of accelerator, the
Graphcore IPU. We do this by extending this family of models in the following
ways: (i) generalising depthwise convolutions to group convolutions; (ii)
adding proxy-normalized activations to match batch normalization performance
with batch-independent statistics; (iii) reducing compute by lowering the
training resolution and inexpensively fine-tuning at higher resolution. We find
that these three methods improve the practical efficiency for both training and
inference. Our code will be made available online.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Noise and Fluctuation of Finite Learning Rate Stochastic Gradient Descent. (arXiv:2012.03636v4 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Liu_K/0/1/0/all/0/1">Kangqiao Liu</a>, <a href="http://arxiv.org/find/stat/1/au:+Ziyin_L/0/1/0/all/0/1">Liu Ziyin</a>, <a href="http://arxiv.org/find/stat/1/au:+Ueda_M/0/1/0/all/0/1">Masahito Ueda</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03636">
                                    <div class="article-summary-box-inner">
                                        <span>In the vanishing learning rate regime, stochastic gradient descent (SGD) is
now relatively well understood. In this work, we propose to study the basic
properties of SGD and its variants in the non-vanishing learning rate regime.
The focus is on deriving exactly solvable results and discussing their
implications. The main contributions of this work are to derive the stationary
distribution for discrete-time SGD in a quadratic loss function with and
without momentum; in particular, one implication of our result is that the
fluctuation caused by discrete-time dynamics takes a distorted shape and is
dramatically larger than a continuous-time theory could predict. Examples of
applications of the proposed theory considered in this work include the
approximation error of variants of SGD, the effect of minibatch noise, the
optimal Bayesian inference, the escape rate from a sharp minimum, and the
stationary covariance of a few second-order methods including damped Newton&#x27;s
method, natural gradient descent, and Adam.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Machine Collaboration. (arXiv:2105.02569v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Liu_Q/0/1/0/all/0/1">Qingfeng Liu</a>, <a href="http://arxiv.org/find/stat/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02569">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a new ensemble framework for supervised learning, called machine
collaboration (MaC), using a collection of base machines for prediction tasks.
Unlike bagging/stacking (a parallel &amp; independent framework) and boosting (a
sequential &amp; top-down framework), MaC is a type of circular &amp; interactive
learning framework. The circular &amp; interactive feature helps the base machines
to transfer information circularly and update their structures and parameters
accordingly. The theoretical result on the risk bound of the estimator from MaC
reveals that the circular &amp; interactive feature can help MaC reduce risk via a
parsimonious ensemble. We conduct extensive experiments on MaC using both
simulated data and 119 benchmark real datasets. The results demonstrate that in
most cases, MaC performs significantly better than several other
state-of-the-art methods, including classification and regression trees, neural
networks, stacking, and boosting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Variational Autoencoders: A Harmonic Perspective. (arXiv:2105.14866v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Camuto_A/0/1/0/all/0/1">Alexander Camuto</a>, <a href="http://arxiv.org/find/stat/1/au:+Willetts_M/0/1/0/all/0/1">Matthew Willetts</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14866">
                                    <div class="article-summary-box-inner">
                                        <span>In this work we study Variational Autoencoders (VAEs) from the perspective of
harmonic analysis. By viewing a VAE&#x27;s latent space as a Gaussian Space, a
variety of measure space, we derive a series of results that show that the
encoder variance of a VAE controls the frequency content of the functions
parameterised by the VAE encoder and decoder neural networks. In particular we
demonstrate that larger encoder variances reduce the high frequency content of
these functions. Our analysis allows us to show that increasing this variance
effectively induces a soft Lipschitz constraint on the decoder network of a
VAE, which is a core contributor to the adversarial robustness of VAEs. We
further demonstrate that adding Gaussian noise to the input of a VAE allows us
to more finely control the frequency content and the Lipschitz constant of the
VAE encoder networks. To support our theoretical analysis we run experiments
with VAEs with small fully-connected neural networks and with larger
convolutional networks, demonstrating empirically that our theory holds for a
variety of neural network architectures.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions. (arXiv:2104.02710v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jennifer J. Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Karigo_T/0/1/0/all/0/1">Tomomi Karigo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1">Dipam Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1">Sharada P. Mohanty</a>, <a href="http://arxiv.org/find/cs/1/au:+Wild_B/0/1/0/all/0/1">Benjamin Wild</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Quan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1">David J. Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1">Pietro Perona</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yisong Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1">Ann Kennedy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02710">
                                    <div class="article-summary-box-inner">
                                        <span>Multi-agent behavior modeling aims to understand the interactions that occur
between agents. We present a multi-agent dataset from behavioral neuroscience,
the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists
of trajectory data of social interactions, recorded from videos of freely
behaving mice in a standard resident-intruder assay. To help accelerate
behavioral studies, the CalMS21 dataset provides benchmarks to evaluate the
performance of automated behavior classification methods in three settings: (1)
for training on large behavioral datasets all annotated by a single annotator,
(2) for style transfer to learn inter-annotator differences in behavior
definitions, and (3) for learning of new behaviors of interest given limited
training data. The dataset consists of 6 million frames of unlabeled tracked
poses of interacting mice, as well as over 1 million frames with tracked poses
and corresponding frame-level behavior annotations. The challenge of our
dataset is to be able to classify behaviors accurately using both labeled and
unlabeled tracking data, as well as being able to generalize to new settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An Integer Linear Programming Framework for Mining Constraints from Data. (arXiv:2006.10836v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1">Tao Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10836">
                                    <div class="article-summary-box-inner">
                                        <span>Structured output prediction problems (e.g., sequential tagging, hierarchical
multi-class classification) often involve constraints over the output label
space. These constraints interact with the learned models to filter infeasible
solutions and facilitate in building an accountable system. However, although
constraints are useful, they are often based on hand-crafted rules. This raises
a question -- \emph{can we mine constraints and rules from data based on a
learning algorithm?}

In this paper, we present a general framework for mining constraints from
data. In particular, we consider the inference in structured output prediction
as an integer linear programming (ILP) problem. Then, given the coefficients of
the objective function and the corresponding solution, we mine the underlying
constraints by estimating the outer and inner polytopes of the feasible set. We
verify the proposed constraint mining algorithm in various synthetic and
real-world applications and demonstrate that the proposed approach successfully
identifies the feasible set at scale.

In particular, we show that our approach can learn to solve 9x9 Sudoku
puzzles and minimal spanning tree problems from examples without providing the
underlying rules. Our algorithm can also integrate with a neural network model
to learn the hierarchical label structure of a multi-label classification task.
Besides, we provide a theoretical analysis about the tightness of the polytopes
and the reliability of the mined constraints.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks. (arXiv:2006.07869v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Papoudakis_G/0/1/0/all/0/1">Georgios Papoudakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1">Filippos Christianos</a>, <a href="http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1">Lukas Sch&#xe4;fer</a>, <a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1">Stefano V. Albrecht</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.07869">
                                    <div class="article-summary-box-inner">
                                        <span>Multi-agent deep reinforcement learning (MARL) suffers from a lack of
commonly-used evaluation tasks and criteria, making comparisons between
approaches difficult. In this work, we consistently evaluate and compare three
different classes of MARL algorithms (independent learning, centralised
multi-agent policy gradient, value decomposition) in a diverse range of
cooperative multi-agent learning tasks. Our experiments serve as a reference
for the expected performance of algorithms across different learning tasks, and
we provide insights regarding the effectiveness of different learning
approaches. We open-source EPyMARL, which extends the PyMARL
codebase~\citep{samvelyan19smac} to include additional algorithms and allow for
flexible configuration of algorithm implementation details such as parameter
sharing. Finally, we open-source two environments for multi-agent research
which focus on coordination under sparse rewards.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hierarchical Reinforcement Learning for Air-to-Air Combat. (arXiv:2105.00990v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pope_A/0/1/0/all/0/1">Adrian P. Pope</a>, <a href="http://arxiv.org/find/cs/1/au:+Ide_J/0/1/0/all/0/1">Jaime S. Ide</a>, <a href="http://arxiv.org/find/cs/1/au:+Micovic_D/0/1/0/all/0/1">Daria Micovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Diaz_H/0/1/0/all/0/1">Henry Diaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenbluth_D/0/1/0/all/0/1">David Rosenbluth</a>, <a href="http://arxiv.org/find/cs/1/au:+Ritholtz_L/0/1/0/all/0/1">Lee Ritholtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Twedt_J/0/1/0/all/0/1">Jason C. Twedt</a>, <a href="http://arxiv.org/find/cs/1/au:+Walker_T/0/1/0/all/0/1">Thayne T. Walker</a>, <a href="http://arxiv.org/find/cs/1/au:+Alcedo_K/0/1/0/all/0/1">Kevin Alcedo</a>, <a href="http://arxiv.org/find/cs/1/au:+Javorsek_D/0/1/0/all/0/1">Daniel Javorsek</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00990">
                                    <div class="article-summary-box-inner">
                                        <span>Artificial Intelligence (AI) is becoming a critical component in the defense
industry, as recently demonstrated by DARPA&#x60;s AlphaDogfight Trials (ADT). ADT
sought to vet the feasibility of AI algorithms capable of piloting an F-16 in
simulated air-to-air combat. As a participant in ADT, Lockheed Martin&#x60;s (LM)
approach combines a hierarchical architecture with maximum-entropy
reinforcement learning (RL), integrates expert knowledge through reward
shaping, and supports modularity of policies. This approach achieved a $2^{nd}$
place finish in the final ADT event (among eight total competitors) and
defeated a graduate of the US Air Force&#x27;s (USAF) F-16 Weapons Instructor Course
in match play.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills. (arXiv:2104.07749v3 [cs.RO] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chebotar_Y/0/1/0/all/0/1">Yevgen Chebotar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1">Karol Hausman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1">Ted Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalashnikov_D/0/1/0/all/0/1">Dmitry Kalashnikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Varley_J/0/1/0/all/0/1">Jake Varley</a>, <a href="http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1">Alex Irpan</a>, <a href="http://arxiv.org/find/cs/1/au:+Eysenbach_B/0/1/0/all/0/1">Benjamin Eysenbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1">Ryan Julian</a>, <a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1">Chelsea Finn</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07749">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of learning useful robotic skills from previously
collected offline data without access to manually specified rewards or
additional online exploration, a setting that is becoming increasingly
important for scaling robot learning by reusing past robotic data. In
particular, we propose the objective of learning a functional understanding of
the environment by learning to reach any goal state in a given dataset. We
employ goal-conditioned Q-learning with hindsight relabeling and develop
several techniques that enable training in a particularly challenging offline
setting. We find that our method can operate on high-dimensional camera images
and learn a variety of skills on real robots that generalize to previously
unseen scenes and objects. We also show that our method can learn to reach
long-horizon goals across multiple episodes through goal chaining, and learn
rich representations that can help with downstream tasks through pre-training
or auxiliary objectives. The videos of our experiments can be found at
https://actionable-models.github.io</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs. (arXiv:2106.06150v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jialin Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1">Da Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lin F. Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1">Geroge Karypis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06150">
                                    <div class="article-summary-box-inner">
                                        <span>Graph neural networks (GNNs) are powerful tools for learning from graph data
and are widely used in various applications such as social network
recommendation, fraud detection, and graph search. The graphs in these
applications are typically large, usually containing hundreds of millions of
nodes. Training GNN models on such large graphs efficiently remains a big
challenge. Despite a number of sampling-based methods have been proposed to
enable mini-batch training on large graphs, these methods have not been proved
to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU
training. The state-of-the-art sampling-based methods are usually not optimized
for these real-world hardware setups, in which data movement between CPUs and
GPUs is a bottleneck. To address this issue, we propose Global Neighborhood
Sampling that aims at training GNNs on giant graphs specifically for
mixed-CPU-GPU training. The algorithm samples a global cache of nodes
periodically for all mini-batches and stores them in GPUs. This global cache
allows in-GPU importance sampling of mini-batches, which drastically reduces
the number of nodes in a mini-batch, especially in the input layer, to reduce
data copy between CPU and GPU and mini-batch computation without compromising
the training convergence rate or model accuracy. We provide a highly efficient
implementation of this method and show that our implementation outperforms an
efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant
graphs. It outperforms an efficient implementation of LADIES with small layers
by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also
theoretically analyze the proposed algorithm and show that with cached node
data of a proper size, it enjoys a comparable convergence rate as the
underlying node-wise sampling method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Weakly Supervised Recovery of Semantic Attributes. (arXiv:2103.11888v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1">Ameen Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Galanti_T/0/1/0/all/0/1">Tomer Galanti</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheltonozhskiy_E/0/1/0/all/0/1">Evgeniy Zheltonozhskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Baskin_C/0/1/0/all/0/1">Chaim Baskin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1">Lior Wolf</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11888">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of the extraction of semantic attributes, supervised
only with classification labels. For example, when learning to classify images
of birds into species, we would like to observe the emergence of features that
zoologists use to classify birds. To tackle this problem, we propose training a
neural network with discrete features in the last layer, which is followed by
two heads: a multi-layered perceptron (MLP) and a decision tree. Since decision
trees utilize simple binary decision stumps we expect those discrete features
to obtain semantic meaning. We present a theoretical analysis as well as a
practical method for learning in the intersection of two hypothesis classes.
Our results on multiple benchmarks show an improved ability to extract a set of
features that are highly correlated with the set of unseen attributes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improved, Deterministic Smoothing for L_1 Certified Robustness. (arXiv:2103.10834v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Levine_A/0/1/0/all/0/1">Alexander Levine</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10834">
                                    <div class="article-summary-box-inner">
                                        <span>Randomized smoothing is a general technique for computing sample-dependent
robustness guarantees against adversarial attacks for deep classifiers. Prior
works on randomized smoothing against L_1 adversarial attacks use additive
smoothing noise and provide probabilistic robustness guarantees. In this work,
we propose a non-additive and deterministic smoothing method, Deterministic
Smoothing with Splitting Noise (DSSN). To develop DSSN, we first develop SSN, a
randomized method which involves generating each noisy smoothing sample by
first randomly splitting the input space and then returning a representation of
the center of the subdivision occupied by the input sample. In contrast to
uniform additive smoothing, the SSN certification does not require the random
noise components used to be independent. Thus, smoothing can be done
effectively in just one dimension and can therefore be efficiently derandomized
for quantized data (e.g., images). To the best of our knowledge, this is the
first work to provide deterministic &quot;randomized smoothing&quot; for a norm-based
adversarial threat model while allowing for an arbitrary classifier (i.e., a
deep model) to be used as a base classifier and without requiring an
exponential number of smoothing samples. On CIFAR-10 and ImageNet datasets, we
provide substantially larger L_1 robustness certificates compared to prior
works, establishing a new state-of-the-art. The determinism of our method also
leads to significantly faster certificate computation. Code is available at:
https://github.com/alevine0/smoothingSplittingNoise</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Active Learning of Continuous-time Bayesian Networks through Interventions. (arXiv:2105.14742v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Linzner_D/0/1/0/all/0/1">Dominik Linzner</a>, <a href="http://arxiv.org/find/stat/1/au:+Koeppl_H/0/1/0/all/0/1">Heinz Koeppl</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14742">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of learning structures and parameters of
Continuous-time Bayesian Networks (CTBNs) from time-course data under minimal
experimental resources. In practice, the cost of generating experimental data
poses a bottleneck, especially in the natural and social sciences. A popular
approach to overcome this is Bayesian optimal experimental design (BOED).
However, BOED becomes infeasible in high-dimensional settings, as it involves
integration over all possible experimental outcomes. We propose a novel
criterion for experimental design based on a variational approximation of the
expected information gain. We show that for CTBNs, a semi-analytical expression
for this criterion can be calculated for structure and parameter learning. By
doing so, we can replace sampling over experimental outcomes by solving the
CTBNs master-equation, for which scalable approximations exist. This alleviates
the computational burden of sampling possible experimental outcomes in
high-dimensions. We employ this framework in order to recommend interventional
sequences. In this context, we extend the CTBN model to conditional CTBNs in
order to incorporate interventions. We demonstrate the performance of our
criterion on synthetic and real-world data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses. (arXiv:2106.06519v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ganesan_K/0/1/0/all/0/1">Karthik Ganesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bamdev_P/0/1/0/all/0/1">Pakhi Bamdev</a>, <a href="http://arxiv.org/find/cs/1/au:+B_J/0/1/0/all/0/1">Jaivarsan B</a>, <a href="http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1">Amresh Venugopal</a>, <a href="http://arxiv.org/find/cs/1/au:+Tushar_A/0/1/0/all/0/1">Abhinav Tushar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06519">
                                    <div class="article-summary-box-inner">
                                        <span>Spoken Language Understanding (SLU) systems parse speech into semantic
structures like dialog acts and slots. This involves the use of an Automatic
Speech Recognizer (ASR) to transcribe speech into multiple text alternatives
(hypotheses). Transcription errors, common in ASRs, impact downstream SLU
performance negatively. Approaches to mitigate such errors involve using richer
information from the ASR, either in form of N-best hypotheses or word-lattices.
We hypothesize that transformer models learn better with a simpler utterance
representation using the concatenation of the N-best ASR alternatives, where
each alternative is separated by a special delimiter [SEP]. In our work, we
test our hypothesis by using concatenated N-best ASR alternatives as the input
to transformer encoder models, namely BERT and XLM-RoBERTa, and achieve
performance equivalent to the prior state-of-the-art model on DSTC2 dataset. We
also show that our approach significantly outperforms the prior
state-of-the-art when subjected to the low data regime. Additionally, this
methodology is accessible to users of third-party ASR APIs which do not provide
word-lattice information.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online Continual Adaptation with Active Self-Training. (arXiv:2106.06526v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shiji Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Han Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanghang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lianzhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1">Heng Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenwu Zhu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06526">
                                    <div class="article-summary-box-inner">
                                        <span>Models trained with offline data often suffer from continual distribution
shifts and expensive labeling in changing environments. This calls for a new
online learning paradigm where the learner can continually adapt to changing
environments with limited labels. In this paper, we propose a new online
setting -- Online Active Continual Adaptation, where the learner aims to
continually adapt to changing distributions using both unlabeled samples and
active queries of limited labels. To this end, we propose Online Self-Adaptive
Mirror Descent (OSAMD), which adopts an online teacher-student structure to
enable online self-training from unlabeled data, and a margin-based criterion
that decides whether to query the labels to track changing distributions.
Theoretically, we show that, in the separable case, OSAMD has an $O({T}^{1/2})$
dynamic regret bound under mild assumptions, which is even tighter than the
lower bound $\Omega(T^{2/3})$ of traditional online learning with full labels.
In the general case, we show a regret bound of $O({\alpha^*}^{1/3} {T}^{2/3} +
\alpha^* T)$, where $\alpha^*$ denotes the separability of domains and is
usually small. Our theoretical results show that OSAMD can fast adapt to
changing environments with active queries. Empirically, we demonstrate that
OSAMD achieves favorable regrets under changing environments with limited
labels on both simulated and real-world data, which corroborates our
theoretical findings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unsupervised Knowledge Graph Alignment by Probabilistic Reasoning and Semantic Embedding. (arXiv:2105.05596v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1">Zhiyuan Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaoyan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1">Yuejia Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yefeng Zheng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05596">
                                    <div class="article-summary-box-inner">
                                        <span>Knowledge Graph (KG) alignment is to discover the mappings (i.e., equivalent
entities, relations, and others) between two KGs. The existing methods can be
divided into the embedding-based models, and the conventional reasoning and
lexical matching based systems. The former compute the similarity of entities
via their cross-KG embeddings, but they usually rely on an ideal supervised
learning setting for good performance and lack appropriate reasoning to avoid
logically wrong mappings; while the latter address the reasoning issue but are
poor at utilizing the KG graph structures and the entity contexts. In this
study, we aim at combining the above two solutions and thus propose an
iterative framework named PRASE which is based on probabilistic reasoning and
semantic embedding. It learns the KG embeddings via entity mappings from a
probabilistic reasoning system named PARIS, and feeds the resultant entity
mappings and embeddings back into PARIS for augmentation. The PRASE framework
is compatible with different embedding-based models, and our experiments on
multiple datasets have demonstrated its state-of-the-art performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-Trained One-class Classification for Unsupervised Anomaly Detection. (arXiv:2106.06115v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jinsung Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kihyuk Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chun-Liang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1">Sercan O. Arik</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Chen-Yu Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1">Tomas Pfister</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06115">
                                    <div class="article-summary-box-inner">
                                        <span>Anomaly detection (AD), separating anomalies from normal data, has various
applications across domains, from manufacturing to healthcare. While most
previous works have shown to be effective for cases with fully or partially
labeled data, they are less practical for AD applications due to tedious data
labeling processes. In this work, we focus on unsupervised AD problems whose
entire training data are unlabeled and may contain both normal and anomalous
samples. To tackle this problem, we build a robust one-class classification
framework via data refinement. To refine the data accurately, we propose an
ensemble of one-class classifiers, each of which is trained on a disjoint
subset of training data. Moreover, we propose a self-training of deep
representation one-class classifiers (STOC) that iteratively refines the data
and deep representations. In experiments, we show the efficacy of our method
for unsupervised anomaly detection on benchmarks from image and tabular data
domains. For example, with a 10% anomaly ratio on CIFAR-10 data, the proposed
method outperforms state-of-the-art one-class classification method by 6.3 AUC
and 12.5 average precision.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Inter-domain Multi-relational Link Prediction. (arXiv:2106.06171v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Phuc_L/0/1/0/all/0/1">Luu Huu Phuc</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1">Koh Takeuchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Okajima_S/0/1/0/all/0/1">Seiji Okajima</a>, <a href="http://arxiv.org/find/cs/1/au:+Tolmachev_A/0/1/0/all/0/1">Arseny Tolmachev</a>, <a href="http://arxiv.org/find/cs/1/au:+Takebayashi_T/0/1/0/all/0/1">Tomoyoshi Takebayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Maruhashi_K/0/1/0/all/0/1">Koji Maruhashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1">Hisashi Kashima</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06171">
                                    <div class="article-summary-box-inner">
                                        <span>Multi-relational graph is a ubiquitous and important data structure, allowing
flexible representation of multiple types of interactions and relations between
entities. Similar to other graph-structured data, link prediction is one of the
most important tasks on multi-relational graphs and is often used for knowledge
completion. When related graphs coexist, it is of great benefit to build a
larger graph via integrating the smaller ones. The integration requires
predicting hidden relational connections between entities belonged to different
graphs (inter-domain link prediction). However, this poses a real challenge to
existing methods that are exclusively designed for link prediction between
entities of the same graph only (intra-domain link prediction). In this study,
we propose a new approach to tackle the inter-domain link prediction problem by
softly aligning the entity distributions between different domains with optimal
transport and maximum mean discrepancy regularizers. Experiments on real-world
datasets show that optimal transport regularizer is beneficial and considerably
improves the performance of baseline methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Model-Free Learning for Two-Player Zero-Sum Partially Observable Markov Games with Perfect Recall. (arXiv:2106.06279v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Kozuno_T/0/1/0/all/0/1">Tadashi Kozuno</a>, <a href="http://arxiv.org/find/stat/1/au:+Menard_P/0/1/0/all/0/1">Pierre M&#xe9;nard</a>, <a href="http://arxiv.org/find/stat/1/au:+Munos_R/0/1/0/all/0/1">R&#xe9;mi Munos</a>, <a href="http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1">Michal Valko</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06279">
                                    <div class="article-summary-box-inner">
                                        <span>We study the problem of learning a Nash equilibrium (NE) in an imperfect
information game (IIG) through self-play. Precisely, we focus on two-player,
zero-sum, episodic, tabular IIG under the perfect-recall assumption where the
only feedback is realizations of the game (bandit feedback). In particular, the
dynamic of the IIG is not known -- we can only access it by sampling or
interacting with a game simulator. For this learning setting, we provide the
Implicit Exploration Online Mirror Descent (IXOMD) algorithm. It is a
model-free algorithm with a high-probability bound on the convergence rate to
the NE of order $1/\sqrt{T}$ where $T$ is the number of played games. Moreover,
IXOMD is computationally efficient as it needs to perform the updates only
along the sampled trajectory.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust Learning via Persistency of Excitation. (arXiv:2106.02078v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1">Kaustubh Sridhar</a>, <a href="http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1">Oleg Sokolsky</a>, <a href="http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1">Insup Lee</a>, <a href="http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1">James Weimer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02078">
                                    <div class="article-summary-box-inner">
                                        <span>Improving adversarial robustness of neural networks remains a major
challenge. Fundamentally, training a network is a parameter estimation problem.
In adaptive control theory, maintaining persistency of excitation (PoE) is
integral to ensuring convergence of parameter estimates in dynamical systems to
their robust optima. In this work, we show that network training using gradient
descent is equivalent to a dynamical system parameter estimation problem.
Leveraging this relationship, we prove a sufficient condition for PoE of
gradient descent is achieved when the learning rate is less than the inverse of
the Lipschitz constant of the gradient of loss function. We provide an
efficient technique for estimating the corresponding Lipschitz constant using
extreme value theory and demonstrate that by only scaling the learning rate
schedule we can increase adversarial accuracy by up to 15% points on benchmark
datasets. Our approach also universally increases the adversarial accuracy by
0.1% to 0.3% points in various state-of-the-art adversarially trained models on
the AutoAttack benchmark, where every small margin of improvement is
significant.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Asymptotics of representation learning in finite Bayesian neural networks. (arXiv:2106.00651v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zavatone_Veth_J/0/1/0/all/0/1">Jacob A. Zavatone-Veth</a>, <a href="http://arxiv.org/find/cs/1/au:+Canatar_A/0/1/0/all/0/1">Abdulkadir Canatar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pehlevan_C/0/1/0/all/0/1">Cengiz Pehlevan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00651">
                                    <div class="article-summary-box-inner">
                                        <span>Recent works have suggested that finite Bayesian neural networks may
outperform their infinite cousins because finite networks can flexibly adapt
their internal representations. However, our theoretical understanding of how
the learned hidden layer representations of finite networks differ from the
fixed representations of infinite networks remains incomplete. Perturbative
finite-width corrections to the network prior and posterior have been studied,
but the asymptotics of learned features have not been fully characterized.
Here, we argue that the leading finite-width corrections to the average feature
kernels for any Bayesian network with linear readout and quadratic cost have a
largely universal form. We illustrate this explicitly for two classes of fully
connected networks: deep linear networks and networks with a single nonlinear
hidden layer. Our results begin to elucidate which features of data wide
Bayesian neural networks learn to represent.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Price graphs: Utilizing the structural information of financial time series for stock prediction. (arXiv:2106.02522v2 [q-fin.ST] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/q-fin/1/au:+Wu_J/0/1/0/all/0/1">Junran Wu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Xu_K/0/1/0/all/0/1">Ke Xu</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Chen_X/0/1/0/all/0/1">Xueyuan Chen</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Li_S/0/1/0/all/0/1">Shangzhe Li</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhao_J/0/1/0/all/0/1">Jichang Zhao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02522">
                                    <div class="article-summary-box-inner">
                                        <span>Stock prediction, with the purpose of forecasting the future price trends of
stocks, is crucial for maximizing profits from stock investments. While great
research efforts have been devoted to exploiting deep neural networks for
improved stock prediction, two major issues still exist in recent studies.
First, the capture of long-range dependencies in time series is not
sufficiently addressed. Second, the chaotic property of financial time series
fundamentally lowers prediction performance. In this study, we propose a novel
framework to address both issues regarding stock prediction. Specifically, in
terms of transforming time series into complex networks, we convert market
price series into graphs. Then, structural information, referring to
associations among temporal points and the node weights, is extracted from the
mapped graphs to resolve the problems regarding long-range dependencies and the
chaotic property. We take graph embeddings to represent the associations among
temporal points as the prediction model inputs. Node weights are used as a
priori knowledge to enhance the learning of temporal attention. The
effectiveness of our proposed framework is validated using real-world stock
data, and our approach obtains the best performance among several
state-of-the-art benchmarks. Moreover, in the conducted trading simulations,
our framework further obtains the highest cumulative profits. Our results
supplement the existing applications of complex network methods in the
financial realm and provide insightful implications for investment applications
regarding decision support in financial markets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction. (arXiv:2105.10446v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1">Kwan Ho Ryan Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yaodong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1">Chong You</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1">Haozhi Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1">John Wright</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yi Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10446">
                                    <div class="article-summary-box-inner">
                                        <span>This work attempts to provide a plausible theoretical framework that aims to
interpret modern deep (convolutional) networks from the principles of data
compression and discriminative representation. We argue that for
high-dimensional multi-class data, the optimal linear discriminative
representation maximizes the coding rate difference between the whole dataset
and the average of all the subsets. We show that the basic iterative gradient
ascent scheme for optimizing the rate reduction objective naturally leads to a
multi-layer deep network, named ReduNet, which shares common characteristics of
modern deep networks. The deep layered architectures, linear and nonlinear
operators, and even parameters of the network are all explicitly constructed
layer-by-layer via forward propagation, although they are amenable to
fine-tuning via back propagation. All components of so-obtained &#x60;&#x60;white-box&#x27;&#x27;
network have precise optimization, statistical, and geometric interpretation.
Moreover, all linear operators of the so-derived network naturally become
multi-channel convolutions when we enforce classification to be rigorously
shift-invariant. The derivation in the invariant setting suggests a trade-off
between sparsity and invariance, and also indicates that such a deep
convolution network is significantly more efficient to construct and learn in
the spectral domain. Our preliminary simulations and experiments clearly verify
the effectiveness of both the rate reduction objective and the associated
ReduNet. All code and data are available at https://github.com/Ma-Lab-Berkeley.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scale-invariant scale-channel networks: Deep networks that generalise to previously unseen scales. (arXiv:2106.06418v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jansson_Y/0/1/0/all/0/1">Ylva Jansson</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1">Tony Lindeberg</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06418">
                                    <div class="article-summary-box-inner">
                                        <span>The ability to handle large scale variations is crucial for many real world
visual tasks. A straightforward approach for handling scale in a deep network
is to process an image at several scales simultaneously in a set of scale
channels. Scale invariance can then, in principle, be achieved by using weight
sharing between the scale channels together with max or average pooling over
the outputs from the scale channels. The ability of such scale channel networks
to generalise to scales not present in the training set over significant scale
ranges has, however, not previously been explored.

In this paper, we present a systematic study of this methodology by
implementing different types of scale channel networks and evaluating their
ability to generalise to previously unseen scales. We develop a formalism for
analysing the covariance and invariance properties of scale channel networks,
and explore how different design choices, unique to scaling transformations,
affect the overall performance of scale channel networks. We first show that
two previously proposed scale channel network designs do not generalise well to
scales not present in the training set. We explain theoretically and
demonstrate experimentally why generalisation fails in these cases.

We then propose a new type of foveated scale channel architecture}, where the
scale channels process increasingly larger parts of the image with decreasing
resolution. This new type of scale channel network is shown to generalise
extremely well, provided sufficient image resolution and the absence of
boundary effects. Our proposed FovMax and FovAvg networks perform almost
identically over a scale range of 8, also when training on single scale
training data, and do also give improved performance when learning from
datasets with large scale variations in the small sample regime.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DRLD-SP: A Deep Reinforcement Learning-based Dynamic Service Placement in Edge-Enabled Internet of Vehicles. (arXiv:2106.06291v1 [cs.NI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Talpur_A/0/1/0/all/0/1">Anum Talpur</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurusamy_M/0/1/0/all/0/1">Mohan Gurusamy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06291">
                                    <div class="article-summary-box-inner">
                                        <span>The growth of 5G and edge computing has enabled the emergence of Internet of
Vehicles. It supports different types of services with different resource and
service requirements. However, limited resources at the edge, high mobility of
vehicles, increasing demand, and dynamicity in service request-types have made
service placement a challenging task. A typical static placement solution is
not effective as it does not consider the traffic mobility and service
dynamics. Handling dynamics in IoV for service placement is an important and
challenging problem which is the primary focus of our work in this paper. We
propose a Deep Reinforcement Learning-based Dynamic Service Placement (DRLD-SP)
framework with the objective of minimizing the maximum edge resource usage and
service delay while considering the vehicle&#x27;s mobility, varying demand, and
dynamics in the requests for different types of services. We use SUMO and
MATLAB to carry out simulation experiments. The experimental results show that
the proposed DRLD-SP approach is effective and outperforms other static and
dynamic placement approaches.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Going Beyond Linear Transformers with Recurrent Fast Weight Programmers. (arXiv:2106.06295v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1">Kazuki Irie</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlag_I/0/1/0/all/0/1">Imanol Schlag</a>, <a href="http://arxiv.org/find/cs/1/au:+Csordas_R/0/1/0/all/0/1">R&#xf3;bert Csord&#xe1;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1">J&#xfc;rgen Schmidhuber</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06295">
                                    <div class="article-summary-box-inner">
                                        <span>Transformers with linearised attention (&quot;linear Transformers&quot;) have
demonstrated the practical scalability and effectiveness of outer product-based
Fast Weight Programmers (FWPs) from the &#x27;90s. However, the original FWP
formulation is more general than the one of linear Transformers: a slow neural
network (NN) continually reprograms the weights of a fast NN with arbitrary NN
architectures. In existing linear Transformers, both NNs are feedforward and
consist of a single layer. Here we explore new variations by adding recurrence
to the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two
synthetic algorithmic tasks (code execution and sequential ListOps),
Wikitext-103 language models, and on the Atari 2600 2D game environment. Our
models exhibit properties of Transformers and RNNs. In the reinforcement
learning setting, we report large improvements over LSTM in several Atari
games. Our code is public.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Catch-A-Waveform: Learning to Generate Audio from a Single Short Example. (arXiv:2106.06426v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Greshler_G/0/1/0/all/0/1">Gal Greshler</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1">Tamar Rott Shaham</a>, <a href="http://arxiv.org/find/cs/1/au:+Michaeli_T/0/1/0/all/0/1">Tomer Michaeli</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06426">
                                    <div class="article-summary-box-inner">
                                        <span>Models for audio generation are typically trained on hours of recordings.
Here, we illustrate that capturing the essence of an audio source is typically
possible from as little as a few tens of seconds from a single training signal.
Specifically, we present a GAN-based generative model that can be trained on
one short audio signal from any domain (e.g. speech, music, etc.) and does not
require pre-training or any other form of external supervision. Once trained,
our model can generate random samples of arbitrary duration that maintain
semantic similarity to the training waveform, yet exhibit new compositions of
its audio primitives. This enables a long line of interesting applications,
including generating new jazz improvisations or new a-cappella rap variants
based on a single short example, producing coherent modifications to famous
songs (e.g. adding a new verse to a Beatles song based solely on the original
recording), filling-in of missing parts (inpainting), extending the bandwidth
of a speech signal (super-resolution), and enhancing old recordings without
access to any clean training example. We show that in all cases, no more than
20 seconds of training audio commonly suffice for our model to achieve
state-of-the-art results. This is despite its complete lack of prior knowledge
about the nature of audio signals in general.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial Robustness through the Lens of Causality. (arXiv:2106.06196v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yonggang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1">Mingming Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1">Gang Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1">Xinmei Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kun Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06196">
                                    <div class="article-summary-box-inner">
                                        <span>The adversarial vulnerability of deep neural networks has attracted
significant attention in machine learning. From a causal viewpoint, adversarial
attacks can be considered as a specific type of distribution change on natural
data. As causal reasoning has an instinct for modeling distribution change, we
propose to incorporate causality into mitigating adversarial vulnerability.
However, causal formulations of the intuition of adversarial attack and the
development of robust DNNs are still lacking in the literature. To bridge this
gap, we construct a causal graph to model the generation process of adversarial
examples and define the adversarial distribution to formalize the intuition of
adversarial attacks. From a causal perspective, we find that the label is
spuriously correlated with the style (content-independent) information when an
instance is given. The spurious correlation implies that the adversarial
distribution is constructed via making the statistical conditional association
between style information and labels drastically different from that in natural
distribution. Thus, DNNs that fit the spurious correlation are vulnerable to
the adversarial distribution. Inspired by the observation, we propose the
adversarial distribution alignment method to eliminate the difference between
the natural distribution and the adversarial distribution. Extensive
experiments demonstrate the efficacy of the proposed method. Our method can be
seen as the first attempt to leverage causality for mitigating adversarial
vulnerability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Guarantees for Tuning the Step Size using a Learning-to-Learn Approach. (arXiv:2006.16495v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1">Xiang Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Yuan_S/0/1/0/all/0/1">Shuai Yuan</a>, <a href="http://arxiv.org/find/stat/1/au:+Wu_C/0/1/0/all/0/1">Chenwei Wu</a>, <a href="http://arxiv.org/find/stat/1/au:+Ge_R/0/1/0/all/0/1">Rong Ge</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.16495">
                                    <div class="article-summary-box-inner">
                                        <span>Choosing the right parameters for optimization algorithms is often the key to
their success in practice. Solving this problem using a learning-to-learn
approach -- using meta-gradient descent on a meta-objective based on the
trajectory that the optimizer generates -- was recently shown to be effective.
However, the meta-optimization problem is difficult. In particular, the
meta-gradient can often explode/vanish, and the learned optimizer may not have
good generalization performance if the meta-objective is not chosen carefully.
In this paper we give meta-optimization guarantees for the learning-to-learn
approach on a simple problem of tuning the step size for quadratic loss. Our
results show that the na\&quot;ive objective suffers from meta-gradient
explosion/vanishing problem. Although there is a way to design the
meta-objective so that the meta-gradient remains polynomially bounded,
computing the meta-gradient directly using backpropagation leads to numerical
issues. We also characterize when it is necessary to compute the meta-objective
on a separate validation set to ensure the generalization performance of the
learned optimizer. Finally, we verify our results empirically and show that a
similar phenomenon appears even for more complicated learned optimizers
parametrized by neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dynamic Game Theoretic Neural Optimizer. (arXiv:2105.03788v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guan-Horng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianrong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1">Evangelos A. Theodorou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03788">
                                    <div class="article-summary-box-inner">
                                        <span>The connection between training deep neural networks (DNNs) and optimal
control theory (OCT) has attracted considerable attention as a principled tool
of algorithmic design. Despite few attempts being made, they have been limited
to architectures where the layer propagation resembles a Markovian dynamical
system. This casts doubts on their flexibility to modern networks that heavily
rely on non-Markovian dependencies between layers (e.g. skip connections in
residual networks). In this work, we propose a novel dynamic game perspective
by viewing each layer as a player in a dynamic game characterized by the DNN
itself. Through this lens, different classes of optimizers can be seen as
matching different types of Nash equilibria, depending on the implicit
information structure of each (p)layer. The resulting method, called Dynamic
Game Theoretic Neural Optimizer (DGNOpt), not only generalizes OCT-inspired
optimizers to richer network class; it also motivates a new training principle
by solving a multi-player cooperative game. DGNOpt shows convergence
improvements over existing methods on image classification datasets with
residual and inception networks. Our work marries strengths from both OCT and
game theory, paving ways to new algorithmic opportunities from robust optimal
control and bandit-based optimization.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unsupervised Anomaly Detection Ensembles using Item Response Theory. (arXiv:2106.06243v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Kandanaarachchi_S/0/1/0/all/0/1">Sevvandi Kandanaarachchi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06243">
                                    <div class="article-summary-box-inner">
                                        <span>Constructing an ensemble from a heterogeneous set of unsupervised anomaly
detection methods is challenging because the class labels or the ground truth
is unknown. Thus, traditional ensemble techniques that use the response
variable or the class labels cannot be used to construct an ensemble for
unsupervised anomaly detection.

We use Item Response Theory (IRT) -- a class of models used in educational
psychometrics to assess student and test question characteristics -- to
construct an unsupervised anomaly detection ensemble. IRT&#x27;s latent trait
computation lends itself to anomaly detection because the latent trait can be
used to uncover the hidden ground truth. Using a novel IRT mapping to the
anomaly detection problem, we construct an ensemble that can downplay noisy,
non-discriminatory methods and accentuate sharper methods. We demonstrate the
effectiveness of the IRT ensemble on an extensive data repository, by comparing
its performance to other ensemble techniques.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On the Robustness of Average Losses for Partial-Label Learning. (arXiv:2106.06152v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1">Jiaqi Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1">Lei Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Miao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1">Bo An</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1">Gang Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1">Xin Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1">Masashi Sugiyama</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06152">
                                    <div class="article-summary-box-inner">
                                        <span>Partial-label (PL) learning is a typical weakly supervised classification
problem, where a PL of an instance is a set of candidate labels such that a
fixed but unknown candidate is the true label. For PL learning, there are two
lines of research: (a) the identification-based strategy (IBS) purifies each
label set and extracts the true label; (b) the average-based strategy (ABS)
treats all candidates equally for training. In the past two decades, IBS was a
much hotter topic than ABS, since it was believed that IBS is more promising.
In this paper, we theoretically analyze ABS and find it also promising in the
sense of the robustness of its loss functions. Specifically, we consider five
problem settings for the generation of clean or noisy PLs, and we prove that
average PL losses with bounded multi-class losses are always robust under mild
assumptions on the domination of true labels, while average PL losses with
unbounded multi-class losses (e.g., the cross-entropy loss) may not be robust.
We also conduct experiments to validate our theoretical findings. Note that IBS
is heuristic, and we cannot prove its robustness by a similar proof technique;
hence, ABS is more advantageous from a theoretical point of view, and it is
worth paying attention to the design of more advanced PL learning methods
following ABS.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1">Antoine Labatie</a>, <a href="http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1">Dominic Masters</a>, <a href="http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1">Zach Eaton-Rosen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1">Carlo Luschi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03743">
                                    <div class="article-summary-box-inner">
                                        <span>We investigate the reasons for the performance degradation incurred with
batch-independent normalization. We find that the prototypical techniques of
layer normalization and instance normalization both induce the appearance of
failure modes in the neural network&#x27;s pre-activations: (i) layer normalization
induces a collapse towards channel-wise constant functions; (ii) instance
normalization induces a lack of variability in instance statistics, symptomatic
of an alteration of the expressivity. To alleviate failure mode (i) without
aggravating failure mode (ii), we introduce the technique &quot;Proxy Normalization&quot;
that normalizes post-activations using a proxy distribution. When combined with
layer normalization or group normalization, this batch-independent
normalization emulates batch normalization&#x27;s behavior and consistently matches
or exceeds its performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Twins: Revisiting the Design of Spatial Attention in Vision Transformers. (arXiv:2104.13840v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1">Xiangxiang Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1">Zhi Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1">Haibing Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiaolin Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1">Huaxia Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13840">
                                    <div class="article-summary-box-inner">
                                        <span>Very recently, a variety of vision transformer architectures for dense
prediction tasks have been proposed and they show that the design of spatial
attention is critical to their success in these tasks. In this work, we revisit
the design of the spatial attention and demonstrate that a carefully-devised
yet simple spatial attention mechanism performs favourably against the
state-of-the-art schemes. As a result, we propose two vision transformer
architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures
are highly-efficient and easy to implement, only involving matrix
multiplications that are highly optimized in modern deep learning frameworks.
More importantly, the proposed architectures achieve excellent performance on a
wide range of visual tasks including imagelevel classification as well as dense
detection and segmentation. The simplicity and strong performance suggest that
our proposed architectures may serve as stronger backbones for many vision
tasks. Our code will be released soon at
https://github.com/Meituan-AutoML/Twins .</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Moreau-Yosida $f$-divergences. (arXiv:2102.13416v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Terjek_D/0/1/0/all/0/1">D&#xe1;vid Terj&#xe9;k</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.13416">
                                    <div class="article-summary-box-inner">
                                        <span>Variational representations of $f$-divergences are central to many machine
learning algorithms, with Lipschitz constrained variants recently gaining
attention. Inspired by this, we define the Moreau-Yosida approximation of
$f$-divergences with respect to the Wasserstein-$1$ metric. The corresponding
variational formulas provide a generalization of a number of recent results,
novel special cases of interest and a relaxation of the hard Lipschitz
constraint. Additionally, we prove that the so-called tight variational
representation of $f$-divergences can be to be taken over the quotient space of
Lipschitz functions, and give a characterization of functions achieving the
supremum in the variational representation. On the practical side, we propose
an algorithm to calculate the tight convex conjugate of $f$-divergences
compatible with automatic differentiation frameworks. As an application of our
results, we propose the Moreau-Yosida $f$-GAN, providing an implementation of
the variational formulas for the Kullback-Leibler, reverse Kullback-Leibler,
$\chi^2$, reverse $\chi^2$, squared Hellinger, Jensen-Shannon, Jeffreys,
triangular discrimination and total variation divergences as GANs trained on
CIFAR-10, leading to competitive results and a simple solution to the problem
of uniqueness of the optimal critic.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">High-Performance FPGA-based Accelerator for Bayesian Neural Networks. (arXiv:2105.09163v2 [cs.AR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Hongxiang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1">Martin Ferianc</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1">Miguel Rodrigues</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hongyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1">Xinyu Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luk_W/0/1/0/all/0/1">Wayne Luk</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09163">
                                    <div class="article-summary-box-inner">
                                        <span>Neural networks (NNs) have demonstrated their potential in a wide range of
applications such as image recognition, decision making or recommendation
systems. However, standard NNs are unable to capture their model uncertainty
which is crucial for many safety-critical applications including healthcare and
autonomous vehicles. In comparison, Bayesian neural networks (BNNs) are able to
express uncertainty in their prediction via a mathematical grounding.
Nevertheless, BNNs have not been as widely used in industrial practice, mainly
because of their expensive computational cost and limited hardware performance.
This work proposes a novel FPGA-based hardware architecture to accelerate BNNs
inferred through Monte Carlo Dropout. Compared with other state-of-the-art BNN
accelerators, the proposed accelerator can achieve up to 4 times higher energy
efficiency and 9 times better compute efficiency. Considering partial Bayesian
inference, an automatic framework is proposed, which explores the trade-off
between hardware and algorithmic performance. Extensive experiments are
conducted to demonstrate that our proposed framework can effectively find the
optimal points in the design space.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning to Extend Molecular Scaffolds with Structural Motifs. (arXiv:2103.03864v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Maziarz_K/0/1/0/all/0/1">Krzysztof Maziarz</a>, <a href="http://arxiv.org/find/cs/1/au:+Jackson_Flux_H/0/1/0/all/0/1">Henry Jackson-Flux</a>, <a href="http://arxiv.org/find/cs/1/au:+Cameron_P/0/1/0/all/0/1">Pashmina Cameron</a>, <a href="http://arxiv.org/find/cs/1/au:+Sirockin_F/0/1/0/all/0/1">Finton Sirockin</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1">Nadine Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Stiefl_N/0/1/0/all/0/1">Nikolaus Stiefl</a>, <a href="http://arxiv.org/find/cs/1/au:+Segler_M/0/1/0/all/0/1">Marwin Segler</a>, <a href="http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1">Marc Brockschmidt</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03864">
                                    <div class="article-summary-box-inner">
                                        <span>Recent advancements in deep learning-based modeling of molecules promise to
accelerate in silico drug discovery. A plethora of generative models is
available, building molecules either atom-by-atom and bond-by-bond or
fragment-by-fragment. However, many drug discovery projects require a fixed
scaffold to be present in the generated molecule, and incorporating that
constraint has only recently been explored. In this work, we propose a new
graph-based model that naturally supports scaffolds as initial seed of the
generative procedure, which is possible because our model is not conditioned on
the generation history. At the same time, our generation procedure can flexibly
choose between adding individual atoms and entire fragments. We show that
training using a randomized generation order is necessary for good performance
when extending scaffolds, and that the results are further improved by
increasing the fragment vocabulary size. Our model pushes the state-of-the-art
of graph-based molecule generation, while being an order of magnitude faster to
train and sample from than existing approaches.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Shapley Value of Classifiers in Ensemble Games. (arXiv:2101.02153v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rozemberczki_B/0/1/0/all/0/1">Benedek Rozemberczki</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_R/0/1/0/all/0/1">Rik Sarkar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02153">
                                    <div class="article-summary-box-inner">
                                        <span>What is the value of an individual model in an ensemble of binary
classifiers? We answer this question by introducing a class of transferable
utility cooperative games called \textit{ensemble games}. In machine learning
ensembles, pre-trained models cooperate to make classification decisions. To
quantify the importance of models in these ensemble games, we define
\textit{Troupe} -- an efficient algorithm which allocates payoffs based on
approximate Shapley values of the classifiers. We argue that the Shapley value
of models in these games is an effective decision metric for choosing a high
performing subset of models from the ensemble. Our analytical findings prove
that our Shapley value estimation scheme is precise and scalable; its
performance increases with size of the dataset and ensemble. Empirical results
on real world graph classification tasks demonstrate that our algorithm
produces high quality estimates of the Shapley value. We find that Shapley
values can be utilized for ensemble pruning, and that adversarial models
receive a low valuation. Complex classifiers are frequently found to be
responsible for both correct and incorrect classification decisions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">RNN with Particle Flow for Probabilistic Spatio-temporal Forecasting. (arXiv:2106.06064v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Pal_S/0/1/0/all/0/1">Soumyasundar Pal</a>, <a href="http://arxiv.org/find/stat/1/au:+Ma_L/0/1/0/all/0/1">Liheng Ma</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1">Yingxue Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Coates_M/0/1/0/all/0/1">Mark Coates</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06064">
                                    <div class="article-summary-box-inner">
                                        <span>Spatio-temporal forecasting has numerous applications in analyzing wireless,
traffic, and financial networks. Many classical statistical models often fall
short in handling the complexity and high non-linearity present in time-series
data. Recent advances in deep learning allow for better modelling of spatial
and temporal dependencies. While most of these models focus on obtaining
accurate point forecasts, they do not characterize the prediction uncertainty.
In this work, we consider the time-series data as a random realization from a
nonlinear state-space model and target Bayesian inference of the hidden states
for probabilistic forecasting. We use particle flow as the tool for
approximating the posterior distribution of the states, as it is shown to be
highly effective in complex, high-dimensional settings. Thorough
experimentation on several real world time-series datasets demonstrates that
our approach provides better characterization of uncertainty while maintaining
comparable accuracy to the state-of-the art point forecasting methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Binary Classification from Multiple Unlabeled Datasets via Surrogate Set Classification. (arXiv:2102.00678v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1">Nan Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1">Shida Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1">Gang Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1">Issei Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1">Masashi Sugiyama</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00678">
                                    <div class="article-summary-box-inner">
                                        <span>To cope with high annotation costs, training a classifier only from weakly
supervised data has attracted a great deal of attention these days. Among
various approaches, strengthening supervision from completely unsupervised
classification is a promising direction, which typically employs class priors
as the only supervision and trains a binary classifier from unlabeled (U)
datasets. While existing risk-consistent methods are theoretically grounded
with high flexibility, they can learn only from two U sets. In this paper, we
propose a new approach for binary classification from $m$ U-sets for $m\ge2$.
Our key idea is to consider an auxiliary classification task called surrogate
set classification (SSC), which is aimed at predicting from which U set each
observed data is drawn. SSC can be solved by a standard (multi-class)
classification method, and we use the SSC solution to obtain the final binary
classifier through a certain linear-fractional transformation. We built our
method in a flexible and efficient end-to-end deep learning framework and prove
it to be classifier-consistent. Through experiments, we demonstrate the
superiority of our proposed method over state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Surface Warping Incorporating Machine Learning Assisted Domain Likelihood Estimation: A New Paradigm in Mine Geology Modelling and Automation. (arXiv:2103.03923v2 [physics.geo-ph] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/physics/1/au:+Leung_R/0/1/0/all/0/1">Raymond Leung</a>, <a href="http://arxiv.org/find/physics/1/au:+Balamurali_M/0/1/0/all/0/1">Mehala Balamurali</a>, <a href="http://arxiv.org/find/physics/1/au:+Lowe_A/0/1/0/all/0/1">Alexander Lowe</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03923">
                                    <div class="article-summary-box-inner">
                                        <span>This paper illustrates an application of machine learning (ML) within a
complex system that performs grade estimation. In surface mining, assay
measurements taken from production drilling often provide useful information
that allows initially inaccurate surfaces created using sparse exploration data
to be revised and subsequently improved. Recently, a Bayesian warping technique
has been proposed to reshape modeled surfaces using geochemical and spatial
constraints imposed by newly acquired blasthole data. This paper focuses on
incorporating machine learning into this warping framework to make the
likelihood computation generalizable. The technique works by adjusting the
position of vertices on the surface to maximize the integrity of modeled
geological boundaries with respect to sparse geochemical observations. Its
foundation is laid by a Bayesian derivation in which the geological domain
likelihood given the chemistry, p(g|c), plays a similar role to p(y(c)|g). This
observation allows a manually calibrated process centered around the latter to
be automated since ML techniques may be used to estimate the former in a
data-driven way. Machine learning performance is evaluated for gradient
boosting, neural network, random forest and other classifiers in a binary and
multi-class context using precision and recall rates. Once ML likelihood
estimators are integrated in the surface warping framework, surface shaping
performance is evaluated using unseen data by examining the categorical
distribution of test samples located above and below the warped surface.
Large-scale validation experiments are performed to assess the overall efficacy
of ML assisted surface warping as a fully integrated component within an ore
grade estimation system where the posterior mean is obtained via Gaussian
Process inference with a Matern 3/2 kernel.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Generative Archimedean Copulas. (arXiv:2102.11351v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1">Yuting Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1">Ali Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Elkhalil_K/0/1/0/all/0/1">Khalil Elkhalil</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1">Vahid Tarokh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11351">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a new generative modeling technique for learning multidimensional
cumulative distribution functions (CDFs) in the form of copulas. Specifically,
we consider certain classes of copulas known as Archimedean and hierarchical
Archimedean copulas, popular for their parsimonious representation and ability
to model different tail dependencies. We consider their representation as
mixture models with Laplace transforms of latent random variables from
generative neural networks. This alternative representation allows for
computational efficiencies and easy sampling, especially in high dimensions. We
describe multiple methods for optimizing the network parameters. Finally, we
present empirical results that demonstrate the efficacy of our proposed method
in learning multidimensional CDFs and its computational efficiency compared to
existing methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Infinite-dimensional Folded-in-time Deep Neural Networks. (arXiv:2101.02966v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Stelzer_F/0/1/0/all/0/1">Florian Stelzer</a> (1, 2 and 3), <a href="http://arxiv.org/find/cs/1/au:+Yanchuk_S/0/1/0/all/0/1">Serhiy Yanchuk</a> (1) ((1) Institute of Mathematics, Technische Universit&#xe4;t Berlin, Germany, (2) Department of Mathematics, Humboldt-Universit&#xe4;t zu Berlin, Germany, (3) Institute of Computer Science, University of Tartu, Estonia)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02966">
                                    <div class="article-summary-box-inner">
                                        <span>The method recently introduced in arXiv:2011.10115 realizes a deep neural
network with just a single nonlinear element and delayed feedback. It is
applicable for the description of physically implemented neural networks. In
this work, we present an infinite-dimensional generalization, which allows for
a more rigorous mathematical analysis and a higher flexibility in choosing the
weight functions. Precisely speaking, the weights are described by Lebesgue
integrable functions instead of step functions. We also provide a functional
back-propagation algorithm, which enables gradient descent training of the
weights. In addition, with a slight modification, our concept realizes
recurrent neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Of Moments and Matching: A Game-Theoretic Framework for Closing the Imitation Gap. (arXiv:2103.03236v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Swamy_G/0/1/0/all/0/1">Gokul Swamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1">Sanjiban Choudhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagnell_J/0/1/0/all/0/1">J. Andrew Bagnell</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiwei Steven Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03236">
                                    <div class="article-summary-box-inner">
                                        <span>We provide a unifying view of a large family of previous imitation learning
algorithms through the lens of moment matching. At its core, our classification
scheme is based on whether the learner attempts to match (1) reward or (2)
action-value moments of the expert&#x27;s behavior, with each option leading to
differing algorithmic approaches. By considering adversarially chosen
divergences between learner and expert behavior, we are able to derive bounds
on policy performance that apply for all algorithms in each of these classes,
the first to our knowledge. We also introduce the notion of moment
recoverability, implicit in many previous analyses of imitation learning, which
allows us to cleanly delineate how well each algorithmic family is able to
mitigate compounding errors. We derive three novel algorithm templates (AdVIL,
AdRIL, and DAeQuIL) with strong guarantees, simple implementation, and
competitive empirical performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Signal Processing on Higher-Order Networks: Livin&#x27; on the Edge ... and Beyond. (arXiv:2101.05510v3 [cs.SI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1">Michael T. Schaub</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Seby_J/0/1/0/all/0/1">Jean-Baptiste Seby</a>, <a href="http://arxiv.org/find/cs/1/au:+Roddenberry_T/0/1/0/all/0/1">T. Mitchell Roddenberry</a>, <a href="http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1">Santiago Segarra</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05510">
                                    <div class="article-summary-box-inner">
                                        <span>In this tutorial, we provide a didactic treatment of the emerging topic of
signal processing on higher-order networks. Drawing analogies from discrete and
graph signal processing, we introduce the building blocks for processing data
on simplicial complexes and hypergraphs, two common higher-order network
abstractions that can incorporate polyadic relationships. We provide brief
introductions to simplicial complexes and hypergraphs, with a special emphasis
on the concepts needed for the processing of signals supported on these
structures. Specifically, we discuss Fourier analysis, signal denoising, signal
interpolation, node embeddings, and nonlinear processing through neural
networks, using these two higher-order network models. In the context of
simplicial complexes, we specifically focus on signal processing using the
Hodge Laplacian matrix, a multi-relational operator that leverages the special
structure of simplicial complexes and generalizes desirable properties of the
Laplacian matrix in graph signal processing. For hypergraphs, we present both
matrix and tensor representations, and discuss the trade-offs in adopting one
or the other. We also highlight limitations and potential research avenues,
both to inform practitioners and to motivate the contribution of new
researchers to the area.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. (arXiv:2102.05918v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1">Chao Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yinfei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Ye Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi-Ting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1">Zarana Parekh</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1">Hieu Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1">Quoc V. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1">Yunhsuan Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Duerig_T/0/1/0/all/0/1">Tom Duerig</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05918">
                                    <div class="article-summary-box-inner">
                                        <span>Pre-trained representations are becoming crucial for many NLP and perception
tasks. While representation learning in NLP has transitioned to training on raw
text without human annotations, visual and vision-language representations
still rely heavily on curated training datasets that are expensive or require
expert knowledge. For vision applications, representations are mostly learned
using datasets with explicit class labels such as ImageNet or OpenImages. For
vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all
involve a non-trivial data collection (and cleaning) process. This costly
curation process limits the size of datasets and hence hinders the scaling of
trained models. In this paper, we leverage a noisy dataset of over one billion
image alt-text pairs, obtained without expensive filtering or post-processing
steps in the Conceptual Captions dataset. A simple dual-encoder architecture
learns to align visual and language representations of the image and text pairs
using a contrastive loss. We show that the scale of our corpus can make up for
its noise and leads to state-of-the-art representations even with such a simple
learning scheme. Our visual representation achieves strong performance when
transferred to classification tasks such as ImageNet and VTAB. The aligned
visual and language representations enables zero-shot image classification and
also set new state-of-the-art results on Flickr30K and MSCOCO image-text
retrieval benchmarks, even when compared with more sophisticated
cross-attention models. The representations also enable cross-modality search
with complex text and text + image queries.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Theory of Label Propagation for Subpopulation Shift. (arXiv:2102.11203v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Tianle Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Ruiqi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jason D. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1">Qi Lei</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11203">
                                    <div class="article-summary-box-inner">
                                        <span>One of the central problems in machine learning is domain adaptation. Unlike
past theoretical work, we consider a new model for subpopulation shift in the
input or representation space. In this work, we propose a provably effective
framework for domain adaptation based on label propagation. In our analysis, we
use a simple but realistic expansion assumption, proposed in
\citet{wei2021theoretical}. Using a teacher classifier trained on the source
domain, our algorithm not only propagates to the target domain but also
improves upon the teacher. By leveraging existing generalization bounds, we
also obtain end-to-end finite-sample guarantees on the entire algorithm. In
addition, we extend our theoretical framework to a more general setting of
source-to-target transfer based on a third unlabeled dataset, which can be
easily applied in various learning scenarios. Inspired by our theory, we adapt
consistency-based semi-supervised learning methods to domain adaptation
settings and gain significant improvements.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization. (arXiv:2102.10707v2 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Cai_H/0/1/0/all/0/1">HanQin Cai</a>, <a href="http://arxiv.org/find/math/1/au:+Lou_Y/0/1/0/all/0/1">Yuchen Lou</a>, <a href="http://arxiv.org/find/math/1/au:+McKenzie_D/0/1/0/all/0/1">Daniel McKenzie</a>, <a href="http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1">Wotao Yin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10707">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the zeroth-order optimization problem in the huge-scale setting,
where the dimension of the problem is so large that performing even basic
vector operations on the decision variables is infeasible. In this paper, we
propose a novel algorithm, coined ZO-BCD, that exhibits favorable overall query
complexity and has a much smaller per-iteration computational complexity. In
addition, we discuss how the memory footprint of ZO-BCD can be reduced even
further by the clever use of circulant measurement matrices. As an application
of our new method, we propose the idea of crafting adversarial attacks on
neural network based classifiers in a wavelet domain, which can result in
problem dimensions of over 1.7 million. In particular, we show that crafting
adversarial examples to audio classifiers in a wavelet domain can achieve the
state-of-the-art attack success rate of 97.9%.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Abstract Representations through Lossy Compression of Multi-Modal Signals. (arXiv:2101.11376v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wilmot_C/0/1/0/all/0/1">Charles Wilmot</a>, <a href="http://arxiv.org/find/cs/1/au:+Triesch_J/0/1/0/all/0/1">Jochen Triesch</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11376">
                                    <div class="article-summary-box-inner">
                                        <span>A key competence for open-ended learning is the formation of increasingly
abstract representations useful for driving complex behavior. Abstract
representations ignore specific details and facilitate generalization. Here we
consider the learning of abstract representations in a multi-modal setting with
two or more input modalities. We treat the problem as a lossy compression
problem and show that generic lossy compression of multimodal sensory input
naturally extracts abstract representations that tend to strip away modalitiy
specific details and preferentially retain information that is shared across
the different modalities. Furthermore, we propose an architecture to learn
abstract representations by identifying and retaining only the information that
is shared across multiple modalities while discarding any modality specific
information.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Two-Way Matrix Reordering for Relational Data Analysis. (arXiv:2103.14203v4 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1">Chihiro Watanabe</a>, <a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1">Taiji Suzuki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14203">
                                    <div class="article-summary-box-inner">
                                        <span>Matrix reordering is a task to permute the rows and columns of a given
observed matrix such that the resulting reordered matrix shows meaningful or
interpretable structural patterns. Most existing matrix reordering techniques
share the common processes of extracting some feature representations from an
observed matrix in a predefined manner, and applying matrix reordering based on
it. However, in some practical cases, we do not always have prior knowledge
about the structural pattern of an observed matrix. To address this problem, we
propose a new matrix reordering method, called deep two-way matrix reordering
(DeepTMR), using a neural network model. The trained network can automatically
extract nonlinear row/column features from an observed matrix, which can then
be used for matrix reordering. Moreover, the proposed DeepTMR provides the
denoised mean matrix of a given observed matrix as an output of the trained
network. This denoised mean matrix can be used to visualize the global
structure of the reordered observed matrix. We demonstrate the effectiveness of
the proposed DeepTMR by applying it to both synthetic and practical datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Certifying L-infinity Robustness using Neural Networks with L-inf-dist Neurons. (arXiv:2102.05363v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Bohang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Tianle Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhou Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1">Di He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05363">
                                    <div class="article-summary-box-inner">
                                        <span>It is well-known that standard neural networks, even with a high
classification accuracy, are vulnerable to small $\ell_\infty$-norm bounded
adversarial perturbations. Although many attempts have been made, most previous
works either can only provide empirical verification of the defense to a
particular attack method, or can only develop a certified guarantee of the
model robustness in limited scenarios. In this paper, we seek for a new
approach to develop a theoretically principled neural network that inherently
resists $\ell_\infty$ perturbations. In particular, we design a novel neuron
that uses $\ell_\infty$-distance as its basic operation (which we call
$\ell_\infty$-dist neuron), and show that any neural network constructed with
$\ell_\infty$-dist neurons (called $\ell_{\infty}$-dist net) is naturally a
1-Lipschitz function with respect to $\ell_\infty$-norm. This directly provides
a rigorous guarantee of the certified robustness based on the margin of
prediction outputs. We then prove that such networks have enough expressive
power to approximate any 1-Lipschitz function with robust generalization
guarantee. We further provide a holistic training strategy that can greatly
alleviate optimization difficulties. Experimental results show that using
$\ell_{\infty}$-dist nets as basic building blocks, we consistently achieve
state-of-the-art performance on commonly used datasets: 93.09% certified
accuracy on MNIST ($\epsilon&#x3D;0.3$), 35.42% on CIFAR-10 ($\epsilon&#x3D;8/255$) and
16.31% on TinyImageNet ($\epsilon&#x3D;1/255$).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data Profiling for Adversarial Training: On the Ruin of Problematic Data. (arXiv:2102.07437v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1">Chengyu Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Liyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1">Jingbo Shang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07437">
                                    <div class="article-summary-box-inner">
                                        <span>There are multiple intriguing problems hovering in adversarial training,
including robustness-accuracy trade-off, robust overfitting, and robustness
overestimation. These problems pose great challenges to both reliable
evaluation and practical deployment. Here, we show that these problems share
one common cause -- low quality samples in the dataset. We first identify an
intrinsic property of the data called \emph{problematic score} and then design
controlled experiments to investigate its connections with these problems.
Specifically, we find that when problematic data is removed, robust overfitting
and robustness overestimation can be largely alleviated; and
robustness-accuracy trade-off becomes less significant. These observations not
only verify our intuition about data quality but also open new opportunities to
advance adversarial training. Interestingly, simply removing problematic data
from adversarial training, while making the training set smaller, yields better
robustness for leading adversarial training strategies.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SparseBERT: Rethinking the Importance Analysis in Self-attention. (arXiv:2102.12871v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1">Han Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiahui Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiaozhe Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1">Xiaodan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1">James T. Kwok</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12871">
                                    <div class="article-summary-box-inner">
                                        <span>Transformer-based models are popularly used in natural language processing
(NLP). Its core component, self-attention, has aroused widespread interest. To
understand the self-attention mechanism, a direct method is to visualize the
attention map of a pre-trained model. Based on the patterns observed, a series
of efficient Transformers with different sparse attention masks have been
proposed. From a theoretical perspective, universal approximability of
Transformer-based models is also recently proved. However, the above
understanding and analysis of self-attention is based on a pre-trained model.
To rethink the importance analysis in self-attention, we study the significance
of different positions in attention matrix during pre-training. A surprising
result is that diagonal elements in the attention map are the least important
compared with other attention positions. We provide a proof showing that these
diagonal elements can indeed be removed without deteriorating model
performance. Furthermore, we propose a Differentiable Attention Mask (DAM)
algorithm, which further guides the design of the SparseBERT. Extensive
experiments verify our interesting findings and illustrate the effect of the
proposed algorithm.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Demystifying Assumptions in Learning to Discover Novel Classes. (arXiv:2102.04002v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1">Haoang Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Feng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenjing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1">Long Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1">Gang Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingyuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1">Masashi Sugiyama</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04002">
                                    <div class="article-summary-box-inner">
                                        <span>In learning to discover novel classes (L2DNC), we are given labeled data from
seen classes and unlabeled data from unseen classes, and we train clustering
models for the unseen classes. However, the rigorous definition of L2DNC is
unexplored, which results in that its implicit assumptions are still unclear.
In this paper, we demystify assumptions behind L2DNC and find that high-level
semantic features should be shared among the seen and unseen classes. This
naturally motivates us to link L2DNC to meta-learning that has exactly the same
assumption as L2DNC. Based on this finding, L2DNC is not only theoretically
solvable, but can also be empirically solved by meta-learning algorithms after
slight modifications. This L2DNC methodology significantly reduces the amount
of unlabeled data needed for training and makes it more practical, as
demonstrated in experiments. The use of very limited data is also justified by
the application scenario of L2DNC: since it is unnatural to label only
seen-class data, L2DNC is sampling instead of labeling in causality. Therefore,
unseen-class data should be collected on the way of collecting seen-class data,
which is why they are novel and first need to be clustered.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MagNet: A Neural Network for Directed Graphs. (arXiv:2102.11391v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xitong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yixuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Brugnone_N/0/1/0/all/0/1">Nathan Brugnone</a>, <a href="http://arxiv.org/find/cs/1/au:+Perlmutter_M/0/1/0/all/0/1">Michael Perlmutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirn_M/0/1/0/all/0/1">Matthew Hirn</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11391">
                                    <div class="article-summary-box-inner">
                                        <span>The prevalence of graph-based data has spurred the rapid development of graph
neural networks (GNNs) and related machine learning algorithms. Yet, despite
the many datasets naturally modeled as directed graphs, including citation,
website, and traffic networks, the vast majority of this research focuses on
undirected graphs. In this paper, we propose MagNet, a spectral GNN for
directed graphs based on a complex Hermitian matrix known as the magnetic
Laplacian. This matrix encodes undirected geometric structure in the magnitude
of its entries and directional information in their phase. A &quot;charge&quot; parameter
attunes spectral information to variation among directed cycles. We apply our
network to a variety of directed graph node classification and link prediction
tasks showing that MagNet performs well on all tasks and that its performance
exceeds all other methods on a majority of such tasks. The underlying
principles of MagNet are such that it can be adapted to other spectral GNN
architectures.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Variance Reduced Training with Stratified Sampling for Forecasting Models. (arXiv:2103.02062v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yucheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1">Youngsuk Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lifan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1">Christopher De Sa</a>, <a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1">Dean Foster</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02062">
                                    <div class="article-summary-box-inner">
                                        <span>In large-scale time series forecasting, one often encounters the situation
where the temporal patterns of time series, while drifting over time, differ
from one another in the same dataset. In this paper, we provably show under
such heterogeneity, training a forecasting model with commonly used stochastic
optimizers (e.g. SGD) potentially suffers large variance on gradient
estimation, and thus incurs long-time training. We show that this issue can be
efficiently alleviated via stratification, which allows the optimizer to sample
from pre-grouped time series strata. For better trading-off gradient variance
and computation complexity, we further propose SCott (Stochastic Stratified
Control Variate Gradient Descent), a variance reduced SGD-style optimizer that
utilizes stratified sampling via control variate. In theory, we provide the
convergence guarantee of SCott on smooth non-convex objectives. Empirically, we
evaluate SCott and other baseline optimizers on both synthetic and real-world
time series forecasting problems, and demonstrate SCott converges faster with
respect to both iterations and wall clock time.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Diffusion Asymptotics for Sequential Experiments. (arXiv:2101.09855v3 [math.ST] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Wager_S/0/1/0/all/0/1">Stefan Wager</a>, <a href="http://arxiv.org/find/math/1/au:+Xu_K/0/1/0/all/0/1">Kuang Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09855">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a new diffusion-asymptotic analysis for sequentially randomized
experiments, including those that arise in solving multi-armed bandit problems.
In an experiment with $ n $ time steps, we let the mean reward gaps between
actions scale to the order $1/\sqrt{n}$ so as to preserve the difficulty of the
learning task as $n$ grows. In this regime, we show that the behavior of a
class of sequentially randomized Markov experiments converges to a diffusion
limit, given as the solution of a stochastic differential equation. The
diffusion limit thus enables us to derive refined, instance-specific
characterization of the stochastic dynamics of adaptive experiments. As an
application of this framework, we use the diffusion limit to obtain several new
insights on the regret and belief evolution of Thompson sampling. We show that
a version of Thompson sampling with an asymptotically uninformative prior
variance achieves nearly-optimal instance-specific regret scaling when the
reward gaps are relatively large. We also demonstrate that, in this regime, the
posterior beliefs underlying Thompson sampling are highly unstable over time.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adapting to Misspecification in Contextual Bandits with Offline Regression Oracles. (arXiv:2102.13240v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1">Sanath Kumar Krishnamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadad_V/0/1/0/all/0/1">Vitor Hadad</a>, <a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1">Susan Athey</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.13240">
                                    <div class="article-summary-box-inner">
                                        <span>Computationally efficient contextual bandits are often based on estimating a
predictive model of rewards given contexts and arms using past data. However,
when the reward model is not well-specified, the bandit algorithm may incur
unexpected regret, so recent work has focused on algorithms that are robust to
misspecification. We propose a simple family of contextual bandit algorithms
that adapt to misspecification error by reverting to a good safe policy when
there is evidence that misspecification is causing a regret increase. Our
algorithm requires only an offline regression oracle to ensure regret
guarantees that gracefully degrade in terms of a measure of the average
misspecification level. Compared to prior work, we attain similar regret
guarantees, but we do no rely on a master algorithm, and do not require more
robust oracles like online or constrained regression oracles (e.g., Foster et
al. (2020a); Krishnamurthy et al. (2020)). This allows us to design algorithms
for more general function approximation classes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design. (arXiv:2103.02438v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1">Adam Foster</a>, <a href="http://arxiv.org/find/stat/1/au:+Ivanova_D/0/1/0/all/0/1">Desi R. Ivanova</a>, <a href="http://arxiv.org/find/stat/1/au:+Malik_I/0/1/0/all/0/1">Ilyas Malik</a>, <a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1">Tom Rainforth</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02438">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of
adaptive Bayesian experimental design that allows experiments to be run in
real-time. Traditional sequential Bayesian optimal experimental design
approaches require substantial computation at each stage of the experiment.
This makes them unsuitable for most real-world applications, where decisions
must typically be made quickly. DAD addresses this restriction by learning an
amortized design network upfront and then using this to rapidly run (multiple)
adaptive experiments at deployment time. This network represents a design
policy which takes as input the data from previous steps, and outputs the next
design using a single forward pass; these design decisions can be made in
milliseconds during the live experiment. To train the network, we introduce
contrastive information bounds that are suitable objectives for the sequential
setting, and propose a customized network architecture that exploits key
symmetries. We demonstrate that DAD successfully amortizes the process of
experimental design, outperforming alternative strategies on a number of
problems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Efficient Competitions and Online Learning with Strategic Forecasters. (arXiv:2102.08358v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Frongillo_R/0/1/0/all/0/1">Rafael Frongillo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_R/0/1/0/all/0/1">Robert Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Thilagar_A/0/1/0/all/0/1">Anish Thilagar</a>, <a href="http://arxiv.org/find/cs/1/au:+Waggoner_B/0/1/0/all/0/1">Bo Waggoner</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08358">
                                    <div class="article-summary-box-inner">
                                        <span>Winner-take-all competitions in forecasting and machine-learning suffer from
distorted incentives. Witkowski et al. 2018 identified this problem and
proposed ELF, a truthful mechanism to select a winner. We show that, from a
pool of $n$ forecasters, ELF requires $\Theta(n\log n)$ events or test data
points to select a near-optimal forecaster with high probability. We then show
that standard online learning algorithms select an $\epsilon$-optimal
forecaster using only $O(\log(n) / \epsilon^2)$ events, by way of a strong
approximate-truthfulness guarantee. This bound matches the best possible even
in the nonstrategic setting. We then apply these mechanisms to obtain the first
no-regret guarantee for non-myopic strategic experts.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Lower-Bounded Proper Losses for Weakly Supervised Classification. (arXiv:2103.02893v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Yoshida_S/0/1/0/all/0/1">Shuhei M. Yoshida</a>, <a href="http://arxiv.org/find/stat/1/au:+Takenouchi_T/0/1/0/all/0/1">Takashi Takenouchi</a>, <a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1">Masashi Sugiyama</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02893">
                                    <div class="article-summary-box-inner">
                                        <span>This paper discusses the problem of weakly supervised classification, in
which instances are given weak labels that are produced by some
label-corruption process. The goal is to derive conditions under which loss
functions for weak-label learning are proper and lower-bounded -- two essential
requirements for the losses used in class-probability estimation. To this end,
we derive a representation theorem for proper losses in supervised learning,
which dualizes the Savage representation. We use this theorem to characterize
proper weak-label losses and find a condition for them to be lower-bounded.
From these theoretical findings, we derive a novel regularization scheme called
generalized logit squeezing, which makes any proper weak-label loss bounded
from below, without losing properness. Furthermore, we experimentally
demonstrate the effectiveness of our proposed approach, as compared to improper
or unbounded losses. The results highlight the importance of properness and
lower-boundedness.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards an efficient approach for the nonconvex $\ell_p$ ball projection: algorithm and analysis. (arXiv:2101.01350v3 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Yang_X/0/1/0/all/0/1">Xiangyu Yang</a>, <a href="http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1">Jiashan Wang</a>, <a href="http://arxiv.org/find/math/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.01350">
                                    <div class="article-summary-box-inner">
                                        <span>This paper primarily focuses on computing the Euclidean projection of a
vector onto the $\ell_{p}$ ball in which $p\in(0,1)$. Such a problem emerges as
the core building block in statistical machine learning and signal processing
tasks because of its ability to promote sparsity. However, efficient numerical
algorithms for finding the projections are still not available, particularly in
large-scale optimization. To meet this challenge, we first derive the
first-order necessary optimality conditions of this problem using Fr\&#x27;echet
normal cone. Based on this characterization, we develop a novel numerical
approach for computing the stationary point through solving a sequence of
projections onto the reweighted $\ell_{1}$-balls. This method is practically
simple to implement and computationally efficient. Moreover, the proposed
algorithm is shown to converge uniquely under mild conditions and has a
worst-case $O(1/\sqrt{k})$ convergence rate. Numerical experiments demonstrate
the efficiency of our proposed algorithm.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving Anytime Prediction with Parallel Cascaded Networks and a Temporal-Difference Loss. (arXiv:2102.09808v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Iuzzolino_M/0/1/0/all/0/1">Michael L. Iuzzolino</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1">Michael C. Mozer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1">Samy Bengio</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09808">
                                    <div class="article-summary-box-inner">
                                        <span>Although deep feedforward neural networks share some characteristics with the
primate visual system, a key distinction is their dynamics. Deep nets typically
operate in serial stages wherein each layer completes its computation before
processing begins in subsequent layers. In contrast, biological systems have
cascaded dynamics: information propagates from neurons at all layers in
parallel but transmission occurs gradually over time, leading to speed-accuracy
trade offs even in feedforward architectures. We explore the consequences of
biologically inspired parallel hardware by constructing cascaded ResNets in
which each residual block has propagation delays but all blocks update in
parallel in a stateful manner. Because information transmitted through skip
connections avoids delays, the functional depth of the architecture increases
over time, yielding anytime predictions that improve with internal-processing
time. We introduce a temporal-difference training loss that achieves a strictly
superior speed-accuracy profile over standard losses and enables the cascaded
architecture to outperform state-of-the-art anytime-prediction methods. The
cascaded architecture has intriguing properties, including: it classifies
typical instances more rapidly than atypical instances; it is more robust to
both persistent and transient noise than is a conventional ResNet; and its
time-varying output trace provides a signal that can be exploited to improve
information processing and inference.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MSPM: A Modularized and Scalable Multi-Agent Reinforcement Learning-based System for Financial Portfolio Management. (arXiv:2102.03502v3 [q-fin.PM] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/q-fin/1/au:+Huang_Z/0/1/0/all/0/1">Zhenhan Huang</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Tanaka_F/0/1/0/all/0/1">Fumihide Tanaka</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03502">
                                    <div class="article-summary-box-inner">
                                        <span>Financial portfolio management is one of the most applicable problems in
reinforcement learning (RL) owing to its sequential decision-making nature.
Existing RL-based approaches, while inspiring, often lack scalability,
reusability, or profundity of intake information to accommodate the
ever-changing capital markets. In this paper, we propose MSPM, a modularized
and scalable, multi-agent RL-based system for financial portfolio management.
MSPM involves two asynchronously updated units: an Evolving Agent Module (EAM)
and Strategic Agent Module (SAM). A self-sustained EAM produces
signal-comprised information for a specific asset using heterogeneous data
inputs, and each EAM employs its reusability to have connections to multiple
SAMs. An SAM is responsible for asset reallocation in a portfolio using
profound information from the connected EAMs. With the elaborate architecture
and the multi-step condensation of volatile market information, MSPM aims to
provide a customizable, stable, and dedicated solution to portfolio management,
unlike existing approaches. We also tackle the data-shortage issue of
newly-listed stocks by transfer learning, and validate the indispensability of
EAM with four different portfolios. Experiments on 8-year U.S. stock market
data prove the effectiveness of MSPM in profit accumulation, by its
outperformance over existing benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">UnNatural Language Inference. (arXiv:2101.00010v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1">Koustuv Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1">Prasanna Parthasarathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1">Joelle Pineau</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1">Adina Williams</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00010">
                                    <div class="article-summary-box-inner">
                                        <span>Recent investigations into the inner-workings of state-of-the-art large-scale
pre-trained Transformer-based Natural Language Understanding (NLU) models
indicate that they appear to know humanlike syntax, at least to some extent. We
provide novel evidence that complicates this claim: we find that
state-of-the-art Natural Language Inference (NLI) models assign the same labels
to permuted examples as they do to the original, i.e. they are largely
invariant to random word-order permutations. This behavior notably differs from
that of humans; we struggle with ungrammatical sentences. To measure the
severity of this issue, we propose a suite of metrics and investigate which
properties of particular permutations lead models to be word-order invariant.
In the MNLI dataset, for example, we find almost all (98.7%) examples contain
at least one permutation which elicits the gold label. Models are sometimes
even able to assign gold labels to permutations that they originally failed to
predict correctly. We provide a comprehensive empirical evaluation of this
phenomenon, and further show that this issue exists for both Transformers and
pre-Transformer RNN / ConvNet based encoders, as well as across multiple
languages (English and Mandarin Chinese). Our code and data are available at
https://github.com/facebookresearch/unlu.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Leveraging Public Data for Practical Private Query Release. (arXiv:2102.08598v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Terrance Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Vietri_G/0/1/0/all/0/1">Giuseppe Vietri</a>, <a href="http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1">Thomas Steinke</a>, <a href="http://arxiv.org/find/cs/1/au:+Ullman_J/0/1/0/all/0/1">Jonathan Ullman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiwei Steven Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08598">
                                    <div class="article-summary-box-inner">
                                        <span>In many statistical problems, incorporating priors can significantly improve
performance. However, the use of prior knowledge in differentially private
query release has remained underexplored, despite such priors commonly being
available in the form of public datasets, such as previous US Census releases.
With the goal of releasing statistics about a private dataset, we present
PMW^Pub, which -- unlike existing baselines -- leverages public data drawn from
a related distribution as prior information. We provide a theoretical analysis
and an empirical evaluation on the American Community Survey (ACS) and ADULT
datasets, which shows that our method outperforms state-of-the-art methods.
Furthermore, PMW^Pub scales well to high-dimensional data domains, where
running many existing methods would be computationally infeasible.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-Task Reinforcement Learning with Context-based Representations. (arXiv:2102.06177v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sodhani_S/0/1/0/all/0/1">Shagun Sodhani</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Amy Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1">Joelle Pineau</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06177">
                                    <div class="article-summary-box-inner">
                                        <span>The benefit of multi-task learning over single-task learning relies on the
ability to use relations across tasks to improve performance on any single
task. While sharing representations is an important mechanism to share
information across tasks, its success depends on how well the structure
underlying the tasks is captured. In some real-world situations, we have access
to metadata, or additional information about a task, that may not provide any
new insight in the context of a single task setup alone but inform relations
across multiple tasks. While this metadata can be useful for improving
multi-task learning performance, effectively incorporating it can be an
additional challenge. We posit that an efficient approach to knowledge transfer
is through the use of multiple context-dependent, composable representations
shared across a family of tasks. In this framework, metadata can help to learn
interpretable representations and provide the context to inform which
representations to compose and how to compose them. We use the proposed
approach to obtain state-of-the-art results in Meta-World, a challenging
multi-task benchmark consisting of 50 distinct robotic manipulation tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Finite-Sample Analysis of Off-Policy Natural Actor-Critic Algorithm. (arXiv:2102.09318v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Khodadadian_S/0/1/0/all/0/1">Sajad Khodadadian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zaiwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1">Siva Theja Maguluri</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09318">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we provide finite-sample convergence guarantees for an
off-policy variant of the natural actor-critic (NAC) algorithm based on
Importance Sampling. In particular, we show that the algorithm converges to a
global optimal policy with a sample complexity of
$\mathcal{O}(\epsilon^{-3}\log^2(1/\epsilon))$ under an appropriate choice of
stepsizes. In order to overcome the issue of large variance due to Importance
Sampling, we propose the $Q$-trace algorithm for the critic, which is inspired
by the V-trace algorithm \cite{espeholt2018impala}. This enables us to
explicitly control the bias and variance, and characterize the trade-off
between them. As an advantage of off-policy sampling, a major feature of our
result is that we do not need any additional assumptions, beyond the ergodicity
of the Markov chain induced by the behavior policy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization. (arXiv:2012.14193v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jastrzebski_S/0/1/0/all/0/1">Stanislaw Jastrzebski</a>, <a href="http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1">Devansh Arpit</a>, <a href="http://arxiv.org/find/cs/1/au:+Astrand_O/0/1/0/all/0/1">Oliver Astrand</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerg_G/0/1/0/all/0/1">Giancarlo Kerg</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1">Richard Socher</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1">Kyunghyun Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1">Krzysztof Geras</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14193">
                                    <div class="article-summary-box-inner">
                                        <span>The early phase of training a deep neural network has a dramatic effect on
the local curvature of the loss function. For instance, using a small learning
rate does not guarantee stable optimization because the optimization trajectory
has a tendency to steer towards regions of the loss surface with increasing
local curvature. We ask whether this tendency is connected to the widely
observed phenomenon that the choice of the learning rate strongly influences
generalization. We first show that stochastic gradient descent (SGD) implicitly
penalizes the trace of the Fisher Information Matrix (FIM), a measure of the
local curvature, from the start of training. We argue it is an implicit
regularizer in SGD by showing that explicitly penalizing the trace of the FIM
can significantly improve generalization. We highlight that poor final
generalization coincides with the trace of the FIM attaining a large value
early in training, to which we refer as catastrophic Fisher explosion. Finally,
to gain insight into the regularization effect of penalizing the trace of the
FIM, we show that it limits memorization by reducing the learning speed of
examples with noisy labels more than that of the examples with clean labels.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Particle Dual Averaging: Optimization of Mean Field Neural Networks with Global Convergence Rate Analysis. (arXiv:2012.15477v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1">Atsushi Nitanda</a>, <a href="http://arxiv.org/find/stat/1/au:+Wu_D/0/1/0/all/0/1">Denny Wu</a>, <a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1">Taiji Suzuki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15477">
                                    <div class="article-summary-box-inner">
                                        <span>We propose the particle dual averaging (PDA) method, which generalizes the
dual averaging method in convex optimization to the optimization over
probability distributions with quantitative runtime guarantee. The algorithm
consists of an inner loop and outer loop: the inner loop utilizes the Langevin
algorithm to approximately solve for a stationary distribution, which is then
optimized in the outer loop. The method can thus be interpreted as an extension
of the Langevin algorithm to naturally handle nonlinear functional on the
probability space. An important application of the proposed method is the
optimization of neural network in the mean field regime, which is theoretically
attractive due to the presence of nonlinear feature learning, but quantitative
convergence rate can be challenging to obtain. By adapting finite-dimensional
convex optimization theory into the space of distributions, we analyze PDA in
regularized empirical / expected risk minimization, and establish quantitative
global convergence in learning two-layer mean field neural networks under more
general settings. Our theoretical results are supported by numerical
simulations on neural networks with reasonable size.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Divergence Regulated Encoder Network for Joint Dimensionality Reduction and Classification. (arXiv:2012.15764v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Peeples_J/0/1/0/all/0/1">Joshua Peeples</a>, <a href="http://arxiv.org/find/cs/1/au:+Walker_S/0/1/0/all/0/1">Sarah Walker</a>, <a href="http://arxiv.org/find/cs/1/au:+McCurley_C/0/1/0/all/0/1">Connor McCurley</a>, <a href="http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1">Alina Zare</a>, <a href="http://arxiv.org/find/cs/1/au:+Keller_J/0/1/0/all/0/1">James Keller</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15764">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we investigate performing joint dimensionality reduction and
classification using a novel histogram neural network. Motivated by a popular
dimensionality reduction approach, t-Distributed Stochastic Neighbor Embedding
(t-SNE), our proposed method incorporates a classification loss computed on
samples in a low-dimensional embedding space. We compare the learned sample
embeddings against coordinates found by t-SNE in terms of classification
accuracy and qualitative assessment. We also explore use of various divergence
measures in the t-SNE objective. The proposed method has several advantages
such as readily embedding out-of-sample points and reducing feature
dimensionality while retaining class discriminability. Our results show that
the proposed approach maintains and/or improves classification performance and
reveals characteristics of features produced by neural networks that may be
helpful for other applications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scene-Agnostic Multi-Microphone Speech Dereverberation. (arXiv:2010.11875v2 [eess.AS] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Yemini_Y/0/1/0/all/0/1">Yochai Yemini</a>, <a href="http://arxiv.org/find/eess/1/au:+Fetaya_E/0/1/0/all/0/1">Ethan Fetaya</a>, <a href="http://arxiv.org/find/eess/1/au:+Maron_H/0/1/0/all/0/1">Haggai Maron</a>, <a href="http://arxiv.org/find/eess/1/au:+Gannot_S/0/1/0/all/0/1">Sharon Gannot</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11875">
                                    <div class="article-summary-box-inner">
                                        <span>Neural networks (NNs) have been widely applied in speech processing tasks,
and, in particular, those employing microphone arrays. Nevertheless, most
existing NN architectures can only deal with fixed and position-specific
microphone arrays. In this paper, we present an NN architecture that can cope
with microphone arrays whose number and positions of the microphones are
unknown, and demonstrate its applicability in the speech dereverberation task.
To this end, our approach harnesses recent advances in deep learning on
set-structured data to design an architecture that enhances the reverberant
log-spectrum. We use noisy and noiseless versions of a simulated reverberant
dataset to test the proposed architecture. Our experiments on the noisy data
show that the proposed scene-agnostic setup outperforms a powerful scene-aware
framework, sometimes even with fewer microphones. With the noiseless dataset we
show that, in most cases, our method outperforms the position-aware network as
well as the state-of-the-art weighted linear prediction error (WPE) algorithm.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Nonparametric Learning of Two-Layer ReLU Residual Units. (arXiv:2008.07648v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhunxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Linyun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1">Chunchuan Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1">Shay B. Cohen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.07648">
                                    <div class="article-summary-box-inner">
                                        <span>We describe an algorithm that learns two-layer residual units with rectified
linear unit (ReLU) activation: suppose the input $\mathbf{x}$ is from a
distribution with support space $\mathbb{R}^d$ and the ground-truth generative
model is such a residual unit, given by \[\mathbf{y}&#x3D;
\boldsymbol{B}^\ast\left[\left(\boldsymbol{A}^\ast\mathbf{x}\right)^+ +
\mathbf{x}\right]\text{,}\] where ground-truth network parameters
$\boldsymbol{A}^\ast \in \mathbb{R}^{d\times d}$ is a nonnegative full-rank
matrix and $\boldsymbol{B}^\ast \in \mathbb{R}^{m\times d}$ is full-rank with
$m \geq d$ and for $\mathbf{c} \in \mathbb{R}^d$, $[\mathbf{c}^{+}]_i &#x3D;
\max\{0, c_i\}$. We design layer-wise objectives as functionals whose analytic
minimizers express the exact ground-truth network in terms of its parameters
and nonlinearities. Following this objective landscape, learning residual units
from finite samples can be formulated using convex optimization of a
nonparametric function: for each layer, we first formulate the corresponding
empirical risk minimization (ERM) as a positive semi-definite quadratic program
(QP), then we show the solution space of the QP can be equivalently determined
by a set of linear inequalities, which can then be efficiently solved by linear
programming (LP). We further prove the statistical strong consistency of our
algorithm, and demonstrate the robustness and sample efficiency of our
algorithm by experiments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Simple and Efficient Hard Label Black-box Adversarial Attacks in Low Query Budget Regimes. (arXiv:2007.07210v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1">Satya Narayan Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahu_A/0/1/0/all/0/1">Anit Kumar Sahu</a>, <a href="http://arxiv.org/find/cs/1/au:+Willmott_D/0/1/0/all/0/1">Devin Willmott</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.07210">
                                    <div class="article-summary-box-inner">
                                        <span>We focus on the problem of black-box adversarial attacks, where the aim is to
generate adversarial examples for deep learning models solely based on
information limited to output label~(hard label) to a queried data input. We
propose a simple and efficient Bayesian Optimization~(BO) based approach for
developing black-box adversarial attacks. Issues with BO&#x27;s performance in high
dimensions are avoided by searching for adversarial examples in a structured
low-dimensional subspace. We demonstrate the efficacy of our proposed attack
method by evaluating both $\ell_\infty$ and $\ell_2$ norm constrained
untargeted and targeted hard label black-box attacks on three standard datasets
- MNIST, CIFAR-10 and ImageNet. Our proposed approach consistently achieves 2x
to 10x higher attack success rate while requiring 10x to 20x fewer queries
compared to the current state-of-the-art black-box adversarial attacks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Distributional Soft Actor-Critic: Off-Policy Reinforcement Learning for Addressing Value Estimation Errors. (arXiv:2001.02811v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jingliang Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1">Yang Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengbo Eben Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yangang Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1">Bo Cheng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.02811">
                                    <div class="article-summary-box-inner">
                                        <span>In reinforcement learning (RL), function approximation errors are known to
easily lead to the Q-value overestimations, thus greatly reducing policy
performance. This paper presents a distributional soft actor-critic (DSAC)
algorithm, which is an off-policy RL method for continuous control setting, to
improve the policy performance by mitigating Q-value overestimations. We first
discover in theory that learning a distribution function of state-action
returns can effectively mitigate Q-value overestimations because it is capable
of adaptively adjusting the update stepsize of the Q-value function. Then, a
distributional soft policy iteration (DSPI) framework is developed by embedding
the return distribution function into maximum entropy RL. Finally, we present a
deep off-policy actor-critic variant of DSPI, called DSAC, which directly
learns a continuous return distribution by keeping the variance of the
state-action returns within a reasonable range to address exploding and
vanishing gradient problems. We evaluate DSAC on the suite of MuJoCo continuous
control tasks, achieving the state-of-the-art performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Can we have it all? On the Trade-off between Spatial and Adversarial Robustness of Neural Networks. (arXiv:2002.11318v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kamath_S/0/1/0/all/0/1">Sandesh Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1">Amit Deshpande</a>, <a href="http://arxiv.org/find/cs/1/au:+Subrahmanyam_K/0/1/0/all/0/1">K V Subrahmanyam</a>, <a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1">Vineeth N Balasubramanian</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.11318">
                                    <div class="article-summary-box-inner">
                                        <span>(Non-)robustness of neural networks to small, adversarial pixel-wise
perturbations, and as more recently shown, to even random spatial
transformations (e.g., translations, rotations) entreats both theoretical and
empirical understanding. Spatial robustness to random translations and
rotations is commonly attained via equivariant models (e.g., StdCNNs, GCNNs)
and training augmentation, whereas adversarial robustness is typically achieved
by adversarial training. In this paper, we prove a quantitative trade-off
between spatial and adversarial robustness in a simple statistical setting. We
complement this empirically by showing that: (a) as the spatial robustness of
equivariant models improves by training augmentation with progressively larger
transformations, their adversarial robustness worsens progressively, and (b) as
the state-of-the-art robust models are adversarially trained with progressively
larger pixel-wise perturbations, their spatial robustness drops progressively.
Towards achieving pareto-optimality in this trade-off, we propose a method
based on curriculum learning that trains gradually on more difficult
perturbations (both spatial and adversarial) to improve spatial and adversarial
robustness simultaneously.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Inference of Causal Effects when Control Variables are Unknown. (arXiv:2012.08154v3 [stat.ME] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Hult_L/0/1/0/all/0/1">Ludvig Hult</a>, <a href="http://arxiv.org/find/stat/1/au:+Zachariah_D/0/1/0/all/0/1">Dave Zachariah</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08154">
                                    <div class="article-summary-box-inner">
                                        <span>Conventional methods in causal effect inferencetypically rely on specifying a
valid set of control variables. When this set is unknown or misspecified,
inferences will be erroneous. We propose a method for inferring average causal
effects when all potential confounders are observed, but thecontrol variables
are unknown. When the data-generating process belongs to the class of acyclical
linear structural causal models, we prove that themethod yields asymptotically
valid confidence intervals. Our results build upon a smooth characterization of
linear directed acyclic graphs. We verify the capability of the method to
produce valid confidence intervals for average causal effects using synthetic
data, even when the appropriate specification of control variables is unknown.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hutch++: Optimal Stochastic Trace Estimation. (arXiv:2010.09649v5 [cs.DS] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Meyer_R/0/1/0/all/0/1">Raphael A. Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1">Cameron Musco</a>, <a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1">Christopher Musco</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09649">
                                    <div class="article-summary-box-inner">
                                        <span>We study the problem of estimating the trace of a matrix $A$ that can only be
accessed through matrix-vector multiplication. We introduce a new randomized
algorithm, Hutch++, which computes a $(1 \pm \epsilon)$ approximation to
$tr(A)$ for any positive semidefinite (PSD) $A$ using just $O(1/\epsilon)$
matrix-vector products. This improves on the ubiquitous Hutchinson&#x27;s estimator,
which requires $O(1/\epsilon^2)$ matrix-vector products. Our approach is based
on a simple technique for reducing the variance of Hutchinson&#x27;s estimator using
a low-rank approximation step, and is easy to implement and analyze. Moreover,
we prove that, up to a logarithmic factor, the complexity of Hutch++ is optimal
amongst all matrix-vector query algorithms, even when queries can be chosen
adaptively. We show that it significantly outperforms Hutchinson&#x27;s method in
experiments. While our theory mainly requires $A$ to be positive semidefinite,
we provide generalized guarantees for general square matrices, and show
empirical gains in such applications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Network Approximation for Smooth Functions. (arXiv:2001.03040v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jianfeng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zuowei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Haizhao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shijun Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.03040">
                                    <div class="article-summary-box-inner">
                                        <span>This paper establishes optimal approximation error characterization of deep
ReLU networks for smooth functions in terms of both width and depth
simultaneously. To that end, we first prove that multivariate polynomials can
be approximated by deep ReLU networks of width $\mathcal{O}(N)$ and depth
$\mathcal{O}(L)$ with an approximation error $\mathcal{O}(N^{-L})$. Through
local Taylor expansions and their deep ReLU network approximations, we show
that deep ReLU networks of width $\mathcal{O}(N\ln N)$ and depth
$\mathcal{O}(L\ln L)$ can approximate $f\in C^s([0,1]^d)$ with a nearly optimal
approximation rate $\mathcal{O}(\|f\|_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})$. Our
estimate is non-asymptotic in the sense that it is valid for arbitrary width
and depth specified by $N\in\mathbb{N}^+$ and $L\in\mathbb{N}^+$, respectively.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Value Alignment Verification. (arXiv:2012.01557v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Daniel S. Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Jordan Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca D. Dragan</a>, <a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1">Scott Niekum</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01557">
                                    <div class="article-summary-box-inner">
                                        <span>As humans interact with autonomous agents to perform increasingly
complicated, potentially risky tasks, it is important to be able to efficiently
evaluate an agent&#x27;s performance and correctness. In this paper we formalize and
theoretically analyze the problem of efficient value alignment verification:
how to efficiently test whether the behavior of another agent is aligned with a
human&#x27;s values. The goal is to construct a kind of &quot;driver&#x27;s test&quot; that a human
can give to any agent which will verify value alignment via a minimal number of
queries. We study alignment verification problems with both idealized humans
that have an explicit reward function as well as problems where they have
implicit values. We analyze verification of exact value alignment for rational
agents and propose and analyze heuristic and approximate value alignment
verification tests in a wide range of gridworlds and a continuous autonomous
driving domain. Finally, we prove that there exist sufficient conditions such
that we can verify exact and approximate alignment across an infinite set of
test environments via a constant-query-complexity alignment test.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Knowledge Distillation Ensemble Framework for Predicting Short and Long-term Hospitalisation Outcomes from Electronic Health Records Data. (arXiv:2011.09361v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1">Zina M Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Bean_D/0/1/0/all/0/1">Daniel Bean</a>, <a href="http://arxiv.org/find/cs/1/au:+Searle_T/0/1/0/all/0/1">Thomas Searle</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Honghan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shek_A/0/1/0/all/0/1">Anthony Shek</a>, <a href="http://arxiv.org/find/cs/1/au:+Kraljevic_Z/0/1/0/all/0/1">Zeljko Kraljevic</a>, <a href="http://arxiv.org/find/cs/1/au:+Galloway_J/0/1/0/all/0/1">James Galloway</a>, <a href="http://arxiv.org/find/cs/1/au:+Norton_S/0/1/0/all/0/1">Sam Norton</a>, <a href="http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1">James T Teo</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1">Richard JB Dobson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09361">
                                    <div class="article-summary-box-inner">
                                        <span>The ability to perform accurate prognosis of patients is crucial for
proactive clinical decision making, informed resource management and
personalised care. Existing outcome prediction models suffer from a low recall
of infrequent positive outcomes. We present a highly-scalable and robust
machine learning framework to automatically predict adversity represented by
mortality and ICU admission from time-series vital signs and laboratory results
obtained within the first 24 hours of hospital admission. The stacked platform
comprises two components: a) an unsupervised LSTM Autoencoder that learns an
optimal representation of the time-series, using it to differentiate the less
frequent patterns which conclude with an adverse event from the majority
patterns that do not, and b) a gradient boosting model, which relies on the
constructed representation to refine prediction, incorporating static features
of demographics, admission details and clinical summaries. The model is used to
assess a patient&#x27;s risk of adversity over time and provides visual
justifications of its prediction based on the patient&#x27;s static features and
dynamic signals. Results of three case studies for predicting mortality and ICU
admission show that the model outperforms all existing outcome prediction
models, achieving PR-AUC of 0.891 (95$%$ CI: 0.878 - 0.969) in predicting
mortality in ICU and general ward settings and 0.908 (95$%$ CI: 0.870-0.935) in
predicting ICU admission.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Possibility results for graph clustering: A novel consistency axiom. (arXiv:1806.06142v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Strazzeri_F/0/1/0/all/0/1">Fabio Strazzeri</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_Garcia_R/0/1/0/all/0/1">Rub&#xe9;n J. S&#xe1;nchez-Garc&#xed;a</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1806.06142">
                                    <div class="article-summary-box-inner">
                                        <span>Kleinberg introduced three natural clustering properties, or axioms, and
showed they cannot be simultaneously satisfied by any clustering algorithm. We
present a new clustering property, Monotonic Consistency, which avoids the
well-known problematic behaviour of Kleinberg&#x27;s Consistency axiom, and the
impossibility result. Namely, we describe a clustering algorithm, Morse
Clustering, inspired by Morse Theory in Differential Topology, which satisfies
Kleinberg&#x27;s original axioms with Consistency replaced by Monotonic Consistency.
Morse clustering uncovers the underlying flow structure on a set or graph and
returns a partition into trees representing basins of attraction of critical
vertices. We also generalise Kleinberg&#x27;s axiomatic approach to sparse graphs,
showing an impossibility result for Consistency, and a possibility result for
Monotonic Consistency and Morse clustering.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Distribution-Dependent Analysis of Meta-Learning. (arXiv:2011.00344v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Konobeev_M/0/1/0/all/0/1">Mikhail Konobeev</a>, <a href="http://arxiv.org/find/stat/1/au:+Kuzborskij_I/0/1/0/all/0/1">Ilja Kuzborskij</a>, <a href="http://arxiv.org/find/stat/1/au:+Szepesvari_C/0/1/0/all/0/1">Csaba Szepesv&#xe1;ri</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00344">
                                    <div class="article-summary-box-inner">
                                        <span>A key problem in the theory of meta-learning is to understand how the task
distributions influence transfer risk, the expected error of a meta-learner on
a new task drawn from the unknown task distribution. In this paper, focusing on
fixed design linear regression with Gaussian noise and a Gaussian task (or
parameter) distribution, we give distribution-dependent lower bounds on the
transfer risk of any algorithm, while we also show that a novel, weighted
version of the so-called biased regularized regression method is able to match
these lower bounds up to a fixed constant factor. Notably, the weighting is
derived from the covariance of the Gaussian task distribution. Altogether, our
results provide a precise characterization of the difficulty of meta-learning
in this Gaussian setting. While this problem setting may appear simple, we show
that it is rich enough to unify the &quot;parameter sharing&quot; and &quot;representation
learning&quot; streams of meta-learning; in particular, representation learning is
obtained as the special case when the covariance matrix of the task
distribution is unknown. For this case we propose to adopt the EM method, which
is shown to enjoy efficient updates in our case. The paper is completed by an
empirical study of EM. In particular, our experimental results show that the EM
algorithm can attain the lower bound as the number of tasks grows, while the
algorithm is also successful in competing with its alternatives when used in a
representation learning context.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Represent Your Own Policies: Reinforcement Learning with Policy-extended Value Function Approximator. (arXiv:2010.09536v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1">Hongyao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1">Zhaopeng Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jianye Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Graves_D/0/1/0/all/0/1">Daniel Graves</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1">Hangyu Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wulong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Changmin Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09536">
                                    <div class="article-summary-box-inner">
                                        <span>We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement
Learning (RL), which extends conventional value function approximator (VFA) to
take as input not only the state (and action) but also an explicit policy
representation. Such an extension enables PeVFA to preserve values of multiple
policies at the same time and brings an appealing characteristic, i.e.,
\emph{value generalization among policies}. We formally analyze the value
generalization under Generalized Policy Iteration (GPI). From theoretical and
empirical lens, we show that generalized value estimates offered by PeVFA may
have lower initial approximation error to true values of successive policies,
which is expected to improve consecutive value approximation during GPI. Based
on above clues, we introduce a new form of GPI with PeVFA which leverages the
value generalization along policy improvement path. Moreover, we propose a
representation learning framework for RL policy, providing several approaches
to learn effective policy embeddings from policy network parameters or
state-action pairs. In our experiments, we evaluate the efficacy of value
generalization offered by PeVFA and policy representation learning in several
OpenAI Gym continuous control tasks. For a representative instance of algorithm
implementation, Proximal Policy Optimization (PPO) re-implemented under the
paradigm of GPI with PeVFA achieves about 40\% performance improvement on its
vanilla counterpart in most environments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">K-shot NAS: Learnable Weight-Sharing for NAS with K-shot Supernets. (arXiv:2106.06442v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1">Xiu Su</a>, <a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1">Shan You</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Mingkai Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Changshui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chang Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06442">
                                    <div class="article-summary-box-inner">
                                        <span>In one-shot weight sharing for NAS, the weights of each operation (at each
layer) are supposed to be identical for all architectures (paths) in the
supernet. However, this rules out the possibility of adjusting operation
weights to cater for different paths, which limits the reliability of the
evaluation results. In this paper, instead of counting on a single supernet, we
introduce $K$-shot supernets and take their weights for each operation as a
dictionary. The operation weight for each path is represented as a convex
combination of items in a dictionary with a simplex code. This enables a matrix
approximation of the stand-alone weight matrix with a higher rank ($K&gt;1$). A
\textit{simplex-net} is introduced to produce architecture-customized code for
each path. As a result, all paths can adaptively learn how to share weights in
the $K$-shot supernets and acquire corresponding weights for better evaluation.
$K$-shot supernets and simplex-net can be iteratively trained, and we further
extend the search to the channel dimension. Extensive experiments on benchmark
datasets validate that K-shot NAS significantly improves the evaluation
accuracy of paths and thus brings in impressive performance improvements.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Black-box Explanation of Object Detectors via Saliency Maps. (arXiv:2006.03204v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Petsiuk_V/0/1/0/all/0/1">Vitali Petsiuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Rajiv Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Manjunatha_V/0/1/0/all/0/1">Varun Manjunatha</a>, <a href="http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1">Vlad I. Morariu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1">Ashutosh Mehra</a>, <a href="http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1">Vicente Ordonez</a>, <a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1">Kate Saenko</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.03204">
                                    <div class="article-summary-box-inner">
                                        <span>We propose D-RISE, a method for generating visual explanations for the
predictions of object detectors. Utilizing the proposed similarity metric that
accounts for both localization and categorization aspects of object detection
allows our method to produce saliency maps that show image areas that most
affect the prediction. D-RISE can be considered &quot;black-box&quot; in the software
testing sense, as it only needs access to the inputs and outputs of an object
detector. Compared to gradient-based methods, D-RISE is more general and
agnostic to the particular type of object detector being tested, and does not
need knowledge of the inner workings of the model. We show that D-RISE can be
easily applied to different object detectors including one-stage detectors such
as YOLOv3 and two-stage detectors such as Faster-RCNN. We present a detailed
analysis of the generated visual explanations to highlight the utilization of
context and possible biases learned by object detectors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning the optimal regularizer for inverse problems. (arXiv:2106.06513v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Alberti_G/0/1/0/all/0/1">Giovanni S. Alberti</a>, <a href="http://arxiv.org/find/stat/1/au:+Vito_E/0/1/0/all/0/1">Ernesto De Vito</a>, <a href="http://arxiv.org/find/stat/1/au:+Lassas_M/0/1/0/all/0/1">Matti Lassas</a>, <a href="http://arxiv.org/find/stat/1/au:+Ratti_L/0/1/0/all/0/1">Luca Ratti</a>, <a href="http://arxiv.org/find/stat/1/au:+Santacesaria_M/0/1/0/all/0/1">Matteo Santacesaria</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06513">
                                    <div class="article-summary-box-inner">
                                        <span>In this work, we consider the linear inverse problem $y&#x3D;Ax+\epsilon$, where
$A\colon X\to Y$ is a known linear operator between the separable Hilbert
spaces $X$ and $Y$, $x$ is a random variable in $X$ and $\epsilon$ is a
zero-mean random process in $Y$. This setting covers several inverse problems
in imaging including denoising, deblurring, and X-ray tomography. Within the
classical framework of regularization, we focus on the case where the
regularization functional is not given a priori but learned from data. Our
first result is a characterization of the optimal generalized Tikhonov
regularizer, with respect to the mean squared error. We find that it is
completely independent of the forward operator $A$ and depends only on the mean
and covariance of $x$. Then, we consider the problem of learning the
regularizer from a finite training set in two different frameworks: one
supervised, based on samples of both $x$ and $y$, and one unsupervised, based
only on samples of $x$. In both cases, we prove generalization bounds, under
some weak assumptions on the distribution of $x$ and $\epsilon$, including the
case of sub-Gaussian variables. Our bounds hold in infinite-dimensional spaces,
thereby showing that finer and finer discretizations do not make this learning
problem harder. The results are validated through numerical simulations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis. (arXiv:2010.05690v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+B_L/0/1/0/all/0/1">Lalith Bharadwaj B</a>, <a href="http://arxiv.org/find/cs/1/au:+Boddeda_R/0/1/0/all/0/1">Rohit Boddeda</a>, <a href="http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1">Sai Vardhan K</a>, <a href="http://arxiv.org/find/cs/1/au:+G_M/0/1/0/all/0/1">Madhu G</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05690">
                                    <div class="article-summary-box-inner">
                                        <span>The issue of COVID-19, increasing with a massive mortality rate. This led to
the WHO declaring it as a pandemic. In this situation, it is crucial to perform
efficient and fast diagnosis. The reverse transcript polymerase chain reaction
(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is
time-consuming and instead chest CT (or Chest X-ray) can be used for a fast and
accurate diagnosis. Automated diagnosis is considered to be important as it
reduces human effort and provides accurate and low-cost tests. The
contributions of our research are three-fold. First, it is aimed to analyse the
behaviour and performance of variant vision models ranging from Inception to
NAS networks with the appropriate fine-tuning procedure. Second, the behaviour
of these models is visually analysed by plotting CAMs for individual networks
and determining classification performance with AUCROC curves. Thirdly, stacked
ensembles techniques are imparted to provide higher generalisation on combining
the fine-tuned models, in which six ensemble neural networks are designed by
combining the existing fine-tuned networks. Implying these stacked ensembles
provides a great generalization to the models. The ensemble model designed by
combining all the fine-tuned networks obtained a state-of-the-art accuracy
score of 99.17%. The precision and recall for the COVID-19 class are 99.99% and
89.79% respectively, which resembles the robustness of the stacked ensembles.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards a Unified Quadrature Framework for Large-Scale Kernel Machines. (arXiv:2011.01668v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fanghui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaolin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yudong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1">Johan A.K. Suykens</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.01668">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we develop a quadrature framework for large-scale kernel
machines via a numerical integration representation. Considering that the
integration domain and measure of typical kernels, e.g., Gaussian kernels,
arc-cosine kernels, are fully symmetric, we leverage deterministic fully
symmetric interpolatory rules to efficiently compute quadrature nodes and
associated weights for kernel approximation. The developed interpolatory rules
are able to reduce the number of needed nodes while retaining a high
approximation accuracy. Further, we randomize the above deterministic rules by
the classical Monte-Carlo sampling and control variates techniques with two
merits: 1) The proposed stochastic rules make the dimension of the feature
mapping flexibly varying, such that we can control the discrepancy between the
original and approximate kernels by tuning the dimnension. 2) Our stochastic
rules have nice statistical properties of unbiasedness and variance reduction
with fast convergence rate. In addition, we elucidate the relationship between
our deterministic/stochastic interpolatory rules and current quadrature rules
for kernel approximation, including the sparse grids quadrature and stochastic
spherical-radial rules, thereby unifying these methods under our framework.
Experimental results on several benchmark datasets show that our methods
compare favorably with other representative kernel approximation based methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Compositional Shape Priors for Few-Shot 3D Reconstruction. (arXiv:2106.06440v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Michalkiewicz_M/0/1/0/all/0/1">Mateusz Michalkiewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsogkas_S/0/1/0/all/0/1">Stavros Tsogkas</a>, <a href="http://arxiv.org/find/cs/1/au:+Parisot_S/0/1/0/all/0/1">Sarah Parisot</a>, <a href="http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1">Mahsa Baktashmotlagh</a>, <a href="http://arxiv.org/find/cs/1/au:+Eriksson_A/0/1/0/all/0/1">Anders Eriksson</a>, <a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1">Eugene Belilovsky</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06440">
                                    <div class="article-summary-box-inner">
                                        <span>The impressive performance of deep convolutional neural networks in
single-view 3D reconstruction suggests that these models perform non-trivial
reasoning about the 3D structure of the output space. Recent work has
challenged this belief, showing that, on standard benchmarks, complex
encoder-decoder architectures perform similarly to nearest-neighbor baselines
or simple linear decoder models that exploit large amounts of per-category
data. However, building large collections of 3D shapes for supervised training
is a laborious process; a more realistic and less constraining task is
inferring 3D shapes for categories with few available training examples,
calling for a model that can successfully generalize to novel object classes.
In this work we experimentally demonstrate that naive baselines fail in this
few-shot learning setting, in which the network must learn informative shape
priors for inference of new categories. We propose three ways to learn a
class-specific global shape prior, directly from data. Using these techniques,
we are able to capture multi-scale information about the 3D shape, and account
for intra-class variability by virtue of an implicit compositional structure.
Experiments on the popular ShapeNet dataset show that our method outperforms a
zero-shot baseline by over 40%, and the current state-of-the-art by over 10%,
in terms of relative performance, in the few-shot setting.12</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SPPL: Probabilistic Programming with Fast Exact Symbolic Inference. (arXiv:2010.03485v3 [cs.PL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Saad_F/0/1/0/all/0/1">Feras A. Saad</a>, <a href="http://arxiv.org/find/cs/1/au:+Rinard_M/0/1/0/all/0/1">Martin C. Rinard</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansinghka_V/0/1/0/all/0/1">Vikash K. Mansinghka</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03485">
                                    <div class="article-summary-box-inner">
                                        <span>We present the Sum-Product Probabilistic Language (SPPL), a new probabilistic
programming language that automatically delivers exact solutions to a broad
range of probabilistic inference queries. SPPL translates probabilistic
programs into sum-product expressions, a new symbolic representation and
associated semantic domain that extends standard sum-product networks to
support mixed-type distributions, numeric transformations, logical formulas,
and pointwise and set-valued constraints. We formalize SPPL via a novel
translation strategy from probabilistic programs to sum-product expressions and
give sound exact algorithms for conditioning on and computing probabilities of
events. SPPL imposes a collection of restrictions on probabilistic programs to
ensure they can be translated into sum-product expressions, which allow the
system to leverage new techniques for improving the scalability of translation
and inference by automatically exploiting probabilistic structure. We implement
a prototype of SPPL with a modular architecture and evaluate it on benchmarks
the system targets, showing that it obtains up to 3500x speedups over
state-of-the-art symbolic systems on tasks such as verifying the fairness of
decision tree classifiers, smoothing hidden Markov models, conditioning
transformed random variables, and computing rare event probabilities.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Evaluating Robustness of Predictive Uncertainty Estimation: Are Dirichlet-based Models Reliable?. (arXiv:2010.14986v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kopetzki_A/0/1/0/all/0/1">Anna-Kathrin Kopetzki</a>, <a href="http://arxiv.org/find/cs/1/au:+Charpentier_B/0/1/0/all/0/1">Bertrand Charpentier</a>, <a href="http://arxiv.org/find/cs/1/au:+Zugner_D/0/1/0/all/0/1">Daniel Z&#xfc;gner</a>, <a href="http://arxiv.org/find/cs/1/au:+Giri_S/0/1/0/all/0/1">Sandhya Giri</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.14986">
                                    <div class="article-summary-box-inner">
                                        <span>Dirichlet-based uncertainty (DBU) models are a recent and promising class of
uncertainty-aware models. DBU models predict the parameters of a Dirichlet
distribution to provide fast, high-quality uncertainty estimates alongside with
class predictions. In this work, we present the first large-scale, in-depth
study of the robustness of DBU models under adversarial attacks. Our results
suggest that uncertainty estimates of DBU models are not robust w.r.t. three
important tasks: (1) indicating correctly and wrongly classified samples; (2)
detecting adversarial examples; and (3) distinguishing between in-distribution
(ID) and out-of-distribution (OOD) data. Additionally, we explore the first
approaches to make DBU models more robust. While adversarial training has a
minor effect, our median smoothing based approach significantly increases
robustness of DBU models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training. (arXiv:2009.03294v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Tianle Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Shengjie Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Keyulu Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1">Di He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tie-Yan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.03294">
                                    <div class="article-summary-box-inner">
                                        <span>Normalization is known to help the optimization of deep neural networks.
Curiously, different architectures require specialized normalization methods.
In this paper, we study what normalization is effective for Graph Neural
Networks (GNNs). First, we adapt and evaluate the existing methods from other
domains to GNNs. Faster convergence is achieved with InstanceNorm compared to
BatchNorm and LayerNorm. We provide an explanation by showing that InstanceNorm
serves as a preconditioner for GNNs, but such preconditioning effect is weaker
with BatchNorm due to the heavy batch noise in graph datasets. Second, we show
that the shift operation in InstanceNorm results in an expressiveness
degradation of GNNs for highly regular graphs. We address this issue by
proposing GraphNorm with a learnable shift. Empirically, GNNs with GraphNorm
converge faster compared to GNNs using other normalization. GraphNorm also
improves the generalization of GNNs, achieving better performance on graph
classification benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Optimal Model Selection in Contextual Bandits with Many Classes via Offline Oracles. (arXiv:2106.06483v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1">Sanath Kumar Krishnamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1">Susan Athey</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06483">
                                    <div class="article-summary-box-inner">
                                        <span>We study the problem of model selection for contextual bandits, in which the
algorithm must balance the bias-variance trade-off for model estimation while
also balancing the exploration-exploitation trade-off. In this paper, we
propose the first reduction of model selection in contextual bandits to offline
model selection oracles, allowing for flexible general purpose algorithms with
computational requirements no worse than those for model selection for
regression. Our main result is a new model selection guarantee for stochastic
contextual bandits. When one of the classes in our set is realizable, up to a
logarithmic dependency on the number of classes, our algorithm attains optimal
realizability-based regret bounds for that class under one of two conditions:
if the time-horizon is large enough, or if an assumption that helps with
detecting misspecification holds. Hence our algorithm adapts to the complexity
of this unknown class. Even when this realizable class is known, we prove
improved regret guarantees in early rounds by relying on simpler model classes
for those rounds and hence further establish the importance of model selection
in contextual bandits.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Architecture Search without Training. (arXiv:2006.04647v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mellor_J/0/1/0/all/0/1">Joseph Mellor</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_J/0/1/0/all/0/1">Jack Turner</a>, <a href="http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1">Amos Storkey</a>, <a href="http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1">Elliot J. Crowley</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04647">
                                    <div class="article-summary-box-inner">
                                        <span>The time and effort involved in hand-designing deep neural networks is
immense. This has prompted the development of Neural Architecture Search (NAS)
techniques to automate this design. However, NAS algorithms tend to be slow and
expensive; they need to train vast numbers of candidate networks to inform the
search process. This could be alleviated if we could partially predict a
network&#x27;s trained accuracy from its initial state. In this work, we examine the
overlap of activations between datapoints in untrained networks and motivate
how this can give a measure which is usefully indicative of a network&#x27;s trained
performance. We incorporate this measure into a simple algorithm that allows us
to search for powerful networks without any training in a matter of seconds on
a single GPU, and verify its effectiveness on NAS-Bench-101, NAS-Bench-201,
NATS-Bench, and Network Design Spaces. Our approach can be readily combined
with more expensive search methods; we examine a simple adaptation of
regularised evolutionary search. Code for reproducing our experiments is
available at https://github.com/BayesWatch/nas-without-training.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PAC-Learning for Strategic Classification. (arXiv:2012.03310v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sundaram_R/0/1/0/all/0/1">Ravi Sundaram</a>, <a href="http://arxiv.org/find/cs/1/au:+Vullikanti_A/0/1/0/all/0/1">Anil Vullikanti</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haifeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1">Fan Yao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03310">
                                    <div class="article-summary-box-inner">
                                        <span>The study of strategic or adversarial manipulation of testing data to fool a
classifier has attracted much recent attention. Most previous works have
focused on two extreme situations where any testing data point either is
completely adversarial or always equally prefers the positive label. In this
paper, we generalize both of these through a unified framework for strategic
classification, and introduce the notion of strategic VC-dimension (SVC) to
capture the PAC-learnability in our general strategic setup. SVC provably
generalizes the recent concept of adversarial VC-dimension (AVC) introduced by
Cullina et al. arXiv:1806.01471. We instantiate our framework for the
fundamental strategic linear classification problem. We fully characterize: (1)
the statistical learnability of linear classifiers by pinning down its SVC; (2)
its computational tractability by pinning down the complexity of the empirical
risk minimization problem. Interestingly, the SVC of linear classifiers is
always upper bounded by its standard VC-dimension. This characterization also
strictly generalizes the AVC bound for linear classifiers in arXiv:1806.01471.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Limitations of Large Width in Neural Networks: A Deep Gaussian Process Perspective. (arXiv:2106.06529v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1">Geoff Pleiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1">John P. Cunningham</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06529">
                                    <div class="article-summary-box-inner">
                                        <span>Large width limits have been a recent focus of deep learning research: modulo
computational practicalities, do wider networks outperform narrower ones?
Answering this question has been challenging, as conventional networks gain
representational power with width, potentially masking any negative effects.
Our analysis in this paper decouples capacity and width via the generalization
of neural networks to Deep Gaussian Processes (Deep GP), a class of
hierarchical models that subsume neural nets. In doing so, we aim to understand
how width affects standard neural networks once they have sufficient capacity
for a given modeling task. Our theoretical and empirical results on Deep GP
suggest that large width is generally detrimental to hierarchical models.
Surprisingly, we prove that even nonparametric Deep GP converge to Gaussian
processes, effectively becoming shallower without any increase in
representational power. The posterior, which corresponds to a mixture of
data-adaptable basis functions, becomes less data-dependent with width. Our
tail analysis demonstrates that width and depth have opposite effects: depth
accentuates a model&#x27;s non-Gaussianity, while width makes models increasingly
Gaussian. We find there is a &quot;sweet spot&quot; that maximizes test set performance
before the limiting GP behavior prevents adaptability, occurring at width &#x3D; 1
or width &#x3D; 2 for nonparametric Deep GP. These results make strong predictions
about the same phenomenon in conventional neural networks: we show empirically
that many neural network architectures need 10 - 500 hidden units for
sufficient capacity - depending on the dataset - but further width degrades
test performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hurricane Forecasting: A Novel Multimodal Machine Learning Framework. (arXiv:2011.06125v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Boussioux_L/0/1/0/all/0/1">L&#xe9;onard Boussioux</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1">Cynthia Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guenais_T/0/1/0/all/0/1">Th&#xe9;o Gu&#xe9;nais</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1">Dimitris Bertsimas</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06125">
                                    <div class="article-summary-box-inner">
                                        <span>This paper describes a machine learning (ML) framework for tropical cyclone
intensity and track forecasting, combining multiple distinct ML techniques and
utilizing diverse data sources. Our framework, which we refer to as Hurricast
(HURR), is built upon the combination of distinct data processing techniques
using gradient-boosted trees and novel encoder-decoder architectures, including
CNN, GRU and Transformers components. We propose a deep-feature extractor
methodology to mix spatial-temporal data with statistical data efficiently. Our
multimodal framework unleashes the potential of making forecasts based on a
wide range of data sources, including historical storm data, and visual data
such as reanalysis atmospheric images. We evaluate our models with current
operational forecasts in North Atlantic and Eastern Pacific basins on 2016-2019
for 24-hour lead time, and show our models consistently outperform
statistical-dynamical models and compete with the best dynamical models, while
computing forecasts in seconds. Furthermore, the inclusion of Hurricast into an
operational forecast consensus model leads to a significant improvement of 5% -
15% over NHC&#x27;s official forecast, thus highlighting the complementary
properties with existing approaches. In summary, our work demonstrates that
combining different data sources and distinct machine learning methodologies
can lead to superior tropical cyclone forecasting. We hope that this work opens
the door for further use of machine learning in meteorological forecasting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Visualizing Classifier Adjacency Relations: A Case Study in Speaker Verification and Voice Anti-Spoofing. (arXiv:2106.06362v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kinnunen_T/0/1/0/all/0/1">Tomi Kinnunen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nautsch_A/0/1/0/all/0/1">Andreas Nautsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Sahidullah_M/0/1/0/all/0/1">Md Sahidullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1">Nicholas Evans</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1">Massimiliano Todisco</a>, <a href="http://arxiv.org/find/cs/1/au:+Delgado_H/0/1/0/all/0/1">H&#xe9;ctor Delgado</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1">Junichi Yamagishi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kong Aik Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06362">
                                    <div class="article-summary-box-inner">
                                        <span>Whether it be for results summarization, or the analysis of classifier
fusion, some means to compare different classifiers can often provide
illuminating insight into their behaviour, (dis)similarity or complementarity.
We propose a simple method to derive 2D representation from detection scores
produced by an arbitrary set of binary classifiers in response to a common
dataset. Based upon rank correlations, our method facilitates a visual
comparison of classifiers with arbitrary scores and with close relation to
receiver operating characteristic (ROC) and detection error trade-off (DET)
analyses. While the approach is fully versatile and can be applied to any
detection task, we demonstrate the method using scores produced by automatic
speaker verification and voice anti-spoofing systems. The former are produced
by a Gaussian mixture model system trained with VoxCeleb data whereas the
latter stem from submissions to the ASVspoof 2019 challenge.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Nystr\&quot;om landmark sampling and regularized Christoffel functions. (arXiv:1905.12346v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fanuel_M/0/1/0/all/0/1">Micha&#xeb;l Fanuel</a>, <a href="http://arxiv.org/find/cs/1/au:+Schreurs_J/0/1/0/all/0/1">Joachim Schreurs</a>, <a href="http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1">Johan A.K. Suykens</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.12346">
                                    <div class="article-summary-box-inner">
                                        <span>Selecting diverse and important items, called landmarks, from a large set is
a problem of interest in machine learning. As a specific example, in order to
deal with large training sets, kernel methods often rely on low rank matrix
Nystr\&quot;om approximations based on the selection or sampling of landmarks. In
this context, we propose a deterministic and a randomized adaptive algorithm
for selecting landmark points within a training data set, which are related to
the minima of a sequence of kernelized Christoffel functions. Beyond the known
connection between Christoffel functions and leverage scores, a connection of
our method with determinantal point processes (DPPs) is also explained. Namely,
our construction promotes diversity among important landmark points in a way
similar to DPPs. Also, we explain how our randomized adaptive algorithm can
influence the accuracy of Kernel Ridge Regression.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Environment Inference for Invariant Learning. (arXiv:2010.07249v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1">Elliot Creager</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobsen_J/0/1/0/all/0/1">J&#xf6;rn-Henrik Jacobsen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1">Richard Zemel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07249">
                                    <div class="article-summary-box-inner">
                                        <span>Learning models that gracefully handle distribution shifts is central to
research on domain generalization, robust optimization, and fairness. A
promising formulation is domain-invariant learning, which identifies the key
issue of learning which features are domain-specific versus domain-invariant.
An important assumption in this area is that the training examples are
partitioned into &quot;domains&quot; or &quot;environments&quot;. Our focus is on the more common
setting where such partitions are not provided. We propose EIIL, a general
framework for domain-invariant learning that incorporates Environment Inference
to directly infer partitions that are maximally informative for downstream
Invariant Learning. We show that EIIL outperforms invariant learning methods on
the CMNIST benchmark without using environment labels, and significantly
outperforms ERM on worst-group performance in the Waterbirds and CivilComments
datasets. Finally, we establish connections between EIIL and algorithmic
fairness, which enables EIIL to improve accuracy and calibration in a fair
prediction problem.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Revealing the Structure of Deep Neural Networks via Convex Duality. (arXiv:2002.09773v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1">Tolga Ergen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1">Mert Pilanci</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.09773">
                                    <div class="article-summary-box-inner">
                                        <span>We study regularized deep neural networks (DNNs) and introduce a convex
analytic framework to characterize the structure of the hidden layers. We show
that a set of optimal hidden layer weights for a norm regularized DNN training
problem can be explicitly found as the extreme points of a convex set. For the
special case of deep linear networks, we prove that each optimal weight matrix
aligns with the previous layers via duality. More importantly, we apply the
same characterization to deep ReLU networks with whitened data and prove the
same weight alignment holds. As a corollary, we also prove that norm
regularized deep ReLU networks yield spline interpolation for one-dimensional
datasets which was previously known only for two-layer networks. Furthermore,
we provide closed-form solutions for the optimal layer weights when data is
rank-one or whitened. The same analysis also applies to architectures with
batch normalization even for arbitrary data. Therefore, we obtain a complete
explanation for a recent empirical observation termed Neural Collapse where
class means collapse to the vertices of a simplex equiangular tight frame.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Finding Physical Adversarial Examples for Autonomous Driving with Fast and Differentiable Image Compositing. (arXiv:2010.08844v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinghan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Boloor_A/0/1/0/all/0/1">Adith Boloor</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1">Ayan Chakrabarti</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1">Yevgeniy Vorobeychik</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08844">
                                    <div class="article-summary-box-inner">
                                        <span>There is considerable evidence that deep neural networks are vulnerable to
adversarial perturbations applied directly to their digital inputs. However, it
remains an open question whether this translates to vulnerabilities in real
systems. For example, an attack on self-driving cars would in practice entail
modifying the driving environment, which then impacts the video inputs to the
car&#x27;s controller, thereby indirectly leading to incorrect driving decisions.
Such attacks require accounting for system dynamics and tracking viewpoint
changes. We propose a scalable approach for finding adversarial modifications
of a simulated autonomous driving environment using a differentiable
approximation for the mapping from environmental modifications (rectangles on
the road) to the corresponding video inputs to the controller neural network.
Given the parameters of the rectangles, our proposed differentiable mapping
composites them onto pre-recorded video streams of the original environment,
accounting for geometric and color variations. Moreover, we propose a multiple
trajectory sampling approach that enables our attacks to be robust to a car&#x27;s
self-correcting behavior. When combined with a neural network-based controller,
our approach allows the design of adversarial modifications through end-to-end
gradient-based optimization. Using the Carla autonomous driving simulator, we
show that our approach is significantly more scalable and far more effective at
identifying autonomous vehicle vulnerabilities in simulation experiments than a
state-of-the-art approach based on Bayesian Optimization.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Statistical Mechanical Analysis of Neural Network Pruning. (arXiv:2006.16617v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Acharyya_R/0/1/0/all/0/1">Rupam Acharyya</a>, <a href="http://arxiv.org/find/cs/1/au:+Chattoraj_A/0/1/0/all/0/1">Ankani Chattoraj</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Boyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Shouman Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Stefankovic_D/0/1/0/all/0/1">Daniel Stefankovic</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.16617">
                                    <div class="article-summary-box-inner">
                                        <span>Deep learning architectures with a huge number of parameters are often
compressed using pruning techniques to ensure computational efficiency of
inference during deployment. Despite multitude of empirical advances, there is
a lack of theoretical understanding of the effectiveness of different pruning
methods. We inspect different pruning techniques under the statistical
mechanics formulation of a teacher-student framework and derive their
generalization error (GE) bounds. It has been shown that Determinantal Point
Process (DPP) based node pruning method is notably superior to competing
approaches when tested on real datasets. Using GE bounds in the aforementioned
setup we provide theoretical guarantees for their empirical observations.
Another consistent finding in literature is that sparse neural networks (edge
pruned) generalize better than dense neural networks (node pruned) for a fixed
number of parameters. We use our theoretical setup to prove this finding and
show that even the baseline random edge pruning method performs better than the
DPP node pruning method. We also validate this empirically on real datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Asynchronous \epsilon-Greedy Bayesian Optimisation. (arXiv:2010.07615v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ath_G/0/1/0/all/0/1">George De Ath</a>, <a href="http://arxiv.org/find/cs/1/au:+Everson_R/0/1/0/all/0/1">Richard M. Everson</a>, <a href="http://arxiv.org/find/cs/1/au:+Fieldsend_J/0/1/0/all/0/1">Jonathan E. Fieldsend</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07615">
                                    <div class="article-summary-box-inner">
                                        <span>Batch Bayesian optimisation (BO) is a successful technique for the
optimisation of expensive black-box functions. Asynchronous BO can reduce
wallclock time by starting a new evaluation as soon as another finishes, thus
maximising resource utilisation. To maximise resource allocation, we develop a
novel asynchronous BO method, AEGiS (Asynchronous $\epsilon$-Greedy Global
Search) that combines greedy search, exploiting the surrogate&#x27;s mean
prediction, with Thompson sampling and random selection from the approximate
Pareto set describing the trade-off between exploitation (surrogate mean
prediction) and exploration (surrogate posterior variance). We demonstrate
empirically the efficacy of AEGiS on synthetic benchmark problems,
meta-surrogate hyperparameter tuning problems and real-world problems, showing
that AEGiS generally outperforms existing methods for asynchronous BO. When a
single worker is available performance is no worse than BO using expected
improvement.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data-Driven Multiscale Design of Cellular Composites with Multiclass Microstructures for Natural Frequency Maximization. (arXiv:2106.06478v1 [cs.CE])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Beek_A/0/1/0/all/0/1">Anton van Beek</a>, <a href="http://arxiv.org/find/cs/1/au:+Da_D/0/1/0/all/0/1">Daicong Da</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_Y/0/1/0/all/0/1">Yu-Chin Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1">Ping Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06478">
                                    <div class="article-summary-box-inner">
                                        <span>For natural frequency optimization of engineering structures, cellular
composites have been shown to possess an edge over solid. However, existing
multiscale design methods for cellular composites are either computationally
exhaustive or confined to a single class of microstructures. In this paper, we
propose a data-driven topology optimization (TO) approach to enable the
multiscale design of cellular structures with various choices of microstructure
classes. The key component is a newly proposed latent-variable Gaussian process
(LVGP) model through which different classes of microstructures are mapped into
a low-dimensional continuous latent space. It provides an interpretable
distance metric between classes and captures their effects on the homogenized
stiffness tensors. By introducing latent vectors as design variables, a
differentiable transition of stiffness matrix between classes can be easily
achieved with an analytical gradient. After integrating LVGP with the
density-based TO, an efficient data-driven cellular composite optimization
process is developed to enable concurrent exploration of microstructure
concepts and the associated volume fractions for natural frequency
optimization. Examples reveal that the proposed cellular designs with
multiclass microstructures achieve higher natural frequencies than both
single-scale and single-class designs. This framework can be easily extended to
other multi-scale TO problems, such as thermal compliance and dynamic response
optimization.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Signed Graph Metric Learning via Gershgorin Disc Perfect Alignment. (arXiv:2006.08816v6 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1">Gene Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wei Hu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.08816">
                                    <div class="article-summary-box-inner">
                                        <span>Given a convex and differentiable objective $Q(\M)$ for a real symmetric
matrix $\M$ in the positive definite (PD) cone -- used to compute Mahalanobis
distances -- we propose a fast general metric learning framework that is
entirely projection-free. We first assume that $\M$ resides in a space $\cS$ of
generalized graph Laplacian matrices corresponding to balanced signed graphs.
$\M \in \cS$ that is also PD is called a graph metric matrix. Unlike low-rank
metric matrices common in the literature, $\cS$ includes the important
diagonal-only matrices as a special case. The key theorem to circumvent full
eigen-decomposition and enable fast metric matrix optimization is Gershgorin
disc perfect alignment (GDPA): given $\M \in \cS$ and diagonal matrix $\S$,
where $S_{ii} &#x3D; 1/v_i$ and $\v$ is $\M$&#x27;s first eigenvector, we prove that
Gershgorin disc left-ends of similarity transform $\B &#x3D; \S \M \S^{-1}$ are
perfectly aligned at the smallest eigenvalue $\lambda_{\min}$. Using this
theorem, we replace the PD cone constraint in the metric learning problem with
tightest possible linear constraints per iteration, so that the alternating
optimization of the diagonal / off-diagonal terms in $\M$ can be solved
efficiently as linear programs via the Frank-Wolfe method. We update $\v$ using
Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) with warm
start as entries in $\M$ are optimized successively. Experiments show that our
graph metric optimization is significantly faster than cone-projection schemes,
and produces competitive binary classification performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Overfitting in Bayesian Optimization: an empirical study and early-stopping solution. (arXiv:2104.08166v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Makarova_A/0/1/0/all/0/1">Anastasia Makarova</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Huibin Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Perrone_V/0/1/0/all/0/1">Valerio Perrone</a>, <a href="http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1">Aaron Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Faddoul_J/0/1/0/all/0/1">Jean Baptiste Faddoul</a>, <a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a>, <a href="http://arxiv.org/find/cs/1/au:+Seeger_M/0/1/0/all/0/1">Matthias Seeger</a>, <a href="http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1">Cedric Archambeau</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08166">
                                    <div class="article-summary-box-inner">
                                        <span>Tuning machine learning models with Bayesian optimization (BO) is a
successful strategy to find good hyperparameters. BO defines an iterative
procedure where a cross-validated metric is evaluated on promising
hyperparameters. In practice, however, an improvement of the validation metric
may not translate in better predictive performance on a test set, especially
when tuning models trained on small datasets. In other words, unlike
conventional wisdom dictates, BO can overfit. In this paper, we carry out the
first systematic investigation of overfitting in BO and demonstrate that this
issue is serious, yet often overlooked in practice. We propose a novel
criterion to early stop BO, which aims to maintain the solution quality while
saving the unnecessary iterations that can lead to overfitting. Experiments on
real-world hyperparameter optimization problems show that our approach
effectively meets these goals and is more adaptive comparing to baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Regularized Softmax Deep Multi-Agent $Q$-Learning. (arXiv:2103.11883v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Ling Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1">Tabish Rashid</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Bei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Longbo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1">Shimon Whiteson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11883">
                                    <div class="article-summary-box-inner">
                                        <span>Tackling overestimation in $Q$-learning is an important problem that has been
extensively studied in single-agent reinforcement learning, but has received
comparatively little attention in the multi-agent setting. In this work, we
empirically demonstrate that QMIX, a popular $Q$-learning algorithm for
cooperative multi-agent reinforcement learning (MARL), suffers from a more
severe overestimation in practice than previously acknowledged, and is not
mitigated by existing approaches. We rectify this with a novel
regularization-based update scheme that penalizes large joint action-values
that deviate from a baseline and demonstrate its effectiveness in stabilizing
learning. Furthermore, we propose to employ a softmax operator, which we
efficiently approximate in a novel way in the multi-agent setting, to further
reduce the potential overestimation bias. Our approach, Regularized Softmax
(RES) Deep Multi-Agent $Q$-Learning, is general and can be applied to any
$Q$-learning based MARL algorithm. We demonstrate that, when applied to QMIX,
RES avoids severe overestimation and significantly improves performance,
yielding state-of-the-art results in a variety of cooperative multi-agent
tasks, including the challenging StarCraft II micromanagement benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models. (arXiv:2104.07788v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rozemberczki_B/0/1/0/all/0/1">Benedek Rozemberczki</a>, <a href="http://arxiv.org/find/cs/1/au:+Scherer_P/0/1/0/all/0/1">Paul Scherer</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yixuan He</a>, <a href="http://arxiv.org/find/cs/1/au:+Panagopoulos_G/0/1/0/all/0/1">George Panagopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Riedel_A/0/1/0/all/0/1">Alexander Riedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Astefanoaei_M/0/1/0/all/0/1">Maria Astefanoaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiss_O/0/1/0/all/0/1">Oliver Kiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Beres_F/0/1/0/all/0/1">Ferenc Beres</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_G/0/1/0/all/0/1">Guzm&#xe1;n L&#xf3;pez</a>, <a href="http://arxiv.org/find/cs/1/au:+Collignon_N/0/1/0/all/0/1">Nicolas Collignon</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkar_R/0/1/0/all/0/1">Rik Sarkar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07788">
                                    <div class="article-summary-box-inner">
                                        <span>We present PyTorch Geometric Temporal a deep learning framework combining
state-of-the-art machine learning algorithms for neural spatiotemporal signal
processing. The main goal of the library is to make temporal geometric deep
learning available for researchers and machine learning practitioners in a
unified easy-to-use framework. PyTorch Geometric Temporal was created with
foundations on existing libraries in the PyTorch eco-system, streamlined neural
network layer definitions, temporal snapshot generators for batching, and
integrated benchmark datasets. These features are illustrated with a
tutorial-like case study. Experiments demonstrate the predictive performance of
the models implemented in the library on real world problems such as
epidemiological forecasting, ridehail demand prediction and web-traffic
management. Our sensitivity analysis of runtime shows that the framework can
potentially operate on web-scale datasets with rich temporal features and
spatial structure.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Generalizable Episodic Memory for Deep Reinforcement Learning. (arXiv:2103.06469v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1">Jianing Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1">Guangxiang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1">Zhizhou Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chongjie Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06469">
                                    <div class="article-summary-box-inner">
                                        <span>Episodic memory-based methods can rapidly latch onto past successful
strategies by a non-parametric memory and improve sample efficiency of
traditional reinforcement learning. However, little effort is put into the
continuous domain, where a state is never visited twice, and previous episodic
methods fail to efficiently aggregate experience across trajectories. To
address this problem, we propose Generalizable Episodic Memory (GEM), which
effectively organizes the state-action values of episodic memory in a
generalizable manner and supports implicit planning on memorized trajectories.
GEM utilizes a double estimator to reduce the overestimation bias induced by
value propagation in the planning process. Empirical evaluation shows that our
method significantly outperforms existing trajectory-based methods on various
MuJoCo continuous control tasks. To further show the general applicability, we
evaluate our method on Atari games with discrete action space, which also shows
a significant improvement over baseline algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Quantifying and Reducing Bias in Maximum Likelihood Estimation of Structured Anomalies. (arXiv:2007.07878v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chitra_U/0/1/0/all/0/1">Uthsav Chitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1">Kimberly Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jasper C.H. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Raphael_B/0/1/0/all/0/1">Benjamin J. Raphael</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.07878">
                                    <div class="article-summary-box-inner">
                                        <span>Anomaly estimation, or the problem of finding a subset of a dataset that
differs from the rest of the dataset, is a classic problem in machine learning
and data mining. In both theoretical work and in applications, the anomaly is
assumed to have a specific structure defined by membership in an
$\textit{anomaly family}$. For example, in temporal data the anomaly family may
be time intervals, while in network data the anomaly family may be connected
subgraphs. The most prominent approach for anomaly estimation is to compute the
Maximum Likelihood Estimator (MLE) of the anomaly; however, it was recently
observed that for normally distributed data, the MLE is a $\textit{biased}$
estimator for some anomaly families. In this work, we demonstrate that in the
normal means setting, the bias of the MLE depends on the size of the anomaly
family. We prove that if the number of sets in the anomaly family that contain
the anomaly is sub-exponential, then the MLE is asymptotically unbiased. We
also provide empirical evidence that the converse is true: if the number of
such sets is exponential, then the MLE is asymptotically biased. Our analysis
unifies a number of earlier results on the bias of the MLE for specific anomaly
families. Next, we derive a new anomaly estimator using a mixture model, and we
prove that our anomaly estimator is asymptotically unbiased regardless of the
size of the anomaly family. We illustrate the advantages of our estimator
versus the MLE on disease outbreak and highway traffic data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online Multi-Object Tracking and Segmentation with GMPHD Filter and Mask-based Affinity Fusion. (arXiv:2009.00100v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Young-min Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1">Young-chul Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1">Kwangjin Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1">Moongu Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1">Witold Pedrycz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.00100">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose a highly practical fully online multi-object
tracking and segmentation (MOTS) method that uses instance segmentation results
as an input. The proposed method is based on the Gaussian mixture probability
hypothesis density (GMPHD) filter, a hierarchical data association (HDA), and a
mask-based affinity fusion (MAF) model to achieve high-performance online
tracking. The HDA consists of two associations: segment-to-track and
track-to-track associations. One affinity, for position and motion, is computed
by using the GMPHD filter, and the other affinity, for appearance is computed
by using the responses from a single object tracker such as a kernalized
correlation filter. These two affinities are simply fused by using a
score-level fusion method such as min-max normalization referred to as MAF. In
addition, to reduce the number of false positive segments, we adopt mask
IoU-based merging (mask merging). The proposed MOTS framework with the key
modules: HDA, MAF, and mask merging, is easily extensible to simultaneously
track multiple types of objects with CPU only execution in parallel processing.
In addition, the developed framework only requires simple parameter tuning
unlike many existing MOTS methods that need intensive hyperparameter
optimization. In the experiments on the two popular MOTS datasets, the key
modules show some improvements. For instance, ID-switch decreases by more than
half compared to a baseline method in the training sets. In conclusion, our
tracker achieves state-of-the-art MOTS performance in the test sets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Symbolic Regression that Scales. (arXiv:2106.06427v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Biggio_L/0/1/0/all/0/1">Luca Biggio</a>, <a href="http://arxiv.org/find/cs/1/au:+Bendinelli_T/0/1/0/all/0/1">Tommaso Bendinelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Neitz_A/0/1/0/all/0/1">Alexander Neitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1">Aurelien Lucchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Parascandolo_G/0/1/0/all/0/1">Giambattista Parascandolo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06427">
                                    <div class="article-summary-box-inner">
                                        <span>Symbolic equations are at the core of scientific discovery. The task of
discovering the underlying equation from a set of input-output pairs is called
symbolic regression. Traditionally, symbolic regression methods use
hand-designed strategies that do not improve with experience. In this paper, we
introduce the first symbolic regression method that leverages large scale
pre-training. We procedurally generate an unbounded set of equations, and
simultaneously pre-train a Transformer to predict the symbolic equation from a
corresponding set of input-output-pairs. At test time, we query the model on a
new set of points and use its output to guide the search for the equation. We
show empirically that this approach can re-discover a set of well-known
physical equations, and that it improves over time with more data and compute.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online and Distribution-Free Robustness: Regression and Contextual Bandits with Huber Contamination. (arXiv:2010.04157v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sitan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1">Frederic Koehler</a>, <a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1">Ankur Moitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Yau_M/0/1/0/all/0/1">Morris Yau</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04157">
                                    <div class="article-summary-box-inner">
                                        <span>In this work we revisit two classic high-dimensional online learning
problems, namely linear regression and contextual bandits, from the perspective
of adversarial robustness. Existing works in algorithmic robust statistics make
strong distributional assumptions that ensure that the input data is evenly
spread out or comes from a nice generative model. Is it possible to achieve
strong robustness guarantees even without distributional assumptions
altogether, where the sequence of tasks we are asked to solve is adaptively and
adversarially chosen?

We answer this question in the affirmative for both linear regression and
contextual bandits. In fact our algorithms succeed where conventional methods
fail. In particular we show strong lower bounds against Huber regression and
more generally any convex M-estimator. Our approach is based on a novel
alternating minimization scheme that interleaves ordinary least-squares with a
simple convex program that finds the optimal reweighting of the distribution
under a spectral constraint. Our results obtain essentially optimal dependence
on the contamination level $\eta$, reach the optimal breakdown point, and
naturally apply to infinite dimensional settings where the feature vectors are
represented implicitly via a kernel map.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Coded-InvNet for Resilient Prediction Serving Systems. (arXiv:2106.06445v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1">Tuan Dinh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kangwook Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06445">
                                    <div class="article-summary-box-inner">
                                        <span>Inspired by a new coded computation algorithm for invertible functions, we
propose Coded-InvNet a new approach to design resilient prediction serving
systems that can gracefully handle stragglers or node failures. Coded-InvNet
leverages recent findings in the deep learning literature such as invertible
neural networks, Manifold Mixup, and domain translation algorithms, identifying
interesting research directions that span across machine learning and systems.
Our experimental results show that Coded-InvNet can outperform existing
approaches, especially when the compute resource overhead is as low as 10%. For
instance, without knowing which of the ten workers is going to fail, our
algorithm can design a backup task so that it can correctly recover the missing
prediction result with an accuracy of 85.9%, significantly outperforming the
previous SOTA by 32.5%.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Compositional Video Synthesis with Action Graphs. (arXiv:2006.15327v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1">Amir Bar</a>, <a href="http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1">Roei Herzig</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1">Anna Rohrbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1">Gal Chechik</a>, <a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1">Trevor Darrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1">Amir Globerson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.15327">
                                    <div class="article-summary-box-inner">
                                        <span>Videos of actions are complex signals containing rich compositional structure
in space and time. Current video generation methods lack the ability to
condition the generation on multiple coordinated and potentially simultaneous
timed actions. To address this challenge, we propose to represent the actions
in a graph structure called Action Graph and present the new &#x60;&#x60;Action Graph To
Video&#x27;&#x27; synthesis task. Our generative model for this task (AG2Vid)
disentangles motion and appearance features, and by incorporating a scheduling
mechanism for actions facilitates a timely and coordinated video generation. We
train and evaluate AG2Vid on the CATER and Something-Something V2 datasets, and
show that the resulting videos have better visual quality and semantic
consistency compared to baselines. Finally, our model demonstrates zero-shot
abilities by synthesizing novel compositions of the learned actions. For code
and pretrained models, see the project page https://roeiherz.github.io/AG2Video</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards the Unification and Robustness of Perturbation and Gradient Based Explanations. (arXiv:2102.10618v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1">Sushant Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jabbari_S/0/1/0/all/0/1">Shahin Jabbari</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1">Chirag Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1">Sohini Upadhyay</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiwei Steven Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1">Himabindu Lakkaraju</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10618">
                                    <div class="article-summary-box-inner">
                                        <span>As machine learning black boxes are increasingly being deployed in critical
domains such as healthcare and criminal justice, there has been a growing
emphasis on developing techniques for explaining these black boxes in a post
hoc manner. In this work, we analyze two popular post hoc interpretation
techniques: SmoothGrad which is a gradient based method, and a variant of LIME
which is a perturbation based method. More specifically, we derive explicit
closed form expressions for the explanations output by these two methods and
show that they both converge to the same explanation in expectation, i.e., when
the number of perturbed samples used by these methods is large. We then
leverage this connection to establish other desirable properties, such as
robustness, for these techniques. We also derive finite sample complexity
bounds for the number of perturbations required for these methods to converge
to their expected explanation. Finally, we empirically validate our theory
using extensive experimentation on both synthetic and real world datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Optimal Complexity in Decentralized Training. (arXiv:2006.08085v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yucheng Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1">Christopher De Sa</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.08085">
                                    <div class="article-summary-box-inner">
                                        <span>Decentralization is a promising method of scaling up parallel machine
learning systems. In this paper, we provide a tight lower bound on the
iteration complexity for such methods in a stochastic non-convex setting. Our
lower bound reveals a theoretical gap in known convergence rates of many
existing decentralized training algorithms, such as D-PSGD. We prove by
construction this lower bound is tight and achievable. Motivated by our
insights, we further propose DeTAG, a practical gossip-style decentralized
algorithm that achieves the lower bound with only a logarithm gap. Empirically,
we compare DeTAG with other decentralized algorithms on image classification
tasks, and we show DeTAG enjoys faster convergence compared to baselines,
especially on unshuffled data and in sparse networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">WAX-ML: A Python library for machine learning and feedback loops on streaming data. (arXiv:2106.06524v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Serie_E/0/1/0/all/0/1">Emmanuel S&#xe9;ri&#xe9;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06524">
                                    <div class="article-summary-box-inner">
                                        <span>Wax is what you put on a surfboard to avoid slipping. It is an essential tool
to go surfing... We introduce WAX-ML a research-oriented Python library
providing tools to design powerful machine learning algorithms and feedback
loops working on streaming data. It strives to complement JAX with tools
dedicated to time series. WAX-ML makes JAX-based programs easy to use for
end-users working with pandas and xarray for data manipulation. It provides a
simple mechanism for implementing feedback loops, allows the implementation of
online learning and reinforcement learning algorithms with functions, and makes
them easy to integrate by end-users working with the object-oriented
reinforcement learning framework from the Gym library. It is released with an
Apache open-source license on GitHub at https://github.com/eserie/wax-ml.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On Robust Mean Estimation under Coordinate-level Corruption. (arXiv:2002.04137v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zifan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jongho Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1">Theodoros Rekatsinas</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1">Christos Tzamos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.04137">
                                    <div class="article-summary-box-inner">
                                        <span>We study the problem of robust mean estimation and introduce a novel Hamming
distance-based measure of distribution shift for coordinate-level corruptions.
We show that this measure yields adversary models that capture more realistic
corruptions than those used in prior works, and present an
information-theoretic analysis of robust mean estimation in these settings. We
show that for structured distributions, methods that leverage the structure
yield information theoretically more accurate mean estimation. We also focus on
practical algorithms for robust mean estimation and study when data
cleaning-inspired approaches that first fix corruptions in the input data and
then perform robust mean estimation can match the information theoretic bounds
of our analysis. We finally demonstrate experimentally that this two-step
approach outperforms structure-agnostic robust estimation and provides accurate
mean estimation even for high-magnitude corruption.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Decoupled Greedy Learning of CNNs for Synchronous and Asynchronous Distributed Learning. (arXiv:2106.06401v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1">Eugene Belilovsky</a> (MILA), <a href="http://arxiv.org/find/cs/1/au:+Leconte_L/0/1/0/all/0/1">Louis Leconte</a> (MLIA, CMAP), <a href="http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1">Lucas Caccia</a> (MILA), <a href="http://arxiv.org/find/cs/1/au:+Eickenberg_M/0/1/0/all/0/1">Michael Eickenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1">Edouard Oyallon</a> (MLIA)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06401">
                                    <div class="article-summary-box-inner">
                                        <span>A commonly cited inefficiency of neural network training using
back-propagation is the update locking problem: each layer must wait for the
signal to propagate through the full network before updating. Several
alternatives that can alleviate this issue have been proposed. In this context,
we consider a simple alternative based on minimal feedback, which we call
Decoupled Greedy Learning (DGL). It is based on a classic greedy relaxation of
the joint training objective, recently shown to be effective in the context of
Convolutional Neural Networks (CNNs) on large-scale image classification. We
consider an optimization of this objective that permits us to decouple the
layer training, allowing for layers or modules in networks to be trained with a
potentially linear parallelization. With the use of a replay buffer we show
that this approach can be extended to asynchronous settings, where modules can
operate and continue to update with possibly large communication delays. To
address bandwidth and memory issues we propose an approach based on online
vector quantization. This allows to drastically reduce the communication
bandwidth between modules and required memory for replay buffers. We show
theoretically and empirically that this approach converges and compare it to
the sequential solvers. We demonstrate the effectiveness of DGL against
alternative approaches on the CIFAR-10 dataset and on the large-scale ImageNet
dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Measuring the sensitivity of Gaussian processes to kernel choice. (arXiv:2106.06510v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Stephenson_W/0/1/0/all/0/1">William T. Stephenson</a>, <a href="http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1">Soumya Ghosh</a>, <a href="http://arxiv.org/find/stat/1/au:+Nguyen_T/0/1/0/all/0/1">Tin D. Nguyen</a>, <a href="http://arxiv.org/find/stat/1/au:+Yurochkin_M/0/1/0/all/0/1">Mikhail Yurochkin</a>, <a href="http://arxiv.org/find/stat/1/au:+Deshpande_S/0/1/0/all/0/1">Sameer K. Deshpande</a>, <a href="http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1">Tamara Broderick</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06510">
                                    <div class="article-summary-box-inner">
                                        <span>Gaussian processes (GPs) are used to make medical and scientific decisions,
including in cardiac care and monitoring of carbon dioxide emissions. But the
choice of GP kernel is often somewhat arbitrary. In particular, uncountably
many kernels typically align with qualitative prior knowledge (e.g. function
smoothness or stationarity). But in practice, data analysts choose among a
handful of convenient standard kernels (e.g. squared exponential). In the
present work, we ask: Would decisions made with a GP differ under other,
qualitatively interchangeable kernels? We show how to formulate this
sensitivity analysis as a constrained optimization problem over a
finite-dimensional space. We can then use standard optimizers to identify
substantive changes in relevant decisions made with a GP. We demonstrate in
both synthetic and real-world examples that decisions made with a GP can
exhibit substantial sensitivity to kernel choice, even when prior draws are
qualitatively interchangeable to a user.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime. (arXiv:2006.12297v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1">Atsushi Nitanda</a>, <a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1">Taiji Suzuki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12297">
                                    <div class="article-summary-box-inner">
                                        <span>We analyze the convergence of the averaged stochastic gradient descent for
overparameterized two-layer neural networks for regression problems. It was
recently found that a neural tangent kernel (NTK) plays an important role in
showing the global convergence of gradient-based methods under the NTK regime,
where the learning dynamics for overparameterized neural networks can be almost
characterized by that for the associated reproducing kernel Hilbert space
(RKHS). However, there is still room for a convergence rate analysis in the NTK
regime. In this study, we show that the averaged stochastic gradient descent
can achieve the minimax optimal convergence rate, with the global convergence
guarantee, by exploiting the complexities of the target function and the RKHS
associated with the NTK. Moreover, we show that the target function specified
by the NTK of a ReLU network can be learned at the optimal convergence rate
through a smooth approximation of a ReLU network under certain conditions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fast Weakly Supervised Action Segmentation Using Mutual Consistency. (arXiv:1904.03116v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Souri_Y/0/1/0/all/0/1">Yaser Souri</a>, <a href="http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1">Mohsen Fayyaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Minciullo_L/0/1/0/all/0/1">Luca Minciullo</a>, <a href="http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1">Gianpiero Francesca</a>, <a href="http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1">Juergen Gall</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.03116">
                                    <div class="article-summary-box-inner">
                                        <span>Action segmentation is the task of predicting the actions for each frame of a
video. As obtaining the full annotation of videos for action segmentation is
expensive, weakly supervised approaches that can learn only from transcripts
are appealing. In this paper, we propose a novel end-to-end approach for weakly
supervised action segmentation based on a two-branch neural network. The two
branches of our network predict two redundant but different representations for
action segmentation and we propose a novel mutual consistency (MuCon) loss that
enforces the consistency of the two redundant representations. Using the MuCon
loss together with a loss for transcript prediction, our proposed approach
achieves the accuracy of state-of-the-art approaches while being $14$ times
faster to train and $20$ times faster during inference. The MuCon loss proves
beneficial even in the fully supervised setting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploration-Exploitation Motivated Variational Auto-Encoder for Recommender Systems. (arXiv:2006.03573v4 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1">Yizi Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Liu_M/0/1/0/all/0/1">Meimei Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.03573">
                                    <div class="article-summary-box-inner">
                                        <span>Recent years have witnessed rapid developments on collaborative filtering
techniques for improving the performance of recommender systems due to the
growing need of companies to help users discover new and relevant items.
However, the majority of existing literature focuses on delivering items which
match the user model learned from users&#x27; past preferences. A good
recommendation model is expected to recommend items that are known to enjoy and
items that are novel to try. In this work, we introduce an
exploitation-exploration motivated variational auto-encoder (XploVAE) to
collaborative filtering. To facilitate personalized recommendations, we
construct user-specific subgraphs, which contain the first-order proximity
capturing observed user-item interactions for exploitation and the high-order
proximity for exploration. A hierarchical latent space model is utilized to
learn the personalized item embedding for a given user, along with the
population distribution of all user subgraphs. Finally, experimental results on
various real-world datasets clearly demonstrate the effectiveness of our
proposed model on leveraging the exploitation and exploration recommendation
tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unsupervised Neural Hidden Markov Models with a Continuous latent state space. (arXiv:2106.06536v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jarboui_F/0/1/0/all/0/1">Firas Jarboui</a>, <a href="http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1">Vianney Perchet</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06536">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a new procedure to neuralize unsupervised Hidden Markov Models
in the continuous case. This provides higher flexibility to solve problems with
underlying latent variables. This approach is evaluated on both synthetic and
real data. On top of generating likely model parameters with comparable
performances to off-the-shelf neural architecture (LSTMs, GRUs,..), the
obtained results are easily interpretable.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Probability Paths and the Structure of Predictions over Time. (arXiv:2106.06515v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhiyuan/0/1/0/all/0/1">Zhiyuan</a> (Jerry)Lin, <a href="http://arxiv.org/find/cs/1/au:+Sheng_H/0/1/0/all/0/1">Hao Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1">Sharad Goel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06515">
                                    <div class="article-summary-box-inner">
                                        <span>In settings ranging from weather forecasts to political prognostications to
financial projections, probability estimates of future binary outcomes often
evolve over time. For example, the estimated likelihood of rain on a specific
day changes by the hour as new information becomes available. Given a
collection of such probability paths, we introduce a Bayesian framework --
which we call the Gaussian latent information martingale, or GLIM -- for
modeling the structure of dynamic predictions over time. Suppose, for example,
that the likelihood of rain in a week is 50%, and consider two hypothetical
scenarios. In the first, one expects the forecast is equally likely to become
either 25% or 75% tomorrow; in the second, one expects the forecast to stay
constant for the next several days. A time-sensitive decision-maker might
select a course of action immediately in the latter scenario, but may postpone
their decision in the former, knowing that new information is imminent. We
model these trajectories by assuming predictions update according to a latent
process of information flow, which is inferred from historical data. In
contrast to general methods for time series analysis, this approach preserves
the martingale structure of probability paths and better quantifies future
uncertainties around probability paths. We show that GLIM outperforms three
popular baseline methods, producing better estimated posterior probability path
distributions measured by three different metrics. By elucidating the dynamic
structure of predictions over time, we hope to help individuals make more
informed choices.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An adaptive cognitive sensor node for ECG monitoring in the Internet of Medical Things. (arXiv:2106.06498v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Scrugli_M/0/1/0/all/0/1">Matteo Antonio Scrugli</a>, <a href="http://arxiv.org/find/cs/1/au:+Loi_D/0/1/0/all/0/1">Daniela Loi</a>, <a href="http://arxiv.org/find/cs/1/au:+Raffo_L/0/1/0/all/0/1">Luigi Raffo</a>, <a href="http://arxiv.org/find/cs/1/au:+Meloni_P/0/1/0/all/0/1">Paolo Meloni</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06498">
                                    <div class="article-summary-box-inner">
                                        <span>The Internet of Medical Things (IoMT) paradigm is becoming mainstream in
multiple clinical trials and healthcare procedures. It relies on novel very
accurate and compact sensing devices and communication infrastructures, opening
previously unmatched possibilities of implementing data collection and
continuous patient monitoring. Nevertheless, to fully exploit the potential of
this technology, some steps forwards are needed. First, the edge-computing
paradigm must be added to the picture. A certain level of near-sensor
processing has to be enabled, to improve the scalability, portability,
reliability, responsiveness of the IoMT nodes. Second, novel, increasingly
accurate, data analysis algorithms, such as those based on artificial
intelligence and Deep Learning, must be exploited. To reach these objectives,
designers, programmers of IoMT nodes, have to face challenging optimization
tasks, in order to execute fairly complex computing tasks on low-power wearable
and portable processing systems, with tight power and battery lifetime budgets.
In this work, we explore the implementation of cognitive data analysis
algorithm on resource-constrained computing platforms. To minimize power
consumption, we add an adaptivity layer that dynamically manages the hardware
and software configuration of the device to adapt it at runtime to the required
operating mode. We have assessed our approach on a use-case using a
convolutional neural network to classify electrocardiogram (ECG) traces on a
low-power microcontroller. Our experimental results show that adapting the node
setup to the workload at runtime can save up to 50% power consumption and a
quantized neural network reaches an accuracy value higher than 98% for
arrhythmia disorders detection on MIT-BIH Arrhythmia dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Object Segmentation Without Labels with Large-Scale Generative Models. (arXiv:2006.04988v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1">Andrey Voynov</a>, <a href="http://arxiv.org/find/cs/1/au:+Morozov_S/0/1/0/all/0/1">Stanislav Morozov</a>, <a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1">Artem Babenko</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04988">
                                    <div class="article-summary-box-inner">
                                        <span>The recent rise of unsupervised and self-supervised learning has dramatically
reduced the dependency on labeled data, providing effective image
representations for transfer to downstream vision tasks. Furthermore, recent
works employed these representations in a fully unsupervised setup for image
classification, reducing the need for human labels on the fine-tuning stage as
well. This work demonstrates that large-scale unsupervised models can also
perform a more challenging object segmentation task, requiring neither
pixel-level nor image-level labeling. Namely, we show that recent unsupervised
GANs allow to differentiate between foreground/background pixels, providing
high-quality saliency masks. By extensive comparison on standard benchmarks, we
outperform existing unsupervised alternatives for object segmentation,
achieving new state-of-the-art.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Recovery of Meteorites Using an Autonomous Drone and Machine Learning. (arXiv:2106.06523v1 [astro-ph.EP])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/astro-ph/1/au:+Citron_R/0/1/0/all/0/1">Robert I. Citron</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Jenniskens_P/0/1/0/all/0/1">Peter Jenniskens</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Watkins_C/0/1/0/all/0/1">Christopher Watkins</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Sinha_S/0/1/0/all/0/1">Sravanthi Sinha</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Shah_A/0/1/0/all/0/1">Amar Shah</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Raissi_C/0/1/0/all/0/1">Chedy Raissi</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Devillepoix_H/0/1/0/all/0/1">Hadrien Devillepoix</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Albers_J/0/1/0/all/0/1">Jim Albers</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06523">
                                    <div class="article-summary-box-inner">
                                        <span>The recovery of freshly fallen meteorites from tracked and triangulated
meteors is critical to determining their source asteroid families. However,
locating meteorite fragments in strewn fields remains a challenge with very few
meteorites being recovered from the meteors triangulated in past and ongoing
meteor camera networks. We examined if locating meteorites can be automated
using machine learning and an autonomous drone. Drones can be programmed to fly
a grid search pattern and take systematic pictures of the ground over a large
survey area. Those images can be analyzed using a machine learning classifier
to identify meteorites in the field among many other features. Here, we
describe a proof-of-concept meteorite classifier that deploys off-line a
combination of different convolution neural networks to recognize meteorites
from images taken by drones in the field. The system was implemented in a
conceptual drone setup and tested in the suspected strewn field of a recent
meteorite fall near Walker Lake, Nevada.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hierarchical Probabilistic Model for Blind Source Separation via Legendre Transformation. (arXiv:1909.11294v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Luo_S/0/1/0/all/0/1">Simon Luo</a>, <a href="http://arxiv.org/find/stat/1/au:+Azizi_L/0/1/0/all/0/1">Lamiae Azizi</a>, <a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1">Mahito Sugiyama</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.11294">
                                    <div class="article-summary-box-inner">
                                        <span>We present a novel blind source separation (BSS) method, called information
geometric blind source separation (IGBSS). Our formulation is based on the
log-linear model equipped with a hierarchically structured sample space, which
has theoretical guarantees to uniquely recover a set of source signals by
minimizing the KL divergence from a set of mixed signals. Source signals,
received signals, and mixing matrices are realized as different layers in our
hierarchical sample space. Our empirical results have demonstrated on images
and time series data that our approach is superior to well established
techniques and is able to separate signals with complex interactions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scalable Polyhedral Verification of Recurrent Neural Networks. (arXiv:2005.13300v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ryou_W/0/1/0/all/0/1">Wonryong Ryou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiayu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1">Mislav Balunovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1">Gagandeep Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Dan_A/0/1/0/all/0/1">Andrei Dan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1">Martin Vechev</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.13300">
                                    <div class="article-summary-box-inner">
                                        <span>We present a scalable and precise verifier for recurrent neural networks,
called Prover based on two novel ideas: (i) a method to compute a set of
polyhedral abstractions for the non-convex and nonlinear recurrent update
functions by combining sampling, optimization, and Fermat&#x27;s theorem, and (ii) a
gradient descent based algorithm for abstraction refinement guided by the
certification problem that combines multiple abstractions for each neuron.
Using Prover, we present the first study of certifying a non-trivial use case
of recurrent neural networks, namely speech classification. To achieve this, we
additionally develop custom abstractions for the non-linear speech
preprocessing pipeline. Our evaluation shows that Prover successfully verifies
several challenging recurrent models in computer vision, speech, and motion
sensor data classification beyond the reach of prior work.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Policy Gradient Bayesian Robust Optimization for Imitation Learning. (arXiv:2106.06499v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Javed_Z/0/1/0/all/0/1">Zaynah Javed</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Daniel S. Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1">Satvik Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jerry Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1">Ashwin Balakrishna</a>, <a href="http://arxiv.org/find/cs/1/au:+Petrik_M/0/1/0/all/0/1">Marek Petrik</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca D. Dragan</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1">Ken Goldberg</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06499">
                                    <div class="article-summary-box-inner">
                                        <span>The difficulty in specifying rewards for many real-world problems has led to
an increased focus on learning rewards from human feedback, such as
demonstrations. However, there are often many different reward functions that
explain the human feedback, leaving agents with uncertainty over what the true
reward function is. While most policy optimization approaches handle this
uncertainty by optimizing for expected performance, many applications demand
risk-averse behavior. We derive a novel policy gradient-style robust
optimization approach, PG-BROIL, that optimizes a soft-robust objective that
balances expected performance and risk. To the best of our knowledge, PG-BROIL
is the first policy optimization algorithm robust to a distribution of reward
hypotheses which can scale to continuous MDPs. Results suggest that PG-BROIL
can produce a family of behaviors ranging from risk-neutral to risk-averse and
outperforms state-of-the-art imitation learning algorithms when learning from
ambiguous demonstrations by hedging against uncertainty, rather than seeking to
uniquely identify the demonstrator&#x27;s reward function.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Locally Sparse Networks for Interpretable Predictions. (arXiv:2106.06468v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Junchen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindenbaum_O/0/1/0/all/0/1">Ofir Lindenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Kluger_Y/0/1/0/all/0/1">Yuval Kluger</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06468">
                                    <div class="article-summary-box-inner">
                                        <span>Despite the enormous success of neural networks, they are still hard to
interpret and often overfit when applied to low-sample-size (LSS) datasets. To
tackle these obstacles, we propose a framework for training locally sparse
neural networks where the local sparsity is learned via a sample-specific
gating mechanism that identifies the subset of most relevant features for each
measurement. The sample-specific sparsity is predicted via a \textit{gating}
network, which is trained in tandem with the \textit{prediction} network. By
learning these subsets and weights of a prediction model, we obtain an
interpretable neural network that can handle LSS data and can remove nuisance
variables, which are irrelevant for the supervised learning task. Using both
synthetic and real-world datasets, we demonstrate that our method outperforms
state-of-the-art models when predicting the target function with far fewer
features per instance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Invariant Information Bottleneck for Domain Generalization. (arXiv:2106.06333v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yifei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yezhen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wenzhen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1">Colorado J. Reed</a>, <a href="http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1">Tong Che</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1">Kurt Keutzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Han Zhao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06333">
                                    <div class="article-summary-box-inner">
                                        <span>The main challenge for domain generalization (DG) is to overcome the
potential distributional shift between multiple training domains and unseen
test domains. One popular class of DG algorithms aims to learn representations
that have an invariant causal relation across the training domains. However,
certain features, called \emph{pseudo-invariant features}, may be invariant in
the training domain but not the test domain and can substantially decreases the
performance of existing algorithms. To address this issue, we propose a novel
algorithm, called Invariant Information Bottleneck (IIB), that learns a
minimally sufficient representation that is invariant across training and
testing domains. By minimizing the mutual information between the
representation and inputs, IIB alleviates its reliance on pseudo-invariant
features, which is desirable for DG. To verify the effectiveness of the IIB
principle, we conduct extensive experiments on large-scale DG benchmarks. The
results show that IIB outperforms invariant learning baseline (e.g. IRM) by an
average of 2.8\% and 3.8\% accuracy over two evaluation metrics.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dictionary and prior learning with unrolled algorithms for unsupervised inverse problems. (arXiv:2106.06338v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Malezieux_B/0/1/0/all/0/1">Beno&#xee;t Mal&#xe9;zieux</a>, <a href="http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1">Thomas Moreau</a>, <a href="http://arxiv.org/find/cs/1/au:+Kowalski_M/0/1/0/all/0/1">Matthieu Kowalski</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06338">
                                    <div class="article-summary-box-inner">
                                        <span>Inverse problems consist in recovering a signal given noisy observations. One
classical resolution approach is to leverage sparsity and integrate prior
knowledge of the signal to the reconstruction algorithm to get a plausible
solution. Still, this prior might not be sufficiently adapted to the data. In
this work, we study Dictionary and Prior learning from degraded measurements as
a bi-level problem, and we take advantage of unrolled algorithms to solve
approximate formulations of Synthesis and Analysis. We provide an empirical and
theoretical analysis of automatic differentiation for Dictionary Learning to
understand better the pros and cons of unrolling in this context. We find that
unrolled algorithms speed up the recovery process for a small number of
iterations by improving the gradient estimation. Then we compare Analysis and
Synthesis by evaluating the performance of unrolled algorithms for inverse
problems, without access to any ground truth data for several classes of
dictionaries and priors. While Analysis can achieve good results,Synthesis is
more robust and performs better. Finally, we illustrate our method on pattern
and structure learning tasks from degraded measurements.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Continuous Herded Gibbs Sampling. (arXiv:2106.06430v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Wolf_L/0/1/0/all/0/1">Laura M. Wolf</a>, <a href="http://arxiv.org/find/stat/1/au:+Baum_M/0/1/0/all/0/1">Marcus Baum</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06430">
                                    <div class="article-summary-box-inner">
                                        <span>Herding is a technique to sequentially generate deterministic samples from a
probability distribution. In this work, we propose a continuous herded Gibbs
sampler, that combines kernel herding on continuous densities with Gibbs
sampling. Our algorithm allows for deterministically sampling from
high-dimensional multivariate probability densities, without directly sampling
from the joint density. Experiments with Gaussian mixture densities indicate
that the L2 error decreases similarly to kernel herding, while the computation
time is significantly lower, i.e., linear in the number of dimensions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Topological Detection of Trojaned Neural Networks. (arXiv:2106.06469v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Songzhu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yikai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_H/0/1/0/all/0/1">Hubert Wagner</a>, <a href="http://arxiv.org/find/cs/1/au:+Goswami_M/0/1/0/all/0/1">Mayank Goswami</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06469">
                                    <div class="article-summary-box-inner">
                                        <span>Deep neural networks are known to have security issues. One particular threat
is the Trojan attack. It occurs when the attackers stealthily manipulate the
model&#x27;s behavior through Trojaned training samples, which can later be
exploited.

Guided by basic neuroscientific principles we discover subtle -- yet critical
-- structural deviation characterizing Trojaned models. In our analysis we use
topological tools. They allow us to model high-order dependencies in the
networks, robustly compare different networks, and localize structural
abnormalities. One interesting observation is that Trojaned models develop
short-cuts from input to output layers.

Inspired by these observations, we devise a strategy for robust detection of
Trojaned models. Compared to standard baselines it displays better performance
on multiple benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Driven Adaptive Prior. (arXiv:2106.06406v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1">Sang-gil Lee</a>, <a href="http://arxiv.org/find/stat/1/au:+Kim_H/0/1/0/all/0/1">Heeseung Kim</a>, <a href="http://arxiv.org/find/stat/1/au:+Shin_C/0/1/0/all/0/1">Chaehun Shin</a>, <a href="http://arxiv.org/find/stat/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1">Chang Liu</a>, <a href="http://arxiv.org/find/stat/1/au:+Meng_Q/0/1/0/all/0/1">Qi Meng</a>, <a href="http://arxiv.org/find/stat/1/au:+Qin_T/0/1/0/all/0/1">Tao Qin</a>, <a href="http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1">Wei Chen</a>, <a href="http://arxiv.org/find/stat/1/au:+Yoon_S/0/1/0/all/0/1">Sungroh Yoon</a>, <a href="http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1">Tie-Yan Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06406">
                                    <div class="article-summary-box-inner">
                                        <span>Denoising diffusion probabilistic models have been recently proposed to
generate high-quality samples by estimating the gradient of the data density.
The framework assumes the prior noise as a standard Gaussian distribution,
whereas the corresponding data distribution may be more complicated than the
standard Gaussian distribution, which potentially introduces inefficiency in
denoising the prior noise into the data sample because of the discrepancy
between the data and the prior. In this paper, we propose PriorGrad to improve
the efficiency of the conditional diffusion model (for example, a vocoder using
a mel-spectrogram as the condition) by applying an adaptive prior derived from
the data statistics based on the conditional information. We formulate the
training and sampling procedures of PriorGrad and demonstrate the advantages of
an adaptive prior through a theoretical analysis. Focusing on the audio domain,
we consider the recently proposed diffusion-based audio generative models based
on both the spectral and time domains and show that PriorGrad achieves a faster
convergence leading to data and parameter efficiency and improved quality, and
thereby demonstrating the efficiency of a data-driven adaptive prior.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Offline Reinforcement Learning as Anti-Exploration. (arXiv:2106.06431v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rezaeifar_S/0/1/0/all/0/1">Shideh Rezaeifar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dadashi_R/0/1/0/all/0/1">Robert Dadashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vieillard_N/0/1/0/all/0/1">Nino Vieillard</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussenot_L/0/1/0/all/0/1">L&#xe9;onard Hussenot</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1">Olivier Bachem</a>, <a href="http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1">Olivier Pietquin</a>, <a href="http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1">Matthieu Geist</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06431">
                                    <div class="article-summary-box-inner">
                                        <span>Offline Reinforcement Learning (RL) aims at learning an optimal control from
a fixed dataset, without interactions with the system. An agent in this setting
should avoid selecting actions whose consequences cannot be predicted from the
data. This is the converse of exploration in RL, which favors such actions. We
thus take inspiration from the literature on bonus-based exploration to design
a new offline RL agent. The core idea is to subtract a prediction-based
exploration bonus from the reward, instead of adding it for exploration. This
allows the policy to stay close to the support of the dataset. We connect this
approach to a more common regularization of the learned policy towards the
data. Instantiated with a bonus based on the prediction error of a variational
autoencoder, we show that our agent is competitive with the state of the art on
a set of continuous control locomotion and manipulation tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">JKOnet: Proximal Optimal Transport Modeling of Population Dynamics. (arXiv:2106.06345v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bunne_C/0/1/0/all/0/1">Charlotte Bunne</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_Papaxanthos_L/0/1/0/all/0/1">Laetitia Meng-Papaxanthos</a>, <a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a>, <a href="http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1">Marco Cuturi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06345">
                                    <div class="article-summary-box-inner">
                                        <span>Consider a heterogeneous population of points evolving with time. While the
population evolves, both in size and nature, we can observe it periodically,
through snapshots taken at different timestamps. Each of these snapshots is
formed by sampling points from the population at that time, and then creating
features to recover point clouds. While these snapshots describe the
population&#x27;s evolution on aggregate, they do not provide directly insights on
individual trajectories. This scenario is encountered in several applications,
notably single-cell genomics experiments, tracking of particles, or when
studying crowd motion. In this paper, we propose to model that dynamic as
resulting from the celebrated Jordan-Kinderlehrer-Otto (JKO) proximal scheme.
The JKO scheme posits that the configuration taken by a population at time $t$
is one that trades off a decrease w.r.t. an energy (the model we seek to learn)
penalized by an optimal transport distance w.r.t. the previous configuration.
To that end, we propose JKOnet, a neural architecture that combines an energy
model on measures, with (small) optimal displacements solved with input convex
neural networks (ICNN). We demonstrate the applicability of our model to
explain and predict population dynamics.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Understanding Generalization via Decomposing Excess Risk Dynamics. (arXiv:2106.06153v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Teng_J/0/1/0/all/0/1">Jiaye Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jianhao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yang Yuan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06153">
                                    <div class="article-summary-box-inner">
                                        <span>Generalization is one of the critical issues in machine learning. However,
traditional methods like uniform convergence are not powerful enough to fully
explain generalization because they may yield vacuous bounds even in
overparameterized linear regression regimes. An alternative solution is to
analyze the generalization dynamics to derive algorithm-dependent bounds, e.g.,
stability. Unfortunately, the stability-based bound is still far from
explaining the remarkable generalization ability of neural networks due to the
coarse-grained analysis of the signal and noise. Inspired by the observation
that neural networks show a slow convergence rate when fitting noise, we
propose decomposing the excess risk dynamics and applying stability-based bound
only on the variance part (which measures how the model performs on pure
noise). We provide two applications for the framework, including a linear case
(overparameterized linear regression with gradient descent) and a non-linear
case (matrix recovery with gradient flow). Under the decomposition framework,
the new bound accords better with the theoretical and empirical evidence
compared to the stability-based bound and uniform convergence bound.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Preferential Temporal Difference Learning. (arXiv:2106.06508v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Anand_N/0/1/0/all/0/1">Nishanth Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06508">
                                    <div class="article-summary-box-inner">
                                        <span>Temporal-Difference (TD) learning is a general and very useful tool for
estimating the value function of a given policy, which in turn is required to
find good policies. Generally speaking, TD learning updates states whenever
they are visited. When the agent lands in a state, its value can be used to
compute the TD-error, which is then propagated to other states. However, it may
be interesting, when computing updates, to take into account other information
than whether a state is visited or not. For example, some states might be more
important than others (such as states which are frequently seen in a successful
trajectory). Or, some states might have unreliable value estimates (for
example, due to partial observability or lack of data), making their values
less desirable as targets. We propose an approach to re-weighting states used
in TD updates, both when they are the input and when they provide the target
for the update. We prove that our approach converges with linear function
approximation and illustrate its desirable empirical behaviour compared to
other TD-style methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Label Noise SGD Provably Prefers Flat Global Minimizers. (arXiv:2106.06530v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Damian_A/0/1/0/all/0/1">Alex Damian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tengyu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jason Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06530">
                                    <div class="article-summary-box-inner">
                                        <span>In overparametrized models, the noise in stochastic gradient descent (SGD)
implicitly regularizes the optimization trajectory and determines which local
minimum SGD converges to. Motivated by empirical studies that demonstrate that
training with noisy labels improves generalization, we study the implicit
regularization effect of SGD with label noise. We show that SGD with label
noise converges to a stationary point of a regularized loss $L(\theta) +\lambda
R(\theta)$, where $L(\theta)$ is the training loss, $\lambda$ is an effective
regularization parameter depending on the step size, strength of the label
noise, and the batch size, and $R(\theta)$ is an explicit regularizer that
penalizes sharp minimizers. Our analysis uncovers an additional regularization
effect of large learning rates beyond the linear scaling rule that penalizes
large eigenvalues of the Hessian more than small ones. We also prove extensions
to classification with general loss functions, SGD with momentum, and SGD with
general noise covariance, significantly strengthening the prior work of Blanc
et al. to global convergence and large learning rates and of HaoChen et al. to
general models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Survey of Image Based Graph Neural Networks. (arXiv:2106.06307v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nazir_U/0/1/0/all/0/1">Usman Nazir</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">He Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Taj_M/0/1/0/all/0/1">Murtaza Taj</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06307">
                                    <div class="article-summary-box-inner">
                                        <span>In this survey paper, we analyze image based graph neural networks and
propose a three-step classification approach. We first convert the image into
superpixels using the Quickshift algorithm so as to reduce 30% of the input
data. The superpixels are subsequently used to generate a region adjacency
graph. Finally, the graph is passed through a state-of-art graph convolutional
neural network to get classification scores. We also analyze the spatial and
spectral convolution filtering techniques in graph neural networks.
Spectral-based models perform better than spatial-based models and classical
CNN with lesser compute cost.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">What Can Knowledge Bring to Machine Learning? -- A Survey of Low-shot Learning for Structured Data. (arXiv:2106.06410v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chapman_A/0/1/0/all/0/1">Adriane Chapman</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_G/0/1/0/all/0/1">Guihua Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hall_D/0/1/0/all/0/1">Dame Wendy Hall</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06410">
                                    <div class="article-summary-box-inner">
                                        <span>Supervised machine learning has several drawbacks that make it difficult to
use in many situations. Drawbacks include: heavy reliance on massive training
data, limited generalizability and poor expressiveness of high-level semantics.
Low-shot Learning attempts to address these drawbacks. Low-shot learning allows
the model to obtain good predictive power with very little or no training data,
where structured knowledge plays a key role as a high-level semantic
representation of human. This article will review the fundamental factors of
low-shot learning technologies, with a focus on the operation of structured
knowledge under different low-shot conditions. We also introduce other
techniques relevant to low-shot learning. Finally, we point out the limitations
of low-shot learning, the prospects and gaps of industrial applications, and
future research directions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Nonmyopic Multifidelity Active Search. (arXiv:2106.06356v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1">Quan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Modiri_A/0/1/0/all/0/1">Arghavan Modiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Garnett_R/0/1/0/all/0/1">Roman Garnett</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06356">
                                    <div class="article-summary-box-inner">
                                        <span>Active search is a learning paradigm where we seek to identify as many
members of a rare, valuable class as possible given a labeling budget. Previous
work on active search has assumed access to a faithful (and expensive) oracle
reporting experimental results. However, some settings offer access to cheaper
surrogates such as computational simulation that may aid in the search. We
propose a model of multifidelity active search, as well as a novel,
computationally efficient policy for this setting that is motivated by
state-of-the-art classical policies. Our policy is nonmyopic and budget aware,
allowing for a dynamic tradeoff between exploration and exploitation. We
evaluate the performance of our solution on real-world datasets and demonstrate
significantly better performance than natural benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Order Matters: Probabilistic Modeling of Node Sequence for Graph Generation. (arXiv:2106.06189v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1">Xiaohui Chen</a>, <a href="http://arxiv.org/find/stat/1/au:+Han_X/0/1/0/all/0/1">Xu Han</a>, <a href="http://arxiv.org/find/stat/1/au:+Hu_J/0/1/0/all/0/1">Jiajing Hu</a>, <a href="http://arxiv.org/find/stat/1/au:+Ruiz_F/0/1/0/all/0/1">Francisco J. R. Ruiz</a>, <a href="http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1">Liping Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06189">
                                    <div class="article-summary-box-inner">
                                        <span>A graph generative model defines a distribution over graphs. One type of
generative model is constructed by autoregressive neural networks, which
sequentially add nodes and edges to generate a graph. However, the likelihood
of a graph under the autoregressive model is intractable, as there are numerous
sequences leading to the given graph; this makes maximum likelihood estimation
challenging. Instead, in this work we derive the exact joint probability over
the graph and the node ordering of the sequential process. From the joint, we
approximately marginalize out the node orderings and compute a lower bound on
the log-likelihood using variational inference. We train graph generative
models by maximizing this bound, without using the ad-hoc node orderings of
previous methods. Our experiments show that the log-likelihood bound is
significantly tighter than the bound of previous schemes. Moreover, the models
fitted with the proposed algorithm can generate high-quality graphs that match
the structures of target graphs not seen during training. We have made our code
publicly available at
\hyperref[https://github.com/tufts-ml/graph-generation-vi]{https://github.com/tufts-ml/graph-generation-vi}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploiting Record Similarity for Practical Vertical Federated Learning. (arXiv:2106.06312v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhaomin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qinbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Bingsheng He</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06312">
                                    <div class="article-summary-box-inner">
                                        <span>As the privacy of machine learning has drawn increasing attention, federated
learning is introduced to enable collaborative learning without revealing raw
data. Notably, \textit{vertical federated learning} (VFL), where parties share
the same set of samples but only hold partial features, has a wide range of
real-world applications. However, existing studies in VFL rarely study the
&#x60;&#x60;record linkage&#x27;&#x27; process. They either design algorithms assuming the data
from different parties have been linked or use simple linkage methods like
exact-linkage or top1-linkage. These approaches are unsuitable for many
applications, such as the GPS location and noisy titles requiring fuzzy
matching. In this paper, we design a novel similarity-based VFL framework,
FedSim, which is suitable for more real-world applications and achieves higher
performance on traditional VFL tasks. Moreover, we theoretically analyze the
privacy risk caused by sharing similarities. Our experiments on three synthetic
datasets and five real-world datasets with various similarity metrics show that
FedSim consistently outperforms other state-of-the-art baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Keyframe-Focused Visual Imitation Learning. (arXiv:2106.06452v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1">Chuan Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jierui Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1">Jianing Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1">Dinesh Jayaraman</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06452">
                                    <div class="article-summary-box-inner">
                                        <span>Imitation learning trains control policies by mimicking pre-recorded expert
demonstrations. In partially observable settings, imitation policies must rely
on observation histories, but many seemingly paradoxical results show better
performance for policies that only access the most recent observation. Recent
solutions ranging from causal graph learning to deep information bottlenecks
have shown promising results, but failed to scale to realistic settings such as
visual imitation. We propose a solution that outperforms these prior approaches
by upweighting demonstration keyframes corresponding to expert action
changepoints. This simple approach easily scales to complex visual imitation
settings. Our experimental results demonstrate consistent performance
improvements over all baselines on image-based Gym MuJoCo continuous control
tasks. Finally, on the CARLA photorealistic vision-based urban driving
simulator, we resolve a long-standing issue in behavioral cloning for driving
by demonstrating effective imitation from observation histories. Supplementary
materials and code at: \url{https://tinyurl.com/imitation-keyframes}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Within-layer Diversity Reduces Generalization Gap. (arXiv:2106.06012v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Laakom_F/0/1/0/all/0/1">Firas Laakom</a>, <a href="http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1">Jenni Raitoharju</a>, <a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1">Alexandros Iosifidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1">Moncef Gabbouj</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06012">
                                    <div class="article-summary-box-inner">
                                        <span>Neural networks are composed of multiple layers arranged in a hierarchical
structure jointly trained with a gradient-based optimization, where the errors
are back-propagated from the last layer back to the first one. At each
optimization step, neurons at a given layer receive feedback from neurons
belonging to higher layers of the hierarchy. In this paper, we propose to
complement this traditional &#x27;between-layer&#x27; feedback with additional
&#x27;within-layer&#x27; feedback to encourage diversity of the activations within the
same layer. To this end, we measure the pairwise similarity between the outputs
of the neurons and use it to model the layer&#x27;s overall diversity. By penalizing
similarities and promoting diversity, we encourage each neuron to learn a
distinctive representation and, thus, to enrich the data representation learned
within the layer and to increase the total capacity of the model. We
theoretically study how the within-layer activation diversity affects the
generalization performance of a neural network and prove that increasing the
diversity of hidden activations reduces the estimation error. In addition to
the theoretical guarantees, we present an empirical study on three datasets
confirming that the proposed approach enhances the performance of
state-of-the-art neural network models and decreases the generalization gap.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dynamic Language Models for Continuously Evolving Content. (arXiv:2106.06297v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hombaiah_S/0/1/0/all/0/1">Spurthi Amba Hombaiah</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Mingyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1">Michael Bendersky</a>, <a href="http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1">Marc Najork</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06297">
                                    <div class="article-summary-box-inner">
                                        <span>The content on the web is in a constant state of flux. New entities, issues,
and ideas continuously emerge, while the semantics of the existing conversation
topics gradually shift. In recent years, pre-trained language models like BERT
greatly improved the state-of-the-art for a large spectrum of content
understanding tasks. Therefore, in this paper, we aim to study how these
language models can be adapted to better handle continuously evolving web
content. In our study, we first analyze the evolution of 2013 - 2019 Twitter
data, and unequivocally confirm that a BERT model trained on past tweets would
heavily deteriorate when directly applied to data from later years. Then, we
investigate two possible sources of the deterioration: the semantic shift of
existing tokens and the sub-optimal or failed understanding of new tokens. To
this end, we both explore two different vocabulary composition methods, as well
as propose three sampling methods which help in efficient incremental training
for BERT-like models. Compared to a new model trained from scratch offline, our
incremental training (a) reduces the training costs, (b) achieves better
performance on evolving content, and (c) is suitable for online deployment. The
superiority of our methods is validated using two downstream tasks. We
demonstrate significant improvements when incrementally evolving the model from
a particular base year, on the task of Country Hashtag Prediction, as well as
on the OffensEval 2019 task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ViT-Inception-GAN for Image Colourising. (arXiv:2106.06321v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bana_T/0/1/0/all/0/1">Tejas Bana</a>, <a href="http://arxiv.org/find/cs/1/au:+Loya_J/0/1/0/all/0/1">Jatan Loya</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1">Siddhant Kulkarni</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06321">
                                    <div class="article-summary-box-inner">
                                        <span>Studies involving colourising images has been garnering researchers&#x27; keen
attention over time, assisted by significant advances in various Machine
Learning techniques and compute power availability. Traditionally, colourising
images have been an intricate task that gave a substantial degree of freedom
during the assignment of chromatic information. In our proposed method, we
attempt to colourise images using Vision Transformer - Inception - Generative
Adversarial Network (ViT-I-GAN), which has an Inception-v3 fusion embedding in
the generator. For a stable and robust network, we have used Vision Transformer
(ViT) as the discriminator. We trained the model on the Unsplash and the COCO
dataset for demonstrating the improvement made by the Inception-v3 embedding.
We have compared the results between ViT-GANs with and without Inception-v3
embedding.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Graph Transformer Networks: Learning Meta-path Graphs to Improve GNNs. (arXiv:2106.06218v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Seongjun Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1">Minbyul Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1">Sungdong Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seunghun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1">Sean S. Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_R/0/1/0/all/0/1">Raehyun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1">Jaewoo Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunwoo J. Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06218">
                                    <div class="article-summary-box-inner">
                                        <span>Graph Neural Networks (GNNs) have been widely applied to various fields due
to their powerful representations of graph-structured data. Despite the success
of GNNs, most existing GNNs are designed to learn node representations on the
fixed and homogeneous graphs. The limitations especially become problematic
when learning representations on a misspecified graph or a heterogeneous graph
that consists of various types of nodes and edges. To address this limitations,
we propose Graph Transformer Networks (GTNs) that are capable of generating new
graph structures, which preclude noisy connections and include useful
connections (e.g., meta-paths) for tasks, while learning effective node
representations on the new graphs in an end-to-end fashion. We further propose
enhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that
improve scalability of graph transformations. Compared to GTNs, FastGTNs are
230x faster and use 100x less memory while allowing the identical graph
transformations as GTNs. In addition, we extend graph transformations to the
semantic proximity of nodes allowing non-local operations beyond meta-paths.
Extensive experiments on both homogeneous graphs and heterogeneous graphs show
that GTNs and FastGTNs with non-local operations achieve the state-of-the-art
performance for node classification tasks. The code is available:
https://github.com/seongjunyun/Graph_Transformer_Networks</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">States of confusion: Eye and Head tracking reveal surgeons&#x27; confusion during arthroscopic surgery. (arXiv:2106.06261v1 [cs.HC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hosp_B/0/1/0/all/0/1">Benedikt Hosp</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1">Myat Su Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Haddawy_p/0/1/0/all/0/1">peter Haddawy</a>, <a href="http://arxiv.org/find/cs/1/au:+Watcharporas_R/0/1/0/all/0/1">Ratthapoom Watcharporas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sa_ngasoonsong_p/0/1/0/all/0/1">paphon Sa-ngasoonsong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1">Enkelejda Kasneci</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06261">
                                    <div class="article-summary-box-inner">
                                        <span>During arthroscopic surgeries, surgeons are faced with challenges like
cognitive re-projection of the 2D screen output into the 3D operating site or
navigation through highly similar tissue. Training of these cognitive processes
takes much time and effort for young surgeons, but is necessary and crucial for
their education. In this study we want to show how to recognize states of
confusion of young surgeons during an arthroscopic surgery, by looking at their
eye and head movements and feeding them to a machine learning model. With an
accuracy of over 94\% and detection speed of 0.039 seconds, our model is a step
towards online diagnostic and training systems for the perceptual-cognitive
processes of surgeons during arthroscopic surgeries.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Conditional Gaussian Mixture Model for Constrained Clustering. (arXiv:2106.06385v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Manduchi_L/0/1/0/all/0/1">Laura Manduchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chin_Cheong_K/0/1/0/all/0/1">Kieran Chin-Cheong</a>, <a href="http://arxiv.org/find/cs/1/au:+Michel_H/0/1/0/all/0/1">Holger Michel</a>, <a href="http://arxiv.org/find/cs/1/au:+Wellmann_S/0/1/0/all/0/1">Sven Wellmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1">Julia E. Vogt</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06385">
                                    <div class="article-summary-box-inner">
                                        <span>Constrained clustering has gained significant attention in the field of
machine learning as it can leverage prior information on a growing amount of
only partially labeled data. Following recent advances in deep generative
models, we propose a novel framework for constrained clustering that is
intuitive, interpretable, and can be trained efficiently in the framework of
stochastic gradient variational inference. By explicitly integrating domain
knowledge in the form of probabilistic relations, our proposed model (DC-GMM)
uncovers the underlying distribution of data conditioned on prior clustering
preferences, expressed as pairwise constraints. These constraints guide the
clustering process towards a desirable partition of the data by indicating
which samples should or should not belong to the same cluster. We provide
extensive experiments to demonstrate that DC-GMM shows superior clustering
performances and robustness compared to state-of-the-art deep constrained
clustering methods on a wide range of data sets. We further demonstrate the
usefulness of our approach on two challenging real-world applications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">HPO-B: A Large-Scale Reproducible Benchmark for Black-Box HPO based on OpenML. (arXiv:2106.06257v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Arango_S/0/1/0/all/0/1">Sebastian Pineda Arango</a>, <a href="http://arxiv.org/find/cs/1/au:+Jomaa_H/0/1/0/all/0/1">Hadi S. Jomaa</a>, <a href="http://arxiv.org/find/cs/1/au:+Wistuba_M/0/1/0/all/0/1">Martin Wistuba</a>, <a href="http://arxiv.org/find/cs/1/au:+Grabocka_J/0/1/0/all/0/1">Josif Grabocka</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06257">
                                    <div class="article-summary-box-inner">
                                        <span>Hyperparameter optimization (HPO) is a core problem for the machine learning
community and remains largely unsolved due to the significant computational
resources required to evaluate hyperparameter configurations. As a result, a
series of recent related works have focused on the direction of transfer
learning for quickly fine-tuning hyperparameters on a dataset. Unfortunately,
the community does not have a common large-scale benchmark for comparing HPO
algorithms. Instead, the de facto practice consists of empirical protocols on
arbitrary small-scale meta-datasets that vary inconsistently across
publications, making reproducibility a challenge. To resolve this major
bottleneck and enable a fair and fast comparison of black-box HPO methods on a
level playing field, we propose HPO-B, a new large-scale benchmark in the form
of a collection of meta-datasets. Our benchmark is assembled and preprocessed
from the OpenML repository and consists of 176 search spaces (algorithms)
evaluated sparsely on 196 datasets with a total of 6.4 million hyperparameter
evaluations. For ensuring reproducibility on our benchmark, we detail explicit
experimental protocols, splits, and evaluation measures for comparing methods
for both non-transfer, as well as, transfer learning HPO.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On Learnability via Gradient Method for Two-Layer ReLU Neural Networks in Teacher-Student Setting. (arXiv:2106.06251v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Akiyama_S/0/1/0/all/0/1">Shunta Akiyama</a>, <a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1">Taiji Suzuki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06251">
                                    <div class="article-summary-box-inner">
                                        <span>Deep learning empirically achieves high performance in many applications, but
its training dynamics has not been fully understood theoretically. In this
paper, we explore theoretical analysis on training two-layer ReLU neural
networks in a teacher-student regression model, in which a student network
learns an unknown teacher network through its outputs. We show that with a
specific regularization and sufficient over-parameterization, the student
network can identify the parameters of the teacher network with high
probability via gradient descent with a norm dependent stepsize even though the
objective function is highly non-convex. The key theoretical tool is the
measure representation of the neural networks and a novel application of a dual
certificate argument for sparse estimation on a measure space. We analyze the
global minima and global convergence property in the measure space.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial Attacks. (arXiv:2106.06235v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gurel_N/0/1/0/all/0/1">Nezihe Merve G&#xfc;rel</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiangyu Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rimanic_L/0/1/0/all/0/1">Luka Rimanic</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ce Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06235">
                                    <div class="article-summary-box-inner">
                                        <span>Despite the great successes achieved by deep neural networks (DNNs), recent
studies show that they are vulnerable against adversarial examples, which aim
to mislead DNNs by adding small adversarial perturbations. Several defenses
have been proposed against such attacks, while many of them have been
adaptively attacked. In this work, we aim to enhance the ML robustness from a
different perspective by leveraging domain knowledge: We propose a Knowledge
Enhanced Machine Learning Pipeline (KEMLP) to integrate domain knowledge (i.e.,
logic relationships among different predictions) into a probabilistic graphical
model via first-order logic rules. In particular, we develop KEMLP by
integrating a diverse set of weak auxiliary models based on their logical
relationships to the main DNN model that performs the target task.
Theoretically, we provide convergence results and prove that, under mild
conditions, the prediction of KEMLP is more robust than that of the main DNN
model. Empirically, we take road sign recognition as an example and leverage
the relationships between road signs and their shapes and contents as domain
knowledge. We show that compared with adversarial training and other baselines,
KEMLP achieves higher robustness against physical attacks, $\mathcal{L}_p$
bounded attacks, unforeseen attacks, and natural corruptions under both
whitebox and blackbox settings, while still maintaining high clean accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Nested and Balanced Entity Recognition using Multi-Task Learning. (arXiv:2106.06216v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Waldis_A/0/1/0/all/0/1">Andreas Waldis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazzola_L/0/1/0/all/0/1">Luca Mazzola</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06216">
                                    <div class="article-summary-box-inner">
                                        <span>Entity Recognition (ER) within a text is a fundamental exercise in Natural
Language Processing, enabling further depending tasks such as Knowledge
Extraction, Text Summarisation, or Keyphrase Extraction. An entity consists of
single words or of a consecutive sequence of terms, constituting the basic
building blocks for communication. Mainstream ER approaches are mainly limited
to flat structures, concentrating on the outermost entities while ignoring the
inner ones. This paper introduces a partly-layered network architecture that
deals with the complexity of overlapping and nested cases. The proposed
architecture consists of two parts: (1) a shared Sequence Layer and (2) a
stacked component with multiple Tagging Layers. The adoption of such an
architecture has the advantage of preventing overfit to a specific word-length,
thus maintaining performance for longer entities despite their lower frequency.
To verify the proposed architecture&#x27;s effectiveness, we train and evaluate this
architecture to recognise two kinds of entities - Concepts (CR) and Named
Entities (NER). Our approach achieves state-of-the-art NER performances, while
it outperforms previous CR approaches. Considering these promising results, we
see the possibility to evolve the architecture for other cases such as the
extraction of events or the detection of argumentative components.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Complexity of Sparse Tensor PCA. (arXiv:2106.06308v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Choo_D/0/1/0/all/0/1">Davin Choo</a>, <a href="http://arxiv.org/find/cs/1/au:+dOrsi_T/0/1/0/all/0/1">Tommaso d&#x27;Orsi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06308">
                                    <div class="article-summary-box-inner">
                                        <span>We study the problem of sparse tensor principal component analysis: given a
tensor $\pmb Y &#x3D; \pmb W + \lambda x^{\otimes p}$ with $\pmb W \in
\otimes^p\mathbb{R}^n$ having i.i.d. Gaussian entries, the goal is to recover
the $k$-sparse unit vector $x \in \mathbb{R}^n$. The model captures both sparse
PCA (in its Wigner form) and tensor PCA.

For the highly sparse regime of $k \leq \sqrt{n}$, we present a family of
algorithms that smoothly interpolates between a simple polynomial-time
algorithm and the exponential-time exhaustive search algorithm. For any $1 \leq
t \leq k$, our algorithms recovers the sparse vector for signal-to-noise ratio
$\lambda \geq \tilde{\mathcal{O}} (\sqrt{t} \cdot (k/t)^{p/2})$ in time
$\tilde{\mathcal{O}}(n^{p+t})$, capturing the state-of-the-art guarantees for
the matrix settings (in both the polynomial-time and sub-exponential time
regimes).

Our results naturally extend to the case of $r$ distinct $k$-sparse signals
with disjoint supports, with guarantees that are independent of the number of
spikes. Even in the restricted case of sparse PCA, known algorithms only
recover the sparse vectors for $\lambda \geq \tilde{\mathcal{O}}(k \cdot r)$
while our algorithms require $\lambda \geq \tilde{\mathcal{O}}(k)$.

Finally, by analyzing the low-degree likelihood ratio, we complement these
algorithmic results with rigorous evidence illustrating the trade-offs between
signal-to-noise ratio and running time. This lower bound captures the known
lower bounds for both sparse PCA and tensor PCA. In this general model, we
observe a more intricate three-way trade-off between the number of samples $n$,
the sparsity $k$, and the tensor power $p$.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">KRADA: Known-region-aware Domain Alignment for Open World Semantic Segmentation. (arXiv:2106.06237v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1">Chenhong Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1">Feng Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Gong_C/0/1/0/all/0/1">Chen Gong</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/eess/1/au:+Cheung_W/0/1/0/all/0/1">William Cheung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06237">
                                    <div class="article-summary-box-inner">
                                        <span>In semantic segmentation, we aim to train a pixel-level classifier to assign
category labels to all pixels in an image, where labeled training images and
unlabeled test images are from the same distribution and share the same label
set. However, in an open world, the unlabeled test images probably contain
unknown categories and have different distributions from the labeled images.
Hence, in this paper, we consider a new, more realistic, and more challenging
problem setting where the pixel-level classifier has to be trained with labeled
images and unlabeled open-world images -- we name it open world semantic
segmentation (OSS). In OSS, the trained classifier is expected to identify
unknown-class pixels and classify known-class pixels well. To solve OSS, we
first investigate which distribution that unknown-class pixels obey. Then,
motivated by the goodness-of-fit test, we use statistical measurements to show
how a pixel fits the distribution of an unknown class and select highly-fitted
pixels to form the unknown region in each image. Eventually, we propose an
end-to-end learning framework, known-region-aware domain alignment (KRADA), to
distinguish unknown classes while aligning distributions of known classes in
labeled and unlabeled open-world images. The effectiveness of KRADA has been
verified on two synthetic tasks and one COVID-19 segmentation task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Novel Approach to Lifelong Learning: The Plastic Support Structure. (arXiv:2106.06298v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kanaan_G/0/1/0/all/0/1">Georges Kanaan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1">Kai Wen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fenaux_L/0/1/0/all/0/1">Lucas Fenaux</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06298">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a novel approach to lifelong learning, introducing a compact
encapsulated support structure which endows a network with the capability to
expand its capacity as needed to learn new tasks while preventing the loss of
learned tasks. This is achieved by splitting neurons with high semantic drift
and constructing an adjacent network to encode the new tasks at hand. We call
this the Plastic Support Structure (PSS), it is a compact structure to learn
new tasks that cannot be efficiently encoded in the existing structure of the
network. We validate the PSS on public datasets against existing lifelong
learning architectures, showing it performs similarly to them but without prior
knowledge of the task and in some cases with fewer parameters and in a more
understandable fashion where the PSS is an encapsulated container for specific
features related to specific tasks, thus making it an ideal &quot;add-on&quot; solution
for endowing a network to learn more tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Courteous Behavior of Automated Vehicles at Unsignalized Intersections via Reinforcement Learning. (arXiv:2106.06369v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shengchao Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Welschehold_T/0/1/0/all/0/1">Tim Welschehold</a>, <a href="http://arxiv.org/find/cs/1/au:+Buscher_D/0/1/0/all/0/1">Daniel B&#xfc;scher</a>, <a href="http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1">Wolfram Burgard</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06369">
                                    <div class="article-summary-box-inner">
                                        <span>The transition from today&#x27;s mostly human-driven traffic to a purely automated
one will be a gradual evolution, with the effect that we will likely experience
mixed traffic in the near future. Connected and automated vehicles can benefit
human-driven ones and the whole traffic system in different ways, for example
by improving collision avoidance and reducing traffic waves. Many studies have
been carried out to improve intersection management, a significant bottleneck
in traffic, with intelligent traffic signals or exclusively automated vehicles.
However, the problem of how to improve mixed traffic at unsignalized
intersections has received less attention. In this paper, we propose a novel
approach to optimizing traffic flow at intersections in mixed traffic
situations using deep reinforcement learning. Our reinforcement learning agent
learns a policy for a centralized controller to let connected autonomous
vehicles at unsignalized intersections give up their right of way and yield to
other vehicles to optimize traffic flow. We implemented our approach and tested
it in the traffic simulator SUMO based on simulated and real traffic data. The
experimental evaluation demonstrates that our method significantly improves
traffic flow through unsignalized intersections in mixed traffic settings and
also provides better performance on a wide range of traffic situations compared
to the state-of-the-art traffic signal controller for the corresponding
signalized intersection.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Framework to Enhance Generalization of Deep Metric Learning methods using General Discriminative Feature Learning and Class Adversarial Neural Networks. (arXiv:2106.06420v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Al_Kaabi_K/0/1/0/all/0/1">Karrar Al-Kaabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Monsefi_R/0/1/0/all/0/1">Reza Monsefi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1">Davood Zabihzadeh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06420">
                                    <div class="article-summary-box-inner">
                                        <span>Metric learning algorithms aim to learn a distance function that brings the
semantically similar data items together and keeps dissimilar ones at a
distance. The traditional Mahalanobis distance learning is equivalent to find a
linear projection. In contrast, Deep Metric Learning (DML) methods are proposed
that automatically extract features from data and learn a non-linear
transformation from input space to a semantically embedding space. Recently,
many DML methods are proposed focused to enhance the discrimination power of
the learned metric by providing novel sampling strategies or loss functions.
This approach is very helpful when both the training and test examples are
coming from the same set of categories. However, it is less effective in many
applications of DML such as image retrieval and person-reidentification. Here,
the DML should learn general semantic concepts from observed classes and employ
them to rank or identify objects from unseen categories. Neglecting the
generalization ability of the learned representation and just emphasizing to
learn a more discriminative embedding on the observed classes may lead to the
overfitting problem. To address this limitation, we propose a framework to
enhance the generalization power of existing DML methods in a Zero-Shot
Learning (ZSL) setting by general yet discriminative representation learning
and employing a class adversarial neural network. To learn a more general
representation, we propose to employ feature maps of intermediate layers in a
deep neural network and enhance their discrimination power through an attention
mechanism. Besides, a class adversarial network is utilized to enforce the deep
model to seek class invariant features for the DML task. We evaluate our work
on widely used machine vision datasets in a ZSL setting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">TOHAN: A One-step Approach towards Few-shot Hypothesis Adaptation. (arXiv:2106.06326v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1">Haoang Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Feng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wenjing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1">Long Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1">William K. Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1">James T. Kwok</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06326">
                                    <div class="article-summary-box-inner">
                                        <span>In few-shot domain adaptation (FDA), classifiers for the target domain are
trained with accessible labeled data in the source domain (SD) and few labeled
data in the target domain (TD). However, data usually contain private
information in the current era, e.g., data distributed on personal phones.
Thus, the private information will be leaked if we directly access data in SD
to train a target-domain classifier (required by FDA methods). In this paper,
to thoroughly prevent the privacy leakage in SD, we consider a very challenging
problem setting, where the classifier for the TD has to be trained using few
labeled target data and a well-trained SD classifier, named few-shot hypothesis
adaptation (FHA). In FHA, we cannot access data in SD, as a result, the private
information in SD will be protected well. To this end, we propose a target
orientated hypothesis adaptation network (TOHAN) to solve the FHA problem,
where we generate highly-compatible unlabeled data (i.e., an intermediate
domain) to help train a target-domain classifier. TOHAN maintains two deep
networks simultaneously, where one focuses on learning an intermediate domain
and the other takes care of the intermediate-to-target distributional
adaptation and the target-risk minimization. Experimental results show that
TOHAN outperforms competitive baselines significantly.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Automatic Risk Adaptation in Distributional Reinforcement Learning. (arXiv:2106.06317v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1">Frederik Schubert</a>, <a href="http://arxiv.org/find/cs/1/au:+Eimer_T/0/1/0/all/0/1">Theresa Eimer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1">Bodo Rosenhahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1">Marius Lindauer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06317">
                                    <div class="article-summary-box-inner">
                                        <span>The use of Reinforcement Learning (RL) agents in practical applications
requires the consideration of suboptimal outcomes, depending on the familiarity
of the agent with its environment. This is especially important in
safety-critical environments, where errors can lead to high costs or damage. In
distributional RL, the risk-sensitivity can be controlled via different
distortion measures of the estimated return distribution. However, these
distortion functions require an estimate of the risk level, which is difficult
to obtain and depends on the current state. In this work, we demonstrate the
suboptimality of a static risk level estimation and propose a method to
dynamically select risk levels at each environment step. Our method ARA
(Automatic Risk Adaptation) estimates the appropriate risk level in both known
and unknown environments using a Random Network Distillation error. We show
reduced failure rates by up to a factor of 7 and improved generalization
performance by up to 14% compared to both risk-aware and risk-agnostic agents
in several locomotion environments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning. (arXiv:2106.06232v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1">Jiajun Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Changnan Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yue Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06232">
                                    <div class="article-summary-box-inner">
                                        <span>Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning
(DRL) via combining deep learning (DL) with reinforcement learning (RL), which
has noticed that the distribution of the acquired data would change during the
training process. DQN found this property might cause instability for training,
so it proposed effective methods to handle the downside of the property.
Instead of focusing on the unfavourable aspects, we find it critical for RL to
ease the gap between the estimated data distribution and the ground truth data
distribution while supervised learning (SL) fails to do so. From this new
perspective, we extend the basic paradigm of RL called the Generalized Policy
Iteration (GPI) into a more generalized version, which is called the
Generalized Data Distribution Iteration (GDI). We see massive RL algorithms and
techniques can be unified into the GDI paradigm, which can be considered as one
of the special cases of GDI. We provide theoretical proof of why GDI is better
than GPI and how it works. Several practical algorithms based on GDI have been
proposed to verify the effectiveness and extensiveness of it. Empirical
experiments prove our state-of-the-art (SOTA) performance on Arcade Learning
Environment (ALE), wherein our algorithm has achieved 9620.98% mean human
normalized score (HNS), 1146.39% median HNS and 22 human world record
breakthroughs (HWRB) using only 200 training frames. Our work aims to lead the
RL research to step into the journey of conquering the human world records and
seek real superhuman agents on both performance and efficiency.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Model Selection for Bayesian Autoencoders. (arXiv:2106.06245v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Tran_B/0/1/0/all/0/1">Ba-Hien Tran</a>, <a href="http://arxiv.org/find/stat/1/au:+Rossi_S/0/1/0/all/0/1">Simone Rossi</a>, <a href="http://arxiv.org/find/stat/1/au:+Milios_D/0/1/0/all/0/1">Dimitrios Milios</a>, <a href="http://arxiv.org/find/stat/1/au:+Michiardi_P/0/1/0/all/0/1">Pietro Michiardi</a>, <a href="http://arxiv.org/find/stat/1/au:+Bonilla_E/0/1/0/all/0/1">Edwin V. Bonilla</a>, <a href="http://arxiv.org/find/stat/1/au:+Filippone_M/0/1/0/all/0/1">Maurizio Filippone</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06245">
                                    <div class="article-summary-box-inner">
                                        <span>We develop a novel method for carrying out model selection for Bayesian
autoencoders (BAEs) by means of prior hyper-parameter optimization. Inspired by
the common practice of type-II maximum likelihood optimization and its
equivalence to Kullback-Leibler divergence minimization, we propose to optimize
the distributional sliced-Wasserstein distance (DSWD) between the output of the
autoencoder and the empirical data distribution. The advantages of this
formulation are that we can estimate the DSWD based on samples and handle
high-dimensional problems. We carry out posterior estimation of the BAE
parameters via stochastic gradient Hamiltonian Monte Carlo and turn our BAE
into a generative model by fitting a flexible Dirichlet mixture model in the
latent space. Consequently, we obtain a powerful alternative to variational
autoencoders, which are the preferred choice in modern applications of
autoencoders for representation learning with uncertainty. We evaluate our
approach qualitatively and quantitatively using a vast experimental campaign on
a number of unsupervised learning tasks and show that, in small-data regimes
where priors matter, our approach provides state-of-the-art results,
outperforming multiple competitive baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm. (arXiv:2106.06300v1 [stat.ME])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Plassier_V/0/1/0/all/0/1">Vincent Plassier</a>, <a href="http://arxiv.org/find/stat/1/au:+Vono_M/0/1/0/all/0/1">Maxime Vono</a>, <a href="http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1">Alain Durmus</a>, <a href="http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1">Eric Moulines</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06300">
                                    <div class="article-summary-box-inner">
                                        <span>Performing reliable Bayesian inference on a big data scale is becoming a
keystone in the modern era of machine learning. A workhorse class of methods to
achieve this task are Markov chain Monte Carlo (MCMC) algorithms and their
design to handle distributed datasets has been the subject of many works.
However, existing methods are not completely either reliable or computationally
efficient. In this paper, we propose to fill this gap in the case where the
dataset is partitioned and stored on computing nodes within a cluster under a
master/slaves architecture. We derive a user-friendly centralised distributed
MCMC algorithm with provable scaling in high-dimensional settings. We
illustrate the relevance of the proposed methodology on both synthetic and real
data experiments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Safe Reinforcement Learning with Linear Function Approximation. (arXiv:2106.06239v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Amani_S/0/1/0/all/0/1">Sanae Amani</a>, <a href="http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1">Christos Thrampoulidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lin F. Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06239">
                                    <div class="article-summary-box-inner">
                                        <span>Safety in reinforcement learning has become increasingly important in recent
years. Yet, existing solutions either fail to strictly avoid choosing unsafe
actions, which may lead to catastrophic results in safety-critical systems, or
fail to provide regret guarantees for settings where safety constraints need to
be learned. In this paper, we address both problems by first modeling safety as
an unknown linear cost function of states and actions, which must always fall
below a certain threshold. We then present algorithms, termed SLUCB-QVI and
RSLUCB-QVI, for episodic Markov decision processes (MDPs) with linear function
approximation. We show that SLUCB-QVI and RSLUCB-QVI, while with \emph{no
safety violation}, achieve a
$\tilde{\mathcal{O}}\left(\kappa\sqrt{d^3H^3T}\right)$ regret, nearly matching
that of state-of-the-art unsafe algorithms, where $H$ is the duration of each
episode, $d$ is the dimension of the feature mapping, $\kappa$ is a constant
characterizing the safety constraints, and $T$ is the total number of action
plays. We further present numerical simulations that corroborate our
theoretical findings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">TrafficStream: A Streaming Traffic Flow Forecasting Framework Based on Graph Neural Networks and Continual Learning. (arXiv:2106.06273v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junshan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1">Kunqing Xie</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06273">
                                    <div class="article-summary-box-inner">
                                        <span>With the rapid growth of traffic sensors deployed, a massive amount of
traffic flow data are collected, revealing the long-term evolution of traffic
flows and the gradual expansion of traffic networks. How to accurately
forecasting these traffic flow attracts the attention of researchers as it is
of great significance for improving the efficiency of transportation systems.
However, existing methods mainly focus on the spatial-temporal correlation of
static networks, leaving the problem of efficiently learning models on networks
with expansion and evolving patterns less studied. To tackle this problem, we
propose a Streaming Traffic Flow Forecasting Framework, TrafficStream, based on
Graph Neural Networks (GNNs) and Continual Learning (CL), achieving accurate
predictions and high efficiency. Firstly, we design a traffic pattern fusion
method, cleverly integrating the new patterns that emerged during the long-term
period into the model. A JS-divergence-based algorithm is proposed to mine new
traffic patterns. Secondly, we introduce CL to consolidate the knowledge
learned previously and transfer them to the current model. Specifically, we
adopt two strategies: historical data replay and parameter smoothing. We
construct a streaming traffic dataset to verify the efficiency and
effectiveness of our model. Extensive experiments demonstrate its excellent
potential to extract traffic patterns with high efficiency on long-term
streaming network scene. The source code is available at
https://github.com/AprLie/TrafficStream.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">LocoProp: Enhancing BackProp via Local Loss Optimization. (arXiv:2106.06199v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Amid_E/0/1/0/all/0/1">Ehsan Amid</a>, <a href="http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1">Rohan Anil</a>, <a href="http://arxiv.org/find/cs/1/au:+Warmuth_M/0/1/0/all/0/1">Manfred K. Warmuth</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06199">
                                    <div class="article-summary-box-inner">
                                        <span>We study a local loss construction approach for optimizing neural networks.
We start by motivating the problem as minimizing a squared loss between the
pre-activations of each layer and a local target, plus a regularizer term on
the weights. The targets are chosen so that the first gradient descent step on
the local objectives recovers vanilla BackProp, while the exact solution to
each problem results in a preconditioned gradient update. We improve the local
loss construction by forming a Bregman divergence in each layer tailored to the
transfer function which keeps the local problem convex w.r.t. the weights. The
generalized local problem is again solved iteratively by taking small gradient
descent steps on the weights, for which the first step recovers BackProp. We
run several ablations and show that our construction consistently improves
convergence, reducing the gap between first-order and second-order methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Probabilistic Koopman: Long-term time-series forecasting under periodic uncertainties. (arXiv:2106.06033v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mallen_A/0/1/0/all/0/1">Alex Mallen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lange_H/0/1/0/all/0/1">Henning Lange</a>, <a href="http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1">J. Nathan Kutz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06033">
                                    <div class="article-summary-box-inner">
                                        <span>Probabilistic forecasting of complex phenomena is paramount to various
scientific disciplines and applications. Despite the generality and importance
of the problem, general mathematical techniques that allow for stable long-term
forecasts with calibrated uncertainty measures are lacking. For most time
series models, the difficulty of obtaining accurate probabilistic future time
step predictions increases with the prediction horizon. In this paper, we
introduce a surprisingly simple approach that characterizes time-varying
distributions and enables reasonably accurate predictions thousands of
timesteps into the future. This technique, which we call Deep Probabilistic
Koopman (DPK), is based on recent advances in linear Koopman operator theory,
and does not require time stepping for future time predictions. Koopman models
also tend to have a small parameter footprint (often less than 10,000
parameters). We demonstrate the long-term forecasting performance of these
models on a diversity of domains, including electricity demand forecasting,
atmospheric chemistry, and neuroscience. For electricity demand modeling, our
domain-agnostic technique outperforms all of 177 domain-specific competitors in
the most recent Global Energy Forecasting Competition.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning to Pool in Graph Neural Networks for Extrapolation. (arXiv:2106.06210v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1">Jihoon Ko</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1">Taehyung Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1">Kijung Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Juho Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06210">
                                    <div class="article-summary-box-inner">
                                        <span>Graph neural networks (GNNs) are one of the most popular approaches to using
deep learning on graph-structured data, and they have shown state-of-the-art
performances on a variety of tasks. However, according to a recent study, a
careful choice of pooling functions, which are used for the aggregation or
readout operation in GNNs, is crucial for enabling GNNs to extrapolate. Without
the ideal combination of pooling functions, which varies across tasks, GNNs
completely fail to generalize to out-of-distribution data, while the number of
possible combinations grows exponentially with the number of layers. In this
paper, we present GNP, a $L^p$ norm-like pooling function that is trainable
end-to-end for any given task. Notably, GNP generalizes most of the widely-used
pooling functions. We verify experimentally that simply replacing all pooling
functions with GNP enables GNNs to extrapolate well on many node-level,
graph-level, and set-related tasks; and GNP sometimes performs even better than
optimal combinations of existing pooling functions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Taylor Expansion of Discount Factors. (arXiv:2106.06170v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yunhao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rowland_M/0/1/0/all/0/1">Mark Rowland</a>, <a href="http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1">R&#xe9;mi Munos</a>, <a href="http://arxiv.org/find/cs/1/au:+Valko_M/0/1/0/all/0/1">Michal Valko</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06170">
                                    <div class="article-summary-box-inner">
                                        <span>In practical reinforcement learning (RL), the discount factor used for
estimating value functions often differs from that used for defining the
evaluation objective. In this work, we study the effect that this discrepancy
of discount factors has during learning, and discover a family of objectives
that interpolate value functions of two distinct discount factors. Our analysis
suggests new ways for estimating value functions and performing policy
optimization updates, which demonstrate empirical performance gains. This
framework also leads to new insights on commonly-used deep RL heuristic
modifications to policy optimization algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial purification with Score-based generative models. (arXiv:2106.06041v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jongmin Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Juho Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06041">
                                    <div class="article-summary-box-inner">
                                        <span>While adversarial training is considered as a standard defense method against
adversarial attacks for image classifiers, adversarial purification, which
purifies attacked images into clean images with a standalone purification
model, has shown promises as an alternative defense method. Recently, an
Energy-Based Model (EBM) trained with Markov-Chain Monte-Carlo (MCMC) has been
highlighted as a purification model, where an attacked image is purified by
running a long Markov-chain using the gradients of the EBM. Yet, the
practicality of the adversarial purification using an EBM remains questionable
because the number of MCMC steps required for such purification is too large.
In this paper, we propose a novel adversarial purification method based on an
EBM trained with Denoising Score-Matching (DSM). We show that an EBM trained
with DSM can quickly purify attacked images within a few steps. We further
introduce a simple yet effective randomized purification scheme that injects
random noises into images before purification. This process screens the
adversarial perturbations imposed on images by the random noises and brings the
images to the regime where the EBM can denoise well. We show that our
purification method is robust against various attacks and demonstrate its
state-of-the-art performances.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Assessing the Effectiveness of Syntactic Structure to Learn Code Edit Representations. (arXiv:2106.06110v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Qureshi_S/0/1/0/all/0/1">Syed Arbaaz Qureshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1">Sonu Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhagwan_R/0/1/0/all/0/1">Ranjita Bhagwan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1">Rahul Kumar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06110">
                                    <div class="article-summary-box-inner">
                                        <span>In recent times, it has been shown that one can use code as data to aid
various applications such as automatic commit message generation, automatic
generation of pull request descriptions and automatic program repair. Take for
instance the problem of commit message generation. Treating source code as a
sequence of tokens, state of the art techniques generate commit messages using
neural machine translation models. However, they tend to ignore the syntactic
structure of programming languages.

Previous work, i.e., code2seq has used structural information from Abstract
Syntax Tree (AST) to represent source code and they use it to automatically
generate method names. In this paper, we elaborate upon this state of the art
approach and modify it to represent source code edits. We determine the effect
of using such syntactic structure for the problem of classifying code edits.
Inspired by the code2seq approach, we evaluate how using structural information
from AST, i.e., paths between AST leaf nodes can help with the task of code
edit classification on two datasets of fine-grained syntactic edits.

Our experiments shows that attempts of adding syntactic structure does not
result in any improvements over less sophisticated methods. The results suggest
that techniques such as code2seq, while promising, have a long way to go before
they can be generically applied to learning code edit representations. We hope
that these results will benefit other researchers and inspire them to work
further on this problem.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Optimization Kernel: Towards Robust Deep Learning. (arXiv:2106.06097v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Lyu_Y/0/1/0/all/0/1">Yueming Lyu</a>, <a href="http://arxiv.org/find/stat/1/au:+Tsang_I/0/1/0/all/0/1">Ivor Tsang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06097">
                                    <div class="article-summary-box-inner">
                                        <span>Recent studies show a close connection between neural networks (NN) and
kernel methods. However, most of these analyses (e.g., NTK) focus on the
influence of (infinite) width instead of the depth of NN models. There remains
a gap between theory and practical network designs that benefit from the depth.
This paper first proposes a novel kernel family named Neural Optimization
Kernel (NOK). Our kernel is defined as the inner product between two $T$-step
updated functionals in RKHS w.r.t. a regularized optimization problem.
Theoretically, we proved the monotonic descent property of our update rule for
both convex and non-convex problems, and a $O(1/T)$ convergence rate of our
updates for convex problems. Moreover, we propose a data-dependent structured
approximation of our NOK, which builds the connection between training deep NNs
and kernel methods associated with NOK. The resultant computational graph is a
ResNet-type finite width NN. Our structured approximation preserved the
monotonic descent property and $O(1/T)$ convergence rate. Namely, a $T$-layer
NN performs $T$-step monotonic descent updates. Notably, we show our
$T$-layered structured NN with ReLU maintains a $O(1/T)$ convergence rate
w.r.t. a convex regularized problem, which explains the success of ReLU on
training deep NN from a NN architecture optimization perspective. For the
unsupervised learning and the shared parameter case, we show the equivalence of
training structured NN with GD and performing functional gradient descent in
RKHS associated with a fixed (data-dependent) NOK at an infinity-width regime.
For finite NOKs, we prove generalization bounds. Remarkably, we show that
overparameterized deep NN (NOK) can increase the expressive power to reduce
empirical risk and reduce the generalization bound at the same time. Extensive
experiments verify the robustness of our structured NOK blocks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">HIFI: Anomaly Detection for Multivariate Time Series with High-order Feature Interactions. (arXiv:2106.06167v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1">Liwei Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xuanhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1">Kai Zheng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06167">
                                    <div class="article-summary-box-inner">
                                        <span>Monitoring complex systems results in massive multivariate time series data,
and anomaly detection of these data is very important to maintain the normal
operation of the systems. Despite the recent emergence of a large number of
anomaly detection algorithms for multivariate time series, most of them ignore
the correlation modeling among multivariate, which can often lead to poor
anomaly detection results. In this work, we propose a novel anomaly detection
model for multivariate time series with \underline{HI}gh-order
\underline{F}eature \underline{I}nteractions (HIFI). More specifically, HIFI
builds multivariate feature interaction graph automatically and uses the graph
convolutional neural network to achieve high-order feature interactions, in
which the long-term temporal dependencies are modeled by attention mechanisms
and a variational encoding technique is utilized to improve the model
performance and robustness. Extensive experiments on three publicly available
datasets demonstrate the superiority of our framework compared with
state-of-the-art approaches.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning the Precise Feature for Cluster Assignment. (arXiv:2106.06159v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1">Yanhai Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xinghui Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Huiyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1">Feng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Junyu Dong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06159">
                                    <div class="article-summary-box-inner">
                                        <span>Clustering is one of the fundamental tasks in computer vision and pattern
recognition. Recently, deep clustering methods (algorithms based on deep
learning) have attracted wide attention with their impressive performance. Most
of these algorithms combine deep unsupervised representation learning and
standard clustering together. However, the separation of representation
learning and clustering will lead to suboptimal solutions because the two-stage
strategy prevents representation learning from adapting to subsequent tasks
(e.g., clustering according to specific cues). To overcome this issue, efforts
have been made in the dynamic adaption of representation and cluster
assignment, whereas current state-of-the-art methods suffer from heuristically
constructed objectives with representation and cluster assignment alternatively
optimized. To further standardize the clustering problem, we audaciously
formulate the objective of clustering as finding a precise feature as the cue
for cluster assignment. Based on this, we propose a general-purpose deep
clustering framework which radically integrates representation learning and
clustering into a single pipeline for the first time. The proposed framework
exploits the powerful ability of recently developed generative models for
learning intrinsic features, and imposes an entropy minimization on the
distribution of the cluster assignment by a dedicated variational algorithm.
Experimental results show that the performance of the proposed method is
superior, or at least comparable to, the state-of-the-art methods on the
handwritten digit recognition, fashion recognition, face recognition and object
recognition benchmark datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hybrid Generative-Contrastive Representation Learning. (arXiv:2106.06162v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Saehoon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sungwoong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Juho Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06162">
                                    <div class="article-summary-box-inner">
                                        <span>Unsupervised representation learning has recently received lots of interest
due to its powerful generalizability through effectively leveraging large-scale
unlabeled data. There are two prevalent approaches for this, contrastive
learning and generative pre-training, where the former learns representations
from instance-wise discrimination tasks and the latter learns them from
estimating the likelihood. These seemingly orthogonal approaches have their own
strengths and weaknesses. Contrastive learning tends to extract semantic
information and discards details irrelevant for classifying objects, making the
representations effective for discriminative tasks while degrading robustness
to out-of-distribution data. On the other hand, the generative pre-training
directly estimates the data distribution, so the representations tend to be
robust but not optimal for discriminative tasks. In this paper, we show that we
could achieve the best of both worlds by a hybrid training scheme.
Specifically, we demonstrated that a transformer-based encoder-decoder
architecture trained with both contrastive and generative losses can learn
highly discriminative and robust representations without hurting the generative
performance. We extensively validate our approach on various tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Is Homophily a Necessity for Graph Neural Networks?. (arXiv:2106.06134v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaorui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1">Neil Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06134">
                                    <div class="article-summary-box-inner">
                                        <span>Graph neural networks (GNNs) have shown great prowess in learning
representations suitable for numerous graph-based machine learning tasks. When
applied to semi-supervised node classification, GNNs are widely believed to
work well due to the homophily assumption (&#x60;&#x60;like attracts like&#x27;&#x27;), and fail to
generalize to heterophilous graphs where dissimilar nodes connect. Recent works
design new architectures to overcome such heterophily-related limitations,
citing poor baseline performance and new architecture improvements on a few
heterophilous graph benchmark datasets as evidence for this notion. In our
experiments, we empirically find that standard graph convolutional networks
(GCNs) can actually achieve better performance than such carefully designed
methods on some commonly used heterophilous graphs. This motivates us to
reconsider whether homophily is truly necessary for good GNN performance. We
find that this claim is not quite true, and in fact, GCNs can achieve strong
performance on heterophilous graphs under certain conditions. Our work
carefully characterizes these conditions, and provides supporting theoretical
understanding and empirical observations. Finally, we examine existing
heterophilous graphs benchmarks and reconcile how the GCN (under)performs on
them based on this understanding.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">NAAQA: A Neural Architecture for Acoustic Question Answering. (arXiv:2106.06147v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Abdelnour_J/0/1/0/all/0/1">Jerome Abdelnour</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1">Jean Rouat</a>, <a href="http://arxiv.org/find/cs/1/au:+Salvi_G/0/1/0/all/0/1">Giampiero Salvi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06147">
                                    <div class="article-summary-box-inner">
                                        <span>The goal of the Acoustic Question Answering (AQA) task is to answer a
free-form text question about the content of an acoustic scene. It was inspired
by the Visual Question Answering (VQA) task. In this paper, based on the
previously introduced CLEAR dataset, we propose a new benchmark for AQA that
emphasizes the specific challenges of acoustic inputs, e.g. variable duration
scenes. We also introduce NAAQA, a neural architecture that leverages specific
properties of acoustic inputs. The usage of time and frequency 1D convolutions
to process 2D spectro-temporal representations of acoustic content shows
promising results and enables reductions in model complexity. NAAQA achieves
91.6% of accuracy on the AQA task with about 7 times fewer parameters than the
previously explored VQA model. We provide a detailed analysis of the results
for the different question types. The effectiveness of coordinate maps in this
acoustic context was also studied and we show that time coordinate maps augment
temporal localization capabilities which enhance performance of the network by
about 17 percentage points.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Generate, Annotate, and Learn: Generative Models Advance Self-Training and Knowledge Distillation. (arXiv:2106.06168v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuanli He</a>, <a href="http://arxiv.org/find/cs/1/au:+Nassar_I/0/1/0/all/0/1">Islam Nassar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiros_J/0/1/0/all/0/1">Jamie Kiros</a>, <a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1">Gholamreza Haffari</a>, <a href="http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1">Mohammad Norouzi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06168">
                                    <div class="article-summary-box-inner">
                                        <span>Semi-Supervised Learning (SSL) has seen success in many application domains,
but this success often hinges on the availability of task-specific unlabeled
data. Knowledge distillation (KD) has enabled compressing deep networks and
ensembles, achieving the best results when distilling knowledge on fresh
task-specific unlabeled examples. However, task-specific unlabeled data can be
challenging to find. We present a general framework called &quot;generate, annotate,
and learn (GAL)&quot; that uses unconditional generative models to synthesize
in-domain unlabeled data, helping advance SSL and KD on different tasks. To
obtain strong task-specific generative models, we adopt generic generative
models, pretrained on open-domain data, and fine-tune them on inputs from
specific tasks. Then, we use existing classifiers to annotate generated
unlabeled examples with soft pseudo labels, which are used for additional
training. When self-training is combined with samples generated from
GPT2-large, fine-tuned on the inputs of each GLUE task, we outperform a strong
RoBERTa-large baseline on the GLUE benchmark. Moreover, KD on GPT-2 samples
yields a new state-of-the-art for 6-layer transformers on the GLUE leaderboard.
Finally, self-training with GAL offers significant gains on image
classification on CIFAR-10 and four tabular tasks from the UCI repository</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning. (arXiv:2106.06135v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jingru Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Wenye Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1">Xiangru Lian</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xia Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Ji Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06135">
                                    <div class="article-summary-box-inner">
                                        <span>Games are abstractions of the real world, where artificial agents learn to
compete and cooperate with other agents. While significant achievements have
been made in various perfect- and imperfect-information games, DouDizhu (a.k.a.
Fighting the Landlord), a three-player card game, is still unsolved. DouDizhu
is a very challenging domain with competition, collaboration, imperfect
information, large state space, and particularly a massive set of possible
actions where the legal actions vary significantly from turn to turn.
Unfortunately, modern reinforcement learning algorithms mainly focus on simple
and small action spaces, and not surprisingly, are shown not to make
satisfactory progress in DouDizhu. In this work, we propose a conceptually
simple yet effective DouDizhu AI system, namely DouZero, which enhances
traditional Monte-Carlo methods with deep neural networks, action encoding, and
parallel actors. Starting from scratch in a single server with four GPUs,
DouZero outperformed all the existing DouDizhu AI programs in days of training
and was ranked the first in the Botzone leaderboard among 344 AI agents.
Through building DouZero, we show that classic Monte-Carlo methods can be made
to deliver strong results in a hard domain with a complex action space. The
code and an online demo are released at https://github.com/kwai/DouZero with
the hope that this insight could motivate future work.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Anomalous Sound Detection Using a Binary Classification Model and Class Centroids. (arXiv:2106.06151v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kuroyanagi_I/0/1/0/all/0/1">Ibuki Kuroyanagi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1">Tomoki Hayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1">Kazuya Takeda</a>, <a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1">Tomoki Toda</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06151">
                                    <div class="article-summary-box-inner">
                                        <span>An anomalous sound detection system to detect unknown anomalous sounds
usually needs to be built using only normal sound data. Moreover, it is
desirable to improve the system by effectively using a small amount of
anomalous sound data, which will be accumulated through the system&#x27;s operation.
As one of the methods to meet these requirements, we focus on a binary
classification model that is developed by using not only normal data but also
outlier data in the other domains as pseudo-anomalous sound data, which can be
easily updated by using anomalous data. In this paper, we implement a new loss
function based on metric learning to learn the distance relationship from each
class centroid in feature space for the binary classification model. The
proposed multi-task learning of the binary classification and the metric
learning makes it possible to build the feature space where the within-class
variance is minimized and the between-class variance is maximized while keeping
normal and anomalous classes linearly separable. We also investigate the
effectiveness of additionally using anomalous sound data for further improving
the binary classification model. Our results showed that multi-task learning
using binary classification and metric learning to consider the distance from
each class centroid in the feature space is effective, and performance can be
significantly improved by using even a small amount of anomalous data during
training.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PyGAD: An Intuitive Genetic Algorithm Python Library. (arXiv:2106.06158v1 [cs.NE])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gad_A/0/1/0/all/0/1">Ahmed Fawzy Gad</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06158">
                                    <div class="article-summary-box-inner">
                                        <span>This paper introduces PyGAD, an open-source easy-to-use Python library for
building the genetic algorithm. PyGAD supports a wide range of parameters to
give the user control over everything in its life cycle. This includes, but is
not limited to, population, gene value range, gene data type, parent selection,
crossover, and mutation. PyGAD is designed as a general-purpose optimization
library that allows the user to customize the fitness function. Its usage
consists of 3 main steps: build the fitness function, create an instance of the
pygad.GA class, and calling the pygad.GA.run() method. The library supports
training deep learning models created either with PyGAD itself or with
frameworks like Keras and PyTorch. Given its stable state, PyGAD is also in
active development to respond to the user&#x27;s requested features and enhancement
received on GitHub https://github.com/ahmedfgad/GeneticAlgorithmPython. PyGAD
comes with documentation https://pygad.readthedocs.io for further details and
examples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A comprehensive solution to retrieval-based chatbot construction. (arXiv:2106.06139v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Moore_K/0/1/0/all/0/1">Kristen Moore</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1">Shenjun Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhen He</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudolf_T/0/1/0/all/0/1">Torsten Rudolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Fisher_N/0/1/0/all/0/1">Nils Fisher</a>, <a href="http://arxiv.org/find/cs/1/au:+Victor_B/0/1/0/all/0/1">Brandon Victor</a>, <a href="http://arxiv.org/find/cs/1/au:+Jindal_N/0/1/0/all/0/1">Neha Jindal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06139">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper we present the results of our experiments in training and
deploying a self-supervised retrieval-based chatbot trained with contrastive
learning for assisting customer support agents. In contrast to most existing
research papers in this area where the focus is on solving just one component
of a deployable chatbot, we present an end-to-end set of solutions to take the
reader from an unlabelled chatlogs to a deployed chatbot. This set of solutions
includes creating a self-supervised dataset and a weakly labelled dataset from
chatlogs, as well as a systematic approach to selecting a fixed list of canned
responses. We present a hierarchical-based RNN architecture for the response
selection model, chosen for its ability to cache intermediate utterance
embeddings, which helped to meet deployment inference speed requirements. We
compare the performance of this architecture across 3 different learning
objectives: self-supervised contrastive learning, binary classification, and
multi-class classification. We find that using a self-supervised contrastive
learning model outperforms training the binary and multi-class classification
models on a weakly labelled dataset. Our results validate that the
self-supervised contrastive learning approach can be effectively used for a
real-world chatbot scenario.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DORO: Distributional and Outlier Robust Optimization. (arXiv:2106.06142v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhai_R/0/1/0/all/0/1">Runtian Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Dan_C/0/1/0/all/0/1">Chen Dan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1">Pradeep Ravikumar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06142">
                                    <div class="article-summary-box-inner">
                                        <span>Many machine learning tasks involve subpopulation shift where the testing
data distribution is a subpopulation of the training distribution. For such
settings, a line of recent work has proposed the use of a variant of empirical
risk minimization(ERM) known as distributionally robust optimization (DRO). In
this work, we apply DRO to real, large-scale tasks with subpopulation shift,
and observe that DRO performs relatively poorly, and moreover has severe
instability. We identify one direct cause of this phenomenon: sensitivity of
DRO to outliers in the datasets. To resolve this issue, we propose the
framework of DORO, for Distributional and Outlier Robust Optimization. At the
core of this approach is a refined risk function which prevents DRO from
overfitting to potential outliers. We instantiate DORO for the Cressie-Read
family of R\&#x27;enyi divergence, and delve into two specific instances of this
family: CVaR and $\chi^2$-DRO. We theoretically prove the effectiveness of the
proposed method, and empirically show that DORO improves the performance and
stability of DRO with experiments on large modern datasets, thereby positively
addressing the open question raised by Hashimoto et al., 2018.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sparse Bayesian Learning via Stepwise Regression. (arXiv:2106.06095v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ament_S/0/1/0/all/0/1">Sebastian Ament</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1">Carla Gomes</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06095">
                                    <div class="article-summary-box-inner">
                                        <span>Sparse Bayesian Learning (SBL) is a powerful framework for attaining sparsity
in probabilistic models. Herein, we propose a coordinate ascent algorithm for
SBL termed Relevance Matching Pursuit (RMP) and show that, as its noise
variance parameter goes to zero, RMP exhibits a surprising connection to
Stepwise Regression. Further, we derive novel guarantees for Stepwise
Regression algorithms, which also shed light on RMP. Our guarantees for Forward
Regression improve on deterministic and probabilistic results for Orthogonal
Matching Pursuit with noise. Our analysis of Backward Regression on determined
systems culminates in a bound on the residual of the optimal solution to the
subset selection problem that, if satisfied, guarantees the optimality of the
result. To our knowledge, this bound is the first that can be computed in
polynomial time and depends chiefly on the smallest singular value of the
matrix. We report numerical experiments using a variety of feature selection
algorithms. Notably, RMP and its limiting variant are both efficient and
maintain strong performance with correlated features.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">High-Performance FPGA-based Accelerator for Bayesian Recurrent Neural Networks. (arXiv:2106.06048v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1">Martin Ferianc</a>, <a href="http://arxiv.org/find/cs/1/au:+Que_Z/0/1/0/all/0/1">Zhiqiang Que</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1">Hongxiang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Luk_W/0/1/0/all/0/1">Wayne Luk</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1">Miguel Rodrigues</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06048">
                                    <div class="article-summary-box-inner">
                                        <span>Neural networks have demonstrated their great performance in a wide range of
tasks. Especially in time-series analysis, recurrent architectures based on
long-short term memory (LSTM) cells have manifested excellent capability to
model time dependencies in real-world data. However, standard recurrent
architectures cannot estimate their uncertainty which is essential for
safety-critical applications such as in medicine. In contrast, Bayesian
recurrent neural networks (RNNs) are able to provide uncertainty estimation
with improved accuracy. Nonetheless, Bayesian RNNs are computationally and
memory demanding, which limits their practicality despite their advantages. To
address this issue, we propose an FPGA-based hardware design to accelerate
Bayesian LSTM-based RNNs. To further improve the overall algorithmic-hardware
performance, a co-design framework is proposed to explore the most optimal
algorithmic-hardware configurations for Bayesian RNNs. We conduct extensive
experiments on health-related tasks to demonstrate the improvement of our
design and the effectiveness of our framework. Compared with GPU
implementation, our FPGA-based design can achieve up to 10 times speedup with
nearly 106 times higher energy efficiency. To the best of our knowledge, this
is the first work targeting the acceleration of Bayesian RNNs on FPGAs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Instance-Level Task Parameters: A Robust Multi-task Weighting Framework. (arXiv:2106.06129v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vasu_P/0/1/0/all/0/1">Pavan Kumar Anasosalu Vasu</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1">Shreyas Saxena</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1">Oncel Tuzel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06129">
                                    <div class="article-summary-box-inner">
                                        <span>Recent works have shown that deep neural networks benefit from multi-task
learning by learning a shared representation across several related tasks.
However, performance of such systems depend on relative weighting between
various losses involved during training. Prior works on loss weighting schemes
assume that instances are equally easy or hard for all tasks. In order to break
this assumption, we let the training process dictate the optimal weighting of
tasks for every instance in the dataset. More specifically, we equip every
instance in the dataset with a set of learnable parameters (instance-level task
parameters) where the cardinality is equal to the number of tasks learned by
the model. These parameters model the weighting of each task for an instance.
They are updated by gradient descent and do not require hand-crafted rules. We
conduct extensive experiments on SURREAL and CityScapes datasets, for human
shape and pose estimation, depth estimation and semantic segmentation tasks. In
these tasks, our approach outperforms recent dynamic loss weighting approaches,
e.g. reducing surface estimation errors by 8.97% on SURREAL. When applied to
datasets where one or more tasks can have noisy annotations, the proposed
method learns to prioritize learning from clean labels for a given task, e.g.
reducing surface estimation errors by up to 60%. We also show that we can
reliably detect corrupt labels for a given task as a by-product from learned
instance-level task parameters.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Meta-Adaptive Nonlinear Control: Theory and Algorithms. (arXiv:2106.06098v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1">Guanya Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1">Kamyar Azizzadenesheli</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1">Soon-Jo Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yisong Yue</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06098">
                                    <div class="article-summary-box-inner">
                                        <span>We present an online multi-task learning approach for adaptive nonlinear
control, which we call Online Meta-Adaptive Control (OMAC). The goal is to
control a nonlinear system subject to adversarial disturbance and unknown
$\textit{environment-dependent}$ nonlinear dynamics, under the assumption that
the environment-dependent dynamics can be well captured with some shared
representation. Our approach is motivated by robot control, where a robotic
system encounters a sequence of new environmental conditions that it must
quickly adapt to. A key emphasis is to integrate online representation learning
with established methods from control theory, in order to arrive at a unified
framework that yields both control-theoretic and learning-theoretic guarantees.
We provide instantiations of our approach under varying conditions, leading to
the first non-asymptotic end-to-end convergence guarantee for multi-task
adaptive nonlinear control. OMAC can also be integrated with deep
representation learning. Experiments show that OMAC significantly outperforms
conventional adaptive control approaches which do not learn the shared
representation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Differentially Private Federated Learning via Inexact ADMM. (arXiv:2106.06127v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ryu_M/0/1/0/all/0/1">Minseok Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kibaek Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06127">
                                    <div class="article-summary-box-inner">
                                        <span>Differential privacy (DP) techniques can be applied to the federated learning
model to protect data privacy against inference attacks to communication among
the learning agents. The DP techniques, however, hinder achieving a greater
learning performance while ensuring strong data privacy. In this paper we
develop a DP inexact alternating direction method of multipliers algorithm that
solves a sequence of trust-region subproblems with the objective perturbation
by random noises generated from a Laplace distribution. We show that our
algorithm provides $\bar{\epsilon}$-DP for every iteration and
$\mathcal{O}(1/T)$ rate of convergence in expectation, where $T$ is the number
of iterations. Using MNIST and FEMNIST datasets for the image classification,
we demonstrate that our algorithm reduces the testing error by at most $22\%$
compared with the existing DP algorithm, while achieving the same level of data
privacy. The numerical experiment also shows that our algorithm converges
faster than the existing algorithm.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Monotonic Neural Network: combining Deep Learning with Domain Knowledge for Chiller Plants Energy Optimization. (arXiv:2106.06143v1 [eess.SP])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Ma_F/0/1/0/all/0/1">Fanhe Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1">Faen Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Ben_S/0/1/0/all/0/1">Shenglan Ben</a>, <a href="http://arxiv.org/find/eess/1/au:+Qin_S/0/1/0/all/0/1">Shuxin Qin</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_P/0/1/0/all/0/1">Pengcheng Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1">Changsheng Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_F/0/1/0/all/0/1">Fengyi Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06143">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we are interested in building a domain knowledge based deep
learning framework to solve the chiller plants energy optimization problems.
Compared to the hotspot applications of deep learning (e.g. image
classification and NLP), it is difficult to collect enormous data for deep
network training in real-world physical systems. Most existing methods reduce
the complex systems into linear model to facilitate the training on small
samples. To tackle the small sample size problem, this paper considers domain
knowledge in the structure and loss design of deep network to build a nonlinear
model with lower redundancy function space. Specifically, the energy
consumption estimation of most chillers can be physically viewed as an
input-output monotonic problem. Thus, we can design a Neural Network with
monotonic constraints to mimic the physical behavior of the system. We verify
the proposed method in a cooling system of a data center, experimental results
show the superiority of our framework in energy optimization compared to the
existing ones.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploiting Large-scale Teacher-Student Training for On-device Acoustic Models. (arXiv:2106.06126v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Swaminathan_R/0/1/0/all/0/1">Rupak Vignesh Swaminathan</a>, <a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_S/0/1/0/all/0/1">Sree Hari Krishnan Parthasarathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1">Chunchuan Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1">Athanasios Mouchtaris</a>, <a href="http://arxiv.org/find/cs/1/au:+Kunzmann_S/0/1/0/all/0/1">Siegfried Kunzmann</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06126">
                                    <div class="article-summary-box-inner">
                                        <span>We present results from Alexa speech teams on semi-supervised learning (SSL)
of acoustic models (AM) with experiments spanning over 3000 hours of GPU time,
making our study one of the largest of its kind. We discuss SSL for AMs in a
small footprint setting, showing that a smaller capacity model trained with 1
million hours of unsupervised data can outperform a baseline supervised system
by 14.3% word error rate reduction (WERR). When increasing the supervised data
to seven-fold, our gains diminish to 7.1% WERR; to improve SSL efficiency at
larger supervised data regimes, we employ a step-wise distillation into a
smaller model, obtaining a WERR of 14.4%. We then switch to SSL using larger
student models in low data regimes; while learning efficiency with unsupervised
data is higher, student models may outperform teacher models in such a setting.
We develop a theoretical sketch to explain this behavior.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Progressive-Scale Boundary Blackbox Attack via Projective Gradient Estimation. (arXiv:2106.06056v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiawei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huichen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaolu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06056">
                                    <div class="article-summary-box-inner">
                                        <span>Boundary based blackbox attack has been recognized as practical and
effective, given that an attacker only needs to access the final model
prediction. However, the query efficiency of it is in general high especially
for high dimensional image data. In this paper, we show that such efficiency
highly depends on the scale at which the attack is applied, and attacking at
the optimal scale significantly improves the efficiency. In particular, we
propose a theoretical framework to analyze and show three key characteristics
to improve the query efficiency. We prove that there exists an optimal scale
for projective gradient estimation. Our framework also explains the
satisfactory performance achieved by existing boundary black-box attacks. Based
on our theoretical framework, we propose Progressive-Scale enabled projective
Boundary Attack (PSBA) to improve the query efficiency via progressive scaling
techniques. In particular, we employ Progressive-GAN to optimize the scale of
projections, which we call PSBA-PGAN. We evaluate our approach on both spatial
and frequency scales. Extensive experiments on MNIST, CIFAR-10, CelebA, and
ImageNet against different models including a real-world face recognition API
show that PSBA-PGAN significantly outperforms existing baseline attacks in
terms of query efficiency and attack success rate. We also observe relatively
stable optimal scales for different models and datasets. The code is publicly
available at https://github.com/AI-secure/PSBA.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Gradual Domain Adaptation in the Wild:When Intermediate Distributions are Absent. (arXiv:2106.06080v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1">Samira Abnar</a>, <a href="http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1">Rianne van den Berg</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1">Golnaz Ghiasi</a>, <a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1">Mostafa Dehghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalchbrenner_N/0/1/0/all/0/1">Nal Kalchbrenner</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1">Hanie Sedghi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06080">
                                    <div class="article-summary-box-inner">
                                        <span>We focus on the problem of domain adaptation when the goal is shifting the
model towards the target distribution, rather than learning domain invariant
representations. It has been shown that under the following two assumptions:
(a) access to samples from intermediate distributions, and (b) samples being
annotated with the amount of change from the source distribution, self-training
can be successfully applied on gradually shifted samples to adapt the model
toward the target distribution. We hypothesize having (a) is enough to enable
iterative self-training to slowly adapt the model to the target distribution,
by making use of an implicit curriculum. In the case where (a) does not hold,
we observe that iterative self-training falls short. We propose GIFT, a method
that creates virtual samples from intermediate distributions by interpolating
representations of examples from source and target domains. We evaluate an
iterative-self-training method on datasets with natural distribution shifts,
and show that when applied on top of other domain adaptation methods, it
improves the performance of the model on the target dataset. We run an analysis
on a synthetic dataset to show that in the presence of (a)
iterative-self-training naturally forms a curriculum of samples. Furthermore,
we show that when (a) does not hold, GIFT performs better than iterative
self-training.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Convergence and Alignment of Gradient Descentwith Random Back propagation Weights. (arXiv:2106.06044v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Song_G/0/1/0/all/0/1">Ganlin Song</a>, <a href="http://arxiv.org/find/stat/1/au:+Xu_R/0/1/0/all/0/1">Ruitu Xu</a>, <a href="http://arxiv.org/find/stat/1/au:+Lafferty_J/0/1/0/all/0/1">John Lafferty</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06044">
                                    <div class="article-summary-box-inner">
                                        <span>Stochastic gradient descent with backpropagation is the workhorse of
artificial neural networks. It has long been recognized that backpropagation
fails to be a biologically plausible algorithm. Fundamentally, it is a
non-local procedure -- updating one neuron&#x27;s synaptic weights requires
knowledge of synaptic weights or receptive fields of downstream neurons. This
limits the use of artificial neural networks as a tool for understanding the
biological principles of information processing in the brain. Lillicrap et al.
(2016) propose a more biologically plausible &quot;feedback alignment&quot; algorithm
that uses random and fixed backpropagation weights, and show promising
simulations. In this paper we study the mathematical properties of the feedback
alignment procedure by analyzing convergence and alignment for two-layer
networks under squared error loss. In the overparameterized setting, we prove
that the error converges to zero exponentially fast, and also that
regularization is necessary in order for the parameters to become aligned with
the random backpropagation weights. Simulations are given that are consistent
with this analysis and suggest further generalizations. These results
contribute to our understanding of how biologically plausible algorithms might
carry out weight learning in a manner different from Hebbian learning, with
performance that is comparable with the full non-local backpropagation
algorithm.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bayesian Optimisation with Formal Guarantees. (arXiv:2106.06067v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Brausse_F/0/1/0/all/0/1">Franz Brau&#xdf;e</a>, <a href="http://arxiv.org/find/cs/1/au:+Khasidashvili_Z/0/1/0/all/0/1">Zurab Khasidashvili</a>, <a href="http://arxiv.org/find/cs/1/au:+Korovin_K/0/1/0/all/0/1">Konstantin Korovin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06067">
                                    <div class="article-summary-box-inner">
                                        <span>Application domains of Bayesian optimization include optimizing black-box

functions or very complex functions. The functions we are interested in
describe

complex real-world systems applied in industrial settings. Even though

they do have explicit representations, standard optimization

techniques fail to provide validated solutions and correctness

guarantees for them.

In this paper we present a combination of Bayesian optimisation and SMT-based
constraint solving to achieve safe and stable solutions with optimality
guarantees.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Verifying Quantized Neural Networks using SMT-Based Model Checking. (arXiv:2106.05997v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sena_L/0/1/0/all/0/1">Luiz Sena</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xidan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Alves_E/0/1/0/all/0/1">Erickson Alves</a>, <a href="http://arxiv.org/find/cs/1/au:+Bessa_I/0/1/0/all/0/1">Iury Bessa</a>, <a href="http://arxiv.org/find/cs/1/au:+Manino_E/0/1/0/all/0/1">Edoardo Manino</a>, <a href="http://arxiv.org/find/cs/1/au:+Cordeiro_L/0/1/0/all/0/1">Lucas Cordeiro</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05997">
                                    <div class="article-summary-box-inner">
                                        <span>Artificial Neural Networks (ANNs) are being deployed on an increasing number
of safety-critical applications, including autonomous cars and medical
diagnosis. However, concerns about their reliability have been raised due to
their black-box nature and apparent fragility to adversarial attacks. Here, we
develop and evaluate a symbolic verification framework using incremental model
checking (IMC) and satisfiability modulo theories (SMT) to check for
vulnerabilities in ANNs. More specifically, we propose several ANN-related
optimizations for IMC, including invariant inference via interval analysis and
the discretization of non-linear activation functions. With this, we can
provide guarantees on the safe behavior of ANNs implemented both in
floating-point and fixed-point (quantized) arithmetic. In this regard, our
verification approach was able to verify and produce adversarial examples for
52 test cases spanning image classification and general machine learning
applications. For small- to medium-sized ANN, our approach completes most of
its verification runs in minutes. Moreover, in contrast to most
state-of-the-art methods, our approach is not restricted to specific choices of
activation functions or non-quantized representations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Unified Framework for Constructing Nonconvex Regularizations. (arXiv:2106.06123v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Zhou_Z/0/1/0/all/0/1">Zhiyong Zhou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06123">
                                    <div class="article-summary-box-inner">
                                        <span>Over the past decades, many individual nonconvex methods have been proposed
to achieve better sparse recovery performance in various scenarios. However,
how to construct a valid nonconvex regularization function remains open in
practice. In this paper, we fill in this gap by presenting a unified framework
for constructing the nonconvex regularization based on the probability density
function. Meanwhile, a new nonconvex sparse recovery method constructed via the
Weibull distribution is studied.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FedBABU: Towards Enhanced Representation for Federated Image Classification. (arXiv:2106.06042v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1">Jaehoon Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sangmook Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06042">
                                    <div class="article-summary-box-inner">
                                        <span>Federated learning has evolved to improve a single global model under data
heterogeneity (as a curse) or to develop multiple personalized models using
data heterogeneity (as a blessing). However, there has been little research
considering both directions simultaneously. In this paper, we first investigate
the relationship between them by analyzing Federated Averaging at the client
level and determine that a better federated global model performance does not
constantly improve personalization. To elucidate the cause of this
personalization performance degradation problem, we decompose the entire
network into the body (i.e., extractor), related to universality, and the head
(i.e., classifier), related to personalization. We then point out that this
problem stems from training the head. Based on this observation, we propose a
novel federated learning algorithm, coined as FedBABU, which updates only the
body of the model during federated training (i.e., the head is randomly
initialized and never updated), and the head is fine-tuned for personalization
during the evaluation process. Extensive experiments show consistent
performance improvements and an efficient personalization of FedBABU.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Collaborative Multidisciplinary Design Optimization with Neural Networks. (arXiv:2106.06092v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Becdelievre_J/0/1/0/all/0/1">Jean de Becdelievre</a>, <a href="http://arxiv.org/find/cs/1/au:+Kroo_I/0/1/0/all/0/1">Ilan Kroo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06092">
                                    <div class="article-summary-box-inner">
                                        <span>The design of complex engineering systems leads to solving very large
optimization problems involving different disciplines. Strategies allowing
disciplines to optimize in parallel by providing sub-objectives and splitting
the problem into smaller parts, such as Collaborative Optimization, are
promising solutions.However, most of them have slow convergence which reduces
their practical use. Earlier efforts to fasten convergence by learning
surrogate models have not yet succeeded at sufficiently improving the
competitiveness of these strategies.This paper shows that, in the case of
Collaborative Optimization, faster and more reliable convergence can be
obtained by solving an interesting instance of binary classification: on top of
the target label, the training data of one of the two classes contains the
distance to the decision boundary and its derivative. Leveraging this
information, we propose to train a neural network with an asymmetric loss
function, a structure that guarantees Lipshitz continuity, and a regularization
towards respecting basic distance function properties. The approach is
demonstrated on a toy learning example, and then applied to a multidisciplinary
aircraft design problem.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Twin Neural Network Regression is a Semi-Supervised Regression Algorithm. (arXiv:2106.06124v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wetzel_S/0/1/0/all/0/1">Sebastian J. Wetzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Melko_R/0/1/0/all/0/1">Roger G. Melko</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamblyn_I/0/1/0/all/0/1">Isaac Tamblyn</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06124">
                                    <div class="article-summary-box-inner">
                                        <span>Twin neural network regression (TNNR) is a semi-supervised regression
algorithm, it can be trained on unlabelled data points as long as other,
labelled anchor data points, are present. TNNR is trained to predict
differences between the target values of two different data points rather than
the targets themselves. By ensembling predicted differences between the targets
of an unseen data point and all training data points, it is possible to obtain
a very accurate prediction for the original regression problem. Since any loop
of predicted differences should sum to zero, loops can be supplied to the
training data, even if the data points themselves within loops are unlabelled.
Semi-supervised training improves TNNR performance, which is already state of
the art, significantly.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1">Sumon Biswas</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1">Hridesh Rajan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06054">
                                    <div class="article-summary-box-inner">
                                        <span>In recent years, many incidents have been reported where machine learning
models exhibited discrimination among people based on race, sex, age, etc.
Research has been conducted to measure and mitigate unfairness in machine
learning models. For a machine learning task, it is a common practice to build
a pipeline that includes an ordered set of data preprocessing stages followed
by a classifier. However, most of the research on fairness has considered a
single classifier based prediction task. What are the fairness impacts of the
preprocessing stages in machine learning pipeline? Furthermore, studies showed
that often the root cause of unfairness is ingrained in the data itself, rather
than the model. But no research has been conducted to measure the unfairness
caused by a specific transformation made in the data preprocessing stage. In
this paper, we introduced the causal method of fairness to reason about the
fairness impact of data preprocessing stages in ML pipeline. We leveraged
existing metrics to define the fairness measures of the stages. Then we
conducted a detailed fairness evaluation of the preprocessing stages in 37
pipelines collected from three different sources. Our results show that certain
data transformers are causing the model to exhibit unfairness. We identified a
number of fairness patterns in several categories of data transformers.
Finally, we showed how the local fairness of a preprocessing stage composes in
the global fairness of the pipeline. We used the fairness composition to choose
appropriate downstream transformer that mitigates unfairness in the machine
learning pipeline.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Domain Transformer: Predicting Samples of Unseen, Future Domains. (arXiv:2106.06057v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Johannes Schneider</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06057">
                                    <div class="article-summary-box-inner">
                                        <span>The data distribution commonly evolves over time leading to problems such as
concept drift that often decrease classifier performance. We seek to predict
unseen data (and their labels) allowing us to tackle challenges due to a
non-constant data distribution in a \emph{proactive} manner rather than
detecting and reacting to already existing changes that might already have led
to errors. To this end, we learn a domain transformer in an unsupervised manner
that allows generating data of unseen domains. Our approach first matches
independently learned latent representations of two given domains obtained from
an auto-encoder using a Cycle-GAN. In turn, a transformation of the original
samples can be learned that can be applied iteratively to extrapolate to unseen
domains. Our evaluation on CNNs on image data confirms the usefulness of the
approach. It also achieves very good results on the well-known problem of
unsupervised domain adaption, where labels but not samples have to be
predicted.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Nonmyopic Approach to Cost-Constrained Bayesian Optimization. (arXiv:2106.06079v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1">Eric Hans Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Eriksson_D/0/1/0/all/0/1">David Eriksson</a>, <a href="http://arxiv.org/find/cs/1/au:+Perrone_V/0/1/0/all/0/1">Valerio Perrone</a>, <a href="http://arxiv.org/find/cs/1/au:+Seeger_M/0/1/0/all/0/1">Matthias Seeger</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06079">
                                    <div class="article-summary-box-inner">
                                        <span>Bayesian optimization (BO) is a popular method for optimizing
expensive-to-evaluate black-box functions. BO budgets are typically given in
iterations, which implicitly assumes each evaluation has the same cost. In
fact, in many BO applications, evaluation costs vary significantly in different
regions of the search space. In hyperparameter optimization, the time spent on
neural network training increases with layer size; in clinical trials, the
monetary cost of drug compounds vary; and in optimal control, control actions
have differing complexities. Cost-constrained BO measures convergence with
alternative cost metrics such as time, money, or energy, for which the sample
efficiency of standard BO methods is ill-suited. For cost-constrained BO, cost
efficiency is far more important than sample efficiency. In this paper, we
formulate cost-constrained BO as a constrained Markov decision process (CMDP),
and develop an efficient rollout approximation to the optimal CMDP policy that
takes both the cost and future iterations into account. We validate our method
on a collection of hyperparameter optimization problems as well as a sensor set
selection application.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Interpreting Expert Annotation Differences in Animal Behavior. (arXiv:2106.06114v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tjandrasuwita_M/0/1/0/all/0/1">Megan Tjandrasuwita</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jennifer J. Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1">Ann Kennedy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1">Swarat Chaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yisong Yue</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06114">
                                    <div class="article-summary-box-inner">
                                        <span>Hand-annotated data can vary due to factors such as subjective differences,
intra-rater variability, and differing annotator expertise. We study
annotations from different experts who labelled the same behavior classes on a
set of animal behavior videos, and observe a variation in annotation styles. We
propose a new method using program synthesis to help interpret annotation
differences for behavior analysis. Our model selects relevant trajectory
features and learns a temporal filter as part of a program, which corresponds
to estimated importance an annotator places on that feature at each timestamp.
Our experiments on a dataset from behavioral neuroscience demonstrate that
compared to baseline approaches, our method is more accurate at capturing
annotator labels and learns interpretable temporal filters. We believe that our
method can lead to greater reproducibility of behavior annotations used in
scientific studies. We plan to release our code.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FiSH: Fair Spatial Hotspots. (arXiv:2106.06049v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+P_D/0/1/0/all/0/1">Deepak P</a>, <a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1">Sowmya S Sundaram</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06049">
                                    <div class="article-summary-box-inner">
                                        <span>Pervasiveness of tracking devices and enhanced availability of spatially
located data has deepened interest in using them for various policy
interventions, through computational data analysis tasks such as spatial hot
spot detection. In this paper, we consider, for the first time to our best
knowledge, fairness in detecting spatial hot spots. We motivate the need for
ensuring fairness through statistical parity over the collective population
covered across chosen hot spots. We then characterize the task of identifying a
diverse set of solutions in the noteworthiness-fairness trade-off spectrum, to
empower the user to choose a trade-off justified by the policy domain. Being a
novel task formulation, we also develop a suite of evaluation metrics for fair
hot spots, motivated by the need to evaluate pertinent aspects of the task. We
illustrate the computational infeasibility of identifying fair hot spots using
naive and/or direct approaches and devise a method, codenamed {\it FiSH}, for
efficiently identifying high-quality, fair and diverse sets of spatial hot
spots. FiSH traverses the tree-structured search space using heuristics that
guide it towards identifying effective and fair sets of spatial hot spots.
Through an extensive empirical analysis over a real-world dataset from the
domain of human development, we illustrate that FiSH generates high-quality
solutions at fast response times.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data-driven battery operation for energy arbitrage using rainbow deep reinforcement learning. (arXiv:2106.06061v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Harrold_D/0/1/0/all/0/1">Daniel J. B. Harrold</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jun Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhong Fan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06061">
                                    <div class="article-summary-box-inner">
                                        <span>As the world seeks to become more sustainable, intelligent solutions are
needed to increase the penetration of renewable energy. In this paper, the
model-free deep reinforcement learning algorithm Rainbow Deep Q-Networks is
used to control a battery in a small microgrid to perform energy arbitrage and
more efficiently utilise solar and wind energy sources. The grid operates with
its own demand and renewable generation based on a dataset collected at Keele
University, as well as using dynamic energy pricing from a real wholesale
energy market. Four scenarios are tested including using demand and price
forecasting produced with local weather data. The algorithm and its
subcomponents are evaluated against two continuous control benchmarks with
Rainbow able to outperform all other method. This research shows the importance
of using the distributional approach for reinforcement learning when working
with complex environments and reward functions, as well as how it can be used
to visualise and contextualise the agent&#x27;s behaviour for real-world
applications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Decentralized Adaptive Momentum Method for Solving a Class of Min-Max Optimization Problems. (arXiv:2106.06075v1 [math.OC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Barazandeh_B/0/1/0/all/0/1">Babak Barazandeh</a>, <a href="http://arxiv.org/find/math/1/au:+Huang_T/0/1/0/all/0/1">Tianjian Huang</a>, <a href="http://arxiv.org/find/math/1/au:+Michailidis_G/0/1/0/all/0/1">George Michailidis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06075">
                                    <div class="article-summary-box-inner">
                                        <span>Min-max saddle point games have recently been intensely studied, due to their
wide range of applications, including training Generative Adversarial
Networks~(GANs). However, most of the recent efforts for solving them are
limited to special regimes such as convex-concave games. Further, it is
customarily assumed that the underlying optimization problem is solved either
by a single machine or in the case of multiple machines connected in
centralized fashion, wherein each one communicates with a central node. The
latter approach becomes challenging, when the underlying communications network
has low bandwidth. In addition, privacy considerations may dictate that certain
nodes can communicate with a subset of other nodes. Hence, it is of interest to
develop methods that solve min-max games in a decentralized manner. To that
end, we develop a decentralized adaptive momentum (ADAM)-type algorithm for
solving min-max optimization problem under the condition that the objective
function satisfies a Minty Variational Inequality condition, which is a
generalization to convex-concave case. The proposed method overcomes
shortcomings of recent non-adaptive gradient-based decentralized algorithms for
min-max optimization problems that do not perform well in practice and require
careful tuning. In this paper, we obtain non-asymptotic rates of convergence of
the proposed algorithm (coined DADAM$^3$) for finding a (stochastic)
first-order Nash equilibrium point and subsequently evaluate its performance on
training GANs. The extensive empirical evaluation shows that DADAM$^3$
outperforms recently developed methods, including decentralized optimistic
stochastic gradient for solving such min-max problems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An Ensemble Approach Towards Adversarial Robustness. (arXiv:2106.05996v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1">Haifeng Qian</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05996">
                                    <div class="article-summary-box-inner">
                                        <span>It is a known phenomenon that adversarial robustness comes at a cost to
natural accuracy. To improve this trade-off, this paper proposes an ensemble
approach that divides a complex robust-classification task into simpler
subtasks. Specifically, fractal divide derives multiple training sets from the
training data, and fractal aggregation combines inference outputs from multiple
classifiers that are trained on those sets. The resulting ensemble classifiers
have a unique property that ensures robustness for an input if certain
don&#x27;t-care conditions are met. The new techniques are evaluated on MNIST and
Fashion-MNIST, with no adversarial training. The MNIST classifier has 99%
natural accuracy, 70% measured robustness and 36.9% provable robustness, within
L2 distance of 2. The Fashion-MNIST classifier has 90% natural accuracy, 54.5%
measured robustness and 28.2% provable robustness, within L2 distance of 1.5.
Both results are new state of the art, and we also present new state-of-the-art
binary results on challenging label-pairs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Modeling Hierarchical Structures with Continuous Recursive Neural Networks. (arXiv:2106.06038v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1">Jishnu Ray Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1">Cornelia Caragea</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06038">
                                    <div class="article-summary-box-inner">
                                        <span>Recursive Neural Networks (RvNNs), which compose sequences according to their
underlying hierarchical syntactic structure, have performed well in several
natural language processing tasks compared to similar models without structural
biases. However, traditional RvNNs are incapable of inducing the latent
structure in a plain text sequence on their own. Several extensions have been
proposed to overcome this limitation. Nevertheless, these extensions tend to
rely on surrogate gradients or reinforcement learning at the cost of higher
bias or variance. In this work, we propose Continuous Recursive Neural Network
(CRvNN) as a backpropagation-friendly alternative to address the aforementioned
limitations. This is done by incorporating a continuous relaxation to the
induced structure. We demonstrate that CRvNN achieves strong performance in
challenging synthetic tasks such as logical inference and ListOps. We also show
that CRvNN performs comparably or better than prior latent structure models on
real-world tasks such as sentiment analysis and natural language inference.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning. (arXiv:2106.06047v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1">Liangqiong Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuyin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1">Paul Pu Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1">Yingda Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Feifei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1">Li Fei-Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1">Ehsan Adeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1">Daniel Rubin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06047">
                                    <div class="article-summary-box-inner">
                                        <span>Federated learning is an emerging research paradigm enabling collaborative
training of machine learning models among different organizations while keeping
data private at each institution. Despite recent progress, there remain
fundamental challenges such as lack of convergence and potential for
catastrophic forgetting in federated learning across real-world heterogeneous
devices. In this paper, we demonstrate that attention-based architectures
(e.g., Transformers) are fairly robust to distribution shifts and hence improve
federated learning over heterogeneous data. Concretely, we conduct the first
rigorous empirical investigation of different neural architectures across a
range of federated algorithms, real-world benchmarks, and heterogeneous data
splits. Our experiments show that simply replacing convolutional networks with
Transformers can greatly reduce catastrophic forgetting of previous devices,
accelerate convergence, and reach a better global model, especially when
dealing with heterogeneous data. We will release our code and pretrained models
at https://github.com/Liangqiong/ViT-FL-main to encourage future exploration in
robust architectures as an alternative to current research efforts on the
optimization front.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scalable Variational Gaussian Processes via Harmonic Kernel Decomposition. (arXiv:2106.05992v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1">Shengyang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jiaxin Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1">Andrew Gordon Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1">Roger Grosse</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05992">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a new scalable variational Gaussian process approximation which
provides a high fidelity approximation while retaining general applicability.
We propose the harmonic kernel decomposition (HKD), which uses Fourier series
to decompose a kernel as a sum of orthogonal kernels. Our variational
approximation exploits this orthogonality to enable a large number of inducing
points at a low computational cost. We demonstrate that, on a range of
regression and classification problems, our approach can exploit input space
symmetries such as translations and reflections, and it significantly
outperforms standard variational methods in scalability and accuracy. Notably,
our approach achieves state-of-the-art results on CIFAR-10 among pure GP
models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Synthesising Reinforcement Learning Policies through Set-Valued Inductive Rule Learning. (arXiv:2106.06009v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Coppens_Y/0/1/0/all/0/1">Youri Coppens</a>, <a href="http://arxiv.org/find/cs/1/au:+Steckelmacher_D/0/1/0/all/0/1">Denis Steckelmacher</a>, <a href="http://arxiv.org/find/cs/1/au:+Jonker_C/0/1/0/all/0/1">Catholijn M. Jonker</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1">Ann Now&#xe9;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06009">
                                    <div class="article-summary-box-inner">
                                        <span>Today&#x27;s advanced Reinforcement Learning algorithms produce black-box
policies, that are often difficult to interpret and trust for a person. We
introduce a policy distilling algorithm, building on the CN2 rule mining
algorithm, that distills the policy into a rule-based decision system. At the
core of our approach is the fact that an RL process does not just learn a
policy, a mapping from states to actions, but also produces extra
meta-information, such as action values indicating the quality of alternative
actions. This meta-information can indicate whether more than one action is
near-optimal for a certain state. We extend CN2 to make it able to leverage
knowledge about equally-good actions to distill the policy into fewer rules,
increasing its interpretability by a person. Then, to ensure that the rules
explain a valid, non-degenerate policy, we introduce a refinement algorithm
that fine-tunes the rules to obtain good performance when executed in the
environment. We demonstrate the applicability of our algorithm on the Mario AI
benchmark, a complex task that requires modern reinforcement learning
algorithms including neural networks. The explanations we produce capture the
learned policy in only a few rules, that allow a person to understand what the
black-box agent learned. Source code:
https://gitlab.ai.vub.ac.be/yocoppen/svcn2</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Information Theoretic Evaluation of Privacy-Leakage, Interpretability, and Transferability for a Novel Trustworthy AI Framework. (arXiv:2106.06046v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1">Mohit Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1">Bernhard A. Moser</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1">Lukas Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Freudenthaler_B/0/1/0/all/0/1">Bernhard Freudenthaler</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06046">
                                    <div class="article-summary-box-inner">
                                        <span>Guidelines and principles of trustworthy AI should be adhered to in practice
during the development of AI systems. This work suggests a novel information
theoretic trustworthy AI framework based on the hypothesis that information
theory enables taking into account the ethical AI principles during the
development of machine learning and deep learning models via providing a way to
study and optimize the inherent tradeoffs between trustworthy AI principles. A
unified approach to &quot;privacy-preserving interpretable and transferable
learning&quot; is presented via introducing the information theoretic measures for
privacy-leakage, interpretability, and transferability. A technique based on
variational optimization, employing conditionally deep autoencoders, is
developed for practically calculating the defined information theoretic
measures for privacy-leakage, interpretability, and transferability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Machine Learning Framework for Sensing and Modeling Interference in IoT Frequency Bands. (arXiv:2106.06010v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Homssi_B/0/1/0/all/0/1">Bassel Al Homssi</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Hourani_A/0/1/0/all/0/1">Akram Al-Hourani</a>, <a href="http://arxiv.org/find/cs/1/au:+Krusevac_Z/0/1/0/all/0/1">Zarko Krusevac</a>, <a href="http://arxiv.org/find/cs/1/au:+Rowe_W/0/1/0/all/0/1">Wayne S T Rowe</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06010">
                                    <div class="article-summary-box-inner">
                                        <span>Spectrum scarcity has surfaced as a prominent concern in wireless radio
communications with the emergence of new technologies over the past few years.
As a result, there is growing need for better understanding of the spectrum
occupancy with newly emerging access technologies supporting the Internet of
Things. In this paper, we present a framework to capture and model the traffic
behavior of short-time spectrum occupancy for IoT applications in the shared
bands to determine the existing interference. The proposed capturing method
utilizes a software defined radio to monitor the short bursts of IoT
transmissions by capturing the time series data which is converted to power
spectral density to extract the observed occupancy. Furthermore, we propose the
use of an unsupervised machine learning technique to enhance conventionally
implemented energy detection methods. Our experimental results show that the
temporal and frequency behavior of the spectrum can be well-captured using the
combination of two models, namely, semi-Markov chains and a
Poisson-distribution arrival rate. We conduct an extensive measurement campaign
in different urban environments and incorporate the spatial effect on the IoT
shared spectrum.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm. (arXiv:2106.06027v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Mingkang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tianlong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06027">
                                    <div class="article-summary-box-inner">
                                        <span>Sparse adversarial attacks can fool deep neural networks (DNNs) by only
perturbing a few pixels (regularized by l_0 norm). Recent efforts combine it
with another l_infty imperceptible on the perturbation magnitudes. The
resultant sparse and imperceptible attacks are practically relevant, and
indicate an even higher vulnerability of DNNs that we usually imagined.
However, such attacks are more challenging to generate due to the optimization
difficulty by coupling the l_0 regularizer and box constraints with a
non-convex objective. In this paper, we address this challenge by proposing a
homotopy algorithm, to jointly tackle the sparsity and the perturbation bound
in one unified framework. Each iteration, the main step of our algorithm is to
optimize an l_0-regularized adversarial loss, by leveraging the nonmonotone
Accelerated Proximal Gradient Method (nmAPG) for nonconvex programming; it is
followed by an l_0 change control step, and an optional post-attack step
designed to escape bad local minima. We also extend the algorithm to handling
the structural sparsity regularizer. We extensively examine the effectiveness
of our proposed homotopy attack for both targeted and non-targeted attack
scenarios, on CIFAR-10 and ImageNet datasets. Compared to state-of-the-art
methods, our homotopy attack leads to significantly fewer perturbations, e.g.,
reducing 42.91% on CIFAR-10 and 75.03% on ImageNet (average case, targeted
attack), at similar maximal perturbation magnitudes, when still achieving 100%
attack success rates. Our codes are available at:
https://github.com/VITA-Group/SparseADV_Homotopy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A self-adapting super-resolution structures framework for automatic design of GAN. (arXiv:2106.06011v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yibo Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haidi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yiming Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shunyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mingliang Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06011">
                                    <div class="article-summary-box-inner">
                                        <span>With the development of deep learning, the single super-resolution image
reconstruction network models are becoming more and more complex. Small changes
in hyperparameters of the models have a greater impact on model performance. In
the existing works, experts have gradually explored a set of optimal model
parameters based on empirical values or performing brute-force search. In this
paper, we introduce a new super-resolution image reconstruction generative
adversarial network framework, and a Bayesian optimization method used to
optimizing the hyperparameters of the generator and discriminator. The
generator is made by self-calibrated convolution, and discriminator is made by
convolution lays. We have defined the hyperparameters such as the number of
network layers and the number of neurons. Our method adopts Bayesian
optimization as a optimization policy of GAN in our model. Not only can find
the optimal hyperparameter solution automatically, but also can construct a
super-resolution image reconstruction network, reducing the manual workload.
Experiments show that Bayesian optimization can search the optimal solution
earlier than the other two optimization algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Coordinate Independent Convolutional Networks -- Isometry and Gauge Equivariant Convolutions on Riemannian Manifolds. (arXiv:2106.06020v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1">Maurice Weiler</a>, <a href="http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1">Patrick Forr&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Verlinde_E/0/1/0/all/0/1">Erik Verlinde</a>, <a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1">Max Welling</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06020">
                                    <div class="article-summary-box-inner">
                                        <span>Motivated by the vast success of deep convolutional networks, there is a
great interest in generalizing convolutions to non-Euclidean manifolds. A major
complication in comparison to flat spaces is that it is unclear in which
alignment a convolution kernel should be applied on a manifold. The underlying
reason for this ambiguity is that general manifolds do not come with a
canonical choice of reference frames (gauge). Kernels and features therefore
have to be expressed relative to arbitrary coordinates. We argue that the
particular choice of coordinatization should not affect a network&#x27;s inference
-- it should be coordinate independent. A simultaneous demand for coordinate
independence and weight sharing is shown to result in a requirement on the
network to be equivariant under local gauge transformations (changes of local
reference frames). The ambiguity of reference frames depends thereby on the
G-structure of the manifold, such that the necessary level of gauge
equivariance is prescribed by the corresponding structure group G. Coordinate
independent convolutions are proven to be equivariant w.r.t. those isometries
that are symmetries of the G-structure. The resulting theory is formulated in a
coordinate free fashion in terms of fiber bundles. To exemplify the design of
coordinate independent convolutions, we implement a convolutional network on
the M\&quot;obius strip. The generality of our differential geometric formulation of
convolutional networks is demonstrated by an extensive literature review which
explains a large number of Euclidean CNNs, spherical CNNs and CNNs on general
surfaces as specific instances of coordinate independent convolutions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DECORE: Deep Compression with Reinforcement Learning. (arXiv:2106.06091v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Alwani_M/0/1/0/all/0/1">Manoj Alwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Madhavan_V/0/1/0/all/0/1">Vashisht Madhavan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06091">
                                    <div class="article-summary-box-inner">
                                        <span>Deep learning has become an increasingly popular and powerful option for
modern pattern recognition systems. However, many deep neural networks have
millions to billions of parameters, making them untenable for real-world
applications with constraints on memory or latency. As a result, powerful
network compression techniques are a must for the widespread adoption of deep
learning. We present DECORE, a reinforcement learning approach to automate the
network compression process. Using a simple policy gradient method to learn
which neurons or channels to keep or remove, we are able to achieve compression
rates 3x to 5x greater than contemporary approaches. In contrast with other
architecture search methods, DECORE is simple and quick to train, requiring
only a few hours of training on 1 GPU. When applied to standard network
architectures on different datasets, our approach achieves 11x to 103x
compression on different architectures while maintaining accuracies similar to
those of the original, large networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scaling Vision with Sparse Mixture of Experts. (arXiv:2106.05974v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Riquelme_C/0/1/0/all/0/1">Carlos Riquelme</a>, <a href="http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1">Joan Puigcerver</a>, <a href="http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1">Basil Mustafa</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_M/0/1/0/all/0/1">Maxim Neumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1">Rodolphe Jenatton</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinto_A/0/1/0/all/0/1">Andr&#xe9; Susano Pinto</a>, <a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1">Daniel Keysers</a>, <a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1">Neil Houlsby</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05974">
                                    <div class="article-summary-box-inner">
                                        <span>Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent
scalability in Natural Language Processing. In Computer Vision, however, almost
all performant networks are &quot;dense&quot;, that is, every input is processed by every
parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision
Transformer, that is scalable and competitive with the largest dense networks.
When applied to image recognition, V-MoE matches the performance of
state-of-the-art networks, while requiring as little as half of the compute at
inference time. Further, we propose an extension to the routing algorithm that
can prioritize subsets of each input across the entire batch, leading to
adaptive per-image compute. This allows V-MoE to trade-off performance and
compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE
to scale vision models, and train a 15B parameter model that attains 90.35% on
ImageNet.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ChemRL-GEM: Geometry Enhanced Molecular Representation Learning for Property Prediction. (arXiv:2106.06130v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1">Xiaomin Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lihang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1">Jieqiong Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1">Donglong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanzhuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingbo Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hua Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haifeng Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06130">
                                    <div class="article-summary-box-inner">
                                        <span>Effective molecular representation learning is of great importance to
facilitate molecular property prediction, which is a fundamental task for the
drug and material industry. Recent advances in graph neural networks (GNNs)
have shown great promise in applying GNNs for molecular representation
learning. Moreover, a few recent studies have also demonstrated successful
applications of self-supervised learning methods to pre-train the GNNs to
overcome the problem of insufficient labeled molecules. However, existing GNNs
and pre-training strategies usually treat molecules as topological graph data
without fully utilizing the molecular geometry information. Whereas, the
three-dimensional (3D) spatial structure of a molecule, a.k.a molecular
geometry, is one of the most critical factors for determining molecular
physical, chemical, and biological properties. To this end, we propose a novel
Geometry Enhanced Molecular representation learning method (GEM) for Chemical
Representation Learning (ChemRL). At first, we design a geometry-based GNN
architecture that simultaneously models atoms, bonds, and bond angles in a
molecule. To be specific, we devised double graphs for a molecule: The first
one encodes the atom-bond relations; The second one encodes bond-angle
relations. Moreover, on top of the devised GNN architecture, we propose several
novel geometry-level self-supervised learning strategies to learn spatial
knowledge by utilizing the local and global molecular 3D structures. We compare
ChemRL-GEM with various state-of-the-art (SOTA) baselines on different
molecular benchmarks and exhibit that ChemRL-GEM can significantly outperform
all baselines in both regression and classification tasks. For example, the
experimental results show an overall improvement of $8.8\%$ on average compared
to SOTA baselines on the regression tasks, demonstrating the superiority of the
proposed method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Graph Neural Networks for Natural Language Processing: A Survey. (arXiv:2106.06090v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lingfei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1">Kai Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiaojie Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1">Hanning Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shucheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1">Jian Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1">Bo Long</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06090">
                                    <div class="article-summary-box-inner">
                                        <span>Deep learning has become the dominant approach in coping with various tasks
in Natural LanguageProcessing (NLP). Although text inputs are typically
represented as a sequence of tokens, there isa rich variety of NLP problems
that can be best expressed with a graph structure. As a result, thereis a surge
of interests in developing new deep learning techniques on graphs for a large
numberof NLP tasks. In this survey, we present a comprehensive overview onGraph
Neural Networks(GNNs) for Natural Language Processing. We propose a new
taxonomy of GNNs for NLP, whichsystematically organizes existing research of
GNNs for NLP along three axes: graph construction,graph representation
learning, and graph based encoder-decoder models. We further introducea large
number of NLP applications that are exploiting the power of GNNs and summarize
thecorresponding benchmark datasets, evaluation metrics, and open-source codes.
Finally, we discussvarious outstanding challenges for making the full use of
GNNs for NLP as well as future researchdirections. To the best of our
knowledge, this is the first comprehensive overview of Graph NeuralNetworks for
Natural Language Processing.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Modeling Sequences as Distributions with Uncertainty for Sequential Recommendation. (arXiv:2106.06165v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Ziwei Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1">Philip S. Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06165">
                                    <div class="article-summary-box-inner">
                                        <span>The sequential patterns within the user interactions are pivotal for
representing the user&#x27;s preference and capturing latent relationships among
items. The recent advancements of sequence modeling by Transformers advocate
the community to devise more effective encoders for the sequential
recommendation. Most existing sequential methods assume users are
deterministic. However, item-item transitions might fluctuate significantly in
several item aspects and exhibit randomness of user interests. This
\textit{stochastic characteristics} brings up a solid demand to include
uncertainties in representing sequences and items. Additionally, modeling
sequences and items with uncertainties expands users&#x27; and items&#x27; interaction
spaces, thus further alleviating cold-start problems.

In this work, we propose a Distribution-based Transformer for Sequential
Recommendation (DT4SR), which injects uncertainties into sequential modeling.
We use Elliptical Gaussian distributions to describe items and sequences with
uncertainty. We describe the uncertainty in items and sequences as Elliptical
Gaussian distribution. And we adopt Wasserstein distance to measure the
similarity between distributions. We devise two novel Trans-formers for
modeling mean and covariance, which guarantees the positive-definite property
of distributions. The proposed method significantly outperforms the
state-of-the-art methods. The experiments on three benchmark datasets also
demonstrate its effectiveness in alleviating cold-start issues. The code is
available inhttps://github.com/DyGRec/DT4SR.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.MM"">cs.MM updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online Multi-Object Tracking and Segmentation with GMPHD Filter and Mask-based Affinity Fusion. (arXiv:2009.00100v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Young-min Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1">Young-chul Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1">Kwangjin Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1">Moongu Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1">Witold Pedrycz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.00100">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose a highly practical fully online multi-object
tracking and segmentation (MOTS) method that uses instance segmentation results
as an input. The proposed method is based on the Gaussian mixture probability
hypothesis density (GMPHD) filter, a hierarchical data association (HDA), and a
mask-based affinity fusion (MAF) model to achieve high-performance online
tracking. The HDA consists of two associations: segment-to-track and
track-to-track associations. One affinity, for position and motion, is computed
by using the GMPHD filter, and the other affinity, for appearance is computed
by using the responses from a single object tracker such as a kernalized
correlation filter. These two affinities are simply fused by using a
score-level fusion method such as min-max normalization referred to as MAF. In
addition, to reduce the number of false positive segments, we adopt mask
IoU-based merging (mask merging). The proposed MOTS framework with the key
modules: HDA, MAF, and mask merging, is easily extensible to simultaneously
track multiple types of objects with CPU only execution in parallel processing.
In addition, the developed framework only requires simple parameter tuning
unlike many existing MOTS methods that need intensive hyperparameter
optimization. In the experiments on the two popular MOTS datasets, the key
modules show some improvements. For instance, ID-switch decreases by more than
half compared to a baseline method in the training sets. In conclusion, our
tracker achieves state-of-the-art MOTS performance in the test sets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Abstract Representations through Lossy Compression of Multi-Modal Signals. (arXiv:2101.11376v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wilmot_C/0/1/0/all/0/1">Charles Wilmot</a>, <a href="http://arxiv.org/find/cs/1/au:+Triesch_J/0/1/0/all/0/1">Jochen Triesch</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11376">
                                    <div class="article-summary-box-inner">
                                        <span>A key competence for open-ended learning is the formation of increasingly
abstract representations useful for driving complex behavior. Abstract
representations ignore specific details and facilitate generalization. Here we
consider the learning of abstract representations in a multi-modal setting with
two or more input modalities. We treat the problem as a lossy compression
problem and show that generic lossy compression of multimodal sensory input
naturally extracts abstract representations that tend to strip away modalitiy
specific details and preferentially retain information that is shared across
the different modalities. Furthermore, we propose an architecture to learn
abstract representations by identifying and retaining only the information that
is shared across multiple modalities while discarding any modality specific
information.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Shallow Optical Flow Three-Stream CNN for Macro- and Micro-Expression Spotting from Long Videos. (arXiv:2106.06489v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liong_G/0/1/0/all/0/1">Gen-Bing Liong</a>, <a href="http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1">John See</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1">Lai-Kuan Wong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06489">
                                    <div class="article-summary-box-inner">
                                        <span>Facial expressions vary from the visible to the subtle. In recent years, the
analysis of micro-expressions $-$ a natural occurrence resulting from the
suppression of one&#x27;s true emotions, has drawn the attention of researchers with
a broad range of potential applications. However, spotting microexpressions in
long videos becomes increasingly challenging when intertwined with normal or
macro-expressions. In this paper, we propose a shallow optical flow
three-stream CNN (SOFTNet) model to predict a score that captures the
likelihood of a frame being in an expression interval. By fashioning the
spotting task as a regression problem, we introduce pseudo-labeling to
facilitate the learning process. We demonstrate the efficacy and efficiency of
the proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art
performance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM
Long Videos.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
        </ul>
    </section>
    <section class="daily-content">
        <h2 class="daily-heading"><time datatime="2021-06-11">2021-06-11</time></h2>
        <ul class="sources card">
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.CL"">cs.CL updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AUGNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation. (arXiv:2106.05589v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xinnuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guoyin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Young-Bum Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sungjin Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05589">
                                    <div class="article-summary-box-inner">
                                        <span>Natural Language Generation (NLG) is a key component in a task-oriented
dialogue system, which converts the structured meaning representation (MR) to
the natural language. For large-scale conversational systems, where it is
common to have over hundreds of intents and thousands of slots, neither
template-based approaches nor model-based approaches are scalable. Recently,
neural NLGs started leveraging transfer learning and showed promising results
in few-shot settings. This paper proposes AUGNLG, a novel data augmentation
approach that combines a self-trained neural retrieval model with a few-shot
learned NLU model, to automatically create MR-to-Text data from open-domain
texts. The proposed system mostly outperforms the state-of-the-art methods on
the FewShotWOZ data in both BLEU and Slot Error Rate. We further confirm
improved results on the FewShotSGD data and provide comprehensive analysis
results on key components of our system. Our code and data are available at
https://github.com/XinnuoXu/AugNLG.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Two-stage Textual Knowledge Distillation for End-to-End Spoken Language Understanding. (arXiv:2010.13105v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seongbin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Gyuwan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1">Seongjin Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sangmin Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13105">
                                    <div class="article-summary-box-inner">
                                        <span>End-to-end approaches open a new way for more accurate and efficient spoken
language understanding (SLU) systems by alleviating the drawbacks of
traditional pipeline systems. Previous works exploit textual information for an
SLU model via pre-training with automatic speech recognition or fine-tuning
with knowledge distillation. To utilize textual information more effectively,
this work proposes a two-stage textual knowledge distillation method that
matches utterance-level representations and predicted logits of two modalities
during pre-training and fine-tuning, sequentially. We use vq-wav2vec BERT as a
speech encoder because it captures general and rich features. Furthermore, we
improve the performance, especially in a low-resource scenario, with data
augmentation methods by randomly masking spans of discrete audio tokens and
contextualized hidden representations. Consequently, we push the
state-of-the-art on the Fluent Speech Commands, achieving 99.7% test accuracy
in the full dataset setting and 99.5% in the 10% subset setting. Throughout the
ablation studies, we empirically verify that all used methods are crucial to
the final performance, providing the best practice for spoken language
understanding. Code is available at https://github.com/clovaai/textual-kd-slu.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Linguistically Informed Masking for Representation Learning in the Patent Domain. (arXiv:2106.05768v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Althammer_S/0/1/0/all/0/1">Sophia Althammer</a>, <a href="http://arxiv.org/find/cs/1/au:+Buckley_M/0/1/0/all/0/1">Mark Buckley</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1">Sebastian Hofst&#xe4;tter</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1">Allan Hanbury</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05768">
                                    <div class="article-summary-box-inner">
                                        <span>Domain-specific contextualized language models have demonstrated substantial
effectiveness gains for domain-specific downstream tasks, like similarity
matching, entity recognition or information retrieval. However successfully
applying such models in highly specific language domains requires domain
adaptation of the pre-trained models. In this paper we propose the empirically
motivated Linguistically Informed Masking (LIM) method to focus
domain-adaptative pre-training on the linguistic patterns of patents, which use
a highly technical sublanguage. We quantify the relevant differences between
patent, scientific and general-purpose language and demonstrate for two
different language models (BERT and SciBERT) that domain adaptation with LIM
leads to systematically improved representations by evaluating the performance
of the domain-adapted representations of patent language on two independent
downstream tasks, the IPC classification and similarity matching. We
demonstrate the impact of balancing the learning from different information
sources during domain adaptation for the patent domain. We make the source code
as well as the domain-adaptive pre-trained patent language models publicly
available at https://github.com/sophiaalthammer/patent-lim.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures. (arXiv:2106.05822v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chelombiev_I/0/1/0/all/0/1">Ivan Chelombiev</a>, <a href="http://arxiv.org/find/cs/1/au:+Justus_D/0/1/0/all/0/1">Daniel Justus</a>, <a href="http://arxiv.org/find/cs/1/au:+Orr_D/0/1/0/all/0/1">Douglas Orr</a>, <a href="http://arxiv.org/find/cs/1/au:+Dietrich_A/0/1/0/all/0/1">Anastasia Dietrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Gressmann_F/0/1/0/all/0/1">Frithjof Gressmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Koliousis_A/0/1/0/all/0/1">Alexandros Koliousis</a>, <a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1">Carlo Luschi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05822">
                                    <div class="article-summary-box-inner">
                                        <span>Attention based language models have become a critical component in
state-of-the-art natural language processing systems. However, these models
have significant computational requirements, due to long training times, dense
operations and large parameter count. In this work we demonstrate a set of
modifications to the structure of a Transformer layer, producing a more
efficient architecture. First, we add a convolutional module to complement the
self-attention module, decoupling the learning of local and global
interactions. Secondly, we rely on grouped transformations to reduce the
computational cost of dense feed-forward layers and convolutions, while
preserving the expressivity of the model. We apply the resulting architecture
to language representation learning and demonstrate its superior performance
compared to BERT models of different scales. We further highlight its improved
efficiency, both in terms of floating-point operations (FLOPs) and
time-to-train.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Progressive Multi-Granularity Training for Non-Autoregressive Translation. (arXiv:2106.05546v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Liang Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Longyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xuebo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1">Derek F. Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhaopeng Tu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05546">
                                    <div class="article-summary-box-inner">
                                        <span>Non-autoregressive translation (NAT) significantly accelerates the inference
process via predicting the entire target sequence. However, recent studies show
that NAT is weak at learning high-mode of knowledge such as one-to-many
translations. We argue that modes can be divided into various granularities
which can be learned from easy to hard. In this study, we empirically show that
NAT models are prone to learn fine-grained lower-mode knowledge, such as words
and phrases, compared with sentences. Based on this observation, we propose
progressive multi-granularity training for NAT. More specifically, to make the
most of the training data, we break down the sentence-level examples into three
types, i.e. words, phrases, sentences, and with the training goes, we
progressively increase the granularities. Experiments on Romanian-English,
English-German, Chinese-English, and Japanese-English demonstrate that our
approach improves the phrase translation accuracy and model reordering ability,
therefore resulting in better translation quality against strong NAT baselines.
Also, we show that more deterministic fine-grained knowledge can further
enhance performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering. (arXiv:2106.05346v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1">Devendra Singh Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Siva Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1">William Hamilton</a>, <a href="http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1">Chris Dyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1">Dani Yogatama</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05346">
                                    <div class="article-summary-box-inner">
                                        <span>We present an end-to-end differentiable training method for
retrieval-augmented open-domain question answering systems that combine
information from multiple retrieved documents when generating answers. We model
retrieval decisions as latent variables over sets of relevant documents. Since
marginalizing over sets of retrieved documents is computationally hard, we
approximate this using an expectation-maximization algorithm. We iteratively
estimate the value of our latent variable (the set of relevant documents for a
given question) and then use this estimate to update the retriever and reader
parameters. We hypothesize that such end-to-end training allows training
signals to flow to the reader and then to the retriever better than staged-wise
training. This results in a retriever that is able to select more relevant
documents for a question and a reader that is trained on more accurate
documents to generate an answer. Experiments on three benchmark datasets
demonstrate that our proposed method outperforms all existing approaches of
comparable size by 2-3% absolute exact match points, achieving new
state-of-the-art results. Our results also demonstrate the feasibility of
learning to retrieve to improve answer generation without explicit supervision
of retrieval decisions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Eye of the Beholder: Improved Relation Generalization for Text-based Reinforcement Learning Agents. (arXiv:2106.05387v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1">Keerthiram Murugesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1">Subhajit Chaudhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Talamadupula_K/0/1/0/all/0/1">Kartik Talamadupula</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05387">
                                    <div class="article-summary-box-inner">
                                        <span>Text-based games (TBGs) have become a popular proving ground for the
demonstration of learning-based agents that make decisions in quasi real-world
settings. The crux of the problem for a reinforcement learning agent in such
TBGs is identifying the objects in the world, and those objects&#x27; relations with
that world. While the recent use of text-based resources for increasing an
agent&#x27;s knowledge and improving its generalization have shown promise, we posit
in this paper that there is much yet to be learned from visual representations
of these same worlds. Specifically, we propose to retrieve images that
represent specific instances of text observations from the world and train our
agents on such images. This improves the agent&#x27;s overall understanding of the
game &#x27;scene&#x27; and objects&#x27; relationships to the world around them, and the
variety of visual representations on offer allow the agent to generate a better
generalization of a relationship. We show that incorporating such images
improves the performance of agents in various TBG settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. (arXiv:2106.05707v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Aly_R/0/1/0/all/0/1">Rami Aly</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhijiang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1">Michael Schlichtkrull</a>, <a href="http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1">James Thorne</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1">Andreas Vlachos</a>, <a href="http://arxiv.org/find/cs/1/au:+Christodoulopoulos_C/0/1/0/all/0/1">Christos Christodoulopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1">Oana Cocarascu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1">Arpit Mittal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05707">
                                    <div class="article-summary-box-inner">
                                        <span>Fact verification has attracted a lot of attention in the machine learning
and natural language processing communities, as it is one of the key methods
for detecting misinformation. Existing large-scale benchmarks for this task
have focused mostly on textual sources, i.e. unstructured information, and thus
ignored the wealth of information available in structured formats, such as
tables. In this paper we introduce a novel dataset and benchmark, Fact
Extraction and VERification Over Unstructured and Structured information
(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated
with evidence in the form of sentences and/or cells from tables in Wikipedia,
as well as a label indicating whether this evidence supports, refutes, or does
not provide enough information to reach a verdict. Furthermore, we detail our
efforts to track and minimize the biases present in the dataset and could be
exploited by models, e.g. being able to predict the label without using
evidence. Finally, we develop a baseline for verifying claims against text and
tables which predicts both the correct evidence and verdict for 18% of the
claims.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DT-grams: Structured Dependency Grammar Stylometry for Cross-Language Authorship Attribution. (arXiv:2106.05677v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Murauer_B/0/1/0/all/0/1">Benjamin Murauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Specht_G/0/1/0/all/0/1">G&#xfc;nther Specht</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05677">
                                    <div class="article-summary-box-inner">
                                        <span>Cross-language authorship attribution problems rely on either translation to
enable the use of single-language features, or language-independent feature
extraction methods. Until recently, the lack of datasets for this problem
hindered the development of the latter, and single-language solutions were
performed on machine-translated corpora. In this paper, we present a novel
language-independent feature for authorship analysis based on dependency graphs
and universal part of speech tags, called DT-grams (dependency tree grams),
which are constructed by selecting specific sub-parts of the dependency graph
of sentences. We evaluate DT-grams by performing cross-language authorship
attribution on untranslated datasets of bilingual authors, showing that, on
average, they achieve a macro-averaged F1 score of 0.081 higher than previous
methods across five different language pairs. Additionally, by providing
results for a diverse set of features for comparison, we provide a baseline on
the previously undocumented task of untranslated cross-language authorship
attribution.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data. (arXiv:2101.07597v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yao Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumatani_K/0/1/0/all/0/1">Kenichi Kumatani</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shujie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1">Michael Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuedong Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07597">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose a unified pre-training approach called UniSpeech to
learn speech representations with both unlabeled and labeled data, in which
supervised phonetic CTC learning and phonetically-aware contrastive
self-supervised learning are conducted in a multi-task learning manner. The
resultant representations can capture information more correlated with phonetic
structures and improve the generalization across languages and domains. We
evaluate the effectiveness of UniSpeech for cross-lingual representation
learning on public CommonVoice corpus. The results show that UniSpeech
outperforms self-supervised pretraining and supervised transfer learning for
speech recognition by a maximum of 13.4% and 17.8% relative phone error rate
reductions respectively (averaged over all testing languages). The
transferability of UniSpeech is also demonstrated on a domain-shift speech
recognition task, i.e., a relative word error rate reduction of 6% against the
previous approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Ruddit: Norms of Offensiveness for English Reddit Comments. (arXiv:2106.05664v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hada_R/0/1/0/all/0/1">Rishav Hada</a>, <a href="http://arxiv.org/find/cs/1/au:+Sudhir_S/0/1/0/all/0/1">Sohi Sudhir</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1">Pushkar Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1">Helen Yannakoudakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1">Saif M. Mohammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1">Ekaterina Shutova</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05664">
                                    <div class="article-summary-box-inner">
                                        <span>On social media platforms, hateful and offensive language negatively impact
the mental well-being of users and the participation of people from diverse
backgrounds. Automatic methods to detect offensive language have largely relied
on datasets with categorical labels. However, comments can vary in their degree
of offensiveness. We create the first dataset of English language Reddit
comments that has \textit{fine-grained, real-valued scores} between -1
(maximally supportive) and 1 (maximally offensive). The dataset was annotated
using \emph{Best--Worst Scaling}, a form of comparative annotation that has
been shown to alleviate known biases of using rating scales. We show that the
method produces highly reliable offensiveness scores. Finally, we evaluate the
ability of widely-used neural models to predict offensiveness scores on this
new dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DESCGEN: A Distantly Supervised Datasetfor Generating Abstractive Entity Descriptions. (arXiv:2106.05365v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Weijia Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1">Mandar Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1">Luke Zettlemoyer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05365">
                                    <div class="article-summary-box-inner">
                                        <span>Short textual descriptions of entities provide summaries of their key
attributes and have been shown to be useful sources of background knowledge for
tasks such as entity linking and question answering. However, generating entity
descriptions, especially for new and long-tail entities, can be challenging
since relevant information is often scattered across multiple sources with
varied content and style. We introduce DESCGEN: given mentions spread over
multiple documents, the goal is to generate an entity summary description.
DESCGEN consists of 37K entity descriptions from Wikipedia and Fandom, each
paired with nine evidence documents on average. The documents were collected
using a combination of entity linking and hyperlinks to the Wikipedia and
Fandom entity pages, which together provide high-quality distant supervision.
The resulting summaries are more abstractive than those found in existing
datasets and provide a better proxy for the challenge of describing new and
emerging entities. We also propose a two-stage extract-then-generate baseline
and show that there exists a large gap (19.9% in ROUGE-L) between
state-of-the-art models and human performance, suggesting that the data will
support significant future work.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Word frequency-rank relationship in tagged texts. (arXiv:2102.10992v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chacoma_A/0/1/0/all/0/1">A. Chacoma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanette_D/0/1/0/all/0/1">D. H. Zanette</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10992">
                                    <div class="article-summary-box-inner">
                                        <span>We analyze the frequency-rank relationship in sub-vocabularies corresponding
to three different grammatical classes (nouns, verbs, and others) in a
collection of literary works in English, whose words have been automatically
tagged according to their grammatical role. Comparing with a null hypothesis
which assumes that words belonging to each class are uniformly distributed
across the frequency-ranked vocabulary of the whole work, we disclose
statistically significant differences between the three classes. This results
point to the fact that frequency-rank relationships may reflect linguistic
features associated with grammatical function.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Programming Puzzles. (arXiv:2106.05784v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1">Tal Schuster</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1">Ashwin Kalyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Polozov_O/0/1/0/all/0/1">Oleksandr Polozov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1">Adam Tauman Kalai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05784">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a new type of programming challenge called programming puzzles,
as an objective and comprehensive evaluation of program synthesis, and release
an open-source dataset of Python Programming Puzzles (P3). Each puzzle is
defined by a short Python program $f$, and the goal is to find an input $x$
which makes $f$ output &quot;True&quot;. The puzzles are objective in that each one is
specified entirely by the source code of its verifier $f$, so evaluating $f(x)$
is all that is needed to test a candidate solution $x$. They do not require an
answer key or input/output examples, nor do they depend on natural language
understanding. The dataset is comprehensive in that it spans problems of a
range of difficulties and domains, ranging from trivial string manipulation
problems that are immediately obvious to human programmers (but not necessarily
to AI), to classic programming puzzles (e.g., Towers of Hanoi), to
interview/competitive-programming problems (e.g., dynamic programming), to
longstanding open problems in algorithms and mathematics (e.g., factoring). The
objective nature of P3 readily supports self-supervised bootstrapping. We
develop baseline enumerative program synthesis and GPT-3 solvers that are
capable of solving easy puzzles -- even without access to any reference
solutions -- by learning from their own past solutions. Based on a small user
study, we find puzzle difficulty to correlate between human programmers and the
baseline AI solvers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning to Perturb Word Embeddings for Out-of-distribution QA. (arXiv:2105.02692v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seanie Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Minki Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Juho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02692">
                                    <div class="article-summary-box-inner">
                                        <span>QA models based on pretrained language mod-els have achieved remarkable
performance onv arious benchmark datasets.However, QA models do not generalize
well to unseen data that falls outside the training distribution, due to
distributional shifts.Data augmentation(DA) techniques which drop/replace words
have shown to be effective in regularizing the model from overfitting to the
training data.Yet, they may adversely affect the QA tasks since they incur
semantic changes that may lead to wrong answers for the QA task. To tackle this
problem, we propose a simple yet effective DA method based on a stochastic
noise generator, which learns to perturb the word embedding of the input
questions and context without changing their semantics. We validate the
performance of the QA models trained with our word embedding perturbation on a
single source dataset, on five different target domains.The results show that
our method significantly outperforms the baselineDA methods. Notably, the model
trained with ours outperforms the model trained with more than 240K
artificially generated QA pairs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Relative Positional Encoding for Transformers with Linear Complexity. (arXiv:2105.08399v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liutkus_A/0/1/0/all/0/1">Antoine Liutkus</a>, <a href="http://arxiv.org/find/cs/1/au:+Cifka_O/0/1/0/all/0/1">Ond&#x159;ej C&#xed;fka</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shih-Lun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Simsekli_U/0/1/0/all/0/1">Umut &#x15e;im&#x15f;ekli</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi-Hsuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1">Ga&#xeb;l Richard</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08399">
                                    <div class="article-summary-box-inner">
                                        <span>Recent advances in Transformer models allow for unprecedented sequence
lengths, due to linear space and time complexity. In the meantime, relative
positional encoding (RPE) was proposed as beneficial for classical Transformers
and consists in exploiting lags instead of absolute positions for inference.
Still, RPE is not available for the recent linear-variants of the Transformer,
because it requires the explicit computation of the attention matrix, which is
precisely what is avoided by such methods. In this paper, we bridge this gap
and present Stochastic Positional Encoding as a way to generate PE that can be
used as a replacement to the classical additive (sinusoidal) PE and provably
behaves like RPE. The main theoretical contribution is to make a connection
between positional encoding and cross-covariance structures of correlated
Gaussian processes. We illustrate the performance of our approach on the
Long-Range Arena benchmark and on music generation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AlloST: Low-resource Speech Translation without Source Transcription. (arXiv:2105.00171v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yao-Fei Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hung-Shin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hsin-Min Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00171">
                                    <div class="article-summary-box-inner">
                                        <span>The end-to-end architecture has made promising progress in speech translation
(ST). However, the ST task is still challenging under low-resource conditions.
Most ST models have shown unsatisfactory results, especially in the absence of
word information from the source speech utterance. In this study, we survey
methods to improve ST performance without using source transcription, and
propose a learning framework that utilizes a language-independent universal
phone recognizer. The framework is based on an attention-based
sequence-to-sequence model, where the encoder generates the phonetic embeddings
and phone-aware acoustic representations, and the decoder controls the fusion
of the two embedding streams to produce the target token sequence. In addition
to investigating different fusion strategies, we explore the specific usage of
byte pair encoding (BPE), which compresses a phone sequence into a
syllable-like segmented sequence. Due to the conversion of symbols, a segmented
sequence represents not only pronunciation but also language-dependent
information lacking in phones. Experiments conducted on the Fisher
Spanish-English and Taigi-Mandarin drama corpora show that our method
outperforms the conformer-based baseline, and the performance is close to that
of the existing best method using source transcription.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations. (arXiv:2106.01093v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1">Ruisheng Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yanbin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Su Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Kai Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01093">
                                    <div class="article-summary-box-inner">
                                        <span>This work aims to tackle the challenging heterogeneous graph encoding problem
in the text-to-SQL task. Previous methods are typically node-centric and merely
utilize different weight matrices to parameterize edge types, which 1) ignore
the rich semantics embedded in the topological structure of edges, and 2) fail
to distinguish local and non-local relations for each node. To this end, we
propose a Line Graph Enhanced Text-to-SQL (LGESQL) model to mine the underlying
relational features without constructing meta-paths. By virtue of the line
graph, messages propagate more efficiently through not only connections between
nodes, but also the topology of directed edges. Furthermore, both local and
non-local relations are integrated distinctively during the graph iteration. We
also design an auxiliary task called graph pruning to improve the
discriminative capability of the encoder. Our framework achieves
state-of-the-art results (62.8% with Glove, 72.0% with Electra) on the
cross-domain text-to-SQL benchmark Spider at the time of writing.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Identifying Populist Paragraphs in Text: A machine-learning approach. (arXiv:2106.03161v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ulinskaite_J/0/1/0/all/0/1">Jogil&#x117; Ulinskait&#x117;</a>, <a href="http://arxiv.org/find/cs/1/au:+Pukelis_L/0/1/0/all/0/1">Lukas Pukelis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03161">
                                    <div class="article-summary-box-inner">
                                        <span>Abstract: In this paper we present an approach to develop a
text-classification model which would be able to identify populist content in
text. The developed BERT-based model is largely successful in identifying
populist content in text and produces only a negligible amount of False
Negatives, which makes it well-suited as a content analysis automation tool,
which shortlists potentially relevant content for human validation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Feature Replacement and Combination for Hybrid ASR Systems. (arXiv:2104.04298v2 [eess.AS] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Vieting_P/0/1/0/all/0/1">Peter Vieting</a>, <a href="http://arxiv.org/find/eess/1/au:+Luscher_C/0/1/0/all/0/1">Christoph L&#xfc;scher</a>, <a href="http://arxiv.org/find/eess/1/au:+Michel_W/0/1/0/all/0/1">Wilfried Michel</a>, <a href="http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1">Ralf Schl&#xfc;ter</a>, <a href="http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1">Hermann Ney</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04298">
                                    <div class="article-summary-box-inner">
                                        <span>Acoustic modeling of raw waveform and learning feature extractors as part of
the neural network classifier has been the goal of many studies in the area of
automatic speech recognition (ASR). Recently, one line of research has focused
on frameworks that can be pre-trained on audio-only data in an unsupervised
fashion and aim at improving downstream ASR tasks. In this work, we investigate
the usefulness of one of these front-end frameworks, namely wav2vec, for hybrid
ASR systems. In addition to deploying a pre-trained feature extractor, we
explore how to make use of an existing acoustic model (AM) trained on the same
task with different features as well. Another neural front-end which is only
trained together with the supervised ASR loss as well as traditional Gammatone
features are applied for comparison. Moreover, it is shown that the AM can be
retrofitted with i-vectors for speaker adaptation. Finally, the described
features are combined in order to further advance the performance. With the
final best system, we obtain a relative improvement of 4% and 6% over our
previous best model on the LibriSpeech test-clean and test-other sets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures. (arXiv:2104.05379v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rossenbach_N/0/1/0/all/0/1">Nick Rossenbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1">Mohammad Zeineldeen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilmes_B/0/1/0/all/0/1">Benedikt Hilmes</a>, <a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1">Ralf Schl&#xfc;ter</a>, <a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1">Hermann Ney</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05379">
                                    <div class="article-summary-box-inner">
                                        <span>Recent publications on automatic-speech-recognition (ASR) have a strong focus
on attention encoder-decoder (AED) architectures which work well for large
datasets, but tend to overfit when applied in low resource scenarios. One
solution to tackle this issue is to generate synthetic data with a trained
text-to-speech system (TTS) if additional text is available. This was
successfully applied in many publications with AED systems. We present a novel
approach of silence correction in the data pre-processing for TTS systems which
increases the robustness when training on corpora targeted for ASR
applications. In this work we do not only show the successful application of
synthetic data for AED systems, but also test the same method on a highly
optimized state-of-the-art Hybrid ASR system and a competitive monophone based
system using connectionist-temporal-classification (CTC). We show that for the
later systems the addition of synthetic data only has a minor effect, but they
still outperform the AED systems by a large margin on LibriSpeech-100h. We
achieve a final word-error-rate of 3.3%/10.0% with a Hybrid system on the
clean/noisy test-sets, surpassing any previous state-of-the-art systems that do
not include unlabeled audio data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Combining Static Word Embeddings and Contextual Representations for Bilingual Lexicon Induction. (arXiv:2106.03084v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jinpeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1">Baijun Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1">Nini Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1">Xiangyu Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yangbin Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1">Weihua Luo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03084">
                                    <div class="article-summary-box-inner">
                                        <span>Bilingual Lexicon Induction (BLI) aims to map words in one language to their
translations in another, and is typically through learning linear projections
to align monolingual word representation spaces. Two classes of word
representations have been explored for BLI: static word embeddings and
contextual representations, but there is no studies to combine both. In this
paper, we propose a simple yet effective mechanism to combine the static word
embeddings and the contextual representations to utilize the advantages of both
paradigms. We test the combination mechanism on various language pairs under
the supervised and unsupervised BLI benchmark settings. Experiments show that
our mechanism consistently improves performances over robust BLI baselines on
all language pairs by averagely improving 3.2 points in the supervised setting,
and 3.1 points in the unsupervised setting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech. (arXiv:2104.11462v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Evain_S/0/1/0/all/0/1">Solene Evain</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Ha Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Hang Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1">Marcely Zanon Boito</a>, <a href="http://arxiv.org/find/cs/1/au:+Mdhaffar_S/0/1/0/all/0/1">Salima Mdhaffar</a>, <a href="http://arxiv.org/find/cs/1/au:+Alisamir_S/0/1/0/all/0/1">Sina Alisamir</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1">Ziyi Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomashenko_N/0/1/0/all/0/1">Natalia Tomashenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinarelli_M/0/1/0/all/0/1">Marco Dinarelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1">Titouan Parcollet</a>, <a href="http://arxiv.org/find/cs/1/au:+Allauzen_A/0/1/0/all/0/1">Alexandre Allauzen</a>, <a href="http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1">Yannick Esteve</a>, <a href="http://arxiv.org/find/cs/1/au:+Lecouteux_B/0/1/0/all/0/1">Benjamin Lecouteux</a>, <a href="http://arxiv.org/find/cs/1/au:+Portet_F/0/1/0/all/0/1">Francois Portet</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossato_S/0/1/0/all/0/1">Solange Rossato</a>, <a href="http://arxiv.org/find/cs/1/au:+Ringeval_F/0/1/0/all/0/1">Fabien Ringeval</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1">Didier Schwab</a>, <a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1">Laurent Besacier</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11462">
                                    <div class="article-summary-box-inner">
                                        <span>Self-Supervised Learning (SSL) using huge unlabeled data has been
successfully explored for image and natural language processing. Recent works
also investigated SSL from speech. They were notably successful to improve
performance on downstream tasks such as automatic speech recognition (ASR).
While these works suggest it is possible to reduce dependence on labeled data
for building efficient speech systems, their evaluation was mostly made on ASR
and using multiple and heterogeneous experimental settings (most of them for
English). This questions the objective comparison of SSL approaches and the
evaluation of their impact on building speech systems. In this paper, we
propose LeBenchmark: a reproducible framework for assessing SSL from speech. It
not only includes ASR (high and low resource) tasks but also spoken language
understanding, speech translation and emotion recognition. We also focus on
speech technologies in a language different than English: French. SSL models of
different sizes are trained from carefully sourced and documented datasets.
Experiments show that SSL is beneficial for most but not all tasks which
confirms the need for exhaustive and reliable benchmarks to evaluate its real
impact. LeBenchmark is shared with the scientific community for reproducible
research in SSL from speech.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SA2SL: From Aspect-Based Sentiment Analysis to Social Listening System for Business Intelligence. (arXiv:2105.15079v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1">Luong Luc Phan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_P/0/1/0/all/0/1">Phuc Huynh Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1">Kim Thi-Thanh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tham Thi Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huynh_S/0/1/0/all/0/1">Sieu Khai Huynh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1">Luan Thanh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1">Tin Van Huynh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1">Kiet Van Nguyen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15079">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we present a process of building a social listening system
based on aspect-based sentiment analysis in Vietnamese from creating a dataset
to building a real application. Firstly, we create UIT-ViSFD, a Vietnamese
Smartphone Feedback Dataset as a new benchmark corpus built based on a strict
annotation schemes for evaluating aspect-based sentiment analysis, consisting
of 11,122 human-annotated comments for mobile e-commerce, which is freely
available for research purposes. We also present a proposed approach based on
the Bi-LSTM architecture with the fastText word embeddings for the Vietnamese
aspect based sentiment task. Our experiments show that our approach achieves
the best performances with the F1-score of 84.48% for the aspect task and
63.06% for the sentiment task, which performs several conventional machine
learning and deep learning systems. Last but not least, we build SA2SL, a
social listening system based on the best performance model on our dataset,
which will inspire more social listening systems in future.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Code Generation from Natural Language with Less Prior and More Monolingual Data. (arXiv:2101.00259v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Norouzi_S/0/1/0/all/0/1">Sajad Norouzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1">Keyi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yanshuai Cao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00259">
                                    <div class="article-summary-box-inner">
                                        <span>Training datasets for semantic parsing are typically small due to the higher
expertise required for annotation than most other NLP tasks. As a result,
models for this application usually need additional prior knowledge to be built
into the architecture or algorithm. The increased dependency on human experts
hinders automation and raises the development and maintenance costs in
practice. This work investigates whether a generic transformer-based seq2seq
model can achieve competitive performance with minimal code-generation-specific
inductive bias design. By exploiting a relatively sizeable monolingual corpus
of the target programming language, which is cheap to mine from the web, we
achieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa.
Both are SOTA to the best of our knowledge. This positive evidence highlights a
potentially easier path toward building accurate semantic parsers in practice.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal Clinical NLP. (arXiv:2011.09625v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">John Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Berlot_Attwell_I/0/1/0/all/0/1">Ian Berlot-Attwell</a>, <a href="http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1">Safwan Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xindi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1">Frank Rudzicz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09625">
                                    <div class="article-summary-box-inner">
                                        <span>Clinical machine learning is increasingly multimodal, collected in both
structured tabular formats and unstructured forms such as freetext. We propose
a novel task of exploring fairness on a multimodal clinical dataset, adopting
equalized odds for the downstream medical prediction tasks. To this end, we
investigate a modality-agnostic fairness algorithm - equalized odds post
processing - and compare it to a text-specific fairness algorithm: debiased
clinical word embeddings. Despite the fact that debiased word embeddings do not
explicitly address equalized odds of protected groups, we show that a
text-specific approach to fairness may simultaneously achieve a good balance of
performance and classical notions of fairness. We hope that our paper inspires
future contributions at the critical intersection of clinical NLP and fairness.
The full source code is available here:
https://github.com/johntiger1/multimodal_fairness</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">NeurST: Neural Speech Translation Toolkit. (arXiv:2012.10018v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chengqi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mingxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10018">
                                    <div class="article-summary-box-inner">
                                        <span>NeurST is an open-source toolkit for neural speech translation. The toolkit
mainly focuses on end-to-end speech translation, which is easy to use, modify,
and extend to advanced speech translation research and products. NeurST aims at
facilitating the speech translation research for NLP researchers and building
reliable benchmarks for this field. It provides step-by-step recipes for
feature extraction, data preprocessing, distributed training, and evaluation.
In this paper, we will introduce the framework design of NeurST and show
experimental results for different benchmark datasets, which can be regarded as
reliable baselines for future research. The toolkit is publicly available at
https://github.com/bytedance/neurst/ and we will continuously update the
performance of NeurST with other counterparts and studies at
https://st-benchmark.github.io/.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Cognitive Regularizer for Language Modeling. (arXiv:2105.07144v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jason Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1">Clara Meister</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07144">
                                    <div class="article-summary-box-inner">
                                        <span>The uniform information density (UID) hypothesis, which posits that speakers
behaving optimally tend to distribute information uniformly across a linguistic
signal, has gained traction in psycholinguistics as an explanation for certain
syntactic, morphological, and prosodic choices. In this work, we explore
whether the UID hypothesis can be operationalized as an inductive bias for
statistical language modeling. Specifically, we augment the canonical MLE
objective for training language models with a regularizer that encodes UID. In
experiments on ten languages spanning five language families, we find that
using UID regularization consistently improves perplexity in language models,
having a larger effect when training data is limited. Moreover, via an analysis
of generated sequences, we find that UID-regularized language models have other
desirable properties, e.g., they generate text that is more lexically diverse.
Our results not only suggest that UID is a reasonable inductive bias for
language modeling, but also provide an alternative validation of the UID
hypothesis using modern-day NLP tools.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On-the-Fly Aligned Data Augmentation for Sequence-to-Sequence ASR. (arXiv:2104.01393v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1">Tsz Kin Lam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohta_M/0/1/0/all/0/1">Mayumi Ohta</a>, <a href="http://arxiv.org/find/cs/1/au:+Schamoni_S/0/1/0/all/0/1">Shigehiko Schamoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1">Stefan Riezler</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01393">
                                    <div class="article-summary-box-inner">
                                        <span>We propose an on-the-fly data augmentation method for automatic speech
recognition (ASR) that uses alignment information to generate effective
training samples. Our method, called Aligned Data Augmentation (ADA) for ASR,
replaces transcribed tokens and the speech representations in an aligned manner
to generate previously unseen training pairs. The speech representations are
sampled from an audio dictionary that has been extracted from the training
corpus and inject speaker variations into the training examples. The
transcribed tokens are either predicted by a language model such that the
augmented data pairs are semantically close to the original data, or randomly
sampled. Both strategies result in training pairs that improve robustness in
ASR training. Our experiments on a Seq-to-Seq architecture show that ADA can be
applied on top of SpecAugment, and achieves about 9-23% and 4-15% relative
improvements in WER over SpecAugment alone on LibriSpeech 100h and LibriSpeech
960h test datasets, respectively.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">KARI: KAnari/QCRI&#x27;s End-to-End systems for the INTERSPEECH 2021 Indian Languages Code-Switching Challenge. (arXiv:2106.05885v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1">Amir Hussein</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1">Shammur Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1">Ahmed Ali</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05885">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we present the Kanari/QCRI (KARI) system and the modeling
strategies used to participate in the Interspeech 2021 Code-switching (CS)
challenge for low-resource Indian languages. The subtask involved developing a
speech recognition system for two CS datasets: Hindi-English and
Bengali-English, collected in a real-life scenario. To tackle the CS
challenges, we use transfer learning for incorporating the publicly available
monolingual Hindi, Bengali, and English speech data. In this work, we study the
effectiveness of two steps transfer learning protocol for low-resourced CS
data: monolingual pretraining, followed by fine-tuning. For acoustic modeling,
we develop an end-to-end convolution-augmented transformer (Conformer). We show
that selecting the percentage of each monolingual data affects model biases
towards using one language character set over the other in a CS scenario. The
models pretrained on well-aligned and accurate monolingual data showed
robustness against misalignment between the segments and the transcription.
Finally, we develop word-level n-gram language models (LM) to rescore ASR
recognition.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for Multimodal Hate. (arXiv:2106.05903v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Botelho_A/0/1/0/all/0/1">Austin Botelho</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1">Bertie Vidgen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1">Scott A. Hale</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05903">
                                    <div class="article-summary-box-inner">
                                        <span>Accurate detection and classification of online hate is a difficult task.
Implicit hate is particularly challenging as such content tends to have unusual
syntax, polysemic words, and fewer markers of prejudice (e.g., slurs). This
problem is heightened with multimodal content, such as memes (combinations of
text and images), as they are often harder to decipher than unimodal content
(e.g., text alone). This paper evaluates the role of semantic and multimodal
context for detecting implicit and explicit hate. We show that both text- and
visual- enrichment improves model performance, with the multimodal model
(0.771) outperforming other models&#x27; F1 scores (0.544, 0.737, and 0.754). While
the unimodal-text context-aware (transformer) model was the most accurate on
the subtask of implicit hate detection, the multimodal model outperformed it
overall because of a lower propensity towards false positives. We find that all
models perform better on content with full annotator agreement and that
multimodal models are best at classifying the content where annotators
disagree. To conduct these investigations, we undertook high-quality annotation
of a sample of 5,000 multimodal entries. Tweets were annotated for primary
category, modality, and strategy. We make this corpus, along with the codebook,
code, and final model, freely available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights. (arXiv:2106.05852v1 [eess.AS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Adiga_D/0/1/0/all/0/1">Devaraja Adiga</a>, <a href="http://arxiv.org/find/eess/1/au:+Kumar_R/0/1/0/all/0/1">Rishabh Kumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Krishna_A/0/1/0/all/0/1">Amrith Krishna</a>, <a href="http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1">Preethi Jyothi</a>, <a href="http://arxiv.org/find/eess/1/au:+Ramakrishnan_G/0/1/0/all/0/1">Ganesh Ramakrishnan</a>, <a href="http://arxiv.org/find/eess/1/au:+Goyal_P/0/1/0/all/0/1">Pawan Goyal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05852">
                                    <div class="article-summary-box-inner">
                                        <span>Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the
various linguistic peculiarities present in the language. The Sanskrit language
is lexically productive, undergoes euphonic assimilation of phones at the word
boundaries and exhibits variations in spelling conventions and in
pronunciations. In this work, we propose the first large scale study of
automatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact
of unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR
dataset for Sanskrit, which faithfully captures several of the linguistic
characteristics expressed by the language. We investigate the role of different
acoustic model and language model units in ASR systems for Sanskrit. We also
propose a new modelling unit, inspired by the syllable level unit selection,
that captures character sequences from one vowel in the word to the next vowel.
We also highlight the importance of choosing graphemic representations for
Sanskrit and show the impact of this choice on word error rates (WER). Finally,
we extend these insights from Sanskrit ASR for building ASR systems in two
other Indic languages, Gujarati and Telugu. For both these languages, our
experimental results show that the use of phonetic based graphemic
representations in ASR results in performance improvements as compared to ASR
systems that use native scripts.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation. (arXiv:2106.05970v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wanrong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Eric Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1">An Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1">Miguel Eckstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05970">
                                    <div class="article-summary-box-inner">
                                        <span>Automatic evaluations for natural language generation (NLG) conventionally
rely on token-level or embedding-level comparisons with the text references.
This is different from human language processing, for which visual imaginations
often improve comprehension. In this work, we propose ImaginE, an
imagination-based automatic evaluation metric for natural language generation.
With the help of CLIP and DALL-E, two cross-modal models pre-trained on
large-scale image-text pairs, we automatically generate an image as the
embodied imagination for the text snippet and compute the imagination
similarity using contextual embeddings. Experiments spanning several text
generation tasks demonstrate that adding imagination with our ImaginE displays
great potential in introducing multi-modal information into NLG evaluation, and
improves existing automatic metrics&#x27; correlations with human similarity
judgments in many circumstances.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Parallel Deep Learning-Driven Sarcasm Detection from Pop Culture Text and English Humor Literature. (arXiv:2106.05752v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Sourav Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolya_A/0/1/0/all/0/1">Anup Kumar Kolya</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05752">
                                    <div class="article-summary-box-inner">
                                        <span>Sarcasm is a sophisticated way of wrapping any immanent truth, mes-sage, or
even mockery within a hilarious manner. The advent of communications using
social networks has mass-produced new avenues of socialization. It can be
further said that humor, irony, sarcasm, and wit are the four chariots of being
socially funny in the modern days. In this paper, we manually extract the
sarcastic word distribution features of a benchmark pop culture sarcasm corpus,
containing sarcastic dialogues and monologues. We generate input sequences
formed of the weighted vectors from such words. We further propose an
amalgamation of four parallel deep long-short term networks (pLSTM), each with
distinctive activation classifier. These modules are primarily aimed at
successfully detecting sarcasm from the text corpus. Our proposed model for
detecting sarcasm peaks a training accuracy of 98.95% when trained with the
discussed dataset. Consecutively, it obtains the highest of 98.31% overall
validation accuracy on two handpicked Project Gutenberg English humor
literature among all the test cases. Our approach transcends previous
state-of-the-art works on several sarcasm corpora and results in a new gold
standard performance for sarcasm detection.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Synthesizing Adversarial Negative Responses for Robust Response Ranking and Evaluation. (arXiv:2106.05894v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1">Prakhar Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1">Yulia Tsvetkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1">Jeffrey P. Bigham</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05894">
                                    <div class="article-summary-box-inner">
                                        <span>Open-domain neural dialogue models have achieved high performance in response
ranking and evaluation tasks. These tasks are formulated as a binary
classification of responses given in a dialogue context, and models generally
learn to make predictions based on context-response content similarity.
However, over-reliance on content similarity makes the models less sensitive to
the presence of inconsistencies, incorrect time expressions and other factors
important for response appropriateness and coherence. We propose approaches for
automatically creating adversarial negative training data to help ranking and
evaluation models learn features beyond content similarity. We propose
mask-and-fill and keyword-guided approaches that generate negative examples for
training more robust dialogue systems. These generated adversarial responses
have high content similarity with the contexts but are either incoherent,
inappropriate or not fluent. Our approaches are fully data-driven and can be
easily incorporated in existing models and datasets. Experiments on
classification, ranking and evaluation tasks across multiple datasets
demonstrate that our approaches outperform strong baselines in providing
informative negative examples for training dialogue systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation. (arXiv:2106.05691v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuanxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05691">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, knowledge distillation (KD) has shown great success in BERT
compression. Instead of only learning from the teacher&#x27;s soft label as in
conventional KD, researchers find that the rich information contained in the
hidden layers of BERT is conducive to the student&#x27;s performance. To better
exploit the hidden knowledge, a common practice is to force the student to
deeply mimic the teacher&#x27;s hidden states of all the tokens in a layer-wise
manner. In this paper, however, we observe that although distilling the
teacher&#x27;s hidden state knowledge (HSK) is helpful, the performance gain
(marginal utility) diminishes quickly as more HSK is distilled. To understand
this effect, we conduct a series of analysis. Specifically, we divide the HSK
of BERT into three dimensions, namely depth, length and width. We first
investigate a variety of strategies to extract crucial knowledge for each
single dimension and then jointly compress the three dimensions. In this way,
we show that 1) the student&#x27;s performance can be improved by extracting and
distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the
same performance as extensive HSK distillation. Based on the second finding, we
further propose an efficient KD paradigm to compress BERT, which does not
require loading the teacher during the training of student. For two kinds of
student models and computing devices, the proposed KD paradigm gives rise to
training speedup of 2.7x ~ 3.4x.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition. (arXiv:2106.05933v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1">Cheng-I Jeff Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Alexander H. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1">Shiyu Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Yi-Lun Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1">Yung-Sung Chuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1">Kaizhi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1">Sameer Khurana</a>, <a href="http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1">David Cox</a>, <a href="http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1">James Glass</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05933">
                                    <div class="article-summary-box-inner">
                                        <span>Recent work on speech self-supervised learning (speech SSL) demonstrated the
benefits of scale in learning rich and transferable representations for
Automatic Speech Recognition (ASR) with limited parallel data. It is then
natural to investigate the existence of sparse and transferrable subnetworks in
pre-trained speech SSL models that can achieve even better low-resource ASR
performance. However, directly applying widely adopted pruning methods such as
the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost
needed. Moreover, contrary to what LTH predicts, the discovered subnetworks
yield minimal performance gain compared to the original dense network. In this
work, we propose Prune-Adjust- Re-Prune (PARP), which discovers and finetunes
subnetworks for much better ASR performance, while only requiring a single
downstream finetuning run. PARP is inspired by our surprising observation that
subnetworks pruned for pre-training tasks only needed to be slightly adjusted
to achieve a sizeable performance boost in downstream ASR tasks. Extensive
experiments on low-resource English and multi-lingual ASR show (1) sparse
subnetworks exist in pre-trained speech SSL, and (2) the computational
advantage and performance gain of PARP over baseline pruning methods. On the
10min Librispeech split without LM decoding, PARP discovers subnetworks from
wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full
model. We demonstrate PARP mitigates performance degradation in cross-lingual
mask transfer, and investigate the possibility of discovering a single
subnetwork for 10 spoken languages in one run.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">VT-SSum: A Benchmark Dataset for Video Transcript Segmentation and Summarization. (arXiv:2106.05606v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1">Tengchao Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1">Lei Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasilijevic_M/0/1/0/all/0/1">Momcilo Vasilijevic</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05606">
                                    <div class="article-summary-box-inner">
                                        <span>Video transcript summarization is a fundamental task for video understanding.
Conventional approaches for transcript summarization are usually built upon the
summarization data for written language such as news articles, while the domain
discrepancy may degrade the model performance on spoken text. In this paper, we
present VT-SSum, a benchmark dataset with spoken language for video transcript
segmentation and summarization, which includes 125K transcript-summary pairs
from 9,616 videos. VT-SSum takes advantage of the videos from VideoLectures.NET
by leveraging the slides content as the weak supervision to generate the
extractive summary for video transcripts. Experiments with a state-of-the-art
deep learning approach show that the model trained with VT-SSum brings a
significant improvement on the AMI spoken text summarization benchmark. VT-SSum
will be publicly available to support the future research of video transcript
segmentation and summarization tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Template-guided Hybrid Pointer Network for Knowledge-basedTask-oriented Dialogue Systems. (arXiv:2106.05830v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dingmin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziyao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1">Wanwei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1">Li Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1">Yunzhe Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Min Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05830">
                                    <div class="article-summary-box-inner">
                                        <span>Most existing neural network based task-oriented dialogue systems follow
encoder-decoder paradigm, where the decoder purely depends on the source texts
to generate a sequence of words, usually suffering from instability and poor
readability. Inspired by the traditional template-based generation approaches,
we propose a template-guided hybrid pointer network for the knowledge-based
task-oriented dialogue system, which retrieves several potentially relevant
answers from a pre-constructed domain-specific conversational repository as
guidance answers, and incorporates the guidance answers into both the encoding
and decoding processes. Specifically, we design a memory pointer network model
with a gating mechanism to fully exploit the semantic correlation between the
retrieved answers and the ground-truth response. We evaluate our model on four
widely used task-oriented datasets, including one simulated and three manually
created datasets. The experimental results demonstrate that the proposed model
achieves significantly better performance than the state-of-the-art methods
over different automatic evaluation metrics.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training. (arXiv:2106.05630v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1">Mingliang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_Z/0/1/0/all/0/1">Zeqian Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1">Tao Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tie-Yan Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05630">
                                    <div class="article-summary-box-inner">
                                        <span>Symbolic music understanding, which refers to the understanding of music from
the symbolic data (e.g., MIDI format, but not audio), covers many music
applications such as genre classification, emotion classification, and music
pieces matching. While good music representations are beneficial for these
applications, the lack of training data hinders representation learning.
Inspired by the success of pre-training models in natural language processing,
in this paper, we develop MusicBERT, a large-scale pre-trained model for music
understanding. To this end, we construct a large-scale symbolic music corpus
that contains more than 1 million music songs. Since symbolic music contains
more structural (e.g., bar, position) and diverse information (e.g., tempo,
instrument, and pitch), simply adopting the pre-training techniques from NLP to
symbolic music only brings marginal gains. Therefore, we design several
mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to
enhance pre-training with symbolic music data. Experiments demonstrate the
advantages of MusicBERT on four music understanding tasks, including melody
completion, accompaniment suggestion, genre classification, and style
classification. Ablation studies also verify the effectiveness of our designs
of OctupleMIDI encoding and bar-level masking strategy in MusicBERT.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploring Unsupervised Pretraining Objectives for Machine Translation. (arXiv:2106.05634v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Baziotis_C/0/1/0/all/0/1">Christos Baziotis</a>, <a href="http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1">Ivan Titov</a>, <a href="http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1">Alexandra Birch</a>, <a href="http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1">Barry Haddow</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05634">
                                    <div class="article-summary-box-inner">
                                        <span>Unsupervised cross-lingual pretraining has achieved strong results in neural
machine translation (NMT), by drastically reducing the need for large parallel
data. Most approaches adapt masked-language modeling (MLM) to
sequence-to-sequence architectures, by masking parts of the input and
reconstructing them in the decoder. In this work, we systematically compare
masking with alternative objectives that produce inputs resembling real (full)
sentences, by reordering and replacing words based on their context. We
pretrain models with different methods on English$\leftrightarrow$German,
English$\leftrightarrow$Nepali and English$\leftrightarrow$Sinhala monolingual
data, and evaluate them on NMT. In (semi-) supervised NMT, varying the
pretraining objective leads to surprisingly small differences in the finetuned
performance, whereas unsupervised NMT is much more sensitive to it. To
understand these results, we thoroughly study the pretrained models using a
series of probes and verify that they encode and use information in different
ways. We conclude that finetuning on parallel data is mostly sensitive to few
properties that are shared by most models, such as a strong decoder, in
contrast to unsupervised NMT that also requires models with strong
cross-lingual abilities.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Text Classification and StackedHeterogeneous Embeddings for Named Entity Recognition in SMM4H 2021. (arXiv:2106.05823v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yaseen_U/0/1/0/all/0/1">Usama Yaseen</a>, <a href="http://arxiv.org/find/cs/1/au:+Langer_S/0/1/0/all/0/1">Stefan Langer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05823">
                                    <div class="article-summary-box-inner">
                                        <span>This paper presents our findings from participating in the SMM4H Shared Task
2021. We addressed Named Entity Recognition (NER) and Text Classification. To
address NER we explored BiLSTM-CRF with Stacked Heterogeneous Embeddings and
linguistic features. We investigated various machine learning algorithms
(logistic regression, Support Vector Machine (SVM) and Neural Networks) to
address text classification. Our proposed approaches can be generalized to
different languages and we have shown its effectiveness for English and
Spanish. Our text classification submissions (team:MIC-NLP) have achieved
competitive performance with F1-score of $0.46$ and $0.90$ on ADE
Classification (Task 1a) and Profession Classification (Task 7a) respectively.
In the case of NER, our submissions scored F1-score of $0.50$ and $0.82$ on ADE
Span Detection (Task 1b) and Profession Span detection (Task 7b) respectively.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals. (arXiv:2106.05544v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1">Yuqi Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1">Deyi Xiong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05544">
                                    <div class="article-summary-box-inner">
                                        <span>Most previous studies integrate cognitive language processing signals (e.g.,
eye-tracking or EEG data) into neural models of natural language processing
(NLP) just by directly concatenating word embeddings with cognitive features,
ignoring the gap between the two modalities (i.e., textual vs. cognitive) and
noise in cognitive features. In this paper, we propose a CogAlign approach to
these issues, which learns to align textual neural representations to cognitive
features. In CogAlign, we use a shared encoder equipped with a modality
discriminator to alternatively encode textual and cognitive inputs to capture
their differences and commonalities. Additionally, a text-aware attention
mechanism is proposed to detect task-related information and to avoid using
noise in cognitive features. Experimental results on three NLP tasks, namely
named entity recognition, sentiment analysis and relation extraction, show that
CogAlign achieves significant improvements with multiple cognitive features
over state-of-the-art models on public datasets. Moreover, our model is able to
transfer cognitive information to other datasets that do not have any cognitive
processing signals.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving multi-speaker TTS prosody variance with a residual encoder and normalizing flows. (arXiv:2106.05762v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Valles_Perez_I/0/1/0/all/0/1">Iv&#xe1;n Vall&#xe9;s-P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_J/0/1/0/all/0/1">Julian Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Beringer_G/0/1/0/all/0/1">Grzegorz Beringer</a>, <a href="http://arxiv.org/find/cs/1/au:+Barra_Chicote_R/0/1/0/all/0/1">Roberto Barra-Chicote</a>, <a href="http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1">Jasha Droppo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05762">
                                    <div class="article-summary-box-inner">
                                        <span>Text-to-speech systems recently achieved almost indistinguishable quality
from human speech. However, the prosody of those systems is generally flatter
than natural speech, producing samples with low expressiveness. Disentanglement
of speaker id and prosody is crucial in text-to-speech systems to improve on
naturalness and produce more variable syntheses. This paper proposes a new
neural text-to-speech model that approaches the disentanglement problem by
conditioning a Tacotron2-like architecture on flow-normalized speaker
embeddings, and by substituting the reference encoder with a new learned latent
distribution responsible for modeling the intra-sentence variability due to the
prosody. By removing the reference encoder dependency, the speaker-leakage
problem typically happening in this kind of systems disappears, producing more
distinctive syntheses at inference time. The new model achieves significantly
higher prosody variance than the baseline in a set of quantitative prosody
features, as well as higher speaker distinctiveness, without decreasing the
speaker intelligibility. Finally, we observe that the normalized speaker
embeddings enable much richer speaker interpolations, substantially improving
the distinctiveness of the new interpolated speakers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">U2++: Unified Two-pass Bidirectional End-to-end Model for Speech Recognition. (arXiv:2106.05642v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Di Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Binbin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhendong Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1">Wenjing Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1">Xin Lei</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05642">
                                    <div class="article-summary-box-inner">
                                        <span>The unified streaming and non-streaming two-pass (U2) end-to-end model for
speech recognition has shown great performance in terms of streaming
capability, accuracy, real-time factor (RTF), and latency. In this paper, we
present U2++, an enhanced version of U2 to further improve the accuracy. The
core idea of U2++ is to use the forward and the backward information of the
labeling sequences at the same time at training to learn richer information,
and combine the forward and backward prediction at decoding to give more
accurate recognition results. We also proposed a new data augmentation method
called SpecSub to help the U2++ model to be more accurate and robust. Our
experiments show that, compared with U2, U2++ shows faster convergence at
training, better robustness to the decoding method, as well as consistent 5\% -
8\% word error rate reduction gain over U2. On the experiment of AISHELL-1, we
achieve a 4.63\% character error rate (CER) with a non-streaming setup and
5.05\% with a streaming setup with 320ms latency by U2++. To the best of our
knowledge, 5.05\% is the best-published streaming result on the AISHELL-1 test
set.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Automatic Construction of Context-Aware Sentiment Lexicon in the Financial Domain Using Direction-Dependent Words. (arXiv:2106.05723v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jihye Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hye Jin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Sungzoon Cho</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05723">
                                    <div class="article-summary-box-inner">
                                        <span>Increasing attention has been drawn to the sentiment analysis of financial
documents. The most popular examples of such documents include analyst reports
and economic news, the analysis of which is frequently used to capture the
trends in market sentiments. On the other hand, the significance of the role
sentiment analysis plays in the financial domain has given rise to the efforts
to construct a financial domain-specific sentiment lexicon. Sentiment lexicons
lend a hand for solving various text mining tasks, such as unsupervised
classification of text data, while alleviating the arduous human labor required
for manual labeling. One of the challenges in the construction of an effective
sentiment lexicon is that the semantic orientation of a word may change
depending on the context in which it appears. For instance, the word &#x60;&#x60;profit&quot;
usually conveys positive sentiments; however, when the word is juxtaposed with
another word &#x60;&#x60;decrease,&quot; the sentiment associated with the phrase &#x60;&#x60;profit
decreases&quot; now becomes negative. Hence, the sentiment of a given word may shift
as one begins to consider the context surrounding the word. In this paper, we
address this issue by incorporating context when building sentiment lexicon
from a given corpus. Specifically, we construct a lexicon named Senti-DD for
the Sentiment lexicon composed of Direction-Dependent words, which expresses
each term a pair of a directional word and a direction-dependent word.
Experiment results show that higher classification performance is achieved with
Senti-DD, proving the effectiveness of our method for automatically
constructing a context-aware sentiment lexicon in the financial domain.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Shades of BLEU, Flavours of Success: The Case of MultiWOZ. (arXiv:2106.05555v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nekvinda_T/0/1/0/all/0/1">Tom&#xe1;&#x161; Nekvinda</a>, <a href="http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1">Ond&#x159;ej Du&#x161;ek</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05555">
                                    <div class="article-summary-box-inner">
                                        <span>The MultiWOZ dataset (Budzianowski et al.,2018) is frequently used for
benchmarking context-to-response abilities of task-oriented dialogue systems.
In this work, we identify inconsistencies in data preprocessing and reporting
of three corpus-based metrics used on this dataset, i.e., BLEU score and Inform
&amp; Success rates. We point out a few problems of the MultiWOZ benchmark such as
unsatisfactory preprocessing, insufficient or under-specified evaluation
metrics, or rigid database. We re-evaluate 7 end-to-end and 6 policy
optimization models in as-fair-as-possible setups, and we show that their
reported scores cannot be directly compared. To facilitate comparison of future
systems, we release our stand-alone standardized evaluation scripts. We also
give basic recommendations for corpus-based benchmarking in future works.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Input Augmentation Improves Constrained Beam Search for Neural Machine Translation: NTT at WAT 2021. (arXiv:2106.05450v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chousa_K/0/1/0/all/0/1">Katsuki Chousa</a>, <a href="http://arxiv.org/find/cs/1/au:+Morishita_M/0/1/0/all/0/1">Makoto Morishita</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05450">
                                    <div class="article-summary-box-inner">
                                        <span>This paper describes our systems that were submitted to the restricted
translation task at WAT 2021. In this task, the systems are required to output
translated sentences that contain all given word constraints. Our system
combined input augmentation and constrained beam search algorithms. Through
experiments, we found that this combination significantly improves translation
accuracy and can save inference time while containing all the constraints in
the output. For both En-&gt;Ja and Ja-&gt;En, our systems obtained the best
evaluation performances in automatic evaluation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models. (arXiv:2106.05505v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1">Tyler A. Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Weijian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhuowen Tu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05505">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we detail the relationship between convolutions and
self-attention in natural language tasks. We show that relative position
embeddings in self-attention layers are equivalent to recently-proposed dynamic
lightweight convolutions, and we consider multiple new ways of integrating
convolutions into Transformer self-attention. Specifically, we propose
composite attention, which unites previous relative position embedding methods
under a convolutional framework. We conduct experiments by training BERT with
composite attention, finding that convolutions consistently improve performance
on multiple downstream tasks, replacing absolute position embeddings. To inform
future work, we present results comparing lightweight convolutions, dynamic
convolutions, and depthwise-separable convolutions in language model
pre-training, considering multiple injection points for convolutions in
self-attention layers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1">Richard Antonello</a>, <a href="http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1">Javier Turek</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1">Vy Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1">Alexander Huth</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05426">
                                    <div class="article-summary-box-inner">
                                        <span>How related are the representations learned by neural language models,
translation models, and language tagging tasks? We answer this question by
adapting an encoder-decoder transfer learning method from computer vision to
investigate the structure among 100 different feature spaces extracted from
hidden representations of various networks trained on language tasks. This
method reveals a low-dimensional structure where language models and
translation models smoothly interpolate between word embeddings, syntactic and
semantic tasks, and future word embeddings. We call this low-dimensional
structure a language representation embedding because it encodes the
relationships between representations needed to process language for a variety
of NLP tasks. We find that this representation embedding can predict how well
each individual feature space maps to human brain responses to natural language
stimuli recorded using fMRI. Additionally, we find that the principal dimension
of this structure can be used to create a metric which highlights the brain&#x27;s
natural language processing hierarchy. This suggests that the embedding
captures some part of the brain&#x27;s natural language representation structure.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AGGGEN: Ordering and Aggregating while Generating. (arXiv:2106.05580v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xinnuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1">Ond&#x159;ej Du&#x161;ek</a>, <a href="http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1">Verena Rieser</a>, <a href="http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1">Ioannis Konstas</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05580">
                                    <div class="article-summary-box-inner">
                                        <span>We present AGGGEN (pronounced &#x27;again&#x27;), a data-to-text model which
re-introduces two explicit sentence planning stages into neural data-to-text
systems: input ordering and input aggregation. In contrast to previous work
using sentence planning, our model is still end-to-end: AGGGEN performs
sentence planning at the same time as generating text by learning latent
alignments (via semantic facts) between input representation and target text.
Experiments on the WebNLG and E2E challenge data show that by using fact-based
alignments our approach is more interpretable, expressive, robust to noise, and
easier to control, while retaining the advantages of end-to-end systems in
terms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Grover&#x27;s Algorithm for Question Answering. (arXiv:2106.05299v1 [quant-ph])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/quant-ph/1/au:+Correia_A/0/1/0/all/0/1">A. D. Correia</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Moortgat_M/0/1/0/all/0/1">M. Moortgat</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Stoof_H/0/1/0/all/0/1">H. T. C. Stoof</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05299">
                                    <div class="article-summary-box-inner">
                                        <span>Grover&#x27;s algorithm, a well-know quantum search algorithm, allows one to find
the correct item in a database, with quadratic speedup. In this paper we adapt
Grover&#x27;s algorithm to the problem of finding a correct answer to a natural
language question in English, thus contributing to the growing field of Quantum
Natural Language Processing. Using a grammar that can be interpreted as tensor
contractions, each word is represented as a quantum state that serves as input
to the quantum circuit. We here introduce a quantum measurement to contract the
representations of words, resulting in the representation of larger text
fragments. Using this framework, a representation for the question is found
that contains all the possible answers in equal quantum superposition, and
allows for the building of an oracle that can detect a correct answer, being
agnostic to the specific question. Furthermore, we show that our construction
can deal with certain types of ambiguous phrases by keeping the various
different meanings in quantum superposition.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data augmentation to improve robustness of image captioning solutions. (arXiv:2106.05437v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bujimalla_S/0/1/0/all/0/1">Shashank Bujimalla</a>, <a href="http://arxiv.org/find/cs/1/au:+Subedar_M/0/1/0/all/0/1">Mahesh Subedar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tickoo_O/0/1/0/all/0/1">Omesh Tickoo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05437">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we study the impact of motion blur, a common quality flaw in
real world images, on a state-of-the-art two-stage image captioning solution,
and notice a degradation in solution performance as blur intensity increases.
We investigate techniques to improve the robustness of the solution to motion
blur using training data augmentation at each or both stages of the solution,
i.e., object detection and captioning, and observe improved results. In
particular, augmenting both the stages reduces the CIDEr-D degradation for high
motion blur intensity from 68.7 to 11.7 on MS COCO dataset, and from 22.4 to
6.8 on Vizwiz dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">How Robust are Model Rankings: A Leaderboard Customization Approach for Equitable Evaluation. (arXiv:2106.05532v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Swaroop Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1">Anjana Arunkumar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05532">
                                    <div class="article-summary-box-inner">
                                        <span>Models that top leaderboards often perform unsatisfactorily when deployed in
real world applications; this has necessitated rigorous and expensive
pre-deployment model testing. A hitherto unexplored facet of model performance
is: Are our leaderboards doing equitable evaluation? In this paper, we
introduce a task-agnostic method to probe leaderboards by weighting samples
based on their &#x60;difficulty&#x27; level. We find that leaderboards can be
adversarially attacked and top performing models may not always be the best
models. We subsequently propose alternate evaluation metrics. Our experiments
on 10 models show changes in model ranking and an overall reduction in
previously reported performance -- thus rectifying the overestimation of AI
systems&#x27; capabilities. Inspired by behavioral testing principles, we further
develop a prototype of a visual analytics tool that enables leaderboard
revamping through customization, based on an end user&#x27;s focus area. This helps
users analyze models&#x27; strengths and weaknesses, and guides them in the
selection of a model best suited for their application scenario. In a user
study, members of various commercial product development teams, covering 5
focus areas, find that our prototype reduces pre-deployment development and
testing effort by 41% on average.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Variational Information Bottleneck for Effective Low-Resource Fine-Tuning. (arXiv:2106.05469v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mahabadi_R/0/1/0/all/0/1">Rabeeh Karimi Mahabadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1">Yonatan Belinkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1">James Henderson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05469">
                                    <div class="article-summary-box-inner">
                                        <span>While large-scale pretrained language models have obtained impressive results
when fine-tuned on a wide variety of tasks, they still often suffer from
overfitting in low-resource scenarios. Since such models are general-purpose
feature extractors, many of these features are inevitably irrelevant for a
given target task. We propose to use Variational Information Bottleneck (VIB)
to suppress irrelevant features when fine-tuning on low-resource target tasks,
and show that our method successfully reduces overfitting. Moreover, we show
that our VIB model finds sentence representations that are more robust to
biases in natural language inference datasets, and thereby obtains better
generalization to out-of-domain datasets. Evaluation on seven low-resource
datasets in different tasks shows that our method significantly improves
transfer learning in low-resource scenarios, surpassing prior work. Moreover,
it improves generalization on 13 out of 15 out-of-domain natural language
inference benchmarks. Our code is publicly available in
https://github.com/rabeehk/vibert.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.CV"">cs.CV updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Context-Free TextSpotter for Real-Time and Mobile End-to-End Text Detection and Recognition. (arXiv:2106.05611v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yoshihashi_R/0/1/0/all/0/1">Ryota Yoshihashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1">Tomohiro Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Doi_K/0/1/0/all/0/1">Kenji Doi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fujino_T/0/1/0/all/0/1">Takumi Fujino</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamashita_N/0/1/0/all/0/1">Naoaki Yamashita</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05611">
                                    <div class="article-summary-box-inner">
                                        <span>In the deployment of scene-text spotting systems on mobile platforms,
lightweight models with low computation are preferable. In concept, end-to-end
(E2E) text spotting is suitable for such purposes because it performs text
detection and recognition in a single model. However, current state-of-the-art
E2E methods rely on heavy feature extractors, recurrent sequence modellings,
and complex shape aligners to pursue accuracy, which means their computations
are still heavy. We explore the opposite direction: How far can we go without
bells and whistles in E2E text spotting? To this end, we propose a
text-spotting method that consists of simple convolutions and a few
post-processes, named Context-Free TextSpotter. Experiments using standard
benchmarks show that Context-Free TextSpotter achieves real-time text spotting
on a GPU with only three million parameters, which is the smallest and fastest
among existing deep text spotters, with an acceptable transcription quality
degradation compared to heavier ones. Further, we demonstrate that our text
spotter can run on a smartphone with affordable latency, which is valuable for
building stand-alone OCR applications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold. (arXiv:2106.05965v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1">Kieran Murphy</a>, <a href="http://arxiv.org/find/cs/1/au:+Esteves_C/0/1/0/all/0/1">Carlos Esteves</a>, <a href="http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1">Varun Jampani</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1">Srikumar Ramalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1">Ameesh Makadia</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05965">
                                    <div class="article-summary-box-inner">
                                        <span>Single image pose estimation is a fundamental problem in many vision and
robotics tasks, and existing deep learning approaches suffer by not completely
modeling and handling: i) uncertainty about the predictions, and ii) symmetric
objects with multiple (sometimes infinite) correct poses. To this end, we
introduce a method to estimate arbitrary, non-parametric distributions on
SO(3). Our key idea is to represent the distributions implicitly, with a neural
network that estimates the probability given the input image and a candidate
pose. Grid sampling or gradient ascent can be used to find the most likely
pose, but it is also possible to evaluate the probability at any pose, enabling
reasoning about symmetries and uncertainty. This is the most general way of
representing distributions on manifolds, and to showcase the rich expressive
power, we introduce a dataset of challenging symmetric and nearly-symmetric
objects. We require no supervision on pose uncertainty -- the model trains only
with a single pose per example. Nonetheless, our implicit model is highly
expressive to handle complex distributions over 3D poses, while still obtaining
accurate pose estimation on standard non-ambiguous environments, achieving
state-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Universal Adder Neural Networks. (arXiv:2105.14202v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hanting Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yunhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chunjing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14202">
                                    <div class="article-summary-box-inner">
                                        <span>Compared with cheap addition operation, multiplication operation is of much
higher computation complexity. The widely-used convolutions in deep neural
networks are exactly cross-correlation to measure the similarity between input
feature and convolution filters, which involves massive multiplications between
float values. In this paper, we present adder networks (AdderNets) to trade
these massive multiplications in deep neural networks, especially convolutional
neural networks (CNNs), for much cheaper additions to reduce computation costs.
In AdderNets, we take the $\ell_1$-norm distance between filters and input
feature as the output response. The influence of this new similarity measure on
the optimization of neural network have been thoroughly analyzed. To achieve a
better performance, we develop a special training approach for AdderNets by
investigating the $\ell_p$-norm. We then propose an adaptive learning rate
strategy to enhance the training procedure of AdderNets according to the
magnitude of each neuron&#x27;s gradient. As a result, the proposed AdderNets can
achieve 75.7% Top-1 accuracy 92.3% Top-5 accuracy using ResNet-50 on the
ImageNet dataset without any multiplication in convolutional layer. Moreover,
we develop a theoretical foundation for AdderNets, by showing that both the
single hidden layer AdderNet and the width-bounded deep AdderNet with ReLU
activation functions are universal function approximators. These results match
those of the traditional neural networks using the more complex multiplication
units. An approximation bound for AdderNets with a single hidden layer is also
presented.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CoviLearn: A Machine Learning Integrated Smart X-Ray Device in Healthcare Cyber-Physical System for Automatic Initial Screening of COVID-19. (arXiv:2106.05861v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Das_D/0/1/0/all/0/1">Debanjan Das</a>, <a href="http://arxiv.org/find/eess/1/au:+Samal_C/0/1/0/all/0/1">Chirag Samal</a>, <a href="http://arxiv.org/find/eess/1/au:+Ukey_D/0/1/0/all/0/1">Deewanshu Ukey</a>, <a href="http://arxiv.org/find/eess/1/au:+Chowdhary_G/0/1/0/all/0/1">Gourav Chowdhary</a>, <a href="http://arxiv.org/find/eess/1/au:+Mohanty_S/0/1/0/all/0/1">Saraju P. Mohanty</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05861">
                                    <div class="article-summary-box-inner">
                                        <span>The pandemic of novel Coronavirus Disease 2019 (COVID-19) is widespread all
over the world causing serious health problems as well as serious impact on the
global economy. Reliable and fast testing of the COVID-19 has been a challenge
for researchers and healthcare practitioners. In this work we present a novel
machine learning (ML) integrated X-ray device in Healthcare Cyber-Physical
System (H-CPS) or smart healthcare framework (called CoviLearn) to allow
healthcare practitioners to perform automatic initial screening of COVID-19
patients. We propose convolutional neural network (CNN) models of X-ray images
integrated into an X-ray device for automatic COVID-19 detection. The proposed
CoviLearn device will be useful in detecting if a person is COVID-19 positive
or negative by considering the chest X-ray image of individuals. CoviLearn will
be useful tool doctors to detect potential COVID-19 infections instantaneously
without taking more intrusive healthcare data samples, such as saliva and
blood. COVID-19 attacks the endothelium tissues that support respiratory tract,
X-rays images can be used to analyze the health of a patient lungs. As all
healthcare centers have X-ray machines, it could be possible to use proposed
CoviLearn X-rays to test for COVID-19 without the especial test kits. Our
proposed automated analysis system CoviLearn which has 99% accuracy will be
able to save valuable time of medical professionals as the X-ray machines come
with a drawback as it needed a radiology expert.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame. (arXiv:2105.06340v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yap_C/0/1/0/all/0/1">Chuin Hong Yap</a>, <a href="http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1">Moi Hoon Yap</a>, <a href="http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1">Adrian K. Davison</a>, <a href="http://arxiv.org/find/cs/1/au:+Cunningham_R/0/1/0/all/0/1">Ryan Cunningham</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06340">
                                    <div class="article-summary-box-inner">
                                        <span>Facial expression spotting is the preliminary step for micro- and
macro-expression analysis. The task of reliably spotting such expressions in
video sequences is currently unsolved. The current best systems depend upon
optical flow methods to extract regional motion features, before categorisation
of that motion into a specific class of facial movement. Optical flow is
susceptible to drift error, which introduces a serious problem for motions with
long-term dependencies, such as high frame-rate macro-expression. We propose a
purely deep learning solution which, rather than track frame differential
motion, compares via a convolutional model, each frame with two temporally
local reference frames. Reference frames are sampled according to calculated
micro- and macro-expression durations. We show that our solution achieves
state-of-the-art performance (F1-score of 0.126) in a dataset of high
frame-rate (200 fps) long video sequences (SAMM-LV) and is competitive in a low
frame-rate (30 fps) dataset (CAS(ME)2). In this paper, we document our deep
learning model and parameters, including how we use local contrast
normalisation, which we show is critical for optimal results. We surpass a
limitation in existing methods, and advance the state of deep learning in the
domain of facial expression spotting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning ordered pooling weights in image classification. (arXiv:2007.01243v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Forcen_J/0/1/0/all/0/1">J.I.Forcen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagola_M/0/1/0/all/0/1">Miguel Pagola</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrenechea_E/0/1/0/all/0/1">Edurne Barrenechea</a>, <a href="http://arxiv.org/find/cs/1/au:+Bustince_H/0/1/0/all/0/1">Humberto Bustince</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01243">
                                    <div class="article-summary-box-inner">
                                        <span>Spatial pooling is an important step in computer vision systems like
Convolutional Neural Networks or the Bag-of-Words method. The spatial pooling
purpose is to combine neighbouring descriptors to obtain a single descriptor
for a given region (local or global). The resultant combined vector must be as
discriminant as possible, in other words, must contain relevant information,
while removing irrelevant and confusing details. Maximum and average are the
most common aggregation functions used in the pooling step. To improve the
aggregation of relevant information without degrading their discriminative
power for image classification, we introduce a simple but effective scheme
based on Ordered Weighted Average (OWA) aggregation operators. We present a
method to learn the weights of the OWA aggregation operator in a Bag-of-Words
framework and in Convolutional Neural Networks, and provide an extensive
evaluation showing that OWA based pooling outperforms classical aggregation
operators.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-resolution Outlier Pooling for Sorghum Classification. (arXiv:2106.05748v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1">Chao Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Dulay_J/0/1/0/all/0/1">Justin Dulay</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolwes_G/0/1/0/all/0/1">Gregory Rolwes</a>, <a href="http://arxiv.org/find/cs/1/au:+Pauli_D/0/1/0/all/0/1">Duke Pauli</a>, <a href="http://arxiv.org/find/cs/1/au:+Shakoor_N/0/1/0/all/0/1">Nadia Shakoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1">Abby Stylianou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05748">
                                    <div class="article-summary-box-inner">
                                        <span>Automated high throughput plant phenotyping involves leveraging sensors, such
as RGB, thermal and hyperspectral cameras (among others), to make large scale
and rapid measurements of the physical properties of plants for the purpose of
better understanding the difference between crops and facilitating rapid plant
breeding programs. One of the most basic phenotyping tasks is to determine the
cultivar, or species, in a particular sensor product. This simple phenotype can
be used to detect errors in planting and to learn the most differentiating
features between cultivars. It is also a challenging visual recognition task,
as a large number of highly related crops are grown simultaneously, leading to
a classification problem with low inter-class variance. In this paper, we
introduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum
captured by a state-of-the-art gantry system, a multi-resolution network
architecture that learns both global and fine-grained features on the crops,
and a new global pooling strategy called Dynamic Outlier Pooling which
outperforms standard global pooling strategies on this task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dataset Condensation with Differentiable Siamese Augmentation. (arXiv:2102.08259v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bo Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1">Hakan Bilen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08259">
                                    <div class="article-summary-box-inner">
                                        <span>In many machine learning problems, large-scale datasets have become the
de-facto standard to train state-of-the-art deep networks at the price of heavy
computation load. In this paper, we focus on condensing large training sets
into significantly smaller synthetic sets which can be used to train deep
neural networks from scratch with minimum drop in performance. Inspired from
the recent training set synthesis methods, we propose Differentiable Siamese
Augmentation that enables effective use of data augmentation to synthesize more
informative synthetic images and thus achieves better performance when training
networks with augmentations. Experiments on multiple image classification
benchmarks demonstrate that the proposed method obtains substantial gains over
the state-of-the-art, 7% improvements on CIFAR10 and CIFAR100 datasets. We show
with only less than 1% data that our method achieves 99.6%, 94.9%, 88.5%, 71.5%
relative performance on MNIST, FashionMNIST, SVHN, CIFAR10 respectively. We
also explore the use of our method in continual learning and neural
architecture search, and show promising results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Low-Light Image and Video Enhancement Using Deep Learning: A Survey. (arXiv:2104.10729v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chongyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Chunle Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Linghao Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Ming-Ming Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinwei Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1">Chen Change Loy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10729">
                                    <div class="article-summary-box-inner">
                                        <span>Low-light image enhancement (LLIE) aims at improving the perception or
interpretability of an image captured in an environment with poor illumination.
Recent advances in this area are dominated by deep learning-based solutions,
where many learning strategies, network structures, loss functions, training
data, etc. have been employed. In this paper, we provide a comprehensive survey
to cover various aspects ranging from algorithm taxonomy to unsolved open
issues. To examine the generalization of existing methods, we propose a
large-scale low-light image and video dataset, in which the images and videos
are taken by different mobile phones&#x27; cameras under diverse illumination
conditions. Besides, for the first time, we provide a unified online platform
that covers many popular LLIE methods, of which the results can be produced
through a user-friendly web interface. In addition to qualitative and
quantitative evaluation of existing methods on publicly available and our
proposed datasets, we also validate their performance in face detection in the
dark. This survey together with the proposed dataset and online platform could
serve as a reference source for future study and promote the development of
this research field. The proposed platform and the collected methods, datasets,
and evaluation metrics are publicly available and will be regularly updated at
https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open.
Our low-light image and video dataset is also available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">To The Point: Correspondence-driven monocular 3D category reconstruction. (arXiv:2106.05662v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kokkinos_F/0/1/0/all/0/1">Filippos Kokkinos</a>, <a href="http://arxiv.org/find/cs/1/au:+Kokkinos_I/0/1/0/all/0/1">Iasonas Kokkinos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05662">
                                    <div class="article-summary-box-inner">
                                        <span>We present To The Point (TTP), a method for reconstructing 3D objects from a
single image using 2D to 3D correspondences learned from weak supervision. We
recover a 3D shape from a 2D image by first regressing the 2D positions
corresponding to the 3D template vertices and then jointly estimating a rigid
camera transform and non-rigid template deformation that optimally explain the
2D positions through the 3D shape projection. By relying on 3D-2D
correspondences we use a simple per-sample optimization problem to replace
CNN-based regression of camera pose and non-rigid deformation and thereby
obtain substantially more accurate 3D reconstructions. We treat this
optimization as a differentiable layer and train the whole system in an
end-to-end manner. We report systematic quantitative improvements on multiple
categories and provide qualitative results comprising diverse shape, pose and
texture prediction examples. Project website:
https://fkokkinos.github.io/to_the_point/.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Co-occurrence of deep convolutional features for image search. (arXiv:2003.13827v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Forcen_J/0/1/0/all/0/1">J.I.Forcen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagola_M/0/1/0/all/0/1">Miguel Pagola</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrenechea_E/0/1/0/all/0/1">Edurne Barrenechea</a>, <a href="http://arxiv.org/find/cs/1/au:+Bustince_H/0/1/0/all/0/1">Humberto Bustince</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.13827">
                                    <div class="article-summary-box-inner">
                                        <span>Image search can be tackled using deep features from pre-trained
Convolutional Neural Networks (CNN). The feature map from the last
convolutional layer of a CNN encodes descriptive information from which a
discriminative global descriptor can be obtained. We propose a new
representation of co-occurrences from deep convolutional features to extract
additional relevant information from this last convolutional layer. Combining
this co-occurrence map with the feature map, we achieve an improved image
representation. We present two different methods to get the co-occurrence
representation, the first one based on direct aggregation of activations, and
the second one, based on a trainable co-occurrence representation. The image
descriptors derived from our methodology improve the performance in very
well-known image retrieval datasets as we prove in the experiments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Using Persistent Homology Topological Features to Characterize Medical Images: Case Studies on Lung and Brain Cancers. (arXiv:2012.12102v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Moon_C/0/1/0/all/0/1">Chul Moon</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1">Guanghua Xiao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.12102">
                                    <div class="article-summary-box-inner">
                                        <span>Tumor shape is a key factor that affects tumor growth and metastasis. This
paper proposes a topological feature computed by persistent homology to
characterize tumor progression from digital pathology and radiology images and
examines its effect on the time-to-event data. The proposed topological
features are invariant to scale-preserving transformation and can summarize
various tumor shape patterns. The topological features are represented in
functional space and used as functional predictors in a functional Cox
proportional hazards model. The proposed model enables interpretable inference
about the association between topological shape features and survival risks.
Two case studies are conducted using consecutive 143 lung cancer and 77 brain
tumor patients. The results of both studies show that the topological features
predict survival prognosis after adjusting clinical variables, and the
predicted high-risk groups have significantly (at the level of 0.01) worse
survival outcomes than the low-risk groups. Also, the topological shape
features found to be positively associated with survival hazards are irregular
and heterogeneous shape patterns, which are known to be related to tumor
progression.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Consistent Instance False Positive Improves Fairness in Face Recognition. (arXiv:2106.05519v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xingkun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuge Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1">Pengcheng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shaoxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jilin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Feiyue Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1">Zhen Cui</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05519">
                                    <div class="article-summary-box-inner">
                                        <span>Demographic bias is a significant challenge in practical face recognition
systems. Existing methods heavily rely on accurate demographic annotations.
However, such annotations are usually unavailable in real scenarios. Moreover,
these methods are typically designed for a specific demographic group and are
not general enough. In this paper, we propose a false positive rate penalty
loss, which mitigates face recognition bias by increasing the consistency of
instance False Positive Rate (FPR). Specifically, we first define the instance
FPR as the ratio between the number of the non-target similarities above a
unified threshold and the total number of the non-target similarities. The
unified threshold is estimated for a given total FPR. Then, an additional
penalty term, which is in proportion to the ratio of instance FPR overall FPR,
is introduced into the denominator of the softmax-based loss. The larger the
instance FPR, the larger the penalty. By such unequal penalties, the instance
FPRs are supposed to be consistent. Compared with the previous debiasing
methods, our method requires no demographic annotations. Thus, it can mitigate
the bias among demographic groups divided by various attributes, and these
attributes are not needed to be previously predefined during training.
Extensive experimental results on popular benchmarks demonstrate the
superiority of our method over state-of-the-art competitors. Code and trained
models are available at https://github.com/Tencent/TFace.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SVMA: A GAN-based model for Monocular 3D Human Pose Estimation. (arXiv:2106.05616v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yicheng Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yongqi Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jiahui Zhu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05616">
                                    <div class="article-summary-box-inner">
                                        <span>Recovering 3D human pose from 2D joints is a highly unconstrained problem,
especially without any video or multi-view information. We present an
unsupervised GAN-based model to recover 3D human pose from 2D joint locations
extracted from a single image. Our model uses a GAN to learn the mapping of
distribution from 2D poses to 3D poses, not the simple 2D-3D correspondence.
Considering the reprojection constraint, our model can estimate the camera so
that we can reproject the estimated 3D pose to the original 2D pose. Based on
this reprojection method, we can rotate and reproject the generated pose to get
our &quot;new&quot; 2D pose and then use a weight sharing generator to estimate the &quot;new&quot;
3D pose and a &quot;new&quot; camera. Through the above estimation process, we can define
the single-view-multi-angle consistency loss during training to simulate
multi-view consistency, which means the 3D poses and cameras estimated from two
angles of a single view should be able to be mixed to generate rich 2D
reprojections, and the 2D reprojections reprojected from the same 3D pose
should be consistent. The experimental results on Human3.6M show that our
method outperforms all the state-of-the-art methods, and results on
MPI-INF-3DHP show that our method outperforms state-of-the-art by approximately
15.0%.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning to Affiliate: Mutual Centralized Learning for Few-shot Classification. (arXiv:2106.05517v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weifeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1">Chao Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1">Tu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1">Deng Cai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05517">
                                    <div class="article-summary-box-inner">
                                        <span>Few-shot learning (FSL) aims to learn a classifier that can be easily adapted
to accommodate new tasks not seen during training, given only a few examples.
To handle the limited-data problem in few-shot regimes, recent methods tend to
collectively use a set of local features to densely represent an image instead
of using a mixed global feature. They generally explore a unidirectional
query-to-support paradigm in FSL, e.g., find the nearest/optimal support
feature for each query feature and aggregate these local matches for a joint
classification. In this paper, we propose a new method Mutual Centralized
Learning (MCL) to fully affiliate the two disjoint sets of dense features in a
bidirectional paradigm. We associate each local feature with a particle that
can bidirectionally random walk in a discrete feature space by the
affiliations. To estimate the class probability, we propose the features&#x27;
accessibility that measures the expected number of visits to the support
features of that class in a Markov process. We relate our method to learning a
centrality on an affiliation network and demonstrate its capability to be
plugged in existing methods by highlighting centralized local features.
Experiments show that our method achieves the state-of-the-art on both
miniImageNet and tieredImageNet.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. (arXiv:2106.05953v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Spurr_A/0/1/0/all/0/1">Adrian Spurr</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahiya_A/0/1/0/all/0/1">Aneesh Dahiya</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xucong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1">Otmar Hilliges</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05953">
                                    <div class="article-summary-box-inner">
                                        <span>Acquiring accurate 3D annotated data for hand pose estimation is a
notoriously difficult problem. This typically requires complex multi-camera
setups and controlled conditions, which in turn creates a domain gap that is
hard to bridge to fully unconstrained settings. Encouraged by the success of
contrastive learning on image classification tasks, we propose a new
self-supervised method for the structured regression task of 3D hand pose
estimation. Contrastive learning makes use of unlabeled data for the purpose of
representation learning via a loss formulation that encourages the learned
feature representations to be invariant under any image transformation. For 3D
hand pose estimation, it too is desirable to have invariance to appearance
transformation such as color jitter. However, the task requires equivariance
under affine transformations, such as rotation and translation. To address this
issue, we propose an equivariant contrastive objective and demonstrate its
effectiveness in the context of 3D hand pose estimation. We experimentally
investigate the impact of invariant and equivariant contrastive objectives and
show that learning equivariant features leads to better representations for the
task of 3D hand pose estimation. Furthermore, we show that a standard
ResNet-152, trained on additional unlabeled data, attains an improvement of
$7.6\%$ in PA-EPE on FreiHAND and thus achieves state-of-the-art performance
without any task specific, specialized architectures.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AlphaNet: Improved Training of Supernets with Alpha-Divergence. (arXiv:2102.07954v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1">Chengyue Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Meng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1">Vikas Chandra</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07954">
                                    <div class="article-summary-box-inner">
                                        <span>Weight-sharing neural architecture search (NAS) is an effective technique for
automating efficient neural architecture design. Weight-sharing NAS builds a
supernet that assembles all the architectures as its sub-networks and jointly
trains the supernet with the sub-networks. The success of weight-sharing NAS
heavily relies on distilling the knowledge of the supernet to the sub-networks.
However, we find that the widely used distillation divergence, i.e., KL
divergence, may lead to student sub-networks that over-estimate or
under-estimate the uncertainty of the teacher supernet, leading to inferior
performance of the sub-networks. In this work, we propose to improve the
supernet training with a more generalized alpha-divergence. By adaptively
selecting the alpha-divergence, we simultaneously prevent the over-estimation
or under-estimation of the uncertainty of the teacher model. We apply the
proposed alpha-divergence based supernets training to both slimmable neural
networks and weight-sharing NAS, and demonstrate significant improvements.
Specifically, our discovered model family, AlphaNet, outperforms prior-art
models on a wide range of FLOPs regimes, including BigNAS, Once-for-All
networks, and AttentiveNAS. We achieve ImageNet top-1 accuracy of 80.0% with
only 444M FLOPs. Our code and pretrained models are available at
https://github.com/facebookresearch/AlphaNet.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal Fusion. (arXiv:2012.02177v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Duzceker_A/0/1/0/all/0/1">Arda D&#xfc;z&#xe7;eker</a>, <a href="http://arxiv.org/find/cs/1/au:+Galliani_S/0/1/0/all/0/1">Silvano Galliani</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogel_C/0/1/0/all/0/1">Christoph Vogel</a>, <a href="http://arxiv.org/find/cs/1/au:+Speciale_P/0/1/0/all/0/1">Pablo Speciale</a>, <a href="http://arxiv.org/find/cs/1/au:+Dusmanu_M/0/1/0/all/0/1">Mihai Dusmanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1">Marc Pollefeys</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02177">
                                    <div class="article-summary-box-inner">
                                        <span>We propose an online multi-view depth prediction approach on posed video
streams, where the scene geometry information computed in the previous time
steps is propagated to the current time step in an efficient and geometrically
plausible way. The backbone of our approach is a real-time capable, lightweight
encoder-decoder that relies on cost volumes computed from pairs of images. We
extend it by placing a ConvLSTM cell at the bottleneck layer, which compresses
an arbitrary amount of past information in its states. The novelty lies in
propagating the hidden state of the cell by accounting for the viewpoint
changes between time steps. At a given time step, we warp the previous hidden
state into the current camera plane using the previous depth prediction. Our
extension brings only a small overhead of computation time and memory
consumption, while improving the depth predictions significantly. As a result,
we outperform the existing state-of-the-art multi-view stereo methods on most
of the evaluated metrics in hundreds of indoor scenes while maintaining a
real-time performance. Code available:
https://github.com/ardaduz/deep-video-mvs</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation. (arXiv:2106.05969v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhengyi Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1">Ryo Hachiuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Ye Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1">Kris Kitani</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05969">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a method for object-aware 3D egocentric pose estimation that
tightly integrates kinematics modeling, dynamics modeling, and scene object
information. Unlike prior kinematics or dynamics-based approaches where the two
components are used disjointly, we synergize the two approaches via
dynamics-regulated training. At each timestep, a kinematic model is used to
provide a target pose using video evidence and simulation state. Then, a
prelearned dynamics model attempts to mimic the kinematic pose in a physics
simulator. By comparing the pose instructed by the kinematic model against the
pose generated by the dynamics model, we can use their misalignment to further
improve the kinematic model. By factoring in the 6DoF pose of objects (e.g.,
chairs, boxes) in the scene, we demonstrate for the first time, the ability to
estimate physically-plausible 3D human-object interactions using a single
wearable camera. We evaluate our egocentric pose estimation method in both
controlled laboratory settings and real-world scenarios.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning to See by Looking at Noise. (arXiv:2106.05963v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Baradad_M/0/1/0/all/0/1">Manel Baradad</a>, <a href="http://arxiv.org/find/cs/1/au:+Wulff_J/0/1/0/all/0/1">Jonas Wulff</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tongzhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1">Phillip Isola</a>, <a href="http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1">Antonio Torralba</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05963">
                                    <div class="article-summary-box-inner">
                                        <span>Current vision systems are trained on huge datasets, and these datasets come
with costs: curation is expensive, they inherit human biases, and there are
concerns over privacy and usage rights. To counter these costs, interest has
surged in learning from cheaper data sources, such as unlabeled images. In this
paper we go a step further and ask if we can do away with real image datasets
entirely, instead learning from noise processes. We investigate a suite of
image generation models that produce images from simple random processes. These
are then used as training data for a visual representation learner with a
contrastive loss. We study two types of noise processes, statistical image
models and deep generative models under different random initializations. Our
findings show that it is important for the noise to capture certain structural
properties of real data but that good performance can be achieved even with
processes that are far from realistic. We also find that diversity is a key
property to learn good representations. Datasets, models, and code are
available at https://mbaradad.github.io/learning_with_noise.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A numerical framework for elastic surface matching, comparison, and interpolation. (arXiv:2006.11652v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1">Martin Bauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Charon_N/0/1/0/all/0/1">Nicolas Charon</a>, <a href="http://arxiv.org/find/cs/1/au:+Harms_P/0/1/0/all/0/1">Philipp Harms</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_H/0/1/0/all/0/1">Hsi-Wei Hsieh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.11652">
                                    <div class="article-summary-box-inner">
                                        <span>Surface comparison and matching is a challenging problem in computer vision.
While reparametrization-invariant Sobolev metrics provide meaningful elastic
distances and point correspondences via the geodesic boundary value problem,
solving this problem numerically tends to be difficult. Square root normal
fields (SRNF) considerably simplify the computation of certain elastic
distances between parametrized surfaces. Yet they leave open the issue of
finding optimal reparametrizations, which induce elastic distances between
unparametrized surfaces. This issue has concentrated much effort in recent
years and led to the development of several numerical frameworks. In this
paper, we take an alternative approach which bypasses the direct estimation of
reparametrizations: we relax the geodesic boundary constraint using an
auxiliary parametrization-blind varifold fidelity metric. This reformulation
has several notable benefits. By avoiding altogether the need for
reparametrizations, it provides the flexibility to deal with simplicial meshes
of arbitrary topologies and sampling patterns. Moreover, the problem lends
itself to a coarse-to-fine multi-resolution implementation, which makes the
algorithm scalable to large meshes. Furthermore, this approach extends readily
to higher-order feature maps such as square root curvature fields and is also
able to include surface textures in the matching problem. We demonstrate these
advantages on several examples, synthetic and real.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced Semi-Supervised Learning. (arXiv:2106.05682v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1">Youngtaek Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dong-Jin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1">In So Kweon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05682">
                                    <div class="article-summary-box-inner">
                                        <span>The capability of the traditional semi-supervised learning (SSL) methods is
far from real-world application since they do not consider (1) class imbalance
and (2) class distribution mismatch between labeled and unlabeled data. This
paper addresses such a relatively under-explored problem, imbalanced
semi-supervised learning, where heavily biased pseudo-labels can harm the model
performance. Interestingly, we find that the semantic pseudo-labels from a
similarity-based classifier in feature space and the traditional pseudo-labels
from the linear classifier show the complementary property. To this end, we
propose a general pseudo-labeling framework to address the bias motivated by
this observation. The key idea is to class-adaptively blend the semantic
pseudo-label to the linear one, depending on the current pseudo-label
distribution. Thereby, the increased semantic pseudo-label component suppresses
the false positives in the majority classes and vice versa. We term the novel
pseudo-labeling framework for imbalanced SSL as Distribution-Aware
Semantics-Oriented (DASO) Pseudo-label. Extensive evaluation on CIFAR10/100-LT
and STL10-LT shows that DASO consistently outperforms both recently proposed
re-balancing methods for label and pseudo-label. Moreover, we demonstrate that
typical SSL algorithms can effectively benefit from unlabeled data with DASO,
especially when (1) class imbalance and (2) class distribution mismatch exist
and even on recent real-world Semi-Aves benchmark.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Spatially Invariant Unsupervised 3D Object Segmentation with Graph Neural Networks. (arXiv:2106.05607v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1">Kee Siong Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Miaomiao Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05607">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we tackle the problem of unsupervised 3D object segmentation
from a point cloud without RGB information. In particular, we propose a
framework,~{\bf SPAIR3D}, to model a point cloud as a spatial mixture model and
jointly learn the multiple-object representation and segmentation in 3D via
Variational Autoencoders (VAE). Inspired by SPAIR, we adopt an
object-specification scheme that describes each object&#x27;s location relative to
its local voxel grid cell rather than the point cloud as a whole. To model the
spatial mixture model on point clouds, we derive the~\emph{Chamfer Likelihood},
which fits naturally into the variational training pipeline. We further design
a new spatially invariant graph neural network to generate a varying number of
3D points as a decoder within our VAE.~Experimental results demonstrate
that~{\bf SPAIR3D} is capable of detecting and segmenting variable number of
objects without appearance information across diverse scenes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Quantized Conditional COT-GAN for Video Prediction. (arXiv:2106.05658v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Xu_T/0/1/0/all/0/1">Tianlin Xu</a>, <a href="http://arxiv.org/find/stat/1/au:+Acciaio_B/0/1/0/all/0/1">Beatrice Acciaio</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05658">
                                    <div class="article-summary-box-inner">
                                        <span>Causal Optimal Transport (COT) results from imposing a temporal causality
constraint on classic optimal transport problems, which naturally generates a
new concept of distances between distributions on path spaces. The first
application of the COT theory for sequential learning was given in Xu et al.
(2020), where COT-GAN was introduced as an adversarial algorithm to train
implicit generative models optimized for producing sequential data. Relying on
Xu et al. (2020), the contribution of the present paper is twofold. First, we
develop a conditional version of COT-GAN suitable for sequence prediction. This
means that the dataset is now used in order to learn how a sequence will evolve
given the observation of its past evolution. Second, we improve on the
convergence results by working with modifications of the empirical measures via
a specific type of quantization due to Backhoff et al. (2020). The resulting
quantized conditional COT-GAN algorithm is illustrated with an application for
video prediction.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-Modal Discrete Representation Learning. (arXiv:2106.05438v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Alexander H. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">SouYoung Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1">Cheng-I Jeff Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1">Andrew Rouditchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1">Aude Oliva</a>, <a href="http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1">James Glass</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05438">
                                    <div class="article-summary-box-inner">
                                        <span>Recent advances in representation learning have demonstrated an ability to
represent information from different modalities such as video, text, and audio
in a single high-level embedding vector. In this work we present a
self-supervised learning framework that is able to learn a representation that
captures finer levels of granularity across different modalities such as
concepts or events represented by visual objects or spoken words. Our framework
relies on a discretized embedding space created via vector quantization that is
shared across different modalities. Beyond the shared embedding space, we
propose a Cross-Modal Code Matching objective that forces the representations
from different views (modalities) to have a similar distribution over the
discrete embedding space such that cross-modal objects/actions localization can
be performed without direct supervision. In our experiments we show that the
proposed discretized multi-modal fine-grained representation (e.g.,
pixel/word/frame) can complement high-level summary representations (e.g.,
video/sentence/waveform) for improved performance on cross-modal retrieval
tasks. We also observe that the discretized representation uses individual
clusters to represent the same semantic concept across modalities.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Match What Matters: Generative Implicit Feature Replay for Continual Learning. (arXiv:2106.05350v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Thandiackal_K/0/1/0/all/0/1">Kevin Thandiackal</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Portenier_T/0/1/0/all/0/1">Tiziano Portenier</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1">Andrea Giovannini</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Gabrani_M/0/1/0/all/0/1">Maria Gabrani</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1">Orcun Goksel</a> (2 and 3) ((1) IBM Research Europe, (2) ETH Zurich, (3) Uppsala University)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05350">
                                    <div class="article-summary-box-inner">
                                        <span>Neural networks are prone to catastrophic forgetting when trained
incrementally on different tasks. In order to prevent forgetting, most existing
methods retain a small subset of previously seen samples, which in turn can be
used for joint training with new tasks. While this is indeed effective, it may
not always be possible to store such samples, e.g., due to data protection
regulations. In these cases, one can instead employ generative models to create
artificial samples or features representing memories from previous tasks.
Following a similar direction, we propose GenIFeR (Generative Implicit Feature
Replay) for class-incremental learning. The main idea is to train a generative
adversarial network (GAN) to generate images that contain realistic features.
While the generator creates images at full resolution, the discriminator only
sees the corresponding features extracted by the continually trained
classifier. Since the classifier compresses raw images into features that are
actually relevant for classification, the GAN can match this target
distribution more accurately. On the other hand, allowing the generator to
create full resolution images has several benefits: In contrast to previous
approaches, the feature extractor of the classifier does not have to be frozen.
In addition, we can employ augmentations on generated images, which not only
boosts classification performance, but also mitigates discriminator overfitting
during GAN training. We empirically show that GenIFeR is superior to both
conventional generative image and feature replay. In particular, we
significantly outperform the state-of-the-art in generative replay for various
settings on the CIFAR-100 and CUB-200 datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Evolving Robust Neural Architectures to Defend from Adversarial Attacks. (arXiv:1906.11667v3 [cs.NE] CROSS LISTED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1">Shashank Kotyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1">Danilo Vasconcellos Vargas</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.11667">
                                    <div class="article-summary-box-inner">
                                        <span>Neural networks are prone to misclassify slightly modified input images.
Recently, many defences have been proposed, but none have improved the
robustness of neural networks consistently. Here, we propose to use adversarial
attacks as a function evaluation to search for neural architectures that can
resist such attacks automatically. Experiments on neural architecture search
algorithms from the literature show that although accurate, they are not able
to find robust architectures. A significant reason for this lies in their
limited search space. By creating a novel neural architecture search with
options for dense layers to connect with convolution layers and vice-versa as
well as the addition of concatenation layers in the search, we were able to
evolve an architecture that is inherently accurate on adversarial samples.
Interestingly, this inherent robustness of the evolved architecture rivals
state-of-the-art defences such as adversarial training while being trained only
on the non-adversarial samples. Moreover, the evolved architecture makes use of
some peculiar traits which might be useful for developing even more robust
ones. Thus, the results here confirm that more robust architectures exist as
well as opens up a new realm of feasibilities for the development and
exploration of neural networks.

Code available at this http URL</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ResMLP: Feedforward networks for image classification with data-efficient training. (arXiv:2105.03404v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1">Hugo Touvron</a>, <a href="http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1">Piotr Bojanowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1">Mathilde Caron</a>, <a href="http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1">Matthieu Cord</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Nouby_A/0/1/0/all/0/1">Alaaeldin El-Nouby</a>, <a href="http://arxiv.org/find/cs/1/au:+Grave_E/0/1/0/all/0/1">Edouard Grave</a>, <a href="http://arxiv.org/find/cs/1/au:+Izacard_G/0/1/0/all/0/1">Gautier Izacard</a>, <a href="http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1">Armand Joulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1">Gabriel Synnaeve</a>, <a href="http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1">Jakob Verbeek</a>, <a href="http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1">Herv&#xe9; J&#xe9;gou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03404">
                                    <div class="article-summary-box-inner">
                                        <span>We present ResMLP, an architecture built entirely upon multi-layer
perceptrons for image classification. It is a simple residual network that
alternates (i) a linear layer in which image patches interact, independently
and identically across channels, and (ii) a two-layer feed-forward network in
which channels interact independently per patch. When trained with a modern
training strategy using heavy data-augmentation and optionally distillation, it
attains surprisingly good accuracy/complexity trade-offs on ImageNet. We also
train ResMLP models in a self-supervised setup, to further remove priors from
employing a labelled dataset. Finally, by adapting our model to machine
translation we achieve surprisingly good results.

We share pre-trained models and our code based on the Timm library.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial Motion Modelling helps Semi-supervised Hand Pose Estimation. (arXiv:2106.05954v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Spurr_A/0/1/0/all/0/1">Adrian Spurr</a>, <a href="http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1">Pavlo Molchanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1">Umar Iqbal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1">Jan Kautz</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1">Otmar Hilliges</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05954">
                                    <div class="article-summary-box-inner">
                                        <span>Hand pose estimation is difficult due to different environmental conditions,
object- and self-occlusion as well as diversity in hand shape and appearance.
Exhaustively covering this wide range of factors in fully annotated datasets
has remained impractical, posing significant challenges for generalization of
supervised methods. Embracing this challenge, we propose to combine ideas from
adversarial training and motion modelling to tap into unlabeled videos. To this
end we propose what to the best of our knowledge is the first motion model for
hands and show that an adversarial formulation leads to better generalization
properties of the hand pose estimator via semi-supervised training on unlabeled
video sequences. In this setting, the pose predictor must produce a valid
sequence of hand poses, as determined by a discriminative adversary. This
adversary reasons both on the structural as well as temporal domain,
effectively exploiting the spatio-temporal structure in the task. The main
advantage of our approach is that we can make use of unpaired videos and joint
sequence data both of which are much easier to attain than paired training
data. We perform extensive evaluation, investigating essential components
needed for the proposed framework and empirically demonstrate in two
challenging settings that the proposed approach leads to significant
improvements in pose estimation accuracy. In the lowest label setting, we
attain an improvement of $40\%$ in absolute mean joint error.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Concealed Object Detection. (arXiv:2102.10274v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1">Deng-Ping Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1">Ge-Peng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Ming-Ming Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1">Ling Shao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10274">
                                    <div class="article-summary-box-inner">
                                        <span>We present the first systematic study on concealed object detection (COD),
which aims to identify objects that are &quot;perfectly&quot; embedded in their
background. The high intrinsic similarities between the concealed objects and
their background make COD far more challenging than traditional object
detection/segmentation. To better understand this task, we collect a
large-scale dataset, called COD10K, which consists of 10,000 images covering
concealed objects in diverse real-world scenarios from 78 object categories.
Further, we provide rich annotations including object categories, object
boundaries, challenging attributes, object-level labels, and instance-level
annotations. Our COD10K is the largest COD dataset to date, with the richest
annotations, which enables comprehensive concealed object understanding and can
even be used to help progress several other vision tasks, such as detection,
segmentation, classification, etc. Motivated by how animals hunt in the wild,
we also design a simple but strong baseline for COD, termed the Search
Identification Network (SINet). Without any bells and whistles, SINet
outperforms 12 cutting-edge baselines on all datasets tested, making them
robust, general architectures that could serve as catalysts for future research
in COD. Finally, we provide some interesting findings and highlight several
potential applications and future directions. To spark research in this new
field, our code, dataset, and online demo are available on our project page:
this http URL</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SemSegLoss: A python package of loss functions for semantic segmentation. (arXiv:2106.05844v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jadon_S/0/1/0/all/0/1">Shruti Jadon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05844">
                                    <div class="article-summary-box-inner">
                                        <span>Image Segmentation has been an active field of research as it has a wide
range of applications, ranging from automated disease detection to self-driving
cars. In recent years, various research papers proposed different loss
functions used in case of biased data, sparse segmentation, and unbalanced
dataset. In this paper, we introduce SemSegLoss, a python package consisting of
some of the well-known loss functions widely used for image segmentation. It is
developed with the intent to help researchers in the development of novel loss
functions and perform an extensive set of experiments on model architectures
for various applications. The ease-of-use and flexibility of the presented
package have allowed reducing the development time and increased evaluation
strategies of machine learning models for semantic segmentation. Furthermore,
different applications that use image segmentation can use SemSegLoss because
of the generality of its functions. This wide range of applications will lead
to the development and growth of AI across all industries.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CALTeC: Content-Adaptive Linear Tensor Completion for Collaborative Intelligence. (arXiv:2106.05531v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Dhondea_A/0/1/0/all/0/1">Ashiv Dhondea</a>, <a href="http://arxiv.org/find/eess/1/au:+Cohen_R/0/1/0/all/0/1">Robert A. Cohen</a>, <a href="http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1">Ivan V. Baji&#x107;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05531">
                                    <div class="article-summary-box-inner">
                                        <span>In collaborative intelligence, an artificial intelligence (AI) model is
typically split between an edge device and the cloud. Feature tensors produced
by the edge sub-model are sent to the cloud via an imperfect communication
channel. At the cloud side, parts of the feature tensor may be missing due to
packet loss. In this paper we propose a method called Content-Adaptive Linear
Tensor Completion (CALTeC) to recover the missing feature data. The proposed
method is fast, data-adaptive, does not require pre-training, and produces
better results than existing methods for tensor data recovery in collaborative
intelligence.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Joint Landmark and Structure Learning for Automatic Evaluation of Developmental Dysplasia of the Hip. (arXiv:2106.05458v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xindi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Limin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1">Wufeng Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengfeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shuangping Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_N/0/1/0/all/0/1">Ning Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1">Dong Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1">Ning Gu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05458">
                                    <div class="article-summary-box-inner">
                                        <span>The ultrasound (US) screening of the infant hip is vital for the early
diagnosis of developmental dysplasia of the hip (DDH). The US diagnosis of DDH
refers to measuring alpha and beta angles that quantify hip joint development.
These two angles are calculated from key anatomical landmarks and structures of
the hip. However, this measurement process is not trivial for sonographers and
usually requires a thorough understanding of complex anatomical structures. In
this study, we propose a multi-task framework to learn the relationships among
landmarks and structures jointly and automatically evaluate DDH. Our multi-task
networks are equipped with three novel modules. Firstly, we adopt Mask R-CNN as
the basic framework to detect and segment key anatomical structures and add one
landmark detection branch to form a new multi-task framework. Secondly, we
propose a novel shape similarity loss to refine the incomplete anatomical
structure prediction robustly and accurately. Thirdly, we further incorporate
the landmark-structure consistent prior to ensure the consistency of the bony
rim estimated from the segmented structure and the detected landmark. In our
experiments, 1,231 US images of the infant hip from 632 patients are collected,
of which 247 images from 126 patients are tested. The average errors in alpha
and beta angles are 2.221 degrees and 2.899 degrees. About 93% and 85%
estimates of alpha and beta angles have errors less than 5 degrees,
respectively. Experimental results demonstrate that the proposed method can
accurately and robustly realize the automatic evaluation of DDH, showing great
potential for clinical application.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An adaptive Origin-Destination flows cluster-detecting method to identify urban mobility trends. (arXiv:2106.05436v1 [cs.CG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1">Mengyuan Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">Luliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1">Zihan Kan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xue Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_T/0/1/0/all/0/1">Tao Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qingquan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chaokui Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05436">
                                    <div class="article-summary-box-inner">
                                        <span>Origin-Destination (OD) flow, as an abstract representation of the object&#x60;s
movement or interaction, has been used to reveal the urban mobility and
human-land interaction pattern. As an important spatial analysis approach, the
clustering methods of point events have been extended to OD flows to identify
the dominant trends and spatial structures of urban mobility. However, the
existing methods for OD flow cluster-detecting are limited both in specific
spatial scale and the uncertain result due to different parameters setting,
which is difficult for complicated OD flows clustering under spatial
heterogeneity. To address these limitations, in this paper, we proposed a novel
OD flows cluster-detecting method based on the OPTICS algorithm which can
identify OD flow clusters with various aggregation scales. The method can
adaptively determine parameter value from the dataset without prior knowledge
and artificial intervention. Experiments indicated that our method outperformed
three state-of-the-art methods with more accurate and complete of clusters and
less noise. As a case study, our method is applied to identify the potential
routes for public transport service settings by detecting OD flow clusters
within urban travel data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers. (arXiv:2106.05392v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Patrick_M/0/1/0/all/0/1">Mandela Patrick</a>, <a href="http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1">Dylan Campbell</a>, <a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1">Yuki M. Asano</a>, <a href="http://arxiv.org/find/cs/1/au:+Metze_I/0/1/0/all/0/1">Ishan Misra Florian Metze</a>, <a href="http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1">Christoph Feichtenhofer</a>, <a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1">Andrea Vedaldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1">Jo\&#xe3;o F. Henriques</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05392">
                                    <div class="article-summary-box-inner">
                                        <span>In video transformers, the time dimension is often treated in the same way as
the two spatial dimensions. However, in a scene where objects or the camera may
move, a physical point imaged at one location in frame $t$ may be entirely
unrelated to what is found at that location in frame $t+k$. These temporal
correspondences should be modeled to facilitate learning about dynamic scenes.
To this end, we propose a new drop-in block for video transformers --
trajectory attention -- that aggregates information along implicitly determined
motion paths. We additionally propose a new method to address the quadratic
dependence of computation and memory on the input size, which is particularly
important for high resolution or long videos. While these ideas are useful in a
range of settings, we apply them to the specific task of video action
recognition with a transformer model and obtain state-of-the-art results on the
Kinetics, Something--Something V2, and Epic-Kitchens datasets. Code and models
are available at: https://github.com/facebookresearch/Motionformer</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation. (arXiv:2106.05970v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wanrong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Eric Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1">An Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1">Miguel Eckstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05970">
                                    <div class="article-summary-box-inner">
                                        <span>Automatic evaluations for natural language generation (NLG) conventionally
rely on token-level or embedding-level comparisons with the text references.
This is different from human language processing, for which visual imaginations
often improve comprehension. In this work, we propose ImaginE, an
imagination-based automatic evaluation metric for natural language generation.
With the help of CLIP and DALL-E, two cross-modal models pre-trained on
large-scale image-text pairs, we automatically generate an image as the
embodied imagination for the text snippet and compute the imagination
similarity using contextual embeddings. Experiments spanning several text
generation tasks demonstrate that adding imagination with our ImaginE displays
great potential in introducing multi-modal information into NLG evaluation, and
improves existing automatic metrics&#x27; correlations with human similarity
judgments in many circumstances.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional Convolution. (arXiv:2105.05003v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lizhe Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaohao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1">Siyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1">Ping Tan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05003">
                                    <div class="article-summary-box-inner">
                                        <span>Modern deep-learning-based lane detection methods are successful in most
scenarios but struggling for lane lines with complex topologies. In this work,
we propose CondLaneNet, a novel top-to-down lane detection framework that
detects the lane instances first and then dynamically predicts the line shape
for each instance. Aiming to resolve lane instance-level discrimination
problem, we introduce a conditional lane detection strategy based on
conditional convolution and row-wise formulation. Further, we design the
Recurrent Instance Module(RIM) to overcome the problem of detecting lane lines
with complex topologies such as dense lines and fork lines. Benefit from the
end-to-end pipeline which requires little post-process, our method has
real-time efficiency. We extensively evaluate our method on three benchmarks of
lane detection. Results show that our method achieves state-of-the-art
performance on all three benchmark datasets. Moreover, our method has the
coexistence of accuracy and efficiency, e.g. a 78.14 F1 score and 220 FPS on
CULane. Our code is available at
https://github.com/aliyun/conditional-lane-detection.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Vision Transformers with Patch Diversification. (arXiv:2104.12753v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1">Chengyue Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Meng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1">Vikas Chandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12753">
                                    <div class="article-summary-box-inner">
                                        <span>Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code will be made publicly available soon.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FetReg: Placental Vessel Segmentation and Registration in Fetoscopy Challenge Dataset. (arXiv:2106.05923v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1">Sophia Bano</a>, <a href="http://arxiv.org/find/cs/1/au:+Casella_A/0/1/0/all/0/1">Alessandro Casella</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1">Francisco Vasconcelos</a>, <a href="http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1">Sara Moccia</a>, <a href="http://arxiv.org/find/cs/1/au:+Attilakos_G/0/1/0/all/0/1">George Attilakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Wimalasundera_R/0/1/0/all/0/1">Ruwan Wimalasundera</a>, <a href="http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1">Anna L. David</a>, <a href="http://arxiv.org/find/cs/1/au:+Paladini_D/0/1/0/all/0/1">Dario Paladini</a>, <a href="http://arxiv.org/find/cs/1/au:+Deprest_J/0/1/0/all/0/1">Jan Deprest</a>, <a href="http://arxiv.org/find/cs/1/au:+Mattos_L/0/1/0/all/0/1">Leonardo S. Mattos</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1">Danail Stoyanov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05923">
                                    <div class="article-summary-box-inner">
                                        <span>Fetoscopy laser photocoagulation is a widely used procedure for the treatment
of Twin-to-Twin Transfusion Syndrome (TTTS), that occur in mono-chorionic
multiple pregnancies due to placental vascular anastomoses. This procedure is
particularly challenging due to limited field of view, poor manoeuvrability of
the fetoscope, poor visibility due to fluid turbidity, variability in light
source, and unusual position of the placenta. This may lead to increased
procedural time and incomplete ablation, resulting in persistent TTTS.
Computer-assisted intervention may help overcome these challenges by expanding
the fetoscopic field of view through video mosaicking and providing better
visualization of the vessel network. However, the research and development in
this domain remain limited due to unavailability of high-quality data to encode
the intra- and inter-procedure variability. Through the Fetoscopic Placental
Vessel Segmentation and Registration (FetReg) challenge, we present a
large-scale multi-centre dataset for the development of generalized and robust
semantic segmentation and video mosaicking algorithms for the fetal environment
with a focus on creating drift-free mosaics from long duration fetoscopy
videos. In this paper, we provide an overview of the FetReg dataset, challenge
tasks, evaluation metrics and baseline methods for both segmentation and
registration. Baseline methods results on the FetReg dataset shows that our
dataset poses interesting challenges, which can be modelled and competed for
through our crowd-sourcing initiative of the FetReg challenge.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CTSpine1K: A Large-Scale Dataset for Spinal Vertebrae Segmentation in Computed Tomography. (arXiv:2105.14711v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1">Yang Deng</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1">Ce Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Hui_Y/0/1/0/all/0/1">Yuan Hui</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1">Qian Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1">Jun Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_S/0/1/0/all/0/1">Shiwei Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_M/0/1/0/all/0/1">Mengke Sun</a>, <a href="http://arxiv.org/find/eess/1/au:+Quan_Q/0/1/0/all/0/1">Quan Quan</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1">Shuxin Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Hao_Y/0/1/0/all/0/1">You Hao</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1">Pengbo Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Xiao_H/0/1/0/all/0/1">Honghu Xiao</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1">Chunpeng Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1">Xinbao Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1">S. Kevin Zhou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14711">
                                    <div class="article-summary-box-inner">
                                        <span>Spine-related diseases have high morbidity and cause a huge burden of social
cost. Spine imaging is an essential tool for noninvasively visualizing and
assessing spinal pathology. Segmenting vertebrae in computed tomography (CT)
images is the basis of quantitative medical image analysis for clinical
diagnosis and surgery planning of spine diseases. Current publicly available
annotated datasets on spinal vertebrae are small in size. Due to the lack of a
large-scale annotated spine image dataset, the mainstream deep learning-based
segmentation methods, which are data-driven, are heavily restricted. In this
paper, we introduce a large-scale spine CT dataset, called CTSpine1K, curated
from multiple sources for vertebra segmentation, which contains 1,005 CT
volumes with over 11,100 labeled vertebrae belonging to different spinal
conditions. Based on this dataset, we conduct several spinal vertebrae
segmentation experiments to set the first benchmark. We believe that this
large-scale dataset will facilitate further research in many spine-related
image analysis tasks, including but not limited to vertebrae segmentation,
labeling, 3D spine reconstruction from biplanar radiographs, image
super-resolution, and enhancement.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Structure Guided Lane Detection. (arXiv:2105.05403v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1">Jinming Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Ke Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Junfeng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiaoming Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xiaolin Wei</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05403">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, lane detection has made great progress with the rapid development
of deep neural networks and autonomous driving. However, there exist three
mainly problems including characterizing lanes, modeling the structural
relationship between scenes and lanes, and supporting more attributes (e.g.,
instance and type) of lanes. In this paper, we propose a novel structure guided
framework to solve these problems simultaneously. In the framework, we first
introduce a new lane representation to characterize each instance. Then a
topdown vanishing point guided anchoring mechanism is proposed to produce
intensive anchors, which efficiently capture various lanes. Next, multi-level
structural constraints are used to improve the perception of lanes. In the
process, pixel-level perception with binary segmentation is introduced to
promote features around anchors and restore lane details from bottom up, a
lane-level relation is put forward to model structures (i.e., parallel) around
lanes, and an image-level attention is used to adaptively attend different
regions of the image from the perspective of scenes. With the help of
structural guidance, anchors are effectively classified and regressed to obtain
precise locations and shapes. Extensive experiments on public benchmark
datasets show that the proposed approach outperforms state-of-the-art methods
with 117 FPS on a single GPU.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases. (arXiv:2103.10697v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+dAscoli_S/0/1/0/all/0/1">St&#xe9;phane d&#x27;Ascoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1">Hugo Touvron</a>, <a href="http://arxiv.org/find/cs/1/au:+Leavitt_M/0/1/0/all/0/1">Matthew Leavitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1">Ari Morcos</a>, <a href="http://arxiv.org/find/cs/1/au:+Biroli_G/0/1/0/all/0/1">Giulio Biroli</a>, <a href="http://arxiv.org/find/cs/1/au:+Sagun_L/0/1/0/all/0/1">Levent Sagun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10697">
                                    <div class="article-summary-box-inner">
                                        <span>Convolutional architectures have proven extremely successful for vision
tasks. Their hard inductive biases enable sample-efficient learning, but come
at the cost of a potentially lower performance ceiling. Vision Transformers
(ViTs) rely on more flexible self-attention layers, and have recently
outperformed CNNs for image classification. However, they require costly
pre-training on large external datasets or distillation from pre-trained
convolutional networks. In this paper, we ask the following question: is it
possible to combine the strengths of these two architectures while avoiding
their respective limitations? To this end, we introduce gated positional
self-attention (GPSA), a form of positional self-attention which can be
equipped with a &#x60;&#x60;soft&quot; convolutional inductive bias. We initialise the GPSA
layers to mimic the locality of convolutional layers, then give each attention
head the freedom to escape locality by adjusting a gating parameter regulating
the attention paid to position versus content information. The resulting
convolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet,
while offering a much improved sample efficiency. We further investigate the
role of locality in learning by first quantifying how it is encouraged in
vanilla self-attention layers, then analysing how it is escaped in GPSA layers.
We conclude by presenting various ablations to better understand the success of
the ConViT. Our code and models are released publicly at
https://github.com/facebookresearch/convit.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Network Space Search for Pareto-Efficient Spaces. (arXiv:2104.11014v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1">Min-Fong Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao-Yun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Min-Hung Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yu-Syuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1">Hsien-Kai Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yi-Min Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hung-Jen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jou_K/0/1/0/all/0/1">Kevin Jou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11014">
                                    <div class="article-summary-box-inner">
                                        <span>Network spaces have been known as a critical factor in both handcrafted
network designs or defining search spaces for Neural Architecture Search (NAS).
However, an effective space involves tremendous prior knowledge and/or manual
effort, and additional constraints are required to discover efficiency-aware
architectures. In this paper, we define a new problem, Network Space Search
(NSS), as searching for favorable network spaces instead of a single
architecture. We propose an NSS method to directly search for efficient-aware
network spaces automatically, reducing the manual effort and immense cost in
discovering satisfactory ones. The resultant network spaces, named Elite
Spaces, are discovered from Expanded Search Space with minimal human expertise
imposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front
under various complexity constraints and can be further served as NAS search
spaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an
averagely 2.3% lower error rate and 3.7% closer to target constraint than the
baseline with around 90% fewer samples required to find satisfactory networks).
Moreover, our NSS approach is capable of searching for superior spaces in
future unexplored spaces, revealing great potential in searching for network
spaces automatically.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MLP-Mixer: An all-MLP Architecture for Vision. (arXiv:2105.01601v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1">Ilya Tolstikhin</a>, <a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1">Neil Houlsby</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1">Alexander Kolesnikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1">Lucas Beyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaohua Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1">Thomas Unterthiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Yung_J/0/1/0/all/0/1">Jessica Yung</a>, <a href="http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1">Andreas Steiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1">Daniel Keysers</a>, <a href="http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1">Jakob Uszkoreit</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1">Mario Lucic</a>, <a href="http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1">Alexey Dosovitskiy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01601">
                                    <div class="article-summary-box-inner">
                                        <span>Convolutional Neural Networks (CNNs) are the go-to model for computer vision.
Recently, attention-based networks, such as the Vision Transformer, have also
become popular. In this paper we show that while convolutions and attention are
both sufficient for good performance, neither of them are necessary. We present
MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs).
MLP-Mixer contains two types of layers: one with MLPs applied independently to
image patches (i.e. &quot;mixing&quot; the per-location features), and one with MLPs
applied across patches (i.e. &quot;mixing&quot; spatial information). When trained on
large datasets, or with modern regularization schemes, MLP-Mixer attains
competitive scores on image classification benchmarks, with pre-training and
inference cost comparable to state-of-the-art models. We hope that these
results spark further research beyond the realms of well established CNNs and
Transformers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">VisImages: a Corpus of Visualizations in the Images of Visualization Publications. (arXiv:2007.04584v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1">Dazhen Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yihong Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1">Xinhuan Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mengye Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1">Siwei Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1">Weiwei Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yingcai Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.04584">
                                    <div class="article-summary-box-inner">
                                        <span>Images in visualization publications contain rich information, e.g., novel
visualization designs and common combinations of visualizations. A systematic
collection of these images can contribute to the community in many aspects,
such as literature analysis and automated tasks for visualization. In this
paper, we build and make public a dataset, VisImages, which collects 12,267
images with captions from 1,397 papers in IEEE InfoVis and VAST. Based on a
refined taxonomy for visualizations in publications, the dataset includes
35,096 annotated visualizations, as well as their positions. We demonstrate the
usefulness of VisImages through three use cases: 1) exploring and analyzing the
evolution of visualizations with VisImages Explorer, 2) training and
benchmarking models for visualization classification, and 3) localizing and
recognizing visualizations in the images automatically.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving state estimation through projection post-processing for activity recognition in football. (arXiv:2102.03310v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ciszewski_M/0/1/0/all/0/1">Micha&#x142; Ciszewski</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohl_J/0/1/0/all/0/1">Jakob S&#xf6;hl</a>, <a href="http://arxiv.org/find/cs/1/au:+Jongbloed_G/0/1/0/all/0/1">Geurt Jongbloed</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03310">
                                    <div class="article-summary-box-inner">
                                        <span>The past decade has seen an increased interest in human activity recognition.
Most commonly, the raw data coming from sensors attached to body parts are
unannotated, which creates a need for fast labelling method. Part of the
procedure is choosing or designing an appropriate performance measure. We
propose a new performance measure, the Locally Time-Shifted Measure, which
addresses the issue of timing uncertainty of state transitions in the
classification result. Our main contribution is a novel post-processing method
for binary activity recognition. It improves the accuracy of the classification
methods, by correcting for unrealistically short activities in the estimate.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Escaping Plato&#x27;s Cave: 3D Shape From Adversarial Rendering. (arXiv:1811.11606v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Henzler_P/0/1/0/all/0/1">Philipp Henzler</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1">Niloy Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1">Tobias Ritschel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1811.11606">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce PlatonicGAN to discover the 3D structure of an object class from
an unstructured collection of 2D images, i.e., where no relation between photos
is known, except that they are showing instances of the same category. The key
idea is to train a deep neural network to generate 3D shapes which, when
rendered to images, are indistinguishable from ground truth images (for a
discriminator) under various camera poses. Discriminating 2D images instead of
3D shapes allows tapping into unstructured 2D photo collections instead of
relying on curated (e.g., aligned, annotated, etc.) 3D data sets. To establish
constraints between 2D image observation and their 3D interpretation, we
suggest a family of rendering layers that are effectively differentiable. This
family includes visual hull, absorption-only (akin to x-ray), and
emission-absorption. We can successfully reconstruct 3D shapes from
unstructured 2D images and extensively evaluate PlatonicGAN on a range of
synthetic and real data sets achieving consistent improvements over baseline
methods. We further show that PlatonicGAN can be combined with 3D supervision
to improve on and in some cases even surpass the quality of 3D-supervised
methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Toward Deep Supervised Anomaly Detection: Reinforcement Learning from Partially Labeled Anomaly Data. (arXiv:2009.06847v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1">Guansong Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1">Anton van den Hengel</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Longbing Cao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06847">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of anomaly detection with a small set of partially
labeled anomaly examples and a large-scale unlabeled dataset. This is a common
scenario in many important applications. Existing related methods either
exclusively fit the limited anomaly examples that typically do not span the
entire set of anomalies, or proceed with unsupervised learning from the
unlabeled data. We propose here instead a deep reinforcement learning-based
approach that enables an end-to-end optimization of the detection of both
labeled and unlabeled anomalies. This approach learns the known abnormality by
automatically interacting with an anomaly-biased simulation environment, while
continuously extending the learned abnormality to novel classes of anomaly
(i.e., unknown anomalies) by actively exploring possible anomalies in the
unlabeled data. This is achieved by jointly optimizing the exploitation of the
small labeled anomaly data and the exploration of the rare unlabeled anomalies.
Extensive experiments on 48 real-world datasets show that our model
significantly outperforms five state-of-the-art competing methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Attention-Enhanced Cross-Task Network for Analysing Multiple Attributes of Lung Nodules in CT. (arXiv:2103.03931v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1">Xiaohang Fu</a>, <a href="http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1">Lei Bi</a>, <a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1">Ashnil Kumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Fulham_M/0/1/0/all/0/1">Michael Fulham</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1">Jinman Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03931">
                                    <div class="article-summary-box-inner">
                                        <span>Accurate characterisation of visual attributes such as spiculation,
lobulation, and calcification of lung nodules is critical in cancer management.
The characterisation of these attributes is often subjective, which may lead to
high inter- and intra-observer variability. Furthermore, lung nodules are often
heterogeneous in the cross-sectional image slices of a 3D volume. Current
state-of-the-art methods that score multiple attributes rely on deep
learning-based multi-task learning (MTL) schemes. These methods, however,
extract shared visual features across attributes and then examine each
attribute without explicitly leveraging their inherent intercorrelations.
Furthermore, current methods either treat each slice with equal importance
without considering their relevance or heterogeneity, which limits performance.
In this study, we address these challenges with a new convolutional neural
network (CNN)-based MTL model that incorporates multiple attention-based
learning modules to simultaneously score 9 visual attributes of lung nodules in
computed tomography (CT) image volumes. Our model processes entire nodule
volumes of arbitrary depth and uses a slice attention module to filter out
irrelevant slices. We also introduce cross-attribute and attribute
specialisation attention modules that learn an optimal amalgamation of
meaningful representations to leverage relationships between attributes. We
demonstrate that our model outperforms previous state-of-the-art methods at
scoring attributes using the well-known public LIDC-IDRI dataset of pulmonary
nodules from over 1,000 patients. Our model also performs competitively when
repurposed for benign-malignant classification. Our attention modules also
provide easy-to-interpret weights that offer insights into the predictions of
the model.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v1 [eess.SP] CROSS LISTED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Thanthrige_U/0/1/0/all/0/1">Udaya S.K.P. Miriya Thanthrige</a>, <a href="http://arxiv.org/find/eess/1/au:+Jung_P/0/1/0/all/0/1">Peter Jung</a>, <a href="http://arxiv.org/find/eess/1/au:+Sezgin_A/0/1/0/all/0/1">Aydin Sezgin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03686">
                                    <div class="article-summary-box-inner">
                                        <span>We address the detection of material defects, which are inside a layered
material structure using compressive sensing based multiple-output (MIMO)
wireless radar. Here, the strong clutter due to the reflection of the layered
structure&#x27;s surface often makes the detection of the defects challenging. Thus,
sophisticated signal separation methods are required for improved defect
detection. In many scenarios, the number of defects that we are interested in
is limited and the signaling response of the layered structure can be modeled
as a low-rank structure. Therefore, we propose joint rank and sparsity
minimization for defect detection. In particular, we propose a non-convex
approach based on the iteratively reweighted nuclear and $\ell_1-$norm (a
double-reweighted approach) to obtain a higher accuracy compared to the
conventional nuclear norm and $\ell_1-$norm minimization. To this end, an
iterative algorithm is designed to estimate the low-rank and sparse
contributions. Further, we propose deep learning to learn the parameters of the
algorithm (i.e., algorithm unfolding) to improve the accuracy and the speed of
convergence of the algorithm. Our numerical results show that the proposed
approach outperforms the conventional approaches in terms of mean square errors
of the recovered low-rank and sparse components and the speed of convergence.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Architecture Search of SPD Manifold Networks. (arXiv:2010.14535v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1">Rhea Sanjay Sukthanker</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhiwu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Suryansh Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Endsjo_E/0/1/0/all/0/1">Erik Goron Endsjo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.14535">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose a new neural architecture search (NAS) problem of
Symmetric Positive Definite (SPD) manifold networks, aiming to automate the
design of SPD neural architectures. To address this problem, we first introduce
a geometrically rich and diverse SPD neural architecture search space for an
efficient SPD cell design. Further, we model our new NAS problem with a
one-shot training process of a single supernet. Based on the supernet modeling,
we exploit a differentiable NAS algorithm on our relaxed continuous search
space for SPD neural architecture search. Statistical evaluation of our method
on drone, action, and emotion recognition tasks mostly provides better results
than the state-of-the-art SPD networks and traditional NAS algorithms.
Empirical results show that our algorithm excels in discovering better
performing SPD network design and provides models that are more than three
times lighter than searched by the state-of-the-art NAS algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Automatic Curation of Large-Scale Datasets for Audio-Visual Representation Learning. (arXiv:2101.10803v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sangho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1">Jiwan Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Youngjae Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Gunhee Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1">Thomas Breuel</a>, <a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1">Gal Chechik</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yale Song</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10803">
                                    <div class="article-summary-box-inner">
                                        <span>Large-scale datasets are the cornerstone of representation learning. Existing
self-supervised approaches extract learning signals by making certain
assumptions about the data, e.g., spatio-temporal continuity and multimodal
correspondence. However, finding large amounts of data that satisfy such
assumptions is not straightforward, and this restricts the community to rely on
datasets collected through laborious annotation and/or manual filtering
processes. In this paper, we propose a subset optimization approach for
automatic dataset curation. Focusing on audio-visual representation learning,
we find a subset that provides the maximum mutual information between audio and
visual channels in videos. We show that self-supervised models trained on our
data, despite being automatically constructed, achieve competitive downstream
performances compared to existing datasets that require annotation and/or
manual filtering. The most significant benefit of our approach is scalability.
We release a dataset of 100M videos with high audio-visual correspondence.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Large Norms of CNN Layers Do Not Hurt Adversarial Robustness. (arXiv:2009.08435v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Youwei Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Dong Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08435">
                                    <div class="article-summary-box-inner">
                                        <span>Since the Lipschitz properties of CNN are widely considered to be related to
adversarial robustness, we theoretically characterize the $\ell_1$ norm and
$\ell_\infty$ norm of 2D multi-channel convolutional layers and provide
efficient methods to compute the exact $\ell_1$ norm and $\ell_\infty$ norm.
Based on our theorem, we propose a novel regularization method termed norm
decay, which can effectively reduce the norms of convolutional layers and
fully-connected layers. Experiments show that norm-regularization methods,
including norm decay, weight decay, and singular value clipping, can improve
generalization of CNNs. However, they can slightly hurt adversarial robustness.
Observing this unexpected phenomenon, we compute the norms of layers in the
CNNs trained with three different adversarial training frameworks and
surprisingly find that adversarially robust CNNs have comparable or even larger
layer norms than their non-adversarially robust counterparts. Furthermore, we
prove that under a mild assumption, adversarially robust classifiers can be
achieved, and can have an arbitrarily large Lipschitz constant. For this
reason, enforcing small norms on CNN layers may be neither necessary nor
effective in achieving adversarial robustness. The code is available at
https://github.com/youweiliang/norm_robustness.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On Information Plane Analyses of Neural Network Classifiers -- A Review. (arXiv:2003.09671v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Geiger_B/0/1/0/all/0/1">Bernhard C. Geiger</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.09671">
                                    <div class="article-summary-box-inner">
                                        <span>We review the current literature concerned with information plane analyses of
neural network classifiers. While the underlying information bottleneck theory
and the claim that information-theoretic compression is causally linked to
generalization are plausible, empirical evidence was found to be both
supporting and conflicting. We review this evidence together with a detailed
analysis of how the respective information quantities were estimated. Our
survey suggests that compression visualized in information planes is not
necessarily information-theoretic, but is rather often compatible with
geometric compression of the latent representations. This insight gives the
information plane a renewed justification.

Aside from this, we shed light on the problem of estimating mutual
information in deterministic neural networks and its consequences.
Specifically, we argue that even in feed-forward neural networks the data
processing inequality need not hold for estimates of mutual information.
Similarly, while a fitting phase, in which the mutual information between the
latent representation and the target increases, is necessary (but not
sufficient) for good classification performance, depending on the specifics of
mutual information estimation such a fitting phase need not be visible in the
information plane.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unsupervised Hyperspectral Mixed Noise Removal Via Spatial-Spectral Constrained Deep Image Prior. (arXiv:2008.09753v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yi-Si Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xi-Le Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1">Tai-Xiang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yu-Bang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yi Chang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09753">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, convolutional neural network (CNN)-based methods are proposed for
hyperspectral images (HSIs) denoising. Among them, unsupervised methods such as
the deep image prior (DIP) have received much attention because these methods
do not require any training data. However, DIP suffers from the
semi-convergence behavior, i.e., the iteration of DIP needs to terminate by
referring to the ground-truth image at the optimal iteration point. In this
paper, we propose the spatial-spectral constrained deep image prior (S2DIP) for
HSI mixed noise removal. Specifically, we incorporate DIP with a
spatial-spectral total variation (SSTV) term to fully preserve the
spatial-spectral local smoothness of the HSI and an $\ell_1$-norm term to
capture the complex sparse noise. The proposed S2DIP jointly leverages the
expressive power brought from the deep CNN without any training data and
exploits the HSI and noise structures via hand-crafted priors. Thus, our method
avoids the semi-convergence behavior, showing higher stabilities than DIP.
Meanwhile, our method largely enhances the HSI denoising ability of DIP. To
tackle the proposed denoising model, we develop an alternating direction
multiplier method algorithm. Extensive experiments demonstrate that the
proposed S2DIP outperforms optimization-based and supervised CNN-based
state-of-the-art HSI denoising methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learn your ABCs: Approximate Bijective Correspondence for isolating factors of variation. (arXiv:2103.03240v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1">Kieran A. Murphy</a>, <a href="http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1">Varun Jampani</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1">Srikumar Ramalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1">Ameesh Makadia</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03240">
                                    <div class="article-summary-box-inner">
                                        <span>Representational learning forms the backbone of most deep learning
applications, and the value of a learned representation is intimately tied to
its information content regarding different factors of variation. Finding good
representations depends on the nature of supervision and the learning
algorithm. We propose a novel algorithm that relies on a weak form of
supervision where the data is partitioned into sets according to certain
inactive factors of variation. Our key insight is that by seeking approximate
correspondence between elements of different sets, we learn strong
representations that exclude the inactive factors of variation and isolate the
active factors which vary within all sets. We demonstrate that the method can
work in a semi-supervised scenario, and that a portion of the unsupervised data
can belong to a different domain entirely. Further control over the content of
the learned representations is possible by folding in data augmentation to
suppress nuisance factors. We outperform competing baselines on the challenging
problem of synthetic-to-real object pose transfer.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dual Attention on Pyramid Feature Maps for Image Captioning. (arXiv:2011.01385v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Litao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qiang Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.01385">
                                    <div class="article-summary-box-inner">
                                        <span>Generating natural sentences from images is a fundamental learning task for
visual-semantic understanding in multimedia. In this paper, we propose to apply
dual attention on pyramid image feature maps to fully explore the
visual-semantic correlations and improve the quality of generated sentences.
Specifically, with the full consideration of the contextual information
provided by the hidden state of the RNN controller, the pyramid attention can
better localize the visually indicative and semantically consistent regions in
images. On the other hand, the contextual information can help re-calibrate the
importance of feature components by learning the channel-wise dependencies, to
improve the discriminative power of visual features for better content
description. We conducted comprehensive experiments on three well-known
datasets: Flickr8K, Flickr30K and MS COCO, which achieved impressive results in
generating descriptive and smooth natural sentences from images. Using either
convolution visual features or more informative bottom-up attention features,
our composite captioning model achieves very promising performance in a
single-model mode. The proposed pyramid attention and dual attention methods
are highly modular, which can be inserted into various image captioning modules
to further improve the performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Curiously Effective Features for Image Quality Prediction. (arXiv:2106.05946v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1">S&#xf6;ren Becker</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiegand_T/0/1/0/all/0/1">Thomas Wiegand</a>, <a href="http://arxiv.org/find/cs/1/au:+Bosse_S/0/1/0/all/0/1">Sebastian Bosse</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05946">
                                    <div class="article-summary-box-inner">
                                        <span>The performance of visual quality prediction models is commonly assumed to be
closely tied to their ability to capture perceptually relevant image aspects.
Models are thus either based on sophisticated feature extractors carefully
designed from extensive domain knowledge or optimized through feature learning.
In contrast to this, we find feature extractors constructed from random noise
to be sufficient to learn a linear regression model whose quality predictions
reach high correlations with human visual quality ratings, on par with a model
with learned features. We analyze this curious result and show that besides the
quality of feature extractors also their quantity plays a crucial role - with
top performances only being achieved in highly overparameterized models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Anatomy X-Net: A Semi-Supervised Anatomy Aware Convolutional Neural Network for Thoracic Disease Classification. (arXiv:2106.05915v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kamal_U/0/1/0/all/0/1">Uday Kamal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zunaed_M/0/1/0/all/0/1">Mohammad Zunaed</a>, <a href="http://arxiv.org/find/cs/1/au:+Nizam_N/0/1/0/all/0/1">Nusrat Binta Nizam</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1">Taufiq Hasan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05915">
                                    <div class="article-summary-box-inner">
                                        <span>Thoracic disease detection from chest radiographs using deep learning methods
has been an active area of research in the last decade. Most previous methods
attempt to focus on the diseased organs of the image by identifying spatial
regions responsible for significant contributions to the model&#x27;s prediction. In
contrast, expert radiologists first locate the prominent anatomical structures
before determining if those regions are anomalous. Therefore, integrating
anatomical knowledge within deep learning models could bring substantial
improvement in automatic disease classification. This work proposes an
anatomy-aware attention-based architecture named Anatomy X-Net, that
prioritizes the spatial features guided by the pre-identified anatomy regions.
We leverage a semi-supervised learning method using the JSRT dataset containing
organ-level annotation to obtain the anatomical segmentation masks (for lungs
and heart) for the NIH and CheXpert datasets. The proposed Anatomy X-Net uses
the pre-trained DenseNet-121 as the backbone network with two corresponding
structured modules, the Anatomy Aware Attention (AAA) and Probabilistic
Weighted Average Pooling (PWAP), in a cohesive framework for anatomical
attention learning. Our proposed method sets new state-of-the-art performance
on the official NIH test set with an AUC score of 0.8439, proving the efficacy
of utilizing the anatomy segmentation knowledge to improve the thoracic disease
classification. Furthermore, the Anatomy X-Net yields an averaged AUC of 0.9020
on the Stanford CheXpert dataset, improving on existing methods that
demonstrate the generalizability of the proposed framework.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">1-Point RANSAC-Based Method for Ground Object Pose Estimation. (arXiv:2008.03718v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jeong-Kyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Baik_Y/0/1/0/all/0/1">Young-Ki Baik</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1">Hankyu Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kang Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Duck Hoon Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.03718">
                                    <div class="article-summary-box-inner">
                                        <span>Solving Perspective-n-Point (PnP) problems is a traditional way of estimating
object poses. Given outlier-contaminated data, a pose of an object is
calculated with PnP algorithms of n &#x3D; {3, 4} in the RANSAC-based scheme.
However, the computational complexity considerably increases along with n and
the high complexity imposes a severe strain on devices which should estimate
multiple object poses in real time. In this paper, we propose an efficient
method based on 1-point RANSAC for estimating a pose of an object on the
ground. In the proposed method, a pose is calculated with 1-DoF
parameterization by using a ground object assumption and a 2D object bounding
box as an additional observation, thereby achieving the fastest performance
among the RANSAC-based methods. In addition, since the method suffers from the
errors of the additional information, we propose a hierarchical robust
estimation method for polishing a rough pose estimate and discovering more
inliers in a coarse-to-fine manner. The experiments in synthetic and real-world
datasets demonstrate the superiority of the proposed method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition. (arXiv:2104.03841v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Massiceti_D/0/1/0/all/0/1">Daniela Massiceti</a>, <a href="http://arxiv.org/find/cs/1/au:+Zintgraf_L/0/1/0/all/0/1">Luisa Zintgraf</a>, <a href="http://arxiv.org/find/cs/1/au:+Bronskill_J/0/1/0/all/0/1">John Bronskill</a>, <a href="http://arxiv.org/find/cs/1/au:+Theodorou_L/0/1/0/all/0/1">Lida Theodorou</a>, <a href="http://arxiv.org/find/cs/1/au:+Harris_M/0/1/0/all/0/1">Matthew Tobias Harris</a>, <a href="http://arxiv.org/find/cs/1/au:+Cutrell_E/0/1/0/all/0/1">Edward Cutrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Morrison_C/0/1/0/all/0/1">Cecily Morrison</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1">Katja Hofmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Stumpf_S/0/1/0/all/0/1">Simone Stumpf</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03841">
                                    <div class="article-summary-box-inner">
                                        <span>Object recognition has made great advances in the last decade, but
predominately still relies on many high-quality training examples per object
category. In contrast, learning new objects from only a few examples could
enable many impactful applications from robotics to user personalization. Most
few-shot learning research, however, has been driven by benchmark datasets that
lack the high variation that these applications will face when deployed in the
real-world. To close this gap, we present the ORBIT dataset and benchmark,
grounded in a real-world application of teachable object recognizers for people
who are blind/low-vision. The dataset contains 3,822 videos of 486 objects
recorded by people who are blind/low-vision on their mobile phones, and the
benchmark reflects a realistic, highly challenging recognition problem,
providing a rich playground to drive research in robustness to few-shot,
high-variation conditions. We set the first state-of-the-art on the benchmark
and show that there is massive scope for further innovation, holding the
potential to impact a broad range of real-world vision applications including
tools for the blind/low-vision community. The dataset is available at
https://bit.ly/2OyElCj and the code to run the benchmark at
https://bit.ly/39YgiUW.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On the Robustness of Human Pose Estimation. (arXiv:1908.06401v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1">Sahil Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1">Naman Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Abhishek Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Arjun Jain</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.06401">
                                    <div class="article-summary-box-inner">
                                        <span>This paper provides a comprehensive and exhaustive study of adversarial
attacks on human pose estimation models and the evaluation of their robustness.
Besides highlighting the important differences between well-studied
classification and human pose-estimation systems w.r.t. adversarial attacks, we
also provide deep insights into the design choices of pose-estimation systems
to shape future work. We benchmark the robustness of several 2D single person
pose-estimation architectures trained on multiple datasets, MPII and COCO. In
doing so, we also explore the problem of attacking non-classification networks
including regression based networks, which has been virtually unexplored in the
past.

\par We find that compared to classification and semantic segmentation, human
pose estimation architectures are relatively robust to adversarial attacks with
the single-step attacks being surprisingly ineffective. Our study shows that
the heatmap-based pose-estimation models are notably robust than their direct
regression-based systems and that the systems which explicitly model
anthropomorphic semantics of human body fare better than their other
counterparts. Besides, targeted attacks are more difficult to obtain than
un-targeted ones and some body-joints are easier to fool than the others. We
present visualizations of universal perturbations to facilitate unprecedented
insights into their workings on pose-estimation. Additionally, we show them to
generalize well across different networks. Finally we perform a user study
about perceptibility of these examples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning by Watching. (arXiv:2106.05966v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jimuyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohn_Bar_E/0/1/0/all/0/1">Eshed Ohn-Bar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05966">
                                    <div class="article-summary-box-inner">
                                        <span>When in a new situation or geographical location, human drivers have an
extraordinary ability to watch others and learn maneuvers that they themselves
may have never performed. In contrast, existing techniques for learning to
drive preclude such a possibility as they assume direct access to an
instrumented ego-vehicle with fully known observations and expert driver
actions. However, such measurements cannot be directly accessed for the non-ego
vehicles when learning by watching others. Therefore, in an application where
data is regarded as a highly valuable asset, current approaches completely
discard the vast portion of the training data that can be potentially obtained
through indirect observation of surrounding vehicles. Motivated by this key
insight, we propose the Learning by Watching (LbW) framework which enables
learning a driving policy without requiring full knowledge of neither the state
nor expert actions. To increase its data, i.e., with new perspectives and
maneuvers, LbW makes use of the demonstrations of other vehicles in a given
scene by (1) transforming the ego-vehicle&#x27;s observations to their points of
view, and (2) inferring their expert actions. Our LbW agent learns more robust
driving policies while enabling data-efficient learning, including quick
adaptation of the policy to rare and novel scenarios. In particular, LbW drives
robustly even with a fraction of available driving data required by existing
methods, achieving an average success rate of 92% on the original CARLA
benchmark with only 30 minutes of total driving data and 82% with only 10
minutes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Implicit Feature Alignment: Learn to Convert Text Recognizer to Text Spotter. (arXiv:2106.05920v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuanzhi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1">Lianwen Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1">Dezhi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1">Mengchao He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongpan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Canjie Luo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05920">
                                    <div class="article-summary-box-inner">
                                        <span>Text recognition is a popular research subject with many associated
challenges. Despite the considerable progress made in recent years, the text
recognition task itself is still constrained to solve the problem of reading
cropped line text images and serves as a subtask of optical character
recognition (OCR) systems. As a result, the final text recognition result is
limited by the performance of the text detector. In this paper, we propose a
simple, elegant and effective paradigm called Implicit Feature Alignment (IFA),
which can be easily integrated into current text recognizers, resulting in a
novel inference mechanism called IFAinference. This enables an ordinary text
recognizer to process multi-line text such that text detection can be
completely freed. Specifically, we integrate IFA into the two most prevailing
text recognition streams (attention-based and CTC-based) and propose
attention-guided dense prediction (ADP) and Extended CTC (ExCTC). Furthermore,
the Wasserstein-based Hollow Aggregation Cross-Entropy (WH-ACE) is proposed to
suppress negative predictions to assist in training ADP and ExCTC. We
experimentally demonstrate that IFA achieves state-of-the-art performance on
end-to-end document recognition tasks while maintaining the fastest speed, and
ADP and ExCTC complement each other on the perspective of different application
scenarios. Code will be available at
https://github.com/WangTianwei/Implicit-feature-alignment.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Space-time Mixing Attention for Video Transformer. (arXiv:2106.05968v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1">Adrian Bulat</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Rua_J/0/1/0/all/0/1">Juan-Manuel Perez-Rua</a>, <a href="http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1">Swathikiran Sudhakaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1">Brais Martinez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1">Georgios Tzimiropoulos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05968">
                                    <div class="article-summary-box-inner">
                                        <span>This paper is on video recognition using Transformers. Very recent attempts
in this area have demonstrated promising results in terms of recognition
accuracy, yet they have been also shown to induce, in many cases, significant
computational overheads due to the additional modelling of the temporal
information. In this work, we propose a Video Transformer model the complexity
of which scales linearly with the number of frames in the video sequence and
hence induces \textit{no overhead} compared to an image-based Transformer
model. To achieve this, our model makes two approximations to the full
space-time attention used in Video Transformers: (a) It restricts time
attention to a local temporal window and capitalizes on the Transformer&#x27;s depth
to obtain full temporal coverage of the video sequence. (b) It uses efficient
space-time mixing to attend \textit{jointly} spatial and temporal locations
without inducing any additional cost on top of a spatial-only attention model.
We also show how to integrate 2 very lightweight mechanisms for global
temporal-only attention which provide additional accuracy improvements at
minimal computational cost. We demonstrate that our model produces very high
recognition accuracy on the most popular video recognition datasets while at
the same time being significantly more efficient than other Video Transformer
models. Code will be made available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CAT: Cross Attention in Vision Transformer. (arXiv:2106.05786v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Hezheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xing Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiangyu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1">Dong Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1">Qing Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1">Wei Yuan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05786">
                                    <div class="article-summary-box-inner">
                                        <span>Since Transformer has found widespread use in NLP, the potential of
Transformer in CV has been realized and has inspired many new approaches.
However, the computation required for replacing word tokens with image patches
for Transformer after the tokenization of the image is vast(e.g., ViT), which
bottlenecks model training and inference. In this paper, we propose a new
attention mechanism in Transformer termed Cross Attention, which alternates
attention inner the image patch instead of the whole image to capture local
information and apply attention between image patches which are divided from
single-channel feature maps capture global information. Both operations have
less computation than standard self-attention in Transformer. By alternately
applying attention inner patch and between patches, we implement cross
attention to maintain the performance with lower computational cost and build a
hierarchical network called Cross Attention Transformer(CAT) for other vision
tasks. Our base model achieves state-of-the-arts on ImageNet-1K, and improves
the performance of other methods on COCO and ADE20K, illustrating that our
network has the potential to serve as general backbones. The code and models
are available at \url{https://github.com/linhezheng19/CAT}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">What Does Rotation Prediction Tell Us about Classifier Accuracy under Varying Testing Environments?. (arXiv:2106.05961v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1">Weijian Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1">Stephen Gould</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Liang Zheng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05961">
                                    <div class="article-summary-box-inner">
                                        <span>Understanding classifier decision under novel environments is central to the
community, and a common practice is evaluating it on labeled test sets.
However, in real-world testing, image annotations are difficult and expensive
to obtain, especially when the test environment is changing. A natural question
then arises: given a trained classifier, can we evaluate its accuracy on
varying unlabeled test sets? In this work, we train semantic classification and
rotation prediction in a multi-task way. On a series of datasets, we report an
interesting finding, i.e., the semantic classification accuracy exhibits a
strong linear relationship with the accuracy of the rotation prediction task
(Pearson&#x27;s Correlation r &gt; 0.88). This finding allows us to utilize linear
regression to estimate classifier performance from the accuracy of rotation
prediction which can be obtained on the test set through the freely generated
rotation labels.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning. (arXiv:2106.05956v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1">Ekdeep Singh Lubana</a>, <a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1">Robert P. Dick</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1">Hidenori Tanaka</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05956">
                                    <div class="article-summary-box-inner">
                                        <span>Inspired by BatchNorm, there has been an explosion of normalization layers in
deep learning. Recent works have identified a multitude of beneficial
properties in BatchNorm to explain its success. However, given the pursuit of
alternative normalization techniques, these properties need to be generalized
so that any given layer&#x27;s success/failure can be accurately predicted. In this
work, we take a first step towards this goal by extending known properties of
BatchNorm in randomly initialized deep neural networks (DNNs) to nine recently
proposed normalization layers. Our primary findings follow: (i) Similar to
BatchNorm, activations-based normalization layers can avoid exploding
activations in ResNets; (ii) Use of GroupNorm ensures rank of activations is at
least $\Omega(\sqrt{\frac{\text{width}}{\text{Group Size}}})$, thus explaining
why LayerNorm witnesses slow optimization speed; (iii) Small group sizes result
in large gradient norm in earlier layers, hence justifying training instability
issues in Instance Normalization and illustrating a speed-stability tradeoff in
GroupNorm. Overall, our analysis reveals several general mechanisms that
explain the success of normalization techniques in deep learning, providing us
with a compass to systematically explore the vast design space of DNN
normalization layers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Enforcing Morphological Information in Fully Convolutional Networks to Improve Cell Instance Segmentation in Fluorescence Microscopy Images. (arXiv:2106.05843v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zamora_Cardenas_W/0/1/0/all/0/1">Willard Zamora-Cardenas</a>, <a href="http://arxiv.org/find/cs/1/au:+Mendez_M/0/1/0/all/0/1">Mauro Mendez</a>, <a href="http://arxiv.org/find/cs/1/au:+Calderon_Ramirez_S/0/1/0/all/0/1">Saul Calderon-Ramirez</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_M/0/1/0/all/0/1">Martin Vargas</a>, <a href="http://arxiv.org/find/cs/1/au:+Monge_G/0/1/0/all/0/1">Gerardo Monge</a>, <a href="http://arxiv.org/find/cs/1/au:+Quiros_S/0/1/0/all/0/1">Steve Quiros</a>, <a href="http://arxiv.org/find/cs/1/au:+Elizondo_D/0/1/0/all/0/1">David Elizondo</a>, <a href="http://arxiv.org/find/cs/1/au:+Elizondo_D/0/1/0/all/0/1">David Elizondo</a>, <a href="http://arxiv.org/find/cs/1/au:+Molina_Cabello_M/0/1/0/all/0/1">Miguel A. Molina-Cabello</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05843">
                                    <div class="article-summary-box-inner">
                                        <span>Cell instance segmentation in fluorescence microscopy images is becoming
essential for cancer dynamics and prognosis. Data extracted from cancer
dynamics allows to understand and accurately model different metabolic
processes such as proliferation. This enables customized and more precise
cancer treatments. However, accurate cell instance segmentation, necessary for
further cell tracking and behavior analysis, is still challenging in scenarios
with high cell concentration and overlapping edges. Within this framework, we
propose a novel cell instance segmentation approach based on the well-known
U-Net architecture. To enforce the learning of morphological information per
pixel, a deep distance transformer (DDT) acts as a back-bone model. The DDT
output is subsequently used to train a top-model. The following top-models are
considered: a three-class (\emph{e.g.,} foreground, background and cell border)
U-net, and a watershed transform. The obtained results suggest a performance
boost over traditional U-Net architectures. This opens an interesting research
line around the idea of injecting morphological information into a fully
convolutional model.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations. (arXiv:2106.05967v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gansbeke_W/0/1/0/all/0/1">Wouter Van Gansbeke</a>, <a href="http://arxiv.org/find/cs/1/au:+Vandenhende_S/0/1/0/all/0/1">Simon Vandenhende</a>, <a href="http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1">Stamatios Georgoulis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05967">
                                    <div class="article-summary-box-inner">
                                        <span>Contrastive self-supervised learning has outperformed supervised pretraining
on many downstream tasks like segmentation and object detection. However,
current methods are still primarily applied to curated datasets like ImageNet.
In this paper, we first study how biases in the dataset affect existing
methods. Our results show that current contrastive approaches work surprisingly
well across: (i) object- versus scene-centric, (ii) uniform versus long-tailed
and (iii) general versus domain-specific datasets. Second, given the generality
of the approach, we try to realize further gains with minor modifications. We
show that learning additional invariances -- through the use of multi-scale
cropping, stronger augmentations and nearest neighbors -- improves the
representations. Finally, we observe that MoCo learns spatially structured
representations when trained with a multi-crop strategy. The representations
can be used for semantic segment retrieval and video instance segmentation
without finetuning. Moreover, the results are on par with specialized models.
We hope this work will serve as a useful study for other researchers. The code
and models will be available at
https://github.com/wvangansbeke/Revisiting-Contrastive-SSL.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for Multimodal Hate. (arXiv:2106.05903v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Botelho_A/0/1/0/all/0/1">Austin Botelho</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1">Bertie Vidgen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1">Scott A. Hale</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05903">
                                    <div class="article-summary-box-inner">
                                        <span>Accurate detection and classification of online hate is a difficult task.
Implicit hate is particularly challenging as such content tends to have unusual
syntax, polysemic words, and fewer markers of prejudice (e.g., slurs). This
problem is heightened with multimodal content, such as memes (combinations of
text and images), as they are often harder to decipher than unimodal content
(e.g., text alone). This paper evaluates the role of semantic and multimodal
context for detecting implicit and explicit hate. We show that both text- and
visual- enrichment improves model performance, with the multimodal model
(0.771) outperforming other models&#x27; F1 scores (0.544, 0.737, and 0.754). While
the unimodal-text context-aware (transformer) model was the most accurate on
the subtask of implicit hate detection, the multimodal model outperformed it
overall because of a lower propensity towards false positives. We find that all
models perform better on content with full annotator agreement and that
multimodal models are best at classifying the content where annotators
disagree. To conduct these investigations, we undertook high-quality annotation
of a sample of 5,000 multimodal entries. Tweets were annotated for primary
category, modality, and strategy. We make this corpus, along with the codebook,
code, and final model, freely available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Pivotal Tuning for Latent-based Editing of Real Images. (arXiv:2106.05744v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Roich_D/0/1/0/all/0/1">Daniel Roich</a>, <a href="http://arxiv.org/find/cs/1/au:+Mokady_R/0/1/0/all/0/1">Ron Mokady</a>, <a href="http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1">Amit H. Bermano</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05744">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, a surge of advanced facial editing techniques have been proposed
that leverage the generative power of a pre-trained StyleGAN. To successfully
edit an image this way, one must first project (or invert) the image into the
pre-trained generator&#x27;s domain. As it turns out, however, StyleGAN&#x27;s latent
space induces an inherent tradeoff between distortion and editability, i.e.
between maintaining the original appearance and convincingly altering some of
its attributes. Practically, this means it is still challenging to apply
ID-preserving facial latent-space editing to faces which are out of the
generator&#x27;s domain. In this paper, we present an approach to bridge this gap.
Our technique slightly alters the generator, so that an out-of-domain image is
faithfully mapped into an in-domain latent code. The key idea is pivotal tuning
- a brief training process that preserves the editing quality of an in-domain
latent region, while changing its portrayed identity and appearance. In Pivotal
Tuning Inversion (PTI), an initial inverted latent code serves as a pivot,
around which the generator is fined-tuned. At the same time, a regularization
term keeps nearby identities intact, to locally contain the effect. This
surgical training process ends up altering appearance features that represent
mostly identity, without affecting editing capabilities. We validate our
technique through inversion and editing metrics, and show preferable scores to
state-of-the-art methods. We further qualitatively demonstrate our technique by
applying advanced edits (such as pose, age, or expression) to numerous images
of well-known and recognizable identities. Finally, we demonstrate resilience
to harder cases, including heavy make-up, elaborate hairstyles and/or headwear,
which otherwise could not have been successfully inverted and edited by
state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The 2021 Hotel-ID to Combat Human Trafficking Competition Dataset. (arXiv:2106.05746v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kamath_R/0/1/0/all/0/1">Rashmi Kamath</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolwes_G/0/1/0/all/0/1">Greg Rolwes</a>, <a href="http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1">Samuel Black</a>, <a href="http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1">Abby Stylianou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05746">
                                    <div class="article-summary-box-inner">
                                        <span>Hotel recognition is an important task for human trafficking investigations
since victims are often photographed in hotel rooms. Identifying these hotels
is vital to trafficking investigations since they can help track down current
and future victims who might be taken to the same places. Hotel recognition is
a challenging fine grained visual classification task as there can be little
similarity between different rooms within the same hotel, and high similarity
between rooms from different hotels (especially if they are from the same
chain). Hotel recognition to combat human trafficking poses additional
challenges as investigative images are often low quality, contain uncommon
camera angles and are highly occluded. Here, we present the 2021 Hotel-ID
dataset to help raise awareness for this problem and generate novel approaches.
The dataset consists of hotel room images that have been crowd-sourced and
uploaded through the TraffickCam mobile application. The quality of these
images is similar to investigative images and hence models trained on these
images have good chances of accurately narrowing down on the correct hotel.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Medical Segmentation Decathlon. (arXiv:2106.05735v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Antonelli_M/0/1/0/all/0/1">Michela Antonelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Reinke_A/0/1/0/all/0/1">Annika Reinke</a>, <a href="http://arxiv.org/find/cs/1/au:+Bakas_S/0/1/0/all/0/1">Spyridon Bakas</a>, <a href="http://arxiv.org/find/cs/1/au:+Farahani_K/0/1/0/all/0/1">Keyvan Farahani</a>, <a href="http://arxiv.org/find/cs/1/au:+AnnetteKopp-Schneider/0/1/0/all/0/1">AnnetteKopp-Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1">Bennett A. Landman</a>, <a href="http://arxiv.org/find/cs/1/au:+Litjens_G/0/1/0/all/0/1">Geert Litjens</a>, <a href="http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1">Bjoern Menze</a>, <a href="http://arxiv.org/find/cs/1/au:+Ronneberger_O/0/1/0/all/0/1">Olaf Ronneberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Summers_R/0/1/0/all/0/1">Ronald M.Summers</a>, <a href="http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1">Bram van Ginneken</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilello_M/0/1/0/all/0/1">Michel Bilello</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilic_P/0/1/0/all/0/1">Patrick Bilic</a>, <a href="http://arxiv.org/find/cs/1/au:+Christ_P/0/1/0/all/0/1">Patrick F. Christ</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_R/0/1/0/all/0/1">Richard K. G. Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Gollub_M/0/1/0/all/0/1">Marc J. Gollub</a>, <a href="http://arxiv.org/find/cs/1/au:+Heckers_S/0/1/0/all/0/1">Stephan H. Heckers</a>, <a href="http://arxiv.org/find/cs/1/au:+Huisman_H/0/1/0/all/0/1">Henkjan Huisman</a>, <a href="http://arxiv.org/find/cs/1/au:+Jarnagin_W/0/1/0/all/0/1">William R. Jarnagin</a>, <a href="http://arxiv.org/find/cs/1/au:+McHugo_M/0/1/0/all/0/1">Maureen K. McHugo</a>, <a href="http://arxiv.org/find/cs/1/au:+Napel_S/0/1/0/all/0/1">Sandy Napel</a>, <a href="http://arxiv.org/find/cs/1/au:+Pernicka_J/0/1/0/all/0/1">Jennifer S. Goli Pernicka</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhode_K/0/1/0/all/0/1">Kawal Rhode</a>, <a href="http://arxiv.org/find/cs/1/au:+Tobon_Gomez_C/0/1/0/all/0/1">Catalina Tobon-Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Vorontsov_E/0/1/0/all/0/1">Eugene Vorontsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Huisman_H/0/1/0/all/0/1">Henkjan Huisman</a>, <a href="http://arxiv.org/find/cs/1/au:+Meakin_J/0/1/0/all/0/1">James A. Meakin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1">Sebastien Ourselin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1">Manuel Wiesenfarth</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1">Pablo Arbelaez</a>, <a href="http://arxiv.org/find/cs/1/au:+Bae_B/0/1/0/all/0/1">Byeonguk Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sihong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Daza_L/0/1/0/all/0/1">Laura Daza</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianjiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Baochun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1">Fabian Isensee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1">Yuanfeng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1">Fucang Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1">Namkug Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1">Ildoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1">Dorit Merhof</a>, <a href="http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1">Akshay Pai</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1">Beomhee Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Perslev_M/0/1/0/all/0/1">Mathias Perslev</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezaiifar_R/0/1/0/all/0/1">Ramin Rezaiifar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rippel_O/0/1/0/all/0/1">Oliver Rippel</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarasua_I/0/1/0/all/0/1">Ignacio Sarasua</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Wei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1">Jaemin Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1">Christian Wachinger</a>, et al. (9 additional authors not shown)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05735">
                                    <div class="article-summary-box-inner">
                                        <span>International challenges have become the de facto standard for comparative
assessment of image analysis algorithms given a specific task. Segmentation is
so far the most widely investigated medical image processing task, but the
various segmentation challenges have typically been organized in isolation,
such that algorithm development was driven by the need to tackle a single
specific clinical problem. We hypothesized that a method capable of performing
well on multiple tasks will generalize well to a previously unseen task and
potentially outperform a custom-designed solution. To investigate the
hypothesis, we organized the Medical Segmentation Decathlon (MSD) - a
biomedical image analysis challenge, in which algorithms compete in a multitude
of both tasks and modalities. The underlying data set was designed to explore
the axis of difficulties typically encountered when dealing with medical
images, such as small data sets, unbalanced labels, multi-site data and small
objects. The MSD challenge confirmed that algorithms with a consistent good
performance on a set of tasks preserved their good average performance on a
different set of previously unseen tasks. Moreover, by monitoring the MSD
winner for two years, we found that this algorithm continued generalizing well
to a wide range of other clinical problems, further confirming our hypothesis.
Three main conclusions can be drawn from this study: (1) state-of-the-art image
segmentation algorithms are mature, accurate, and generalize well when
retrained on unseen tasks; (2) consistent algorithmic performance across
multiple tasks is a strong surrogate of algorithmic generalizability; (3) the
training of accurate AI segmentation models is now commoditized to non AI
experts.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Implicit Surface Point Prediction Networks. (arXiv:2106.05779v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Venkatesh_R/0/1/0/all/0/1">Rahul Venkatesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Karmali_T/0/1/0/all/0/1">Tejan Karmali</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1">Sarthak Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1">Aurobrata Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeni_L/0/1/0/all/0/1">L&#xe1;szl&#xf3; A. Jeni</a>, <a href="http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1">R. Venkatesh Babu</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Maneesh Singh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05779">
                                    <div class="article-summary-box-inner">
                                        <span>Deep neural representations of 3D shapes as implicit functions have been
shown to produce high fidelity models surpassing the resolution-memory
trade-off faced by the explicit representations using meshes and point clouds.
However, most such approaches focus on representing closed shapes. Unsigned
distance function (UDF) based approaches have been proposed recently as a
promising alternative to represent both open and closed shapes. However, since
the gradients of UDFs vanish on the surface, it is challenging to estimate
local (differential) geometric properties like the normals and tangent planes
which are needed for many downstream applications in vision and graphics. There
are additional challenges in computing these properties efficiently with a
low-memory footprint. This paper presents a novel approach that models such
surfaces using a new class of implicit representations called the closest
surface-point (CSP) representation. We show that CSP allows us to represent
complex surfaces of any topology (open or closed) with high fidelity. It also
allows for accurate and efficient computation of local geometric properties. We
further demonstrate that it leads to efficient implementation of downstream
algorithms like sphere-tracing for rendering the 3D surface as well as to
create explicit mesh-based representations. Extensive experimental evaluation
on the ShapeNet dataset validate the above contributions with results
surpassing the state-of-the-art.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unsupervised Co-part Segmentation through Assembly. (arXiv:2106.05897v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1">Qingzhe Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Libin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Baoquan Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05897">
                                    <div class="article-summary-box-inner">
                                        <span>Co-part segmentation is an important problem in computer vision for its rich
applications. We propose an unsupervised learning approach for co-part
segmentation from images. For the training stage, we leverage motion
information embedded in videos and explicitly extract latent representations to
segment meaningful object parts. More importantly, we introduce a dual
procedure of part-assembly to form a closed loop with part-segmentation,
enabling an effective self-supervision. We demonstrate the effectiveness of our
approach with a host of extensive experiments, ranging from human bodies,
hands, quadruped, and robot arms. We show that our approach can achieve
meaningful and compact part segmentation, outperforming state-of-the-art
approaches on diverse benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adaptive Streaming Perception using Deep Reinforcement Learning. (arXiv:2106.05665v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1">Anurag Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nambi_A/0/1/0/all/0/1">Akshay Nambi</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Aditya Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+YVS_H/0/1/0/all/0/1">Harish YVS</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganu_T/0/1/0/all/0/1">Tanuja Ganu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05665">
                                    <div class="article-summary-box-inner">
                                        <span>Executing computer vision models on streaming visual data, or streaming
perception is an emerging problem, with applications in self-driving, embodied
agents, and augmented/virtual reality. The development of such systems is
largely governed by the accuracy and latency of the processing pipeline. While
past work has proposed numerous approximate execution frameworks, their
decision functions solely focus on optimizing latency, accuracy, or energy,
etc. This results in sub-optimum decisions, affecting the overall system
performance. We argue that the streaming perception systems should holistically
maximize the overall system performance (i.e., considering both accuracy and
latency simultaneously). To this end, we describe a new approach based on deep
reinforcement learning to learn these tradeoffs at runtime for streaming
perception. This tradeoff optimization is formulated as a novel deep contextual
bandit problem and we design a new reward function that holistically integrates
latency and accuracy into a single metric. We show that our agent can learn a
competitive policy across multiple decision dimensions, which outperforms
state-of-the-art policies on public datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">End-to-end lung nodule detection framework with model-based feature projection block. (arXiv:2106.05741v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Drokin_I/0/1/0/all/0/1">Ivan Drokin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ericheva_E/0/1/0/all/0/1">Elena Ericheva</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05741">
                                    <div class="article-summary-box-inner">
                                        <span>This paper proposes novel end-to-end framework for detecting suspicious
pulmonary nodules in chest CT scans. The method core idea is a new nodule
segmentation architecture with a model-based feature projection block on
three-dimensional convolutions. This block acts as a preliminary feature
extractor for a two-dimensional U-Net-like convolutional network. Using the
proposed approach along with an axial, coronal, and sagittal projection
analysis makes it possible to abandon the widely used false positives reduction
step. The proposed method achieves SOTA on LUNA2016 with 0.959 average
sensitivity, and 0.936 sensitivity if the false-positive level per scan is
0.25. The paper describes the proposed approach and represents the experimental
results on LUNA2016 as well as ablation studies.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep neural network loses attention to adversarial images. (arXiv:2106.05657v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1">Shashank Kotyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1">Danilo Vasconcellos Vargas</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05657">
                                    <div class="article-summary-box-inner">
                                        <span>Adversarial algorithms have shown to be effective against neural networks for
a variety of tasks. Some adversarial algorithms perturb all the pixels in the
image minimally for the image classification task in image classification. In
contrast, some algorithms perturb few pixels strongly. However, very little
information is available regarding why these adversarial samples so diverse
from each other exist. Recently, Vargas et al. showed that the existence of
these adversarial samples might be due to conflicting saliency within the
neural network. We test this hypothesis of conflicting saliency by analysing
the Saliency Maps (SM) and Gradient-weighted Class Activation Maps (Grad-CAM)
of original and few different types of adversarial samples. We also analyse how
different adversarial samples distort the attention of the neural network
compared to original samples. We show that in the case of Pixel Attack,
perturbed pixels either calls the network attention to themselves or divert the
attention from them. Simultaneously, the Projected Gradient Descent Attack
perturbs pixels so that intermediate layers inside the neural network lose
attention for the correct class. We also show that both attacks affect the
saliency map and activation maps differently. Thus, shedding light on why some
defences successful against some attacks remain vulnerable against other
attacks. We hope that this analysis will improve understanding of the existence
and the effect of adversarial samples and enable the community to develop more
robust neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MiDeCon: Unsupervised and Accurate Fingerprint and Minutia Quality Assessment based on Minutia Detection Confidence. (arXiv:2106.05601v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Terhorst_P/0/1/0/all/0/1">Philipp Terh&#xf6;rst</a>, <a href="http://arxiv.org/find/cs/1/au:+Boller_A/0/1/0/all/0/1">Andr&#xe9; Boller</a>, <a href="http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1">Naser Damer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1">Florian Kirchbuchner</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1">Arjan Kuijper</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05601">
                                    <div class="article-summary-box-inner">
                                        <span>An essential factor to achieve high accuracies in fingerprint recognition
systems is the quality of its samples. Previous works mainly proposed
supervised solutions based on image properties that neglects the minutiae
extraction process, despite that most fingerprint recognition techniques are
based on detected minutiae. Consequently, a fingerprint image might be assigned
a high quality even if the utilized minutia extractor produces unreliable
information. In this work, we propose a novel concept of assessing minutia and
fingerprint quality based on minutia detection confidence (MiDeCon). MiDeCon
can be applied to an arbitrary deep learning based minutia extractor and does
not require quality labels for learning. We propose using the detection
reliability of the extracted minutia as its quality indicator. By combining the
highest minutia qualities, MiDeCon also accurately determines the quality of a
full fingerprint. Experiments are conducted on the publicly available databases
of the FVC 2006 and compared against several baselines, such as NIST&#x27;s
widely-used fingerprint image quality software NFIQ1 and NFIQ2. The results
demonstrate a significantly stronger quality assessment performance of the
proposed MiDeCon-qualities as related works on both, minutia- and
fingerprint-level. The implementation is publicly available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Dataset And Benchmark Of Underwater Object Detection For Robot Picking. (arXiv:2106.05681v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chongwei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haojie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shuchang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Ming Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhihui Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05681">
                                    <div class="article-summary-box-inner">
                                        <span>Underwater object detection for robot picking has attracted a lot of
interest. However, it is still an unsolved problem due to several challenges.
We take steps towards making it more realistic by addressing the following
challenges. Firstly, the currently available datasets basically lack the test
set annotations, causing researchers must compare their method with other SOTAs
on a self-divided test set (from the training set). Training other methods lead
to an increase in workload and different researchers divide different datasets,
resulting there is no unified benchmark to compare the performance of different
algorithms. Secondly, these datasets also have other shortcomings, e.g., too
many similar images or incomplete labels. Towards these challenges we introduce
a dataset, Detecting Underwater Objects (DUO), and a corresponding benchmark,
based on the collection and re-annotation of all relevant datasets. DUO
contains a collection of diverse underwater images with more rational
annotations. The corresponding benchmark provides indicators of both efficiency
and accuracy of SOTAs (under the MMDtection framework) for academic research
and industrial applications, where JETSON AGX XAVIER is used to assess detector
speed to simulate the robot-embedded environment.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hierarchical Agglomerative Graph Clustering in Nearly-Linear Time. (arXiv:2106.05610v1 [cs.DS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dhulipala_L/0/1/0/all/0/1">Laxman Dhulipala</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisenstat_D/0/1/0/all/0/1">David Eisenstat</a>, <a href="http://arxiv.org/find/cs/1/au:+Lacki_J/0/1/0/all/0/1">Jakub &#x141;&#x105;cki</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1">Vahab Mirrokni</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jessica Shi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05610">
                                    <div class="article-summary-box-inner">
                                        <span>We study the widely used hierarchical agglomerative clustering (HAC)
algorithm on edge-weighted graphs. We define an algorithmic framework for
hierarchical agglomerative graph clustering that provides the first efficient
$\tilde{O}(m)$ time exact algorithms for classic linkage measures, such as
complete- and WPGMA-linkage, as well as other measures. Furthermore, for
average-linkage, arguably the most popular variant of HAC, we provide an
algorithm that runs in $\tilde{O}(n\sqrt{m})$ time. For this variant, this is
the first exact algorithm that runs in subquadratic time, as long as
$m&#x3D;n^{2-\epsilon}$ for some constant $\epsilon &gt; 0$. We complement this result
with a simple $\epsilon$-close approximation algorithm for average-linkage in
our framework that runs in $\tilde{O}(m)$ time. As an application of our
algorithms, we consider clustering points in a metric space by first using
$k$-NN to generate a graph from the point set, and then running our algorithms
on the resulting weighted graph. We validate the performance of our algorithms
on publicly available datasets, and show that our approach can speed up
clustering of point datasets by a factor of 20.7--76.5x.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MST: Masked Self-Supervised Transformer for Visual Representation. (arXiv:2106.05656v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhaowen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhiyang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Wei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yousong Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chaoyang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1">Rui Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Liwei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1">Rui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Ming Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinqiao Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05656">
                                    <div class="article-summary-box-inner">
                                        <span>Transformer has been widely used for self-supervised pre-training in Natural
Language Processing (NLP) and achieved great success. However, it has not been
fully explored in visual self-supervised learning. Meanwhile, previous methods
only consider the high-level feature and learning representation from a global
perspective, which may fail to transfer to the downstream dense prediction
tasks focusing on local features. In this paper, we present a novel Masked
Self-supervised Transformer approach named MST, which can explicitly capture
the local context of an image while preserving the global semantic information.
Specifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose
a masked token strategy based on the multi-head self-attention map, which
dynamically masks some tokens of local patches without damaging the crucial
structure for self-supervised learning. More importantly, the masked tokens
together with the remaining tokens are further recovered by a global image
decoder, which preserves the spatial information of the image and is more
friendly to the downstream dense prediction tasks. The experiments on multiple
datasets demonstrate the effectiveness and generality of the proposed method.
For instance, MST achieves Top-1 accuracy of 76.9% with DeiT-S only using
300-epoch pre-training by linear evaluation, which outperforms supervised
methods with the same epoch by 0.4% and its comparable variant DINO by 1.0\%.
For dense prediction tasks, MST also achieves 42.7% mAP on MS COCO object
detection and 74.04% mIoU on Cityscapes segmentation only with 100-epoch
pre-training.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Face mask detection using convolution neural network. (arXiv:2106.05728v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Riya Shah Rutva Shah</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05728">
                                    <div class="article-summary-box-inner">
                                        <span>In the recent times, the Coronaviruses that are a big family of different
viruses have become very common, contagious and dangerous to the whole human
kind. It spreads human to human by exhaling the infection breath, which leaves
droplets of the virus on different surface which is then inhaled by other
person and catches the infection too. So it has become very important to
protect ourselves and the people around us from this situation. We can take
precautions such as social distancing, washing hands every two hours, using
sanitizer, maintaining social distance and the most important wearing a mask.
Public use of wearing a masks has become very common everywhere in the whole
world now. From that the most affected and devastating condition is of India
due to its extreme population in small area. This paper proposes a method to
detect the face mask is put on or not for offices, or any other work place with
a lot of people coming to work. We have used convolutional neural network for
the same. The model is trained on a real world dataset and tested with live
video streaming with a good accuracy. Further the accuracy of the model with
different hyper parameters and multiple people at different distance and
location of the frame is done.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Date Estimation in the Wild of Scanned Historical Photos: An Image Retrieval Approach. (arXiv:2106.05618v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Molina_A/0/1/0/all/0/1">Adri&#xe0; Molina</a>, <a href="http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1">Pau Riba</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1">Lluis Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramos_Terrades_O/0/1/0/all/0/1">Oriol Ramos-Terrades</a>, <a href="http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1">Josep Llad&#xf3;s</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05618">
                                    <div class="article-summary-box-inner">
                                        <span>This paper presents a novel method for date estimation of historical
photographs from archival sources. The main contribution is to formulate the
date estimation as a retrieval task, where given a query, the retrieved images
are ranked in terms of the estimated date similarity. The closer are their
embedded representations the closer are their dates. Contrary to the
traditional models that design a neural network that learns a classifier or a
regressor, we propose a learning objective based on the nDCG ranking metric. We
have experimentally evaluated the performance of the method in two different
tasks: date estimation and date-sensitive image retrieval, using the DEW public
database, overcoming the baseline methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Supervising the Transfer of Reasoning Patterns in VQA. (arXiv:2106.05597v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kervadec_C/0/1/0/all/0/1">Corentin Kervadec</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1">Christian Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Antipov_G/0/1/0/all/0/1">Grigory Antipov</a>, <a href="http://arxiv.org/find/cs/1/au:+Baccouche_M/0/1/0/all/0/1">Moez Baccouche</a>, <a href="http://arxiv.org/find/cs/1/au:+Nadri_M/0/1/0/all/0/1">Madiha Nadri</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05597">
                                    <div class="article-summary-box-inner">
                                        <span>Methods for Visual Question Anwering (VQA) are notorious for leveraging
dataset biases rather than performing reasoning, hindering generalization. It
has been recently shown that better reasoning patterns emerge in attention
layers of a state-of-the-art VQA model when they are trained on perfect
(oracle) visual inputs. This provides evidence that deep neural networks can
learn to reason when training conditions are favorable enough. However,
transferring this learned knowledge to deployable models is a challenge, as
much of it is lost during the transfer. We propose a method for knowledge
transfer based on a regularization term in our loss function, supervising the
sequence of required reasoning operations. We provide a theoretical analysis
based on PAC-learning, showing that such program prediction can lead to
decreased sample complexity under mild hypotheses. We also demonstrate the
effectiveness of this approach experimentally on the GQA dataset and show its
complementarity to BERT-like self-supervised pre-training.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-Dataset Benchmarks for Masked Identification using Contrastive Representation Learning. (arXiv:2106.05596v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Seneviratne_S/0/1/0/all/0/1">Sachith Seneviratne</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasthuriaarachchi_N/0/1/0/all/0/1">Nuran Kasthuriaarachchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasnayaka_S/0/1/0/all/0/1">Sanka Rasnayaka</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05596">
                                    <div class="article-summary-box-inner">
                                        <span>The COVID-19 pandemic has drastically changed accepted norms globally. Within
the past year, masks have been used as a public health response to limit the
spread of the virus. This sudden change has rendered many face recognition
based access control, authentication and surveillance systems ineffective.
Official documents such as passports, driving license and national identity
cards are enrolled with fully uncovered face images. However, in the current
global situation, face matching systems should be able to match these reference
images with masked face images. As an example, in an airport or security
checkpoint it is safer to match the unmasked image of the identifying document
to the masked person rather than asking them to remove the mask. We find that
current facial recognition techniques are not robust to this form of occlusion.

To address this unique requirement presented due to the current circumstance,
we propose a set of re-purposed datasets and a benchmark for researchers to
use. We also propose a contrastive visual representation learning based
pre-training workflow which is specialized to masked vs unmasked face matching.
We ensure that our method learns robust features to differentiate people across
varying data collection scenarios. We achieve this by training over many
different datasets and validating our result by testing on various holdout
datasets. The specialized weights trained by our method outperform standard
face recognition features for masked to unmasked face matching. We believe the
provided synthetic mask generating code, our novel training approach and the
trained weights from the masked face models will help in adopting existing face
recognition systems to operate in the current global environment. We
open-source all contributions for broader use by the research community.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Progressive Stage-wise Learning for Unsupervised Feature Representation Enhancement. (arXiv:2106.05554v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zefan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chenxi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1">Bingbing Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Wen Gao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05554">
                                    <div class="article-summary-box-inner">
                                        <span>Unsupervised learning methods have recently shown their competitiveness
against supervised training. Typically, these methods use a single objective to
train the entire network. But one distinct advantage of unsupervised over
supervised learning is that the former possesses more variety and freedom in
designing the objective. In this work, we explore new dimensions of
unsupervised learning by proposing the Progressive Stage-wise Learning (PSL)
framework. For a given unsupervised task, we design multilevel tasks and define
different learning stages for the deep network. Early learning stages are
forced to focus on lowlevel tasks while late stages are guided to extract
deeper information through harder tasks. We discover that by progressive
stage-wise learning, unsupervised feature representation can be effectively
enhanced. Our extensive experiments show that PSL consistently improves results
for the leading unsupervised learning methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Validation of Simulation-Based Testing: Bypassing Domain Shift with Label-to-Image Synthesis. (arXiv:2106.05549v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rosenzweig_J/0/1/0/all/0/1">Julia Rosenzweig</a>, <a href="http://arxiv.org/find/cs/1/au:+Brito_E/0/1/0/all/0/1">Eduardo Brito</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobialka_H/0/1/0/all/0/1">Hans-Ulrich Kobialka</a>, <a href="http://arxiv.org/find/cs/1/au:+Akila_M/0/1/0/all/0/1">Maram Akila</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_N/0/1/0/all/0/1">Nico M. Schmidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlicht_P/0/1/0/all/0/1">Peter Schlicht</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Jan David Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Huger_F/0/1/0/all/0/1">Fabian H&#xfc;ger</a>, <a href="http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1">Matthias Rottmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Houben_S/0/1/0/all/0/1">Sebastian Houben</a>, <a href="http://arxiv.org/find/cs/1/au:+Wirtz_T/0/1/0/all/0/1">Tim Wirtz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05549">
                                    <div class="article-summary-box-inner">
                                        <span>Many machine learning applications can benefit from simulated data for
systematic validation - in particular if real-life data is difficult to obtain
or annotate. However, since simulations are prone to domain shift w.r.t.
real-life data, it is crucial to verify the transferability of the obtained
results. We propose a novel framework consisting of a generative label-to-image
synthesis model together with different transferability measures to inspect to
what extent we can transfer testing results of semantic segmentation models
from synthetic data to equivalent real-life data. With slight modifications,
our approach is extendable to, e.g., general multi-class classification tasks.
Grounded on the transferability analysis, our approach additionally allows for
extensive testing by incorporating controlled simulations. We validate our
approach empirically on a semantic segmentation task on driving scenes.
Transferability is tested using correlation analysis of IoU and a learned
discriminator. Although the latter can distinguish between real-life and
synthetic tests, in the former we observe surprisingly strong correlations of
0.7 for both cars and pedestrians.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Tensor feature hallucination for few-shot learning. (arXiv:2106.05321v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lazarou_M/0/1/0/all/0/1">Michalis Lazarou</a>, <a href="http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1">Yannis Avrithis</a>, <a href="http://arxiv.org/find/cs/1/au:+Stathaki_T/0/1/0/all/0/1">Tania Stathaki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05321">
                                    <div class="article-summary-box-inner">
                                        <span>Few-shot classification addresses the challenge of classifying examples given
not just limited supervision but limited data as well. An attractive solution
is synthetic data generation. However, most such methods are overly
sophisticated, focusing on high-quality, realistic data in the input space. It
is unclear whether adapting them to the few-shot regime and using them for the
downstream task of classification is the right approach. Previous works on
synthetic data generation for few-shot classification focus on exploiting
complex models, e.g. a Wasserstein GAN with multiple regularizers or a network
that transfers latent diversities from known to novel classes.

We follow a different approach and investigate how a simple and
straightforward synthetic data generation method can be used effectively. We
make two contributions, namely we show that: (1) using a simple loss function
is more than enough for training a feature generator in the few-shot setting;
and (2) learning to generate tensor features instead of vector features is
superior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets show
that our method sets a new state of the art, outperforming more sophisticated
few-shot data augmentation methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AFAN: Augmented Feature Alignment Network for Cross-Domain Object Detection. (arXiv:2106.05499v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongsong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1">Shengcai Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1">Ling Shao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05499">
                                    <div class="article-summary-box-inner">
                                        <span>Unsupervised domain adaptation for object detection is a challenging problem
with many real-world applications. Unfortunately, it has received much less
attention than supervised object detection. Models that try to address this
task tend to suffer from a shortage of annotated training samples. Moreover,
existing methods of feature alignments are not sufficient to learn
domain-invariant representations. To address these limitations, we propose a
novel augmented feature alignment network (AFAN) which integrates intermediate
domain image generation and domain-adversarial training into a unified
framework. An intermediate domain image generator is proposed to enhance
feature alignments by domain-adversarial training with automatically generated
soft domain labels. The synthetic intermediate domain images progressively
bridge the domain divergence and augment the annotated source domain training
data. A feature pyramid alignment is designed and the corresponding feature
discriminator is used to align multi-scale convolutional features of different
semantic levels. Last but not least, we introduce a region feature alignment
and an instance discriminator to learn domain-invariant features for object
proposals. Our approach significantly outperforms the state-of-the-art methods
on standard benchmarks for both similar and dissimilar domain adaptations.
Further extensive experiments verify the effectiveness of each component and
demonstrate that the proposed network can learn domain-invariant
representations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Super-Resolution Image Reconstruction Based on Self-Calibrated Convolutional GAN. (arXiv:2106.05545v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yibo Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haidi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yiming Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shunyao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1">Mingliang Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05545">
                                    <div class="article-summary-box-inner">
                                        <span>With the effective application of deep learning in computer vision,
breakthroughs have been made in the research of super-resolution images
reconstruction. However, many researches have pointed out that the
insufficiency of the neural network extraction on image features may bring the
deteriorating of newly reconstructed image. On the other hand, the generated
pictures are sometimes too artificial because of over-smoothing. In order to
solve the above problems, we propose a novel self-calibrated convolutional
generative adversarial networks. The generator consists of feature extraction
and image reconstruction. Feature extraction uses self-calibrated convolutions,
which contains four portions, and each portion has specific functions. It can
not only expand the range of receptive fields, but also obtain long-range
spatial and inter-channel dependencies. Then image reconstruction is performed,
and finally a super-resolution image is reconstructed. We have conducted
thorough experiments on different datasets including set5, set14 and BSD100
under the SSIM evaluation method. The experimental results prove the
effectiveness of the proposed network.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving White-box Robustness of Pre-processing Defenses via Joint Adversarial Training. (arXiv:2106.05453v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Dawei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05453">
                                    <div class="article-summary-box-inner">
                                        <span>Deep neural networks (DNNs) are vulnerable to adversarial noise. A range of
adversarial defense techniques have been proposed to mitigate the interference
of adversarial noise, among which the input pre-processing methods are scalable
and show great potential to safeguard DNNs. However, pre-processing methods may
suffer from the robustness degradation effect, in which the defense reduces
rather than improving the adversarial robustness of a target model in a
white-box setting. A potential cause of this negative effect is that
adversarial training examples are static and independent to the pre-processing
model. To solve this problem, we investigate the influence of full adversarial
examples which are crafted against the full model, and find they indeed have a
positive impact on the robustness of defenses. Furthermore, we find that simply
changing the adversarial training examples in pre-processing methods does not
completely alleviate the robustness degradation effect. This is due to the
adversarial risk of the pre-processed model being neglected, which is another
cause of the robustness degradation effect. Motivated by above analyses, we
propose a method called Joint Adversarial Training based Pre-processing (JATP)
defense. Specifically, we formulate a feature similarity based adversarial risk
for the pre-processing model by using full adversarial examples found in a
feature space. Unlike standard adversarial training, we only update the
pre-processing model, which prompts us to introduce a pixel-wise loss to
improve its cross-model transferability. We then conduct a joint adversarial
training on the pre-processing model to minimize this overall risk. Empirical
results show that our method could effectively mitigate the robustness
degradation effect across different target models in comparison to previous
state-of-the-art approaches.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Optimizing Reusable Knowledge for Continual Learning via Metalearning. (arXiv:2106.05390v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1">Julio Hurtado</a>, <a href="http://arxiv.org/find/cs/1/au:+Raymond_Saez_A/0/1/0/all/0/1">Alain Raymond-Saez</a>, <a href="http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1">Alvaro Soto</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05390">
                                    <div class="article-summary-box-inner">
                                        <span>When learning tasks over time, artificial neural networks suffer from a
problem known as Catastrophic Forgetting (CF). This happens when the weights of
a network are overwritten during the training of a new task causing forgetting
of old information. To address this issue, we propose MetA Reusable Knowledge
or MARK, a new method that fosters weight reusability instead of overwriting
when learning a new task. Specifically, MARK keeps a set of shared weights
among tasks. We envision these shared weights as a common Knowledge Base (KB)
that is not only used to learn new tasks, but also enriched with new knowledge
as the model learns new tasks. Key components behind MARK are two-fold. On the
one hand, a metalearning approach provides the key mechanism to incrementally
enrich the KB with new knowledge and to foster weight reusability among tasks.
On the other hand, a set of trainable masks provides the key mechanism to
selectively choose from the KB relevant weights to solve each task. By using
MARK, we achieve state of the art results in several popular benchmarks,
surpassing the best performing methods in terms of average accuracy by over 10%
on the 20-Split-MiniImageNet dataset, while achieving almost zero forgetfulness
using 55% of the number of parameters. Furthermore, an ablation study provides
evidence that, indeed, MARK is learning reusable knowledge that is selectively
used by each task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline. (arXiv:2106.05304v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Ankit Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Law_H/0/1/0/all/0/1">Hei Law</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bowei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Newell_A/0/1/0/all/0/1">Alejandro Newell</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jia Deng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05304">
                                    <div class="article-summary-box-inner">
                                        <span>Processing point cloud data is an important component of many real-world
systems. As such, a wide variety of point-based approaches have been proposed,
reporting steady benchmark improvements over time. We study the key ingredients
of this progress and uncover two critical results. First, we find that
auxiliary factors like different evaluation schemes, data augmentation
strategies, and loss functions, which are independent of the model
architecture, make a large difference in performance. The differences are large
enough that they obscure the effect of architecture. When these factors are
controlled for, PointNet++, a relatively older network, performs competitively
with recent methods. Second, a very simple projection-based method, which we
refer to as SimpleView, performs surprisingly well. It achieves on par or
better results than sophisticated state-of-the-art methods on ModelNet40 while
being half the size of PointNet++. It also outperforms state-of-the-art methods
on ScanObjectNN, a real-world point cloud benchmark, and demonstrates better
cross-dataset generalization. Code is available at
https://github.com/princeton-vl/SimpleView.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-domain Contrastive Learning for Unsupervised Domain Adaptation. (arXiv:2106.05528v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zuxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1">Zejia Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jingjing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1">Guo-Jun Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05528">
                                    <div class="article-summary-box-inner">
                                        <span>Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a fully-labeled source domain to a different unlabeled target domain. Most
existing UDA methods learn domain-invariant feature representations by
minimizing feature distances across domains. In this work, we build upon
contrastive self-supervised learning to align features so as to reduce the
domain discrepancy between training and testing sets. Exploring the same set of
categories shared by both domains, we introduce a simple yet effective
framework CDCL, for domain alignment. In particular, given an anchor image from
one domain, we minimize its distances to cross-domain samples from the same
class relative to those from different categories. Since target labels are
unavailable, we use a clustering-based approach with carefully initialized
centers to produce pseudo labels. In addition, we demonstrate that CDCL is a
general framework and can be adapted to the data-free setting, where the source
data are unavailable during training, with minimal modification. We conduct
experiments on two widely used domain adaptation benchmarks, i.e., Office-31
and VisDA-2017, and demonstrate that CDCL achieves state-of-the-art performance
on both datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Plan2Scene: Converting Floorplans to 3D Scenes. (arXiv:2106.05375v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vidanapathirana_M/0/1/0/all/0/1">Madhawa Vidanapathirana</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qirui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1">Yasutaka Furukawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1">Angel X. Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1">Manolis Savva</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05375">
                                    <div class="article-summary-box-inner">
                                        <span>We address the task of converting a floorplan and a set of associated photos
of a residence into a textured 3D mesh model, a task which we call Plan2Scene.
Our system 1) lifts a floorplan image to a 3D mesh model; 2) synthesizes
surface textures based on the input photos; and 3) infers textures for
unobserved surfaces using a graph neural network architecture. To train and
evaluate our system we create indoor surface texture datasets, and augment a
dataset of floorplans and photos from prior work with rectified surface crops
and additional annotations. Our approach handles the challenge of producing
tileable textures for dominant surfaces such as floors, walls, and ceilings
from a sparse set of unaligned photos that only partially cover the residence.
Qualitative and quantitative evaluations show that our system produces
realistic 3D interior models, outperforming baseline approaches on a suite of
texture quality metrics and as measured by a holistic user study.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Raman spectral analysis of mixtures with one-dimensional convolutional neural network. (arXiv:2106.05316v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mozaffari_M/0/1/0/all/0/1">M. Hamed Mozaffari</a>, <a href="http://arxiv.org/find/cs/1/au:+Tay_L/0/1/0/all/0/1">Li-Lin Tay</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05316">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, the combination of robust one-dimensional convolutional neural
networks (1-D CNNs) and Raman spectroscopy has shown great promise in rapid
identification of unknown substances with good accuracy. Using this technique,
researchers can recognize a pure compound and distinguish it from unknown
substances in a mixture. The novelty of this approach is that the trained
neural network operates automatically without any pre- or post-processing of
data. Some studies have attempted to extend this technique to the
classification of pure compounds in an unknown mixture. However, the
application of 1-D CNNs has typically been restricted to binary classifications
of pure compounds. Here we will highlight a new approach in spectral
recognition and quantification of chemical components in a multicomponent
mixture. Two 1-D CNN models, RaMixNet I and II, have been developed for this
purpose. The former is for rapid classification of components in a mixture
while the latter is for quantitative determination of those constituents. In
the proposed method, there is no limit to the number of compounds in a mixture.
A data augmentation method is also introduced by adding random baselines to the
Raman spectra. The experimental results revealed that the classification
accuracy of RaMixNet I and II is 100% for analysis of unknown test mixtures; at
the same time, the RaMixNet II model may achieve a regression accuracy of 88%
for the quantification of each component.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DUET: Detection Utilizing Enhancement for Text in Scanned or Captured Documents. (arXiv:2106.05542v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jung_E/0/1/0/all/0/1">Eun-Soo Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1">HyeongGwan Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1">Kyusam Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_Y/0/1/0/all/0/1">Yongkeun Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1">Soonhwan Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Min Soo Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05542">
                                    <div class="article-summary-box-inner">
                                        <span>We present a novel deep neural model for text detection in document images.
For robust text detection in noisy scanned documents, the advantages of
multi-task learning are adopted by adding an auxiliary task of text
enhancement. Namely, our proposed model is designed to perform noise reduction
and text region enhancement as well as text detection. Moreover, we enrich the
training data for the model with synthesized document images that are fully
labeled for text detection and enhancement, thus overcome the insufficiency of
labeled document image data. For the effective exploitation of the synthetic
and real data, the training process is separated in two phases. The first phase
is training only synthetic data in a fully-supervised manner. Then real data
with only detection labels are added in the second phase. The enhancement task
for the real data is weakly-supervised with information from their detection
labels. Our methods are demonstrated in a real document dataset with
performances exceeding those of other text detection methods. Moreover,
ablations are conducted and the results confirm the effectiveness of the
synthetic data, auxiliary task, and weak-supervision. Whereas the existing text
detection studies mostly focus on the text in scenes, our proposed method is
optimized to the applications for the text in scanned documents.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">3D Semantic Mapping from Arthroscopy using Out-of-distribution Pose and Depth and In-distribution Segmentation Training. (arXiv:2106.05525v1 [cs.RO])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jonmohamadi_Y/0/1/0/all/0/1">Yaqub Jonmohamadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1">Shahnewaz Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fengbei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1">Jonathan Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Crawford_R/0/1/0/all/0/1">Ross Crawford</a>, <a href="http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1">Gustavo Carneiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1">Ajay K. Pandey</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05525">
                                    <div class="article-summary-box-inner">
                                        <span>Minimally invasive surgery (MIS) has many documented advantages, but the
surgeon&#x27;s limited visual contact with the scene can be problematic. Hence,
systems that can help surgeons navigate, such as a method that can produce a 3D
semantic map, can compensate for the limitation above. In theory, we can borrow
3D semantic mapping techniques developed for robotics, but this requires
finding solutions to the following challenges in MIS: 1) semantic segmentation,
2) depth estimation, and 3) pose estimation. In this paper, we propose the
first 3D semantic mapping system from knee arthroscopy that solves the three
challenges above. Using out-of-distribution non-human datasets, where pose
could be labeled, we jointly train depth+pose estimators using selfsupervised
and supervised losses. Using an in-distribution human knee dataset, we train a
fully-supervised semantic segmentation system to label arthroscopic image
pixels into femur, ACL, and meniscus. Taking testing images from human knees,
we combine the results from these two systems to automatically create 3D
semantic maps of the human knee. The result of this work opens the pathway to
the generation of intraoperative 3D semantic mapping, registration with
pre-operative data, and robotic-assisted arthroscopy</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Very Compact Clusters with Structural Regularization via Similarity and Connectivity. (arXiv:2106.05430v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1">Won Hwa Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05430">
                                    <div class="article-summary-box-inner">
                                        <span>Clustering algorithms have significantly improved along with Deep Neural
Networks which provide effective representation of data. Existing methods are
built upon deep autoencoder and self-training process that leverages the
distribution of cluster assignments of samples. However, as the fundamental
objective of the autoencoder is focused on efficient data reconstruction, the
learnt space may be sub-optimal for clustering. Moreover, it requires highly
effective codes (i.e., representation) of data, otherwise the initial cluster
centers often cause stability issues during self-training. Many
state-of-the-art clustering algorithms use convolution operation to extract
efficient codes but their applications are limited to image data. In this
regard, we propose an end-to-end deep clustering algorithm, i.e., Very Compact
Clusters (VCC), for the general datasets, which takes advantage of
distributions of local relationships of samples near the boundary of clusters,
so that they can be properly separated and pulled to cluster centers to form
compact clusters. Experimental results on various datasets illustrate that our
proposed approach achieves better clustering performance over most of the
state-of-the-art clustering methods, and the data embeddings learned by VCC
without convolution for image data are even comparable with specialized
convolutional methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Visual Sensor Pose Optimisation Using Rendering-based Visibility Models for Robust Cooperative Perception. (arXiv:2106.05308v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Arnold_E/0/1/0/all/0/1">Eduardo Arnold</a>, <a href="http://arxiv.org/find/cs/1/au:+Mozaffari_S/0/1/0/all/0/1">Sajjad Mozaffari</a>, <a href="http://arxiv.org/find/cs/1/au:+Dianati_M/0/1/0/all/0/1">Mehrdad Dianati</a>, <a href="http://arxiv.org/find/cs/1/au:+Jennings_P/0/1/0/all/0/1">Paul Jennings</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05308">
                                    <div class="article-summary-box-inner">
                                        <span>Visual Sensor Networks can be used in a variety of perception applications
such as infrastructure support for autonomous driving in complex road segments.
The pose of the sensors in such networks directly determines the coverage of
the environment and objects therein, which impacts the performance of
applications such as object detection and tracking. Existing sensor pose
optimisation methods in the literature either maximise the coverage of ground
surfaces, or consider the visibility of the target objects as binary variables,
which cannot represent various degrees of visibility. Such formulations cannot
guarantee the visibility of the target objects as they fail to consider
occlusions. This paper proposes two novel sensor pose optimisation methods,
based on gradient-ascent and Integer Programming techniques, which maximise the
visibility of multiple target objects in cluttered environments. Both methods
consider a realistic visibility model based on a rendering engine that provides
pixel-level visibility information about the target objects. The proposed
methods are evaluated in a complex environment and compared to existing methods
in the literature. The evaluation results indicate that explicitly modelling
the visibility of target objects is critical to avoid occlusions in cluttered
environments. Furthermore, both methods significantly outperform existing
methods in terms of object visibility.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unsupervised Video Person Re-identification via Noise and Hard frame Aware Clustering. (arXiv:2106.05441v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1">Pengyu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamasaki_T/0/1/0/all/0/1">Toshihiko Yamasaki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05441">
                                    <div class="article-summary-box-inner">
                                        <span>Unsupervised video-based person re-identification (re-ID) methods extract
richer features from video tracklets than image-based ones. The
state-of-the-art methods utilize clustering to obtain pseudo-labels and train
the models iteratively. However, they underestimate the influence of two kinds
of frames in the tracklet: 1) noise frames caused by detection errors or heavy
occlusions exist in the tracklet, which may be allocated with unreliable labels
during clustering; 2) the tracklet also contains hard frames caused by pose
changes or partial occlusions, which are difficult to distinguish but
informative. This paper proposes a Noise and Hard frame Aware Clustering (NHAC)
method. NHAC consists of a graph trimming module and a node re-sampling module.
The graph trimming module obtains stable graphs by removing noise frame nodes
to improve the clustering accuracy. The node re-sampling module enhances the
training of hard frame nodes to learn rich tracklet information. Experiments
conducted on two video-based datasets demonstrate the effectiveness of the
proposed NHAC under the unsupervised re-ID setting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation. (arXiv:2106.05487v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1">Khoa Tuan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_G/0/1/0/all/0/1">Ganghee Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1">Won-ki Jeong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05487">
                                    <div class="article-summary-box-inner">
                                        <span>The segmentation of nanoscale electron microscopy (EM) images is crucial but
challenging in connectomics. Recent advances in deep learning have demonstrated
the significant potential of automatic segmentation for tera-scale EM images.
However, none of the existing segmentation methods are error-free, and they
require proofreading, which is typically implemented as an interactive,
semi-automatic process via manual intervention. Herein, we propose a fully
automatic proofreading method based on reinforcement learning. The main idea is
to model the human decision process in proofreading using a reinforcement agent
to achieve fully automatic proofreading. We systematically design the proposed
system by combining multiple reinforcement learning agents in a hierarchical
manner, where each agent focuses only on a specific task while preserving
dependency between agents. Furthermore, we also demonstrate that the episodic
task setting of reinforcement learning can efficiently manage a combination of
merge and split errors concurrently presented in the input. We demonstrate the
efficacy of the proposed system by comparing it with state-of-the-art
proofreading methods using various testing examples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data augmentation to improve robustness of image captioning solutions. (arXiv:2106.05437v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bujimalla_S/0/1/0/all/0/1">Shashank Bujimalla</a>, <a href="http://arxiv.org/find/cs/1/au:+Subedar_M/0/1/0/all/0/1">Mahesh Subedar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tickoo_O/0/1/0/all/0/1">Omesh Tickoo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05437">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we study the impact of motion blur, a common quality flaw in
real world images, on a state-of-the-art two-stage image captioning solution,
and notice a degradation in solution performance as blur intensity increases.
We investigate techniques to improve the robustness of the solution to motion
blur using training data augmentation at each or both stages of the solution,
i.e., object detection and captioning, and observe improved results. In
particular, augmenting both the stages reduces the CIDEr-D degradation for high
motion blur intensity from 68.7 to 11.7 on MS COCO dataset, and from 22.4 to
6.8 on Vizwiz dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.IR"">cs.IR updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity. (arXiv:2106.01300v2 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1">Tao Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fangzhao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chuhan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yongfeng Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01300">
                                    <div class="article-summary-box-inner">
                                        <span>Personalized news recommendation methods are widely used in online news
services. These methods usually recommend news based on the matching between
news content and user interest inferred from historical behaviors. However,
these methods usually have difficulties in making accurate recommendations to
cold-start users, and tend to recommend similar news with those users have
read. In general, popular news usually contain important information and can
attract users with different interests. Besides, they are usually diverse in
content and topic. Thus, in this paper we propose to incorporate news
popularity information to alleviate the cold-start and diversity problems for
personalized news recommendation. In our method, the ranking score for
recommending a candidate news to a target user is the combination of a
personalized matching score and a news popularity score. The former is used to
capture the personalized user interest in news. The latter is used to measure
time-aware popularity of candidate news, which is predicted based on news
content, recency, and real-time CTR using a unified framework. Besides, we
propose a popularity-aware user encoder to eliminate the popularity bias in
user behaviors for accurate interest modeling. Experiments on two real-world
datasets show our method can effectively improve the accuracy and diversity for
news recommendation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Position-wise Interaction Network for CTR Prediction. (arXiv:2106.05482v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jianqiang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1">Ke Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1">Qingtao Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingjian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1">Yi Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1">Jia Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1">Jun Lei</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05482">
                                    <div class="article-summary-box-inner">
                                        <span>Click-through rate (CTR) prediction plays an important role in online
advertising and recommender systems. In practice, the training of CTR models
depends on click data which is intrinsically biased towards higher positions
since higher position has higher CTR by nature. Existing methods such as actual
position training with fixed position inference and inverse propensity weighted
training with no position inference alleviate the bias problem to some extend.
However, the different treatment of position information between training and
inference will inevitably lead to inconsistency and sub-optimal online
performance. Meanwhile, the basic assumption of these methods, i.e., the click
probability is the product of examination probability and relevance
probability, is oversimplified and insufficient to model the rich interaction
between position and other information. In this paper, we propose a Deep
Position-wise Interaction Network (DPIN) to efficiently combine all candidate
items and positions for estimating CTR at each position, achieving consistency
between offline and online as well as modeling the deep non-linear interaction
among position, user, context and item under the limit of serving performance.
Following our new treatment to the position bias in CTR prediction, we propose
a new evaluation metrics named PAUC (position-wise AUC) that is suitable for
measuring the ranking quality at a given position. Through extensive
experiments on a real world dataset, we show empirically that our method is
both effective and efficient in solving position bias problem. We have also
deployed our method in production and observed statistically significant
improvement over a highly optimized baseline in a rigorous A/B test.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PARADE: Passage Representation Aggregation for Document Reranking. (arXiv:2008.09093v2 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Canjia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1">Andrew Yates</a>, <a href="http://arxiv.org/find/cs/1/au:+MacAvaney_S/0/1/0/all/0/1">Sean MacAvaney</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Ben He</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yingfei Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09093">
                                    <div class="article-summary-box-inner">
                                        <span>Pretrained transformer models, such as BERT and T5, have shown to be highly
effective at ad-hoc passage and document ranking. Due to inherent sequence
length limits of these models, they need to be run over a document&#x27;s passages,
rather than processing the entire document sequence at once. Although several
approaches for aggregating passage-level signals have been proposed, there has
yet to be an extensive comparison of these techniques. In this work, we explore
strategies for aggregating relevance signals from a document&#x27;s passages into a
final ranking score. We find that passage representation aggregation techniques
can significantly improve over techniques proposed in prior work, such as
taking the maximum passage score. We call this new approach PARADE. In
particular, PARADE can significantly improve results on collections with broad
information needs where relevance signals can be spread throughout the document
(such as TREC Robust04 and GOV2). Meanwhile, less complex aggregation
techniques may work better on collections with an information need that can
often be pinpointed to a single passage (such as TREC DL and TREC Genomics). We
also conduct efficiency analyses, and highlight several strategies for
improving transformer-based aggregation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Disentangled Self-Attentive Neural Networks for Click-Through Rate Prediction. (arXiv:2101.03654v2 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yichen Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yanqiao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Feng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shu Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.03654">
                                    <div class="article-summary-box-inner">
                                        <span>Click-through rate (CTR) prediction, whose aim is to predict the probability
of whether a user will click on an item, is an essential task for many online
applications. Due to the nature of data sparsity and high dimensionality in CTR
prediction, a key to making effective prediction is to model high-order feature
interaction among feature fields. To explicitly model high-order feature
interaction, an efficient way is to perform inner product of feature embeddings
with self-attentive neural networks. To better model complex feature
interaction, in this paper we propose a novel DisentanglEd Self-atTentIve
NEtwork (DESTINE) framework for CTR prediction that explicitly decouples the
computation of unary importance from pairwise interaction. Specifically, the
unary term models the general impact of one feature on all other features,
whereas the whitened pairwise interaction term contributes to learning the pure
importance score for each feature interaction. We conduct extensive experiments
framework using two real-world benchmark datasets. The results show that
DESTINE not only maintains computational efficiency but obtains performance
improvements over state-of-the-art baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Linguistically Informed Masking for Representation Learning in the Patent Domain. (arXiv:2106.05768v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Althammer_S/0/1/0/all/0/1">Sophia Althammer</a>, <a href="http://arxiv.org/find/cs/1/au:+Buckley_M/0/1/0/all/0/1">Mark Buckley</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1">Sebastian Hofst&#xe4;tter</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1">Allan Hanbury</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05768">
                                    <div class="article-summary-box-inner">
                                        <span>Domain-specific contextualized language models have demonstrated substantial
effectiveness gains for domain-specific downstream tasks, like similarity
matching, entity recognition or information retrieval. However successfully
applying such models in highly specific language domains requires domain
adaptation of the pre-trained models. In this paper we propose the empirically
motivated Linguistically Informed Masking (LIM) method to focus
domain-adaptative pre-training on the linguistic patterns of patents, which use
a highly technical sublanguage. We quantify the relevant differences between
patent, scientific and general-purpose language and demonstrate for two
different language models (BERT and SciBERT) that domain adaptation with LIM
leads to systematically improved representations by evaluating the performance
of the domain-adapted representations of patent language on two independent
downstream tasks, the IPC classification and similarity matching. We
demonstrate the impact of balancing the learning from different information
sources during domain adaptation for the patent domain. We make the source code
as well as the domain-adaptive pre-trained patent language models publicly
available at https://github.com/sophiaalthammer/patent-lim.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GRASP: Graph Alignment through Spectral Signatures. (arXiv:2106.05729v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hermanns_J/0/1/0/all/0/1">Judith Hermanns</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsitsulin_A/0/1/0/all/0/1">Anton Tsitsulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Munkhoeva_M/0/1/0/all/0/1">Marina Munkhoeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1">Alex Bronstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Mottin_D/0/1/0/all/0/1">Davide Mottin</a>, <a href="http://arxiv.org/find/cs/1/au:+Karras_P/0/1/0/all/0/1">Panagiotis Karras</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05729">
                                    <div class="article-summary-box-inner">
                                        <span>What is the best way to match the nodes of two graphs? This graph alignment
problem generalizes graph isomorphism and arises in applications from social
network analysis to bioinformatics. Some solutions assume that auxiliary
information on known matches or node or edge attributes is available, or
utilize arbitrary graph features. Such methods fare poorly in the pure form of
the problem, in which only graph structures are given. Other proposals
translate the problem to one of aligning node embeddings, yet, by doing so,
provide only a single-scale view of the graph.In this paper, we transfer the
shape-analysis concept of functional maps from the continuous to the discrete
case, and treat the graph alignment problem as a special case of the problem of
finding a mapping between functions on graphs. We present GRASP, a method that
first establishes a correspondence between functions derived from Laplacian
matrix eigenvectors, which capture multiscale structural characteristics,and
then exploits this correspondence to align nodes. Our experimental study,
featuring noise levels higher than anything used in previous studies, shows
that GRASP outperforms state-of-the-art methods for graph alignment across
noise levels and graph types.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dynamic Search -- Optimizing the Game of Information Seeking. (arXiv:1909.12425v2 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhiwen Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Grace Hui Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.12425">
                                    <div class="article-summary-box-inner">
                                        <span>This article presents the emerging topic of dynamic search (DS). To position
dynamic search in a larger research landscape, the article discusses in detail
its relationship to related research topics and disciplines. The article
reviews approaches to modeling dynamics during information seeking, with an
emphasis on Reinforcement Learning (RL)-enabled methods. Details are given for
how different approaches are used to model interactions among the human user,
the search system, and the environment. The paper ends with a review of
evaluations of dynamic search systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Citation Recommendation for Research Papers via Knowledge Graphs. (arXiv:2106.05633v1 [cs.DL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Brack_A/0/1/0/all/0/1">Arthur Brack</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoppe_A/0/1/0/all/0/1">Anett Hoppe</a>, <a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1">Ralph Ewerth</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05633">
                                    <div class="article-summary-box-inner">
                                        <span>Citation recommendation for research papers is a valuable task that can help
researchers improve the quality of their work by suggesting relevant related
work. Current approaches for this task rely primarily on the text of the papers
and the citation network. In this paper, we propose to exploit an additional
source of information, namely research knowledge graphs (KG) that interlink
research papers based on mentioned scientific concepts. Our experimental
results demonstrate that the combination of information from research KGs with
existing state-of-the-art approaches is beneficial. Experimental results are
presented for the STM-KG (STM: Science, Technology, Medicine), which is an
automatically populated knowledge graph based on the scientific concepts
extracted from papers of ten domains. The proposed approach outperforms the
state of the art with a mean average precision of 20.6% (+0.8) for the top-50
retrieved results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Analyzing Non-Textual Content Elements to Detect Academic Plagiarism. (arXiv:2106.05764v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1">Norman Meuschke</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05764">
                                    <div class="article-summary-box-inner">
                                        <span>Identifying academic plagiarism is a pressing problem, among others, for
research institutions, publishers, and funding organizations. Detection
approaches proposed so far analyze lexical, syntactical, and semantic text
similarity. These approaches find copied, moderately reworded, and literally
translated text. However, reliably detecting disguised plagiarism, such as
strong paraphrases, sense-for-sense translations, and the reuse of non-textual
content and ideas, is an open research problem.

The thesis addresses this problem by proposing plagiarism detection
approaches that implement a different concept: analyzing non-textual content in
academic documents, specifically citations, images, and mathematical content.

To validate the effectiveness of the proposed detection approaches, the
thesis presents five evaluations that use real cases of academic plagiarism and
exploratory searches for unknown cases.

The evaluation results show that non-textual content elements contain a high
degree of semantic information, are language-independent, and largely immutable
to the alterations that authors typically perform to conceal plagiarism.
Analyzing non-textual content complements text-based detection approaches and
increases the detection effectiveness, particularly for disguised forms of
academic plagiarism.

To demonstrate the benefit of combining non-textual and text-based detection
methods, the thesis describes the first plagiarism detection system that
integrates the analysis of citation-based, image-based, math-based, and
text-based document similarity. The system&#x27;s user interface employs
visualizations that significantly reduce the effort and time users must invest
in examining content similarity.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering. (arXiv:2106.05346v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1">Devendra Singh Sachan</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1">Siva Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1">William Hamilton</a>, <a href="http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1">Chris Dyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1">Dani Yogatama</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05346">
                                    <div class="article-summary-box-inner">
                                        <span>We present an end-to-end differentiable training method for
retrieval-augmented open-domain question answering systems that combine
information from multiple retrieved documents when generating answers. We model
retrieval decisions as latent variables over sets of relevant documents. Since
marginalizing over sets of retrieved documents is computationally hard, we
approximate this using an expectation-maximization algorithm. We iteratively
estimate the value of our latent variable (the set of relevant documents for a
given question) and then use this estimate to update the retriever and reader
parameters. We hypothesize that such end-to-end training allows training
signals to flow to the reader and then to the retriever better than staged-wise
training. This results in a retriever that is able to select more relevant
documents for a question and a reader that is trained on more accurate
documents to generate an answer. Experiments on three benchmark datasets
demonstrate that our proposed method outperforms all existing approaches of
comparable size by 2-3% absolute exact match points, achieving new
state-of-the-art results. Our results also demonstrate the feasibility of
learning to retrieve to improve answer generation without explicit supervision
of retrieval decisions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training. (arXiv:2106.05630v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1">Mingliang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_Z/0/1/0/all/0/1">Zeqian Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1">Tao Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tie-Yan Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05630">
                                    <div class="article-summary-box-inner">
                                        <span>Symbolic music understanding, which refers to the understanding of music from
the symbolic data (e.g., MIDI format, but not audio), covers many music
applications such as genre classification, emotion classification, and music
pieces matching. While good music representations are beneficial for these
applications, the lack of training data hinders representation learning.
Inspired by the success of pre-training models in natural language processing,
in this paper, we develop MusicBERT, a large-scale pre-trained model for music
understanding. To this end, we construct a large-scale symbolic music corpus
that contains more than 1 million music songs. Since symbolic music contains
more structural (e.g., bar, position) and diverse information (e.g., tempo,
instrument, and pitch), simply adopting the pre-training techniques from NLP to
symbolic music only brings marginal gains. Therefore, we design several
mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to
enhance pre-training with symbolic music data. Experiments demonstrate the
advantages of MusicBERT on four music understanding tasks, including melody
completion, accompaniment suggestion, genre classification, and style
classification. Ablation studies also verify the effectiveness of our designs
of OctupleMIDI encoding and bar-level masking strategy in MusicBERT.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.LG"">cs.LG updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Super-Resolution Image Reconstruction Based on Self-Calibrated Convolutional GAN. (arXiv:2106.05545v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1">Yibo Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1">Haidi Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1">Yiming Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1">Shunyao Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1">Mingliang Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05545">
                                    <div class="article-summary-box-inner">
                                        <span>With the effective application of deep learning in computer vision,
breakthroughs have been made in the research of super-resolution images
reconstruction. However, many researches have pointed out that the
insufficiency of the neural network extraction on image features may bring the
deteriorating of newly reconstructed image. On the other hand, the generated
pictures are sometimes too artificial because of over-smoothing. In order to
solve the above problems, we propose a novel self-calibrated convolutional
generative adversarial networks. The generator consists of feature extraction
and image reconstruction. Feature extraction uses self-calibrated convolutions,
which contains four portions, and each portion has specific functions. It can
not only expand the range of receptive fields, but also obtain long-range
spatial and inter-channel dependencies. Then image reconstruction is performed,
and finally a super-resolution image is reconstructed. We have conducted
thorough experiments on different datasets including set5, set14 and BSD100
under the SSIM evaluation method. The experimental results prove the
effectiveness of the proposed network.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Anatomy X-Net: A Semi-Supervised Anatomy Aware Convolutional Neural Network for Thoracic Disease Classification. (arXiv:2106.05915v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Kamal_U/0/1/0/all/0/1">Uday Kamal</a>, <a href="http://arxiv.org/find/eess/1/au:+Zunaed_M/0/1/0/all/0/1">Mohammad Zunaed</a>, <a href="http://arxiv.org/find/eess/1/au:+Nizam_N/0/1/0/all/0/1">Nusrat Binta Nizam</a>, <a href="http://arxiv.org/find/eess/1/au:+Hasan_T/0/1/0/all/0/1">Taufiq Hasan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05915">
                                    <div class="article-summary-box-inner">
                                        <span>Thoracic disease detection from chest radiographs using deep learning methods
has been an active area of research in the last decade. Most previous methods
attempt to focus on the diseased organs of the image by identifying spatial
regions responsible for significant contributions to the model&#x27;s prediction. In
contrast, expert radiologists first locate the prominent anatomical structures
before determining if those regions are anomalous. Therefore, integrating
anatomical knowledge within deep learning models could bring substantial
improvement in automatic disease classification. This work proposes an
anatomy-aware attention-based architecture named Anatomy X-Net, that
prioritizes the spatial features guided by the pre-identified anatomy regions.
We leverage a semi-supervised learning method using the JSRT dataset containing
organ-level annotation to obtain the anatomical segmentation masks (for lungs
and heart) for the NIH and CheXpert datasets. The proposed Anatomy X-Net uses
the pre-trained DenseNet-121 as the backbone network with two corresponding
structured modules, the Anatomy Aware Attention (AAA) and Probabilistic
Weighted Average Pooling (PWAP), in a cohesive framework for anatomical
attention learning. Our proposed method sets new state-of-the-art performance
on the official NIH test set with an AUC score of 0.8439, proving the efficacy
of utilizing the anatomy segmentation knowledge to improve the thoracic disease
classification. Furthermore, the Anatomy X-Net yields an averaged AUC of 0.9020
on the Stanford CheXpert dataset, improving on existing methods that
demonstrate the generalizability of the proposed framework.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition. (arXiv:2106.05933v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1">Cheng-I Jeff Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Alexander H. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1">Shiyu Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Yi-Lun Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1">Yung-Sung Chuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1">Kaizhi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1">Sameer Khurana</a>, <a href="http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1">David Cox</a>, <a href="http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1">James Glass</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05933">
                                    <div class="article-summary-box-inner">
                                        <span>Recent work on speech self-supervised learning (speech SSL) demonstrated the
benefits of scale in learning rich and transferable representations for
Automatic Speech Recognition (ASR) with limited parallel data. It is then
natural to investigate the existence of sparse and transferrable subnetworks in
pre-trained speech SSL models that can achieve even better low-resource ASR
performance. However, directly applying widely adopted pruning methods such as
the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost
needed. Moreover, contrary to what LTH predicts, the discovered subnetworks
yield minimal performance gain compared to the original dense network. In this
work, we propose Prune-Adjust- Re-Prune (PARP), which discovers and finetunes
subnetworks for much better ASR performance, while only requiring a single
downstream finetuning run. PARP is inspired by our surprising observation that
subnetworks pruned for pre-training tasks only needed to be slightly adjusted
to achieve a sizeable performance boost in downstream ASR tasks. Extensive
experiments on low-resource English and multi-lingual ASR show (1) sparse
subnetworks exist in pre-trained speech SSL, and (2) the computational
advantage and performance gain of PARP over baseline pruning methods. On the
10min Librispeech split without LM decoding, PARP discovers subnetworks from
wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full
model. We demonstrate PARP mitigates performance degradation in cross-lingual
mask transfer, and investigate the possibility of discovering a single
subnetwork for 10 spoken languages in one run.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights. (arXiv:2106.05852v1 [eess.AS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Adiga_D/0/1/0/all/0/1">Devaraja Adiga</a>, <a href="http://arxiv.org/find/eess/1/au:+Kumar_R/0/1/0/all/0/1">Rishabh Kumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Krishna_A/0/1/0/all/0/1">Amrith Krishna</a>, <a href="http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1">Preethi Jyothi</a>, <a href="http://arxiv.org/find/eess/1/au:+Ramakrishnan_G/0/1/0/all/0/1">Ganesh Ramakrishnan</a>, <a href="http://arxiv.org/find/eess/1/au:+Goyal_P/0/1/0/all/0/1">Pawan Goyal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05852">
                                    <div class="article-summary-box-inner">
                                        <span>Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the
various linguistic peculiarities present in the language. The Sanskrit language
is lexically productive, undergoes euphonic assimilation of phones at the word
boundaries and exhibits variations in spelling conventions and in
pronunciations. In this work, we propose the first large scale study of
automatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact
of unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR
dataset for Sanskrit, which faithfully captures several of the linguistic
characteristics expressed by the language. We investigate the role of different
acoustic model and language model units in ASR systems for Sanskrit. We also
propose a new modelling unit, inspired by the syllable level unit selection,
that captures character sequences from one vowel in the word to the next vowel.
We also highlight the importance of choosing graphemic representations for
Sanskrit and show the impact of this choice on word error rates (WER). Finally,
we extend these insights from Sanskrit ASR for building ASR systems in two
other Indic languages, Gujarati and Telugu. For both these languages, our
experimental results show that the use of phonetic based graphemic
representations in ASR results in performance improvements as compared to ASR
systems that use native scripts.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations. (arXiv:2106.05967v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gansbeke_W/0/1/0/all/0/1">Wouter Van Gansbeke</a>, <a href="http://arxiv.org/find/cs/1/au:+Vandenhende_S/0/1/0/all/0/1">Simon Vandenhende</a>, <a href="http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1">Stamatios Georgoulis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05967">
                                    <div class="article-summary-box-inner">
                                        <span>Contrastive self-supervised learning has outperformed supervised pretraining
on many downstream tasks like segmentation and object detection. However,
current methods are still primarily applied to curated datasets like ImageNet.
In this paper, we first study how biases in the dataset affect existing
methods. Our results show that current contrastive approaches work surprisingly
well across: (i) object- versus scene-centric, (ii) uniform versus long-tailed
and (iii) general versus domain-specific datasets. Second, given the generality
of the approach, we try to realize further gains with minor modifications. We
show that learning additional invariances -- through the use of multi-scale
cropping, stronger augmentations and nearest neighbors -- improves the
representations. Finally, we observe that MoCo learns spatially structured
representations when trained with a multi-crop strategy. The representations
can be used for semantic segment retrieval and video instance segmentation
without finetuning. Moreover, the results are on par with specialized models.
We hope this work will serve as a useful study for other researchers. The code
and models will be available at
https://github.com/wvangansbeke/Revisiting-Contrastive-SSL.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Does Knowledge Distillation Really Work?. (arXiv:2106.05945v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Stanton_S/0/1/0/all/0/1">Samuel Stanton</a>, <a href="http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1">Pavel Izmailov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirichenko_P/0/1/0/all/0/1">Polina Kirichenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1">Alexander A. Alemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1">Andrew Gordon Wilson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05945">
                                    <div class="article-summary-box-inner">
                                        <span>Knowledge distillation is a popular technique for training a small student
network to emulate a larger teacher model, such as an ensemble of networks. We
show that while knowledge distillation can improve student generalization, it
does not typically work as it is commonly understood: there often remains a
surprisingly large discrepancy between the predictive distributions of the
teacher and the student, even in cases when the student has the capacity to
perfectly match the teacher. We identify difficulties in optimization as a key
reason for why the student is unable to match the teacher. We also show how the
details of the dataset used for distillation play a role in how closely the
student matches the teacher -- and that more closely matching the teacher
paradoxically does not always lead to better student generalization.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FetReg: Placental Vessel Segmentation and Registration in Fetoscopy Challenge Dataset. (arXiv:2106.05923v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1">Sophia Bano</a>, <a href="http://arxiv.org/find/cs/1/au:+Casella_A/0/1/0/all/0/1">Alessandro Casella</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1">Francisco Vasconcelos</a>, <a href="http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1">Sara Moccia</a>, <a href="http://arxiv.org/find/cs/1/au:+Attilakos_G/0/1/0/all/0/1">George Attilakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Wimalasundera_R/0/1/0/all/0/1">Ruwan Wimalasundera</a>, <a href="http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1">Anna L. David</a>, <a href="http://arxiv.org/find/cs/1/au:+Paladini_D/0/1/0/all/0/1">Dario Paladini</a>, <a href="http://arxiv.org/find/cs/1/au:+Deprest_J/0/1/0/all/0/1">Jan Deprest</a>, <a href="http://arxiv.org/find/cs/1/au:+Mattos_L/0/1/0/all/0/1">Leonardo S. Mattos</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1">Danail Stoyanov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05923">
                                    <div class="article-summary-box-inner">
                                        <span>Fetoscopy laser photocoagulation is a widely used procedure for the treatment
of Twin-to-Twin Transfusion Syndrome (TTTS), that occur in mono-chorionic
multiple pregnancies due to placental vascular anastomoses. This procedure is
particularly challenging due to limited field of view, poor manoeuvrability of
the fetoscope, poor visibility due to fluid turbidity, variability in light
source, and unusual position of the placenta. This may lead to increased
procedural time and incomplete ablation, resulting in persistent TTTS.
Computer-assisted intervention may help overcome these challenges by expanding
the fetoscopic field of view through video mosaicking and providing better
visualization of the vessel network. However, the research and development in
this domain remain limited due to unavailability of high-quality data to encode
the intra- and inter-procedure variability. Through the Fetoscopic Placental
Vessel Segmentation and Registration (FetReg) challenge, we present a
large-scale multi-centre dataset for the development of generalized and robust
semantic segmentation and video mosaicking algorithms for the fetal environment
with a focus on creating drift-free mosaics from long duration fetoscopy
videos. In this paper, we provide an overview of the FetReg dataset, challenge
tasks, evaluation metrics and baseline methods for both segmentation and
registration. Baseline methods results on the FetReg dataset shows that our
dataset poses interesting challenges, which can be modelled and competed for
through our crowd-sourcing initiative of the FetReg challenge.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Second look at Exponential and Cosine Step Sizes: Simplicity, Adaptivity, and Performance. (arXiv:2002.05273v4 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1">Xiaoyu Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhuang_Z/0/1/0/all/0/1">Zhenxun Zhuang</a>, <a href="http://arxiv.org/find/stat/1/au:+Orabona_F/0/1/0/all/0/1">Francesco Orabona</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.05273">
                                    <div class="article-summary-box-inner">
                                        <span>Stochastic Gradient Descent (SGD) is a popular tool in training large-scale
machine learning models. Its performance, however, is highly variable,
depending crucially on the choice of the step sizes. Accordingly, a variety of
strategies for tuning the step sizes have been proposed, ranging from
coordinate-wise approaches (a.k.a. &#x60;&#x60;adaptive&#x27;&#x27; step sizes) to sophisticated
heuristics to change the step size in each iteration. In this paper, we study
two step size schedules whose power has been repeatedly confirmed in practice:
the exponential and the cosine step sizes. For the first time, we provide
theoretical support for them proving convergence rates for smooth non-convex
functions, with and without the Polyak-\L{}ojasiewicz (PL) condition. Moreover,
we show the surprising property that these two strategies are \emph{adaptive}
to the noise level in the stochastic gradients of PL functions. That is,
contrary to polynomial step sizes, they achieve almost optimal performance
without needing to know the noise level nor tuning their hyperparameters based
on it. Finally, we conduct a fair and comprehensive empirical evaluation of
real-world datasets with deep learning architectures. Results show that, even
if only requiring at most two hyperparameters to tune, these two strategies
best or match the performance of various finely-tuned state-of-the-art
strategies.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">EventDrop: data augmentation for event-based learning. (arXiv:2106.05836v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gu_F/0/1/0/all/0/1">Fuqiang Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sng_W/0/1/0/all/0/1">Weicong Sng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xuke Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fangwen Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05836">
                                    <div class="article-summary-box-inner">
                                        <span>The advantages of event-sensing over conventional sensors (e.g., higher
dynamic range, lower time latency, and lower power consumption) have spurred
research into machine learning for event data. Unsurprisingly, deep learning
has emerged as a competitive methodology for learning with event sensors; in
typical setups, discrete and asynchronous events are first converted into
frame-like tensors on which standard deep networks can be applied. However,
over-fitting remains a challenge, particularly since event datasets remain
small relative to conventional datasets (e.g., ImageNet). In this paper, we
introduce EventDrop, a new method for augmenting asynchronous event data to
improve the generalization of deep models. By dropping events selected with
various strategies, we are able to increase the diversity of training data
(e.g., to simulate various levels of occlusion). From a practical perspective,
EventDrop is simple to implement and computationally low-cost. Experiments on
two event datasets (N-Caltech101 and N-Cars) demonstrate that EventDrop can
significantly improve the generalization performance across a variety of deep
networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Improving Generalization in Meta-learning via Task Augmentation. (arXiv:2007.13040v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Huaxiu Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Longkai Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Linjun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Ying Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1">Li Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Junzhou Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenhui Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.13040">
                                    <div class="article-summary-box-inner">
                                        <span>Meta-learning has proven to be a powerful paradigm for transferring the
knowledge from previous tasks to facilitate the learning of a novel task.
Current dominant algorithms train a well-generalized model initialization which
is adapted to each task via the support set. The crux lies in optimizing the
generalization capability of the initialization, which is measured by the
performance of the adapted model on the query set of each task. Unfortunately,
this generalization measure, evidenced by empirical results, pushes the
initialization to overfit the meta-training tasks, which significantly impairs
the generalization and adaptation to novel tasks. To address this issue, we
actively augment a meta-training task with &quot;more data&quot; when evaluating the
generalization. Concretely, we propose two task augmentation methods, including
MetaMix and Channel Shuffle. MetaMix linearly combines features and labels of
samples from both the support and query sets. For each class of samples,
Channel Shuffle randomly replaces a subset of their channels with the
corresponding ones from a different class. Theoretical studies show how task
augmentation improves the generalization of meta-learning. Moreover, both
MetaMix and Channel Shuffle outperform state-of-the-art results by a large
margin across many datasets and are compatible with existing meta-learning
algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Semi-Supervised Ordinal Regression Based on Empirical Risk Minimization. (arXiv:1901.11351v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tsuchiya_T/0/1/0/all/0/1">Taira Tsuchiya</a>, <a href="http://arxiv.org/find/cs/1/au:+Charoenphakdee_N/0/1/0/all/0/1">Nontawat Charoenphakdee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1">Issei Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1">Masashi Sugiyama</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1901.11351">
                                    <div class="article-summary-box-inner">
                                        <span>Ordinal regression is aimed at predicting an ordinal class label. In this
paper, we consider its semi-supervised formulation, in which we have unlabeled
data along with ordinal-labeled data to train an ordinal regressor. There are
several metrics to evaluate the performance of ordinal regression, such as the
mean absolute error, mean zero-one error, and mean squared error. However, the
existing studies do not take the evaluation metric into account, have a
restriction on the model choice, and have no theoretical guarantee. To overcome
these problems, we propose a novel generic framework for semi-supervised
ordinal regression based on the empirical risk minimization principle that is
applicable to optimizing all of the metrics mentioned above. Besides, our
framework has flexible choices of models, surrogate losses, and optimization
algorithms without the common geometric assumption on unlabeled data such as
the cluster assumption or manifold assumption. We further provide an estimation
error bound to show that our risk estimator is consistent. Finally, we conduct
experiments to show the usefulness of our framework.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Optimal Transport Kernels for Sequential and Parallel Neural Architecture Search. (arXiv:2006.07593v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Vu Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1">Tam Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1">Makoto Yamada</a>, <a href="http://arxiv.org/find/cs/1/au:+Osborne_M/0/1/0/all/0/1">Michael A Osborne</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.07593">
                                    <div class="article-summary-box-inner">
                                        <span>Neural architecture search (NAS) automates the design of deep neural
networks. One of the main challenges in searching complex and non-continuous
architectures is to compare the similarity of networks that the conventional
Euclidean metric may fail to capture. Optimal transport (OT) is resilient to
such complex structure by considering the minimal cost for transporting a
network into another. However, the OT is generally not negative definite which
may limit its ability to build the positive-definite kernels required in many
kernel-dependent frameworks. Building upon tree-Wasserstein (TW), which is a
negative definite variant of OT, we develop a novel discrepancy for neural
architectures, and demonstrate it within a Gaussian process surrogate model for
the sequential NAS settings. Furthermore, we derive a novel parallel NAS, using
quality k-determinantal point process on the GP posterior, to select diverse
and high-performing architectures from a discrete set of candidates.
Empirically, we demonstrate that our TW-based approaches outperform other
baselines in both sequential and parallel NAS.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Co-occurrence of deep convolutional features for image search. (arXiv:2003.13827v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Forcen_J/0/1/0/all/0/1">J.I.Forcen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagola_M/0/1/0/all/0/1">Miguel Pagola</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrenechea_E/0/1/0/all/0/1">Edurne Barrenechea</a>, <a href="http://arxiv.org/find/cs/1/au:+Bustince_H/0/1/0/all/0/1">Humberto Bustince</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.13827">
                                    <div class="article-summary-box-inner">
                                        <span>Image search can be tackled using deep features from pre-trained
Convolutional Neural Networks (CNN). The feature map from the last
convolutional layer of a CNN encodes descriptive information from which a
discriminative global descriptor can be obtained. We propose a new
representation of co-occurrences from deep convolutional features to extract
additional relevant information from this last convolutional layer. Combining
this co-occurrence map with the feature map, we achieve an improved image
representation. We present two different methods to get the co-occurrence
representation, the first one based on direct aggregation of activations, and
the second one, based on a trainable co-occurrence representation. The image
descriptors derived from our methodology improve the performance in very
well-known image retrieval datasets as we prove in the experiments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning. (arXiv:2102.12560v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Filos_A/0/1/0/all/0/1">Angelos Filos</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyle_C/0/1/0/all/0/1">Clare Lyle</a>, <a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1">Yarin Gal</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1">Natasha Jaques</a>, <a href="http://arxiv.org/find/cs/1/au:+Farquhar_G/0/1/0/all/0/1">Gregory Farquhar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12560">
                                    <div class="article-summary-box-inner">
                                        <span>We study reinforcement learning (RL) with no-reward demonstrations, a setting
in which an RL agent has access to additional data from the interaction of
other agents with the same environment. However, it has no access to the
rewards or goals of these agents, and their objectives and levels of expertise
may vary widely. These assumptions are common in multi-agent settings, such as
autonomous driving. To effectively use this data, we turn to the framework of
successor features. This allows us to disentangle shared features and dynamics
of the environment from agent-specific rewards and policies. We propose a
multi-task inverse reinforcement learning (IRL) algorithm, called \emph{inverse
temporal difference learning} (ITD), that learns shared state features,
alongside per-agent successor features and preference vectors, purely from
demonstrations without reward labels. We further show how to seamlessly
integrate ITD with learning from online environment interactions, arriving at a
novel algorithm for reinforcement learning with demonstrations, called $\Psi
\Phi$-learning (pronounced &#x60;Sci-Fi&#x27;). We provide empirical evidence for the
effectiveness of $\Psi \Phi$-learning as a method for improving RL, IRL,
imitation, and few-shot transfer, and derive worst-case bounds for its
performance in zero-shot transfer to new tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Domain Specific Transporter Framework to Detect Fractures in Ultrasound. (arXiv:2106.05929v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1">Arpan Tripathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rakkunedeth_A/0/1/0/all/0/1">Abhilash Rakkunedeth</a>, <a href="http://arxiv.org/find/cs/1/au:+Panicker_M/0/1/0/all/0/1">Mahesh Raveendranatha Panicker</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jack Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Boora_N/0/1/0/all/0/1">Naveenjyote Boora</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaremko_J/0/1/0/all/0/1">Jacob Jaremko</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05929">
                                    <div class="article-summary-box-inner">
                                        <span>Ultrasound examination for detecting fractures is ideally suited for
Emergency Departments (ED) as it is relatively fast, safe (from ionizing
radiation), has dynamic imaging capability and is easily portable. High
interobserver variability in manual assessment of ultrasound scans has piqued
research interest in automatic assessment techniques using Deep Learning (DL).
Most DL techniques are supervised and are trained on large numbers of labeled
data which is expensive and requires many hours of careful annotation by
experts. In this paper, we propose an unsupervised, domain specific transporter
framework to identify relevant keypoints from wrist ultrasound scans. Our
framework provides a concise geometric representation highlighting regions with
high structural variation in a 3D ultrasound (3DUS) sequence. We also
incorporate domain specific information represented by instantaneous local
phase (LP) which detects bone features from 3DUS. We validate the technique on
3DUS videos obtained from 30 subjects. Each ultrasound scan was independently
assessed by three readers to identify fractures along with the corresponding
x-ray. Saliency of keypoints detected in the image\ are compared against manual
assessment based on distance from relevant features.The transporter neural
network was able to accurately detect 180 out of 250 bone regions sampled from
wrist ultrasound videos. We expect this technique to increase the applicability
of ultrasound in fracture detection.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Linear-time inference for Gaussian Processes on one dimension. (arXiv:2003.05554v4 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Loper_J/0/1/0/all/0/1">Jackson Loper</a>, <a href="http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1">David Blei</a>, <a href="http://arxiv.org/find/stat/1/au:+Cunningham_J/0/1/0/all/0/1">John P. Cunningham</a>, <a href="http://arxiv.org/find/stat/1/au:+Paninski_L/0/1/0/all/0/1">Liam Paninski</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.05554">
                                    <div class="article-summary-box-inner">
                                        <span>Gaussian Processes (GPs) provide powerful probabilistic frameworks for
interpolation, forecasting, and smoothing, but have been hampered by
computational scaling issues. Here we investigate data sampled on one dimension
(e.g., a scalar or vector time series sampled at arbitrarily-spaced intervals),
for which state-space models are popular due to their linearly-scaling
computational costs. It has long been conjectured that state-space models are
general, able to approximate any one-dimensional GP. We provide the first
general proof of this conjecture, showing that any stationary GP on one
dimension with vector-valued observations governed by a Lebesgue-integrable
continuous kernel can be approximated to any desired precision using a
specifically-chosen state-space model: the Latent Exponentially Generated (LEG)
family. This new family offers several advantages compared to the general
state-space model: it is always stable (no unbounded growth), the covariance
can be computed in closed form, and its parameter space is unconstrained
(allowing straightforward estimation via gradient descent). The theorem&#x27;s proof
also draws connections to Spectral Mixture Kernels, providing insight about
this popular family of kernels. We develop parallelized algorithms for
performing inference and learning in the LEG model, test the algorithm on real
and synthetic data, and demonstrate scaling to datasets with billions of
samples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learn your ABCs: Approximate Bijective Correspondence for isolating factors of variation. (arXiv:2103.03240v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1">Kieran A. Murphy</a>, <a href="http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1">Varun Jampani</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1">Srikumar Ramalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1">Ameesh Makadia</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03240">
                                    <div class="article-summary-box-inner">
                                        <span>Representational learning forms the backbone of most deep learning
applications, and the value of a learned representation is intimately tied to
its information content regarding different factors of variation. Finding good
representations depends on the nature of supervision and the learning
algorithm. We propose a novel algorithm that relies on a weak form of
supervision where the data is partitioned into sets according to certain
inactive factors of variation. Our key insight is that by seeking approximate
correspondence between elements of different sets, we learn strong
representations that exclude the inactive factors of variation and isolate the
active factors which vary within all sets. We demonstrate that the method can
work in a semi-supervised scenario, and that a portion of the unsupervised data
can belong to a different domain entirely. Further control over the content of
the learned representations is possible by folding in data augmentation to
suppress nuisance factors. We outperform competing baselines on the challenging
problem of synthetic-to-real object pose transfer.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning. (arXiv:2010.01062v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zintgraf_L/0/1/0/all/0/1">Luisa Zintgraf</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1">Leo Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Cong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Igl_M/0/1/0/all/0/1">Maximilian Igl</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartikainen_K/0/1/0/all/0/1">Kristian Hartikainen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1">Katja Hofmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1">Shimon Whiteson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01062">
                                    <div class="article-summary-box-inner">
                                        <span>To rapidly learn a new task, it is often essential for agents to explore
efficiently -- especially when performance matters from the first timestep. One
way to learn such behaviour is via meta-learning. Many existing methods however
rely on dense rewards for meta-training, and can fail catastrophically if the
rewards are sparse. Without a suitable reward signal, the need for exploration
during meta-training is exacerbated. To address this, we propose HyperX, which
uses novel reward bonuses for meta-training to explore in approximate
hyper-state space (where hyper-states represent the environment state and the
agent&#x27;s task belief). We show empirically that HyperX meta-learns better
task-exploration and adapts more successfully to new tasks than existing
methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Investigation of Uncertainty of Deep Learning-based Object Classification on Radar Spectra. (arXiv:2106.05870v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1">Kanil Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Beluch_W/0/1/0/all/0/1">William Beluch</a>, <a href="http://arxiv.org/find/cs/1/au:+Rambach_K/0/1/0/all/0/1">Kilian Rambach</a>, <a href="http://arxiv.org/find/cs/1/au:+Cozma_A/0/1/0/all/0/1">Adriana-Eliza Cozma</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_M/0/1/0/all/0/1">Michael Pfeiffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bin Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05870">
                                    <div class="article-summary-box-inner">
                                        <span>Deep learning (DL) has recently attracted increasing interest to improve
object type classification for automotive radar.In addition to high accuracy,
it is crucial for decision making in autonomous vehicles to evaluate the
reliability of the predictions; however, decisions of DL networks are
non-transparent. Current DL research has investigated how uncertainties of
predictions can be quantified, and in this article, we evaluate the potential
of these methods for safe, automotive radar perception. In particular we
evaluate how uncertainty quantification can support radar perception under (1)
domain shift, (2) corruptions of input signals, and (3) in the presence of
unknown objects. We find that in agreement with phenomena observed in the
literature,deep radar classifiers are overly confident, even in their wrong
predictions. This raises concerns about the use of the confidence values for
decision making under uncertainty, as the model fails to notify when it cannot
handle an unknown situation. Accurate confidence values would allow optimal
integration of multiple information sources, e.g. via sensor fusion. We show
that by applying state-of-the-art post-hoc uncertainty calibration, the quality
of confidence measures can be significantly improved,thereby partially
resolving the over-confidence problem. Our investigation shows that further
research into training and calibrating DL networks is necessary and offers
great potential for safe automotive object classification with radar sensors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Model Distillation for Revenue Optimization: Interpretable Personalized Pricing. (arXiv:2007.01903v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Biggs_M/0/1/0/all/0/1">Max Biggs</a>, <a href="http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1">Wei Sun</a>, <a href="http://arxiv.org/find/stat/1/au:+Ettl_M/0/1/0/all/0/1">Markus Ettl</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01903">
                                    <div class="article-summary-box-inner">
                                        <span>Data-driven pricing strategies are becoming increasingly common, where
customers are offered a personalized price based on features that are
predictive of their valuation of a product. It is desirable for this pricing
policy to be simple and interpretable, so it can be verified, checked for
fairness, and easily implemented. However, efforts to incorporate machine
learning into a pricing framework often lead to complex pricing policies which
are not interpretable, resulting in slow adoption in practice. We present a
customized, prescriptive tree-based algorithm that distills knowledge from a
complex black-box machine learning algorithm, segments customers with similar
valuations and prescribes prices in such a way that maximizes revenue while
maintaining interpretability. We quantify the regret of a resulting policy and
demonstrate its efficacy in applications with both synthetic and real-world
datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU Networks. (arXiv:2011.05530v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1">Ramy E. Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1">Jinhyun So</a>, <a href="http://arxiv.org/find/cs/1/au:+Avestimehr_A/0/1/0/all/0/1">A. Salman Avestimehr</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.05530">
                                    <div class="article-summary-box-inner">
                                        <span>Outsourcing neural network inference tasks to an untrusted cloud raises data
privacy and integrity concerns. To address these challenges, several
privacy-preserving and verifiable inference techniques have been proposed based
on replacing the non-polynomial activation functions such as the rectified
linear unit (ReLU) function with polynomial activation functions. Such
techniques usually require polynomials with integer coefficients or polynomials
over finite fields. Motivated by such requirements, several works proposed
replacing the ReLU activation function with the square activation function. In
this work, we empirically show that the square function is not the best
degree-$2$ polynomial that can replace the ReLU function even when restricting
the polynomials to have integer coefficients. We instead propose a degree-$2$
polynomial activation function with a first order term and empirically show
that it can lead to much better models. Our experiments on the CIFAR-$10$ and
CIFAR-$100$ datasets on various architectures show that our proposed activation
function improves the test accuracy by up to $9.4\%$ compared to the square
function.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">What Does Rotation Prediction Tell Us about Classifier Accuracy under Varying Testing Environments?. (arXiv:2106.05961v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1">Weijian Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1">Stephen Gould</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Liang Zheng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05961">
                                    <div class="article-summary-box-inner">
                                        <span>Understanding classifier decision under novel environments is central to the
community, and a common practice is evaluating it on labeled test sets.
However, in real-world testing, image annotations are difficult and expensive
to obtain, especially when the test environment is changing. A natural question
then arises: given a trained classifier, can we evaluate its accuracy on
varying unlabeled test sets? In this work, we train semantic classification and
rotation prediction in a multi-task way. On a series of datasets, we report an
interesting finding, i.e., the semantic classification accuracy exhibits a
strong linear relationship with the accuracy of the rotation prediction task
(Pearson&#x27;s Correlation r &gt; 0.88). This finding allows us to utilize linear
regression to estimate classifier performance from the accuracy of rotation
prediction which can be obtained on the test set through the freely generated
rotation labels.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Flow-based sampling for fermionic lattice field theories. (arXiv:2106.05934v1 [hep-lat])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/hep-lat/1/au:+Albergo_M/0/1/0/all/0/1">Michael S. Albergo</a>, <a href="http://arxiv.org/find/hep-lat/1/au:+Kanwar_G/0/1/0/all/0/1">Gurtej Kanwar</a>, <a href="http://arxiv.org/find/hep-lat/1/au:+Racaniere_S/0/1/0/all/0/1">S&#xe9;bastien Racani&#xe8;re</a>, <a href="http://arxiv.org/find/hep-lat/1/au:+Rezende_D/0/1/0/all/0/1">Danilo J. Rezende</a>, <a href="http://arxiv.org/find/hep-lat/1/au:+Urban_J/0/1/0/all/0/1">Julian M. Urban</a>, <a href="http://arxiv.org/find/hep-lat/1/au:+Boyda_D/0/1/0/all/0/1">Denis Boyda</a>, <a href="http://arxiv.org/find/hep-lat/1/au:+Cranmer_K/0/1/0/all/0/1">Kyle Cranmer</a>, <a href="http://arxiv.org/find/hep-lat/1/au:+Hackett_D/0/1/0/all/0/1">Daniel C. Hackett</a>, <a href="http://arxiv.org/find/hep-lat/1/au:+Shanahan_P/0/1/0/all/0/1">Phiala E. Shanahan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05934">
                                    <div class="article-summary-box-inner">
                                        <span>Algorithms based on normalizing flows are emerging as promising machine
learning approaches to sampling complicated probability distributions in a way
that can be made asymptotically exact. In the context of lattice field theory,
proof-of-principle studies have demonstrated the effectiveness of this approach
for scalar theories, gauge theories, and statistical systems. This work
develops approaches that enable flow-based sampling of theories with dynamical
fermions, which is necessary for the technique to be applied to lattice field
theory studies of the Standard Model of particle physics and many condensed
matter systems. As a practical demonstration, these methods are applied to the
sampling of field configurations for a two-dimensional theory of massless
staggered fermions coupled to a scalar field via a Yukawa interaction.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sample-Efficient L0-L2 Constrained Structure Learning of Sparse Ising Models. (arXiv:2012.01744v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Dedieu_A/0/1/0/all/0/1">Antoine Dedieu</a>, <a href="http://arxiv.org/find/stat/1/au:+Lazaro_Gredilla_M/0/1/0/all/0/1">Miguel L&#xe1;zaro-Gredilla</a>, <a href="http://arxiv.org/find/stat/1/au:+George_D/0/1/0/all/0/1">Dileep George</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01744">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of learning the underlying graph of a sparse Ising
model with $p$ nodes from $n$ i.i.d. samples. The most recent and best
performing approaches combine an empirical loss (the logistic regression loss
or the interaction screening loss) with a regularizer (an L1 penalty or an L1
constraint). This results in a convex problem that can be solved separately for
each node of the graph. In this work, we leverage the cardinality constraint L0
norm, which is known to properly induce sparsity, and further combine it with
an L2 norm to better model the non-zero coefficients. We show that our proposed
estimators achieve an improved sample complexity, both (a) theoretically, by
reaching new state-of-the-art upper bounds for recovery guarantees, and (b)
empirically, by showing sharper phase transitions between poor and full
recovery for graph topologies studied in the literature, when compared to their
L1-based state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hybrid Machine Learning Forecasts for the UEFA EURO 2020. (arXiv:2106.05799v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Groll_A/0/1/0/all/0/1">Andreas Groll</a>, <a href="http://arxiv.org/find/cs/1/au:+Hvattum_L/0/1/0/all/0/1">Lars Magnus Hvattum</a>, <a href="http://arxiv.org/find/cs/1/au:+Ley_C/0/1/0/all/0/1">Christophe Ley</a>, <a href="http://arxiv.org/find/cs/1/au:+Popp_F/0/1/0/all/0/1">Franziska Popp</a>, <a href="http://arxiv.org/find/cs/1/au:+Schauberger_G/0/1/0/all/0/1">Gunther Schauberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Eetvelde_H/0/1/0/all/0/1">Hans Van Eetvelde</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeileis_A/0/1/0/all/0/1">Achim Zeileis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05799">
                                    <div class="article-summary-box-inner">
                                        <span>Three state-of-the-art statistical ranking methods for forecasting football
matches are combined with several other predictors in a hybrid machine learning
model. Namely an ability estimate for every team based on historic matches; an
ability estimate for every team based on bookmaker consensus; average
plus-minus player ratings based on their individual performances in their home
clubs and national teams; and further team covariates (e.g., market value, team
structure) and country-specific socio-economic factors (population, GDP). The
proposed combined approach is used for learning the number of goals scored in
the matches from the four previous UEFA EUROs 2004-2016 and then applied to
current information to forecast the upcoming UEFA EURO 2020. Based on the
resulting estimates, the tournament is simulated repeatedly and winning
probabilities are obtained for all teams. A random forest model favors the
current World Champion France with a winning probability of 14.8% before
England (13.5%) and Spain (12.3%). Additionally, we provide survival
probabilities for all teams and at all tournament stages.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Disentangled Attention as Intrinsic Regularization for Bimanual Multi-Object Manipulation. (arXiv:2106.05907v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Minghao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jian_P/0/1/0/all/0/1">Pingcheng Jian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Huazhe Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05907">
                                    <div class="article-summary-box-inner">
                                        <span>We address the problem of solving complex bimanual robot manipulation tasks
on multiple objects with sparse rewards. Such complex tasks can be decomposed
into sub-tasks that are accomplishable by different robots concurrently or
sequentially for better efficiency. While previous reinforcement learning
approaches primarily focus on modeling the compositionality of sub-tasks, two
fundamental issues are largely ignored particularly when learning cooperative
strategies for two robots: (i) domination, i.e., one robot may try to solve a
task by itself and leaves the other idle; (ii) conflict, i.e., one robot can
easily interrupt another&#x27;s workspace when executing different sub-tasks
simultaneously. To tackle these two issues, we propose a novel technique called
disentangled attention, which provides an intrinsic regularization for two
robots to focus on separate sub-tasks and objects. We evaluate our method on
four bimanual manipulation tasks. Experimental results show that our proposed
intrinsic regularization successfully avoids domination and reduces conflicts
for the policies, which leads to significantly more effective cooperative
strategies than all the baselines. Our project page with videos is at
https://mehooz.github.io/bimanual-attention.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data-Free Knowledge Distillation for Heterogeneous Federated Learning. (arXiv:2105.10056v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhuangdi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Junyuan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jiayu Zhou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10056">
                                    <div class="article-summary-box-inner">
                                        <span>Federated Learning (FL) is a decentralized machine-learning paradigm, in
which a global server iteratively averages the model parameters of local users
without accessing their data. User heterogeneity has imposed significant
challenges to FL, which can incur drifted global models that are slow to
converge. Knowledge Distillation has recently emerged to tackle this issue, by
refining the server model using aggregated knowledge from heterogeneous users,
other than directly averaging their model parameters. This approach, however,
depends on a proxy dataset, making it impractical unless such a prerequisite is
satisfied. Moreover, the ensemble knowledge is not fully utilized to guide
local model learning, which may in turn affect the quality of the aggregated
model. Inspired by the prior art, we propose a data-free knowledge
distillation} approach to address heterogeneous FL, where the server learns a
lightweight generator to ensemble user information in a data-free manner, which
is then broadcasted to users, regulating local training using the learned
knowledge as an inductive bias. Empirical studies powered by theoretical
implications show that, our approach facilitates FL with better generalization
performance using fewer communication rounds, compared with the
state-of-the-art.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SemSegLoss: A python package of loss functions for semantic segmentation. (arXiv:2106.05844v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jadon_S/0/1/0/all/0/1">Shruti Jadon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05844">
                                    <div class="article-summary-box-inner">
                                        <span>Image Segmentation has been an active field of research as it has a wide
range of applications, ranging from automated disease detection to self-driving
cars. In recent years, various research papers proposed different loss
functions used in case of biased data, sparse segmentation, and unbalanced
dataset. In this paper, we introduce SemSegLoss, a python package consisting of
some of the well-known loss functions widely used for image segmentation. It is
developed with the intent to help researchers in the development of novel loss
functions and perform an extensive set of experiments on model architectures
for various applications. The ease-of-use and flexibility of the presented
package have allowed reducing the development time and increased evaluation
strategies of machine learning models for semantic segmentation. Furthermore,
different applications that use image segmentation can use SemSegLoss because
of the generality of its functions. This wide range of applications will lead
to the development and growth of AI across all industries.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">State Entropy Maximization with Random Encoders for Efficient Exploration. (arXiv:2102.09430v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1">Younggyo Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lili Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1">Jinwoo Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Honglak Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kimin Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09430">
                                    <div class="article-summary-box-inner">
                                        <span>Recent exploration methods have proven to be a recipe for improving
sample-efficiency in deep reinforcement learning (RL). However, efficient
exploration in high-dimensional observation spaces still remains a challenge.
This paper presents Random Encoders for Efficient Exploration (RE3), an
exploration method that utilizes state entropy as an intrinsic reward. In order
to estimate state entropy in environments with high-dimensional observations,
we utilize a k-nearest neighbor entropy estimator in the low-dimensional
representation space of a convolutional encoder. In particular, we find that
the state entropy can be estimated in a stable and compute-efficient manner by
utilizing a randomly initialized encoder, which is fixed throughout training.
Our experiments show that RE3 significantly improves the sample-efficiency of
both model-free and model-based RL methods on locomotion and navigation tasks
from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3
allows learning diverse behaviors without extrinsic rewards, effectively
improving sample-efficiency in downstream tasks. Source code and videos are
available at https://sites.google.com/view/re3-rl.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A concise method for feature selection via normalized frequencies. (arXiv:2106.05814v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1">Song Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xia He</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05814">
                                    <div class="article-summary-box-inner">
                                        <span>Feature selection is an important part of building a machine learning model.
By eliminating redundant or misleading features from data, the machine learning
model can achieve better performance while reducing the demand on com-puting
resources. Metaheuristic algorithms are mostly used to implement feature
selection such as swarm intelligence algorithms and evolutionary algorithms.
However, they suffer from the disadvantage of relative complexity and slowness.
In this paper, a concise method is proposed for universal feature selection.
The proposed method uses a fusion of the filter method and the wrapper method,
rather than a combination of them. In the method, one-hoting encoding is used
to preprocess the dataset, and random forest is utilized as the classifier. The
proposed method uses normalized frequencies to assign a value to each feature,
which will be used to find the optimal feature subset. Furthermore, we propose
a novel approach to exploit the outputs of mutual information, which allows for
a better starting point for the experiments. Two real-world dataset in the
field of intrusion detection were used to evaluate the proposed method. The
evaluation results show that the proposed method outperformed several
state-of-the-art related works in terms of accuracy, precision, recall, F-score
and AUC.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DMIDAS: Deep Mixed Data Sampling Regression for Long Multi-Horizon Time Series Forecasting. (arXiv:2106.05860v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Challu_C/0/1/0/all/0/1">Cristian Challu</a>, <a href="http://arxiv.org/find/cs/1/au:+Olivares_K/0/1/0/all/0/1">Kin G. Olivares</a>, <a href="http://arxiv.org/find/cs/1/au:+Welter_G/0/1/0/all/0/1">Gus Welter</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1">Artur Dubrawski</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05860">
                                    <div class="article-summary-box-inner">
                                        <span>Neural forecasting has shown significant improvements in the accuracy of
large-scale systems, yet predicting extremely long horizons remains a
challenging task. Two common problems are the volatility of the predictions and
their computational complexity; we addressed them by incorporating smoothness
regularization and mixed data sampling techniques to a well-performing
multi-layer perceptron based architecture (NBEATS). We validate our proposed
method, DMIDAS, on high-frequency healthcare and electricity price data with
long forecasting horizons (~1000 timestamps) where we improve the prediction
accuracy by 5% over state-of-the-art models, reducing the number of parameters
of NBEATS by nearly 70%.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Backdoor Smoothing: Demystifying Backdoor Attacks on Deep Neural Networks. (arXiv:2006.06721v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Grosse_K/0/1/0/all/0/1">Kathrin Grosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1">Taesung Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1">Battista Biggio</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1">Youngja Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1">Michael Backes</a>, <a href="http://arxiv.org/find/cs/1/au:+Molloy_I/0/1/0/all/0/1">Ian Molloy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.06721">
                                    <div class="article-summary-box-inner">
                                        <span>Backdoor attacks aim to mislead machine-learning models to output an
attacker-specified class when presented a specific trigger at test time. These
attacks require poisoning the training data or compromising the learning
algorithm, e.g., by injecting poisoning samples containing the trigger into the
training set, along with the desired class label. Despite the increasing number
of studies on backdoor attacks and defenses, the underlying factors affecting
the success of backdoor attacks, along with their impact on the learning
algorithm, are not yet well understood. In this work, we aim to shed light on
this issue. In particular, we unveil that backdoor attacks work by inducing a
smoother decision function around the triggered samples -- a phenomenon which
we refer to as \textit{backdoor smoothing}. We quantify backdoor smoothing by
defining a measure that evaluates the uncertainty associated to the predictions
of a classifier around the input samples.

Our experiments show that smoothness increases when the trigger is added to
the input samples, and that the phenomenon is more pronounced for more
successful attacks.

However, our experiments also show that patterns fulfilling backdoor
smoothing can be crafted

even without poisoning the training data.

Although our measure may not be directly exploited as a defense mechanism, it
unveils an important phenomenon which may pave the way towards understanding
the limitations of current defenses that rely on a smooth decision output for
backdoors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Rare event estimation using stochastic spectral embedding. (arXiv:2106.05824v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wagner_P/0/1/0/all/0/1">P.-R. Wagner</a>, <a href="http://arxiv.org/find/cs/1/au:+Marelli_S/0/1/0/all/0/1">S. Marelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Papaioannou_I/0/1/0/all/0/1">I. Papaioannou</a>, <a href="http://arxiv.org/find/cs/1/au:+Straub_D/0/1/0/all/0/1">D. Straub</a>, <a href="http://arxiv.org/find/cs/1/au:+Sudret_B/0/1/0/all/0/1">B. Sudret</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05824">
                                    <div class="article-summary-box-inner">
                                        <span>Estimating the probability of rare failure events is an essential step in the
reliability assessment of engineering systems. Computing this failure
probability for complex non-linear systems is challenging, and has recently
spurred the development of active-learning reliability methods. These methods
approximate the limit-state function (LSF) using surrogate models trained with
a sequentially enriched set of model evaluations. A recently proposed method
called stochastic spectral embedding (SSE) aims to improve the local
approximation accuracy of global, spectral surrogate modelling techniques by
sequentially embedding local residual expansions in subdomains of the input
space. In this work we apply SSE to the LSF, giving rise to a stochastic
spectral embedding-based reliability (SSER) method. The resulting partition of
the input space decomposes the failure probability into a set of
easy-to-compute domain-wise failure probabilities. We propose a set of
modifications that tailor the algorithm to efficiently solve rare event
estimation problems. These modifications include specialized refinement domain
selection, partitioning and enrichment strategies. We showcase the algorithm
performance on four benchmark problems of various dimensionality and complexity
in the LSF.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning by Watching. (arXiv:2106.05966v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jimuyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohn_Bar_E/0/1/0/all/0/1">Eshed Ohn-Bar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05966">
                                    <div class="article-summary-box-inner">
                                        <span>When in a new situation or geographical location, human drivers have an
extraordinary ability to watch others and learn maneuvers that they themselves
may have never performed. In contrast, existing techniques for learning to
drive preclude such a possibility as they assume direct access to an
instrumented ego-vehicle with fully known observations and expert driver
actions. However, such measurements cannot be directly accessed for the non-ego
vehicles when learning by watching others. Therefore, in an application where
data is regarded as a highly valuable asset, current approaches completely
discard the vast portion of the training data that can be potentially obtained
through indirect observation of surrounding vehicles. Motivated by this key
insight, we propose the Learning by Watching (LbW) framework which enables
learning a driving policy without requiring full knowledge of neither the state
nor expert actions. To increase its data, i.e., with new perspectives and
maneuvers, LbW makes use of the demonstrations of other vehicles in a given
scene by (1) transforming the ego-vehicle&#x27;s observations to their points of
view, and (2) inferring their expert actions. Our LbW agent learns more robust
driving policies while enabling data-efficient learning, including quick
adaptation of the policy to rare and novel scenarios. In particular, LbW drives
robustly even with a fraction of available driving data required by existing
methods, achieving an average success rate of 92% on the original CARLA
benchmark with only 30 minutes of total driving data and 82% with only 10
minutes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MC-LSTM: Mass-Conserving LSTM. (arXiv:2101.05186v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hoedt_P/0/1/0/all/0/1">Pieter-Jan Hoedt</a>, <a href="http://arxiv.org/find/cs/1/au:+Kratzert_F/0/1/0/all/0/1">Frederik Kratzert</a>, <a href="http://arxiv.org/find/cs/1/au:+Klotz_D/0/1/0/all/0/1">Daniel Klotz</a>, <a href="http://arxiv.org/find/cs/1/au:+Halmich_C/0/1/0/all/0/1">Christina Halmich</a>, <a href="http://arxiv.org/find/cs/1/au:+Holzleitner_M/0/1/0/all/0/1">Markus Holzleitner</a>, <a href="http://arxiv.org/find/cs/1/au:+Nearing_G/0/1/0/all/0/1">Grey Nearing</a>, <a href="http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1">Sepp Hochreiter</a>, <a href="http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1">G&#xfc;nter Klambauer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05186">
                                    <div class="article-summary-box-inner">
                                        <span>The success of Convolutional Neural Networks (CNNs) in computer vision is
mainly driven by their strong inductive bias, which is strong enough to allow
CNNs to solve vision-related tasks with random weights, meaning without
learning. Similarly, Long Short-Term Memory (LSTM) has a strong inductive bias
towards storing information over time. However, many real-world systems are
governed by conservation laws, which lead to the redistribution of particular
quantities -- e.g. in physical and economical systems. Our novel
Mass-Conserving LSTM (MC-LSTM) adheres to these conservation laws by extending
the inductive bias of LSTM to model the redistribution of those stored
quantities. MC-LSTMs set a new state-of-the-art for neural arithmetic units at
learning arithmetic operations, such as addition tasks, which have a strong
conservation law, as the sum is constant over time. Further, MC-LSTM is applied
to traffic forecasting, modelling a pendulum, and a large benchmark dataset in
hydrology, where it sets a new state-of-the-art for predicting peak flows. In
the hydrology example, we show that MC-LSTM states correlate with real-world
processes and are therefore interpretable.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MolGrow: A Graph Normalizing Flow for Hierarchical Molecular Generation. (arXiv:2106.05856v1 [physics.chem-ph])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/physics/1/au:+Kuznetsov_M/0/1/0/all/0/1">Maksim Kuznetsov</a>, <a href="http://arxiv.org/find/physics/1/au:+Polykovskiy_D/0/1/0/all/0/1">Daniil Polykovskiy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05856">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a hierarchical normalizing flow model for generating molecular
graphs. The model produces new molecular structures from a single-node graph by
recursively splitting every node into two. All operations are invertible and
can be used as plug-and-play modules. The hierarchical nature of the latent
codes allows for precise changes in the resulting graph: perturbations in the
top layer cause global structural changes, while perturbations in the
consequent layers change the resulting molecule marginally. The proposed model
outperforms existing generative graph models on the distribution learning task.
We also show successful experiments on global and constrained optimization of
chemical properties using latent codes of the model.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Near-Optimal High Probability Complexity Bounds for Non-Smooth Stochastic Optimization with Heavy-Tailed Noise. (arXiv:2106.05958v1 [math.OC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Gorbunov_E/0/1/0/all/0/1">Eduard Gorbunov</a>, <a href="http://arxiv.org/find/math/1/au:+Danilova_M/0/1/0/all/0/1">Marina Danilova</a>, <a href="http://arxiv.org/find/math/1/au:+Shibaev_I/0/1/0/all/0/1">Innokentiy Shibaev</a>, <a href="http://arxiv.org/find/math/1/au:+Dvurechensky_P/0/1/0/all/0/1">Pavel Dvurechensky</a>, <a href="http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1">Alexander Gasnikov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05958">
                                    <div class="article-summary-box-inner">
                                        <span>Thanks to their practical efficiency and random nature of the data,
stochastic first-order methods are standard for training large-scale machine
learning models. Random behavior may cause a particular run of an algorithm to
result in a highly suboptimal objective value, whereas theoretical guarantees
are usually proved for the expectation of the objective value. Thus, it is
essential to theoretically guarantee that algorithms provide small objective
residual with high probability. Existing methods for non-smooth stochastic
convex optimization have complexity bounds with the dependence on the
confidence level that is either negative-power or logarithmic but under an
additional assumption of sub-Gaussian (light-tailed) noise distribution that
may not hold in practice, e.g., in several NLP tasks. In our paper, we resolve
this issue and derive the first high-probability convergence results with
logarithmic dependence on the confidence level for non-smooth convex stochastic
optimization problems with non-sub-Gaussian (heavy-tailed) noise. To derive our
results, we propose novel stepsize rules for two stochastic methods with
gradient clipping. Moreover, our analysis works for generalized smooth
objectives with H\&quot;older-continuous gradients, and for both methods, we provide
an extension for strongly convex problems. Finally, our results imply that the
first (accelerated) method we consider also has optimal iteration and oracle
complexity in all the regimes, and the second one is optimal in the non-smooth
setting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Group Equivariant Subsampling. (arXiv:2106.05886v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Hyunjik Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1">Tom Rainforth</a>, <a href="http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1">Yee Whye Teh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05886">
                                    <div class="article-summary-box-inner">
                                        <span>Subsampling is used in convolutional neural networks (CNNs) in the form of
pooling or strided convolutions, to reduce the spatial dimensions of feature
maps and to allow the receptive fields to grow exponentially with depth.
However, it is known that such subsampling operations are not translation
equivariant, unlike convolutions that are translation equivariant. Here, we
first introduce translation equivariant subsampling/upsampling layers that can
be used to construct exact translation equivariant CNNs. We then generalise
these layers beyond translations to general groups, thus proposing group
equivariant subsampling/upsampling. We use these layers to construct group
equivariant autoencoders (GAEs) that allow us to learn low-dimensional
equivariant representations. We empirically verify on images that the
representations are indeed equivariant to input translations and rotations, and
thus generalise well to unseen positions and orientations. We further use GAEs
in models that learn object-centric representations on multi-object datasets,
and show improved data efficiency and decomposition compared to non-equivariant
baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DeepVideoMVS: Multi-View Stereo on Video with Recurrent Spatio-Temporal Fusion. (arXiv:2012.02177v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Duzceker_A/0/1/0/all/0/1">Arda D&#xfc;z&#xe7;eker</a>, <a href="http://arxiv.org/find/cs/1/au:+Galliani_S/0/1/0/all/0/1">Silvano Galliani</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogel_C/0/1/0/all/0/1">Christoph Vogel</a>, <a href="http://arxiv.org/find/cs/1/au:+Speciale_P/0/1/0/all/0/1">Pablo Speciale</a>, <a href="http://arxiv.org/find/cs/1/au:+Dusmanu_M/0/1/0/all/0/1">Mihai Dusmanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1">Marc Pollefeys</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02177">
                                    <div class="article-summary-box-inner">
                                        <span>We propose an online multi-view depth prediction approach on posed video
streams, where the scene geometry information computed in the previous time
steps is propagated to the current time step in an efficient and geometrically
plausible way. The backbone of our approach is a real-time capable, lightweight
encoder-decoder that relies on cost volumes computed from pairs of images. We
extend it by placing a ConvLSTM cell at the bottleneck layer, which compresses
an arbitrary amount of past information in its states. The novelty lies in
propagating the hidden state of the cell by accounting for the viewpoint
changes between time steps. At a given time step, we warp the previous hidden
state into the current camera plane using the previous depth prediction. Our
extension brings only a small overhead of computation time and memory
consumption, while improving the depth predictions significantly. As a result,
we outperform the existing state-of-the-art multi-view stereo methods on most
of the evaluated metrics in hundreds of indoor scenes while maintaining a
real-time performance. Code available:
https://github.com/ardaduz/deep-video-mvs</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Evolving Robust Neural Architectures to Defend from Adversarial Attacks. (arXiv:1906.11667v3 [cs.NE] CROSS LISTED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1">Shashank Kotyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1">Danilo Vasconcellos Vargas</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.11667">
                                    <div class="article-summary-box-inner">
                                        <span>Neural networks are prone to misclassify slightly modified input images.
Recently, many defences have been proposed, but none have improved the
robustness of neural networks consistently. Here, we propose to use adversarial
attacks as a function evaluation to search for neural architectures that can
resist such attacks automatically. Experiments on neural architecture search
algorithms from the literature show that although accurate, they are not able
to find robust architectures. A significant reason for this lies in their
limited search space. By creating a novel neural architecture search with
options for dense layers to connect with convolution layers and vice-versa as
well as the addition of concatenation layers in the search, we were able to
evolve an architecture that is inherently accurate on adversarial samples.
Interestingly, this inherent robustness of the evolved architecture rivals
state-of-the-art defences such as adversarial training while being trained only
on the non-adversarial samples. Moreover, the evolved architecture makes use of
some peculiar traits which might be useful for developing even more robust
ones. Thus, the results here confirm that more robust architectures exist as
well as opens up a new realm of feasibilities for the development and
exploration of neural networks.

Code available at this http URL</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Raman spectral analysis of mixtures with one-dimensional convolutional neural network. (arXiv:2106.05316v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mozaffari_M/0/1/0/all/0/1">M. Hamed Mozaffari</a>, <a href="http://arxiv.org/find/cs/1/au:+Tay_L/0/1/0/all/0/1">Li-Lin Tay</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05316">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, the combination of robust one-dimensional convolutional neural
networks (1-D CNNs) and Raman spectroscopy has shown great promise in rapid
identification of unknown substances with good accuracy. Using this technique,
researchers can recognize a pure compound and distinguish it from unknown
substances in a mixture. The novelty of this approach is that the trained
neural network operates automatically without any pre- or post-processing of
data. Some studies have attempted to extend this technique to the
classification of pure compounds in an unknown mixture. However, the
application of 1-D CNNs has typically been restricted to binary classifications
of pure compounds. Here we will highlight a new approach in spectral
recognition and quantification of chemical components in a multicomponent
mixture. Two 1-D CNN models, RaMixNet I and II, have been developed for this
purpose. The former is for rapid classification of components in a mixture
while the latter is for quantitative determination of those constituents. In
the proposed method, there is no limit to the number of compounds in a mixture.
A data augmentation method is also introduced by adding random baselines to the
Raman spectra. The experimental results revealed that the classification
accuracy of RaMixNet I and II is 100% for analysis of unknown test mixtures; at
the same time, the RaMixNet II model may achieve a regression accuracy of 88%
for the quantification of each component.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Network Space Search for Pareto-Efficient Spaces. (arXiv:2104.11014v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1">Min-Fong Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hao-Yun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Min-Hung Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yu-Syuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1">Hsien-Kai Kuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yi-Min Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hung-Jen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jou_K/0/1/0/all/0/1">Kevin Jou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11014">
                                    <div class="article-summary-box-inner">
                                        <span>Network spaces have been known as a critical factor in both handcrafted
network designs or defining search spaces for Neural Architecture Search (NAS).
However, an effective space involves tremendous prior knowledge and/or manual
effort, and additional constraints are required to discover efficiency-aware
architectures. In this paper, we define a new problem, Network Space Search
(NSS), as searching for favorable network spaces instead of a single
architecture. We propose an NSS method to directly search for efficient-aware
network spaces automatically, reducing the manual effort and immense cost in
discovering satisfactory ones. The resultant network spaces, named Elite
Spaces, are discovered from Expanded Search Space with minimal human expertise
imposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front
under various complexity constraints and can be further served as NAS search
spaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an
averagely 2.3% lower error rate and 3.7% closer to target constraint than the
baseline with around 90% fewer samples required to find satisfactory networks).
Moreover, our NSS approach is capable of searching for superior spaces in
future unexplored spaces, revealing great potential in searching for network
spaces automatically.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient Descent. (arXiv:2102.05855v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Chourasia_R/0/1/0/all/0/1">Rishav Chourasia</a>, <a href="http://arxiv.org/find/stat/1/au:+Ye_J/0/1/0/all/0/1">Jiayuan Ye</a>, <a href="http://arxiv.org/find/stat/1/au:+Shokri_R/0/1/0/all/0/1">Reza Shokri</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05855">
                                    <div class="article-summary-box-inner">
                                        <span>What is the information leakage of an iterative learning algorithm about its
training data, when the internal state of the algorithm is \emph{not}
observable? How much is the contribution of each specific training epoch to the
final leakage? We study this problem for noisy gradient descent algorithms, and
model the \emph{dynamics} of R\&#x27;enyi differential privacy loss throughout the
training process. Our analysis traces a provably tight bound on the R\&#x27;enyi
divergence between the pair of probability distributions over parameters of
models with neighboring datasets. We prove that the privacy loss converges
exponentially fast, for smooth and strongly convex loss functions, which is a
significant improvement over composition theorems. For Lipschitz, smooth, and
strongly convex loss functions, we prove optimal utility for differential
privacy algorithms with a small gradient complexity.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust Explanations for Private Support Vector Machines. (arXiv:2102.03785v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mochaourab_R/0/1/0/all/0/1">Rami Mochaourab</a>, <a href="http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1">Sugandh Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Greenstein_S/0/1/0/all/0/1">Stanley Greenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Papapetrou_P/0/1/0/all/0/1">Panagiotis Papapetrou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03785">
                                    <div class="article-summary-box-inner">
                                        <span>We consider counterfactual explanations for private support vector machines
(SVM), where the privacy mechanism that publicly releases the classifier
guarantees differential privacy. While privacy preservation is essential when
dealing with sensitive data, there is a consequent degradation in the
classification accuracy due to the introduced perturbations in the classifier
weights. For such classifiers, counterfactual explanations need to be robust
against the uncertainties in the SVM weights in order to ensure, with high
confidence, that the classification of the data instance to be explained is
different than its explanation. We model the uncertainties in the SVM weights
through a random vector, and formulate the explanation problem as an
optimization problem with probabilistic constraint. Subsequently, we
characterize the problem&#x27;s deterministic equivalent and study its solution. For
linear SVMs, the problem is a convex second-order cone program. For non-linear
SVMs, the problem is non-convex. Thus, we propose a sub-optimal solution that
is based on the bisection method. The results show that, contrary to non-robust
explanations, the quality of explanations from the robust solution degrades
with increasing privacy in order to guarantee a prespecified confidence level
for correct classifications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for Multimodal Hate. (arXiv:2106.05903v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Botelho_A/0/1/0/all/0/1">Austin Botelho</a>, <a href="http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1">Bertie Vidgen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1">Scott A. Hale</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05903">
                                    <div class="article-summary-box-inner">
                                        <span>Accurate detection and classification of online hate is a difficult task.
Implicit hate is particularly challenging as such content tends to have unusual
syntax, polysemic words, and fewer markers of prejudice (e.g., slurs). This
problem is heightened with multimodal content, such as memes (combinations of
text and images), as they are often harder to decipher than unimodal content
(e.g., text alone). This paper evaluates the role of semantic and multimodal
context for detecting implicit and explicit hate. We show that both text- and
visual- enrichment improves model performance, with the multimodal model
(0.771) outperforming other models&#x27; F1 scores (0.544, 0.737, and 0.754). While
the unimodal-text context-aware (transformer) model was the most accurate on
the subtask of implicit hate detection, the multimodal model outperformed it
overall because of a lower propensity towards false positives. We find that all
models perform better on content with full annotator agreement and that
multimodal models are best at classifying the content where annotators
disagree. To conduct these investigations, we undertook high-quality annotation
of a sample of 5,000 multimodal entries. Tweets were annotated for primary
category, modality, and strategy. We make this corpus, along with the codebook,
code, and final model, freely available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Shift Invariance Can Reduce Adversarial Robustness. (arXiv:2103.02695v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1">Songwei Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_V/0/1/0/all/0/1">Vasu Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1">Ronen Basri</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1">David Jacobs</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02695">
                                    <div class="article-summary-box-inner">
                                        <span>Shift invariance is a critical property of CNNs that improves performance on
classification. However, we show that invariance to circular shifts can also
lead to greater sensitivity to adversarial attacks. We first characterize the
margin between classes when a shift-invariant linear classifier is used. We
show that the margin can only depend on the DC component of the signals. Then,
using results about infinitely wide networks, we show that in some simple
cases, fully connected and shift-invariant neural networks produce linear
decision boundaries. Using this, we prove that shift invariance in neural
networks produces adversarial examples for the simple case of two classes, each
consisting of a single image with a black or white dot on a gray background.
This is more than a curiosity; we show empirically that with real datasets and
realistic architectures, shift invariance reduces adversarial robustness.
Finally, we describe initial experiments using synthetic data to probe the
source of this connection.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Reinforcement Learning for Orientation Estimation Using Inertial Sensors with Performance Guarantee. (arXiv:2103.02357v2 [cs.RO] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1">Liang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yujie Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhipeng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1">Wei Pan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02357">
                                    <div class="article-summary-box-inner">
                                        <span>This paper presents a deep reinforcement learning (DRL) algorithm for
orientation estimation using inertial sensors combined with magnetometer. The
Lyapunov method in control theory is employed to prove the convergence of
orientation estimation errors. Based on the theoretical results, the estimator
gains and a Lyapunov function are parametrized by deep neural networks and
learned from samples. The DRL estimator is compared with three well-known
orientation estimation methods on both numerical simulations and real datasets
collected from commercially available sensors. The results show that the
proposed algorithm is superior for arbitrary estimation initialization and can
adapt to very large angular velocities for which other algorithms can be hardly
applicable. To the best of our knowledge, this is the first DRL-based
orientation estimation method with estimation error boundedness guarantee.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fair Normalizing Flows. (arXiv:2106.05937v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1">Mislav Balunovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruoss_A/0/1/0/all/0/1">Anian Ruoss</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1">Martin Vechev</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05937">
                                    <div class="article-summary-box-inner">
                                        <span>Fair representation learning is an attractive approach that promises fairness
of downstream predictors by encoding sensitive data. Unfortunately, recent work
has shown that strong adversarial predictors can still exhibit unfairness by
recovering sensitive attributes from these representations. In this work, we
present Fair Normalizing Flows (FNF), a new approach offering more rigorous
fairness guarantees for learned representations. Specifically, we consider a
practical setting where we can estimate the probability density for sensitive
groups. The key idea is to model the encoder as a normalizing flow trained to
minimize the statistical distance between the latent representations of
different groups. The main advantage of FNF is that its exact likelihood
computation allows us to obtain guarantees on the maximum unfairness of any
potentially adversarial downstream predictor. We experimentally demonstrate the
effectiveness of FNF in enforcing various group fairness notions, as well as
other attractive properties such as interpretability and transfer learning, on
a variety of challenging real-world datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Optimizing Reusable Knowledge for Continual Learning via Metalearning. (arXiv:2106.05390v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1">Julio Hurtado</a>, <a href="http://arxiv.org/find/cs/1/au:+Raymond_Saez_A/0/1/0/all/0/1">Alain Raymond-Saez</a>, <a href="http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1">Alvaro Soto</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05390">
                                    <div class="article-summary-box-inner">
                                        <span>When learning tasks over time, artificial neural networks suffer from a
problem known as Catastrophic Forgetting (CF). This happens when the weights of
a network are overwritten during the training of a new task causing forgetting
of old information. To address this issue, we propose MetA Reusable Knowledge
or MARK, a new method that fosters weight reusability instead of overwriting
when learning a new task. Specifically, MARK keeps a set of shared weights
among tasks. We envision these shared weights as a common Knowledge Base (KB)
that is not only used to learn new tasks, but also enriched with new knowledge
as the model learns new tasks. Key components behind MARK are two-fold. On the
one hand, a metalearning approach provides the key mechanism to incrementally
enrich the KB with new knowledge and to foster weight reusability among tasks.
On the other hand, a set of trainable masks provides the key mechanism to
selectively choose from the KB relevant weights to solve each task. By using
MARK, we achieve state of the art results in several popular benchmarks,
surpassing the best performing methods in terms of average accuracy by over 10%
on the 20-Split-MiniImageNet dataset, while achieving almost zero forgetfulness
using 55% of the number of parameters. Furthermore, an ablation study provides
evidence that, indeed, MARK is learning reusable knowledge that is selectively
used by each task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Early-stopped neural networks are consistent. (arXiv:2106.05932v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1">Ziwei Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Justin D. Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Telgarsky_M/0/1/0/all/0/1">Matus Telgarsky</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05932">
                                    <div class="article-summary-box-inner">
                                        <span>This work studies the behavior of neural networks trained with the logistic
loss via gradient descent on binary classification data where the underlying
data distribution is general, and the (optimal) Bayes risk is not necessarily
zero. In this setting, it is shown that gradient descent with early stopping
achieves population risk arbitrarily close to optimal in terms of not just
logistic and misclassification losses, but also in terms of calibration,
meaning the sigmoid mapping of its outputs approximates the true underlying
conditional distribution arbitrarily finely. Moreover, the necessary iteration,
sample, and architectural complexities of this analysis all scale naturally
with a certain complexity measure of the true conditional model. Lastly, while
it is not shown that early stopping is necessary, it is shown that any
univariate classifier satisfying a local interpolation property is necessarily
inconsistent.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data. (arXiv:2101.07597v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yao Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumatani_K/0/1/0/all/0/1">Kenichi Kumatani</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shujie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1">Michael Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuedong Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07597">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose a unified pre-training approach called UniSpeech to
learn speech representations with both unlabeled and labeled data, in which
supervised phonetic CTC learning and phonetically-aware contrastive
self-supervised learning are conducted in a multi-task learning manner. The
resultant representations can capture information more correlated with phonetic
structures and improve the generalization across languages and domains. We
evaluate the effectiveness of UniSpeech for cross-lingual representation
learning on public CommonVoice corpus. The results show that UniSpeech
outperforms self-supervised pretraining and supervised transfer learning for
speech recognition by a maximum of 13.4% and 17.8% relative phone error rate
reductions respectively (averaged over all testing languages). The
transferability of UniSpeech is also demonstrated on a domain-shift speech
recognition task, i.e., a relative word error rate reduction of 6% against the
previous approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Think Global and Act Local: Bayesian Optimisation over High-Dimensional Categorical and Mixed Search Spaces. (arXiv:2102.07188v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1">Xingchen Wan</a>, <a href="http://arxiv.org/find/stat/1/au:+Nguyen_V/0/1/0/all/0/1">Vu Nguyen</a>, <a href="http://arxiv.org/find/stat/1/au:+Ha_H/0/1/0/all/0/1">Huong Ha</a>, <a href="http://arxiv.org/find/stat/1/au:+Ru_B/0/1/0/all/0/1">Binxin Ru</a>, <a href="http://arxiv.org/find/stat/1/au:+Lu_C/0/1/0/all/0/1">Cong Lu</a>, <a href="http://arxiv.org/find/stat/1/au:+Osborne_M/0/1/0/all/0/1">Michael A. Osborne</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07188">
                                    <div class="article-summary-box-inner">
                                        <span>High-dimensional black-box optimisation remains an important yet notoriously
challenging problem. Despite the success of Bayesian optimisation methods on
continuous domains, domains that are categorical, or that mix continuous and
categorical variables, remain challenging. We propose a novel solution -- we
combine local optimisation with a tailored kernel design, effectively handling
high-dimensional categorical and mixed search spaces, whilst retaining sample
efficiency. We further derive convergence guarantee for the proposed approach.
Finally, we demonstrate empirically that our method outperforms the current
baselines on a variety of synthetic and real-world tasks in terms of
performance, computational costs, or both.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Probabilistic Time Series Forecasting using Augmented Recurrent Input for Dynamic Systems. (arXiv:2106.05848v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haitao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Changjun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xiaomo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xudong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuhua Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaofang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05848">
                                    <div class="article-summary-box-inner">
                                        <span>The demand of probabilistic time series forecasting has been recently raised
in various dynamic system scenarios, for example, system identification and
prognostic and health management of machines. To this end, we combine the
advances in both deep generative models and state space model (SSM) to come up
with a novel, data-driven deep probabilistic sequence model. Specially, we
follow the popular encoder-decoder generative structure to build the recurrent
neural networks (RNN) assisted variational sequence model on an augmented
recurrent input space, which could induce rich stochastic sequence dependency.
Besides, in order to alleviate the issue of inconsistency between training and
predicting as well as improving the mining of dynamic patterns, we (i) propose
using a hybrid output as input at next time step, which brings training and
predicting into alignment; and (ii) further devise a generalized
auto-regressive strategy that encodes all the historical dependencies at
current time step. Thereafter, we first investigate the methodological
characteristics of the proposed deep probabilistic sequence model on toy cases,
and then comprehensively demonstrate the superiority of our model against
existing deep probabilistic SSM models through extensive numerical experiments
on eight system identification benchmarks from various dynamic systems.
Finally, we apply our sequence model to a real-world centrifugal compressor
sensor data forecasting problem, and again verify its outstanding performance
by quantifying the time series predictive distribution.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Conic Blackwell Algorithm: Parameter-Free Convex-Concave Saddle-Point Solving. (arXiv:2105.13203v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Grand_Clement_J/0/1/0/all/0/1">Julien Grand-Cl&#xe9;ment</a>, <a href="http://arxiv.org/find/cs/1/au:+Kroer_C/0/1/0/all/0/1">Christian Kroer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13203">
                                    <div class="article-summary-box-inner">
                                        <span>We develop new parameter and scale-free algorithms for solving convex-concave
saddle-point problems. Our results are based on a new simple regret minimizer,
the Conic Blackwell Algorithm$^+$ (CBA$^+$), which attains $O(1/\sqrt{T})$
average regret. Intuitively, our approach generalizes to other decision sets of
interest ideas from the Counterfactual Regret minimization (CFR$^+$) algorithm,
which has very strong practical performance for solving sequential games on
simplexes. We show how to implement CBA$^+$ for the simplex, $\ell_{p}$ norm
balls, and ellipsoidal confidence regions in the simplex, and we present
numerical experiments for solving matrix games and distributionally robust
optimization problems. Our empirical results show that CBA$^+$ is a simple
algorithm that outperforms state-of-the-art methods on synthetic data and real
data instances, without the need for any choice of step sizes or other
algorithmic parameters.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures. (arXiv:2106.05822v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chelombiev_I/0/1/0/all/0/1">Ivan Chelombiev</a>, <a href="http://arxiv.org/find/cs/1/au:+Justus_D/0/1/0/all/0/1">Daniel Justus</a>, <a href="http://arxiv.org/find/cs/1/au:+Orr_D/0/1/0/all/0/1">Douglas Orr</a>, <a href="http://arxiv.org/find/cs/1/au:+Dietrich_A/0/1/0/all/0/1">Anastasia Dietrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Gressmann_F/0/1/0/all/0/1">Frithjof Gressmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Koliousis_A/0/1/0/all/0/1">Alexandros Koliousis</a>, <a href="http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1">Carlo Luschi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05822">
                                    <div class="article-summary-box-inner">
                                        <span>Attention based language models have become a critical component in
state-of-the-art natural language processing systems. However, these models
have significant computational requirements, due to long training times, dense
operations and large parameter count. In this work we demonstrate a set of
modifications to the structure of a Transformer layer, producing a more
efficient architecture. First, we add a convolutional module to complement the
self-attention module, decoupling the learning of local and global
interactions. Secondly, we rely on grouped transformations to reduce the
computational cost of dense feed-forward layers and convolutions, while
preserving the expressivity of the model. We apply the resulting architecture
to language representation learning and demonstrate its superior performance
compared to BERT models of different scales. We further highlight its improved
efficiency, both in terms of floating-point operations (FLOPs) and
time-to-train.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On the overlooked issue of defining explanation objectives for local-surrogate explainers. (arXiv:2106.05810v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Poyiadzi_R/0/1/0/all/0/1">Rafael Poyiadzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Renard_X/0/1/0/all/0/1">Xavier Renard</a>, <a href="http://arxiv.org/find/cs/1/au:+Laugel_T/0/1/0/all/0/1">Thibault Laugel</a>, <a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1">Raul Santos-Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Detyniecki_M/0/1/0/all/0/1">Marcin Detyniecki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05810">
                                    <div class="article-summary-box-inner">
                                        <span>Local surrogate approaches for explaining machine learning model predictions
have appealing properties, such as being model-agnostic and flexible in their
modelling. Several methods exist that fit this description and share this goal.
However, despite their shared overall procedure, they set out different
objectives, extract different information from the black-box, and consequently
produce diverse explanations, that are -- in general -- incomparable. In this
work we review the similarities and differences amongst multiple methods, with
a particular focus on what information they extract from the model, as this has
large impact on the output: the explanation. We discuss the implications of the
lack of agreement, and clarity, amongst the methods&#x27; objectives on the research
and practice of explainability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An On-Device Federated Learning Approach for Cooperative Anomaly Detection. (arXiv:2002.12301v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ito_R/0/1/0/all/0/1">Rei Ito</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsukada_M/0/1/0/all/0/1">Mineto Tsukada</a>, <a href="http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1">Hiroki Matsutani</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.12301">
                                    <div class="article-summary-box-inner">
                                        <span>Most edge AI focuses on prediction tasks on resource-limited edge devices
while the training is done at server machines. However, retraining or
customizing a model is required at edge devices as the model is becoming
outdated due to environmental changes over time. To follow such a concept
drift, a neural-network based on-device learning approach is recently proposed,
so that edge devices train incoming data at runtime to update their model. In
this case, since a training is done at distributed edge devices, the issue is
that only a limited amount of training data can be used for each edge device.
To address this issue, one approach is a cooperative learning or federated
learning, where edge devices exchange their trained results and update their
model by using those collected from the other devices. In this paper, as an
on-device learning algorithm, we focus on OS-ELM (Online Sequential Extreme
Learning Machine) to sequentially train a model based on recent samples and
combine it with autoencoder for anomaly detection. We extend it for an
on-device federated learning so that edge devices can exchange their trained
results and update their model by using those collected from the other edge
devices. This cooperative model update is one-shot while it can be repeatedly
applied to synchronize their model. Our approach is evaluated with anomaly
detection tasks generated from a driving dataset of cars, a human activity
dataset, and MNIST dataset. The results demonstrate that the proposed on-device
federated learning can produce a merged model by integrating trained results
from multiple edge devices as accurately as traditional backpropagation based
neural networks and a traditional federated learning approach with lower
computation or communication cost.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Classifying high-dimensional Gaussian mixtures: Where kernel methods fail and neural networks succeed. (arXiv:2102.11742v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Refinetti_M/0/1/0/all/0/1">Maria Refinetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldt_S/0/1/0/all/0/1">Sebastian Goldt</a>, <a href="http://arxiv.org/find/cs/1/au:+Krzakala_F/0/1/0/all/0/1">Florent Krzakala</a>, <a href="http://arxiv.org/find/cs/1/au:+Zdeborova_L/0/1/0/all/0/1">Lenka Zdeborov&#xe1;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11742">
                                    <div class="article-summary-box-inner">
                                        <span>A recent series of theoretical works showed that the dynamics of neural
networks with a certain initialisation are well-captured by kernel methods.
Concurrent empirical work demonstrated that kernel methods can come close to
the performance of neural networks on some image classification tasks. These
results raise the question of whether neural networks only learn successfully
if kernels also learn successfully, despite neural networks being more
expressive. Here, we show theoretically that two-layer neural networks (2LNN)
with only a few hidden neurons can beat the performance of kernel learning on a
simple Gaussian mixture classification task. We study the high-dimensional
limit where the number of samples is linearly proportional to the input
dimension, and show that while small 2LNN achieve near-optimal performance on
this task, lazy training approaches such as random features and kernel methods
do not. Our analysis is based on the derivation of a closed set of equations
that track the learning dynamics of the 2LNN and thus allow to extract the
asymptotic performance of the network as a function of signal-to-noise ratio
and other hyperparameters. We finally illustrate how over-parametrising the
neural network leads to faster convergence, but does not improve its final
performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-Supervised VQ-VAE for One-Shot Music Style Transfer. (arXiv:2102.05749v2 [cs.SD] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cifka_O/0/1/0/all/0/1">Ond&#x159;ej C&#xed;fka</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozerov_A/0/1/0/all/0/1">Alexey Ozerov</a>, <a href="http://arxiv.org/find/cs/1/au:+Simsekli_U/0/1/0/all/0/1">Umut &#x15e;im&#x15f;ekli</a>, <a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1">Ga&#xeb;l Richard</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05749">
                                    <div class="article-summary-box-inner">
                                        <span>Neural style transfer, allowing to apply the artistic style of one image to
another, has become one of the most widely showcased computer vision
applications shortly after its introduction. In contrast, related tasks in the
music audio domain remained, until recently, largely untackled. While several
style conversion methods tailored to musical signals have been proposed, most
lack the &#x27;one-shot&#x27; capability of classical image style transfer algorithms. On
the other hand, the results of existing one-shot audio style transfer methods
on musical inputs are not as compelling. In this work, we are specifically
interested in the problem of one-shot timbre transfer. We present a novel
method for this task, based on an extension of the vector-quantized variational
autoencoder (VQ-VAE), along with a simple self-supervised learning strategy
designed to obtain disentangled representations of timbre and pitch. We
evaluate the method using a set of objective metrics and show that it is able
to outperform selected baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases. (arXiv:2103.10697v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+dAscoli_S/0/1/0/all/0/1">St&#xe9;phane d&#x27;Ascoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1">Hugo Touvron</a>, <a href="http://arxiv.org/find/cs/1/au:+Leavitt_M/0/1/0/all/0/1">Matthew Leavitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1">Ari Morcos</a>, <a href="http://arxiv.org/find/cs/1/au:+Biroli_G/0/1/0/all/0/1">Giulio Biroli</a>, <a href="http://arxiv.org/find/cs/1/au:+Sagun_L/0/1/0/all/0/1">Levent Sagun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10697">
                                    <div class="article-summary-box-inner">
                                        <span>Convolutional architectures have proven extremely successful for vision
tasks. Their hard inductive biases enable sample-efficient learning, but come
at the cost of a potentially lower performance ceiling. Vision Transformers
(ViTs) rely on more flexible self-attention layers, and have recently
outperformed CNNs for image classification. However, they require costly
pre-training on large external datasets or distillation from pre-trained
convolutional networks. In this paper, we ask the following question: is it
possible to combine the strengths of these two architectures while avoiding
their respective limitations? To this end, we introduce gated positional
self-attention (GPSA), a form of positional self-attention which can be
equipped with a &#x60;&#x60;soft&quot; convolutional inductive bias. We initialise the GPSA
layers to mimic the locality of convolutional layers, then give each attention
head the freedom to escape locality by adjusting a gating parameter regulating
the attention paid to position versus content information. The resulting
convolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet,
while offering a much improved sample efficiency. We further investigate the
role of locality in learning by first quantifying how it is encouraged in
vanilla self-attention layers, then analysing how it is escaped in GPSA layers.
We conclude by presenting various ablations to better understand the success of
the ConViT. Our code and models are released publicly at
https://github.com/facebookresearch/convit.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GIST: Distributed Training for Large-Scale Graph Convolutional Networks. (arXiv:2102.10424v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1">Cameron R. Wolfe</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingkang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1">Arindam Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1">Chen Dun</a>, <a href="http://arxiv.org/find/cs/1/au:+Bayer_A/0/1/0/all/0/1">Artun Bayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1">Santiago Segarra</a>, <a href="http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1">Anastasios Kyrillidis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10424">
                                    <div class="article-summary-box-inner">
                                        <span>The graph convolutional network (GCN) is a go-to solution for machine
learning on graphs, but its training is notoriously difficult to scale both in
terms of graph size and the number of model parameters. Although some work has
explored training on large-scale graphs (e.g., GraphSAGE, ClusterGCN, etc.), we
pioneer efficient training of large-scale GCN models (i.e., ultra-wide,
overparameterized models) with the proposal of a novel, distributed training
framework. Our proposed training methodology, called GIST, disjointly
partitions the parameters of a GCN model into several, smaller sub-GCNs that
are trained independently and in parallel. In addition to being compatible with
any GCN architecture, GIST improves model performance, scales to training on
arbitrarily large graphs, significantly decreases wall-clock training time, and
enables the training of markedly overparameterized GCN models. Remarkably, with
GIST, we train an astonishgly-wide 32,768-dimensional GraphSAGE model, which
exceeds the capacity of a single GPU by a factor of 8X, to SOTA performance on
the Amazon2M dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ATOM3D: Tasks On Molecules in Three Dimensions. (arXiv:2012.04035v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Townshend_R/0/1/0/all/0/1">Raphael J.L. Townshend</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogele_M/0/1/0/all/0/1">Martin V&#xf6;gele</a>, <a href="http://arxiv.org/find/cs/1/au:+Suriana_P/0/1/0/all/0/1">Patricia Suriana</a>, <a href="http://arxiv.org/find/cs/1/au:+Derry_A/0/1/0/all/0/1">Alexander Derry</a>, <a href="http://arxiv.org/find/cs/1/au:+Powers_A/0/1/0/all/0/1">Alexander Powers</a>, <a href="http://arxiv.org/find/cs/1/au:+Laloudakis_Y/0/1/0/all/0/1">Yianni Laloudakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Balachandar_S/0/1/0/all/0/1">Sidhika Balachandar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1">Bowen Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1">Brandon Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Eismann_S/0/1/0/all/0/1">Stephan Eismann</a>, <a href="http://arxiv.org/find/cs/1/au:+Kondor_R/0/1/0/all/0/1">Risi Kondor</a>, <a href="http://arxiv.org/find/cs/1/au:+Altman_R/0/1/0/all/0/1">Russ B. Altman</a>, <a href="http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1">Ron O. Dror</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04035">
                                    <div class="article-summary-box-inner">
                                        <span>Computational methods that operate on three-dimensional molecular structure
have the potential to solve important questions in biology and chemistry. In
particular, deep neural networks have gained significant attention, but their
widespread adoption in the biomolecular domain has been limited by a lack of
either systematic performance benchmarks or a unified toolkit for interacting
with molecular data. To address this, we present ATOM3D, a collection of both
novel and existing benchmark datasets spanning several key classes of
biomolecules. We implement several classes of three-dimensional molecular
learning methods for each of these tasks and show that they consistently
improve performance relative to methods based on one- and two-dimensional
representations. The specific choice of architecture proves to be critical for
performance, with three-dimensional convolutional networks excelling at tasks
involving complex geometries, graph networks performing well on systems
requiring detailed positional information, and the more recently developed
equivariant networks showing significant promise. Our results indicate that
many molecular problems stand to gain from three-dimensional molecular
learning, and that there is potential for improvement on many tasks which
remain underexplored. To lower the barrier to entry and facilitate further
developments in the field, we also provide a comprehensive suite of tools for
dataset processing, model training, and evaluation in our open-source atom3d
Python package. All datasets are available for download from
https://www.atom3d.ai .</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">UnICORNN: A recurrent model for learning very long time dependencies. (arXiv:2103.05487v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rusch_T/0/1/0/all/0/1">T. Konstantin Rusch</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Siddhartha Mishra</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05487">
                                    <div class="article-summary-box-inner">
                                        <span>The design of recurrent neural networks (RNNs) to accurately process
sequential inputs with long-time dependencies is very challenging on account of
the exploding and vanishing gradient problem. To overcome this, we propose a
novel RNN architecture which is based on a structure preserving discretization
of a Hamiltonian system of second-order ordinary differential equations that
models networks of oscillators. The resulting RNN is fast, invertible (in
time), memory efficient and we derive rigorous bounds on the hidden state
gradients to prove the mitigation of the exploding and vanishing gradient
problem. A suite of experiments are presented to demonstrate that the proposed
RNN provides state of the art performance on a variety of learning tasks with
(very) long-time dependencies.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Benign Overfitting of Constant-Stepsize SGD for Linear Regression. (arXiv:2103.12692v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1">Difan Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jingfeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1">Vladimir Braverman</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1">Quanquan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1">Sham M. Kakade</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12692">
                                    <div class="article-summary-box-inner">
                                        <span>There is an increasing realization that algorithmic inductive biases are
central in preventing overfitting; empirically, we often see a benign
overfitting phenomenon in overparameterized settings for natural learning
algorithms, such as stochastic gradient descent (SGD), where little to no
explicit regularization has been employed. This work considers this issue in
arguably the most basic setting: constant-stepsize SGD (with iterate averaging)
for linear regression in the overparameterized regime. Our main result provides
a sharp excess risk bound, stated in terms of the full eigenspectrum of the
data covariance matrix, that reveals a bias-variance decomposition
characterizing when generalization is possible: (i) the variance bound is
characterized in terms of an effective dimension (specific for SGD) and (ii)
the bias bound provides a sharp geometric characterization in terms of the
location of the initial iterate (and how it aligns with the data covariance
matrix). We reflect on a number of notable differences between the algorithmic
regularization afforded by (unregularized) SGD in comparison to ordinary least
squares (minimum-norm interpolation) and ridge regression.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v1 [eess.SP] CROSS LISTED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Thanthrige_U/0/1/0/all/0/1">Udaya S.K.P. Miriya Thanthrige</a>, <a href="http://arxiv.org/find/eess/1/au:+Jung_P/0/1/0/all/0/1">Peter Jung</a>, <a href="http://arxiv.org/find/eess/1/au:+Sezgin_A/0/1/0/all/0/1">Aydin Sezgin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03686">
                                    <div class="article-summary-box-inner">
                                        <span>We address the detection of material defects, which are inside a layered
material structure using compressive sensing based multiple-output (MIMO)
wireless radar. Here, the strong clutter due to the reflection of the layered
structure&#x27;s surface often makes the detection of the defects challenging. Thus,
sophisticated signal separation methods are required for improved defect
detection. In many scenarios, the number of defects that we are interested in
is limited and the signaling response of the layered structure can be modeled
as a low-rank structure. Therefore, we propose joint rank and sparsity
minimization for defect detection. In particular, we propose a non-convex
approach based on the iteratively reweighted nuclear and $\ell_1-$norm (a
double-reweighted approach) to obtain a higher accuracy compared to the
conventional nuclear norm and $\ell_1-$norm minimization. To this end, an
iterative algorithm is designed to estimate the low-rank and sparse
contributions. Further, we propose deep learning to learn the parameters of the
algorithm (i.e., algorithm unfolding) to improve the accuracy and the speed of
convergence of the algorithm. Our numerical results show that the proposed
approach outperforms the conventional approaches in terms of mean square errors
of the recovered low-rank and sparse components and the speed of convergence.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes. (arXiv:2102.06356v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nado_Z/0/1/0/all/0/1">Zachary Nado</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilmer_J/0/1/0/all/0/1">Justin M. Gilmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Shallue_C/0/1/0/all/0/1">Christopher J. Shallue</a>, <a href="http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1">Rohan Anil</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahl_G/0/1/0/all/0/1">George E. Dahl</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06356">
                                    <div class="article-summary-box-inner">
                                        <span>Recently the LARS and LAMB optimizers have been proposed for training neural
networks faster using large batch sizes. LARS and LAMB add layer-wise
normalization to the update rules of Heavy-ball momentum and Adam,
respectively, and have become popular in prominent benchmarks and deep learning
libraries. However, without fair comparisons to standard optimizers, it remains
an open question whether LARS and LAMB have any benefit over traditional,
generic algorithms. In this work we demonstrate that standard optimization
algorithms such as Nesterov momentum and Adam can match or exceed the results
of LARS and LAMB at large batch sizes. Our results establish new, stronger
baselines for future comparisons at these batch sizes and shed light on the
difficulties of comparing optimizers for neural network training more
generally.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Classification of head impacts based on the spectral density of measurable kinematics. (arXiv:2104.09082v2 [q-bio.QM] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/q-bio/1/au:+Zhan_X/0/1/0/all/0/1">Xianghao Zhan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1">Yiheng Li</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_Y/0/1/0/all/0/1">Yuzhe Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Cecchi_N/0/1/0/all/0/1">Nicholas J. Cecchi</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Raymond_S/0/1/0/all/0/1">Samuel J. Raymond</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhou_Z/0/1/0/all/0/1">Zhou Zhou</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Alizadeh_H/0/1/0/all/0/1">Hossein Vahid Alizadeh</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ruan_J/0/1/0/all/0/1">Jesse Ruan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Barbat_S/0/1/0/all/0/1">Saeed Barbat</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tiernan_S/0/1/0/all/0/1">Stephen Tiernan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gevaert_O/0/1/0/all/0/1">Olivier Gevaert</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zeineh_M/0/1/0/all/0/1">Michael M. Zeineh</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Grant_G/0/1/0/all/0/1">Gerald A. Grant</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Camarillo_D/0/1/0/all/0/1">David B. Camarillo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09082">
                                    <div class="article-summary-box-inner">
                                        <span>Traumatic brain injury can be caused by head impacts, but many brain injury
risk estimation models are less accurate across the variety of impacts that
patients may undergo. We investigated the spectral characteristics of different
head impact types with kinematics classification. Data was analyzed from 3,262
head impacts from lab reconstruction, American football, mixed martial arts,
and publicly available car crash data. A random forest classifier with spectral
densities of linear acceleration and angular velocity was built to classify
head impact types (e.g., football), reaching a median accuracy of 96% over
1,000 random partitions of training and test sets. To test the classifier on
data from different measurement devices, another 271 lab-reconstructed impacts
were obtained from 5 other instrumented mouthguards with the classifier
reaching over 96% accuracy. The most important features in the classification
included both low-frequency and high-frequency features, both linear
acceleration features and angular velocity features. Different head impact
types had different distributions of spectral densities in low-frequency and
high-frequency ranges (e.g., the spectral densities of MMA impacts were higher
in high-frequency range than in the low-frequency range). Finally, with the
classifier, type-specific, nearest-neighbor regression models were built for
95th percentile maximum principal strain, 95th percentile maximum principal
strain in corpus callosum, and cumulative strain damage (15th percentile). This
showed a generally higher R2-value than baseline models. The classifier enables
a better understanding of the impact kinematics in different sports, and it can
be applied to evaluate the quality of impact-simulation systems and on-field
data augmentation. Key words: traumatic brain injury, head impacts,
classification, impact kinematics</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Vision Transformers with Patch Diversification. (arXiv:2104.12753v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1">Chengyue Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dilin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Meng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1">Vikas Chandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qiang Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12753">
                                    <div class="article-summary-box-inner">
                                        <span>Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code will be made publicly available soon.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning. (arXiv:2007.04938v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kimin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1">Michael Laskin</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1">Aravind Srinivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.04938">
                                    <div class="article-summary-box-inner">
                                        <span>Model-free deep reinforcement learning (RL) has been successful in a range of
challenging domains. However, there are some remaining issues, such as
stabilizing the optimization of nonlinear function approximators, preventing
error propagation due to the Bellman backup in Q-learning, and efficient
exploration. To mitigate these issues, we present SUNRISE, a simple unified
ensemble method, which is compatible with various off-policy RL algorithms.
SUNRISE integrates three key ingredients: (a) bootstrap with random
initialization which improves the stability of the learning process by training
a diverse ensemble of agents, (b) weighted Bellman backups, which prevent error
propagation in Q-learning by reweighing sample transitions based on uncertainty
estimates from the ensembles, and (c) an inference method that selects actions
using highest upper-confidence bounds for efficient exploration. Our
experiments show that SUNRISE significantly improves the performance of
existing off-policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN,
for both continuous and discrete control tasks on both low-dimensional and
high-dimensional environments. Our training code is available at
https://github.com/pokaxpoka/sunrise.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 Prediction. (arXiv:2105.00620v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Er_S/0/1/0/all/0/1">Siawpeng Er</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shihao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1">Tuo Zhao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00620">
                                    <div class="article-summary-box-inner">
                                        <span>The global spread of COVID-19, the disease caused by the novel coronavirus
SARS-CoV-2, has cast a significant threat to mankind. As the COVID-19 situation
continues to evolve, predicting localized disease severity is crucial for
advanced resource allocation. This paper proposes a method named COURAGE
(COUnty aggRegation mixup AuGmEntation) to generate a short-term prediction of
2-week-ahead COVID-19 related deaths for each county in the United States,
leveraging modern deep learning techniques. Specifically, our method adopts a
self-attention model from Natural Language Processing, known as the transformer
model, to capture both short-term and long-term dependencies within the time
series while enjoying computational efficiency. Our model fully utilizes
publicly available information of COVID-19 related confirmed cases, deaths,
community mobility trends and demographic information, and can produce
state-level prediction as an aggregation of the corresponding county-level
predictions. Our numerical experiments demonstrate that our model achieves the
state-of-the-art performance among the publicly available benchmark models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SCC: an efficient deep reinforcement learning agent mastering the game of StarCraft II. (arXiv:2012.13169v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiangjun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Junxiao Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1">Penghui Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1">Peng Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhenkun Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weimin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pi_X/0/1/0/all/0/1">Xiongjun Pi</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jujie He</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1">Chao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_H/0/1/0/all/0/1">Haitao Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1">Quan Yuan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13169">
                                    <div class="article-summary-box-inner">
                                        <span>AlphaStar, the AI that reaches GrandMaster level in StarCraft II, is a
remarkable milestone demonstrating what deep reinforcement learning can achieve
in complex Real-Time Strategy (RTS) games. However, the complexities of the
game, algorithms and systems, and especially the tremendous amount of
computation needed are big obstacles for the community to conduct further
research in this direction. We propose a deep reinforcement learning agent,
StarCraft Commander (SCC). With order of magnitude less computation, it
demonstrates top human performance defeating GrandMaster players in test
matches and top professional players in a live event. Moreover, it shows strong
robustness to various human strategies and discovers novel strategies unseen
from human plays. In this paper, we will share the key insights and
optimizations on efficient imitation learning and reinforcement learning for
StarCraft II full game.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The query complexity of sampling from strongly log-concave distributions in one dimension. (arXiv:2105.14163v2 [math.ST] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Chewi_S/0/1/0/all/0/1">Sinho Chewi</a>, <a href="http://arxiv.org/find/math/1/au:+Gerber_P/0/1/0/all/0/1">Patrik Gerber</a>, <a href="http://arxiv.org/find/math/1/au:+Lu_C/0/1/0/all/0/1">Chen Lu</a>, <a href="http://arxiv.org/find/math/1/au:+Gouic_T/0/1/0/all/0/1">Thibaut Le Gouic</a>, <a href="http://arxiv.org/find/math/1/au:+Rigollet_P/0/1/0/all/0/1">Philippe Rigollet</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14163">
                                    <div class="article-summary-box-inner">
                                        <span>We establish the first tight lower bound of $\Omega(\log\log\kappa)$ on the
query complexity of sampling from the class of strongly log-concave and
log-smooth distributions with condition number $\kappa$ in one dimension.
Whereas existing guarantees for MCMC-based algorithms scale polynomially in
$\kappa$, we introduce a novel algorithm based on rejection sampling that
closes this doubly exponential gap.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial Graph Augmentation to Improve Graph Contrastive Learning. (arXiv:2106.05819v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1">Susheel Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1">Cong Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1">Jennifer Neville</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05819">
                                    <div class="article-summary-box-inner">
                                        <span>Self-supervised learning of graph neural networks (GNN) is in great need
because of the widespread label scarcity issue in real-world graph/network
data. Graph contrastive learning (GCL), by training GNNs to maximize the
correspondence between the representations of the same graph in its different
augmented forms, may yield robust and transferable GNNs even without using
labels. However, GNNs trained by traditional GCL often risk capturing redundant
graph features and thus may be brittle and provide sub-par performance in
downstream tasks. Here, we propose a novel principle, termed adversarial-GCL
(AD-GCL), which enables GNNs to avoid capturing redundant information during
the training by optimizing adversarial graph augmentation strategies used in
GCL. We pair AD-GCL with theoretical explanations and design a practical
instantiation based on trainable edge-dropping graph augmentation. We
experimentally validate AD-GCL by comparing with the state-of-the-art GCL
methods and achieve performance gains of up-to $14\%$ in unsupervised, $6\%$ in
transfer, and $3\%$ in semi-supervised learning settings overall with 18
different benchmark datasets for the tasks of molecule property regression and
classification, and social network classification.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Large Norms of CNN Layers Do Not Hurt Adversarial Robustness. (arXiv:2009.08435v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Youwei Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Dong Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08435">
                                    <div class="article-summary-box-inner">
                                        <span>Since the Lipschitz properties of CNN are widely considered to be related to
adversarial robustness, we theoretically characterize the $\ell_1$ norm and
$\ell_\infty$ norm of 2D multi-channel convolutional layers and provide
efficient methods to compute the exact $\ell_1$ norm and $\ell_\infty$ norm.
Based on our theorem, we propose a novel regularization method termed norm
decay, which can effectively reduce the norms of convolutional layers and
fully-connected layers. Experiments show that norm-regularization methods,
including norm decay, weight decay, and singular value clipping, can improve
generalization of CNNs. However, they can slightly hurt adversarial robustness.
Observing this unexpected phenomenon, we compute the norms of layers in the
CNNs trained with three different adversarial training frameworks and
surprisingly find that adversarially robust CNNs have comparable or even larger
layer norms than their non-adversarially robust counterparts. Furthermore, we
prove that under a mild assumption, adversarially robust classifiers can be
achieved, and can have an arbitrarily large Lipschitz constant. For this
reason, enforcing small norms on CNN layers may be neither necessary nor
effective in achieving adversarial robustness. The code is available at
https://github.com/youweiliang/norm_robustness.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Stability and Convergence of Stochastic Gradient Clipping: Beyond Lipschitz Continuity and Smoothness. (arXiv:2102.06489v2 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Mai_V/0/1/0/all/0/1">Vien V. Mai</a>, <a href="http://arxiv.org/find/math/1/au:+Johansson_M/0/1/0/all/0/1">Mikael Johansson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06489">
                                    <div class="article-summary-box-inner">
                                        <span>Stochastic gradient algorithms are often unstable when applied to functions
that do not have Lipschitz-continuous and/or bounded gradients. Gradient
clipping is a simple and effective technique to stabilize the training process
for problems that are prone to the exploding gradient problem. Despite its
widespread popularity, the convergence properties of the gradient clipping
heuristic are poorly understood, especially for stochastic problems. This paper
establishes both qualitative and quantitative convergence results of the
clipped stochastic (sub)gradient method (SGD) for non-smooth convex functions
with rapidly growing subgradients. Our analyses show that clipping enhances the
stability of SGD and that the clipped SGD algorithm enjoys finite convergence
rates in many cases. We also study the convergence of a clipped method with
momentum, which includes clipped SGD as a special case, for weakly convex
problems under standard assumptions. With a novel Lyapunov analysis, we show
that the proposed method achieves the best-known rate for the considered class
of problems, demonstrating the effectiveness of clipped methods also in this
regime. Numerical results confirm our theoretical developments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Empirical observations on the effects of data transformation in machine learning classification of geological domains. (arXiv:2106.05855v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Leung_R/0/1/0/all/0/1">Raymond Leung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05855">
                                    <div class="article-summary-box-inner">
                                        <span>In the literature, a large body of work advocates the use of log-ratio
transformation for multivariate statistical analysis of compositional data. In
contrast, few studies have looked at how data transformation changes the
efficacy of machine learning classifiers within geoscience. This letter
presents experiment results and empirical observations to further explore this
issue. The objective is to study the effects of data transformation on geozone
classification performance when machine learning (ML) classifiers/estimators
are trained using geochemical data. The training input consists of exploration
hole assay samples obtained from a Pilbara iron-ore deposit in Western
Australia, and geozone labels assigned based on stratigraphic units, the
absence or presence and type of mineralization. The ML techniques considered
are multinomial logistic regression, Gaussian na\&quot;{i}ve Bayes, kNN, linear
support vector classifier, RBF-SVM, gradient boosting and extreme GB, random
forest (RF) and multi-layer perceptron (MLP). The transformations examined
include isometric log-ratio (ILR), center log-ratio (CLR) coupled with
principal component analysis (PCA) or independent component analysis (ICA), and
a manifold learning approach based on local linear embedding (LLE). The results
reveal that different ML classifiers exhibit varying sensitivity to these
transformations, with some clearly more advantageous or deleterious than
others. Overall, the best performing candidate is ILR which is unsurprising
considering the compositional nature of the data. The performance of pairwise
log-ratio (PWLR) transformation is better than ILR for ensemble and tree-based
learners such as boosting and RF; but worse for MLP, SVM and other classifiers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Causality in Neural Networks -- An Extended Abstract. (arXiv:2106.05842v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1">Abbavaram Gowtham Reddy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05842">
                                    <div class="article-summary-box-inner">
                                        <span>Causal reasoning is the main learning and explanation tool used by humans. AI
systems should possess causal reasoning capabilities to be deployed in the real
world with trust and reliability. Introducing the ideas of causality to machine
learning helps in providing better learning and explainable models.
Explainability, causal disentanglement are some important aspects of any
machine learning model. Causal explanations are required to believe in a
model&#x27;s decision and causal disentanglement learning is important for transfer
learning applications. We exploit the ideas of causality to be used in deep
learning models to achieve better and causally explainable models that are
useful in fairness, disentangled representation, etc.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Simple Graph Convolutional Networks. (arXiv:2106.05809v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pasa_L/0/1/0/all/0/1">Luca Pasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Navarin_N/0/1/0/all/0/1">Nicol&#xf2; Navarin</a>, <a href="http://arxiv.org/find/cs/1/au:+Erb_W/0/1/0/all/0/1">Wolfgang Erb</a>, <a href="http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1">Alessandro Sperduti</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05809">
                                    <div class="article-summary-box-inner">
                                        <span>Many neural networks for graphs are based on the graph convolution operator,
proposed more than a decade ago. Since then, many alternative definitions have
been proposed, that tend to add complexity (and non-linearity) to the model. In
this paper, we follow the opposite direction by proposing simple graph
convolution operators, that can be implemented in single-layer graph
convolutional networks. We show that our convolution operators are more
theoretically grounded than many proposals in literature, and exhibit
state-of-the-art predictive performance on the considered benchmark datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Analysis and Design of Thompson Sampling for Stochastic Partial Monitoring. (arXiv:2006.09668v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Tsuchiya_T/0/1/0/all/0/1">Taira Tsuchiya</a>, <a href="http://arxiv.org/find/stat/1/au:+Honda_J/0/1/0/all/0/1">Junya Honda</a>, <a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1">Masashi Sugiyama</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.09668">
                                    <div class="article-summary-box-inner">
                                        <span>We investigate finite stochastic partial monitoring, which is a general model
for sequential learning with limited feedback. While Thompson sampling is one
of the most promising algorithms on a variety of online decision-making
problems, its properties for stochastic partial monitoring have not been
theoretically investigated, and the existing algorithm relies on a heuristic
approximation of the posterior distribution. To mitigate these problems, we
present a novel Thompson-sampling-based algorithm, which enables us to exactly
sample the target parameter from the posterior distribution. Besides, we prove
that the new algorithm achieves the logarithmic problem-dependent expected
pseudo-regret $\mathrm{O}(\log T)$ for a linearized variant of the problem with
local observability. This result is the first regret bound of Thompson sampling
for partial monitoring, which also becomes the first logarithmic regret bound
of Thompson sampling for linear bandits.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures. (arXiv:2104.05379v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rossenbach_N/0/1/0/all/0/1">Nick Rossenbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1">Mohammad Zeineldeen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hilmes_B/0/1/0/all/0/1">Benedikt Hilmes</a>, <a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1">Ralf Schl&#xfc;ter</a>, <a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1">Hermann Ney</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05379">
                                    <div class="article-summary-box-inner">
                                        <span>Recent publications on automatic-speech-recognition (ASR) have a strong focus
on attention encoder-decoder (AED) architectures which work well for large
datasets, but tend to overfit when applied in low resource scenarios. One
solution to tackle this issue is to generate synthetic data with a trained
text-to-speech system (TTS) if additional text is available. This was
successfully applied in many publications with AED systems. We present a novel
approach of silence correction in the data pre-processing for TTS systems which
increases the robustness when training on corpora targeted for ASR
applications. In this work we do not only show the successful application of
synthetic data for AED systems, but also test the same method on a highly
optimized state-of-the-art Hybrid ASR system and a competitive monophone based
system using connectionist-temporal-classification (CTC). We show that for the
later systems the addition of synthetic data only has a minor effect, but they
still outperform the AED systems by a large margin on LibriSpeech-100h. We
achieve a final word-error-rate of 3.3%/10.0% with a Hybrid system on the
clean/noisy test-sets, surpassing any previous state-of-the-art systems that do
not include unlabeled audio data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust MAML: Prioritization task buffer with adaptive learning process for model-agnostic meta-learning. (arXiv:2103.08233v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Thanh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luu_T/0/1/0/all/0/1">Tung Luu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1">Trung Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Rakhimkul_S/0/1/0/all/0/1">Sanzhar Rakhimkul</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1">Chang D. Yoo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08233">
                                    <div class="article-summary-box-inner">
                                        <span>Model agnostic meta-learning (MAML) is a popular state-of-the-art
meta-learning algorithm that provides good weight initialization of a model
given a variety of learning tasks. The model initialized by provided weight can
be fine-tuned to an unseen task despite only using a small amount of samples
and within a few adaptation steps. MAML is simple and versatile but requires
costly learning rate tuning and careful design of the task distribution which
affects its scalability and generalization. This paper proposes a more robust
MAML based on an adaptive learning scheme and a prioritization task buffer(PTB)
referred to as Robust MAML (RMAML) for improving scalability of training
process and alleviating the problem of distribution mismatch. RMAML uses
gradient-based hyper-parameter optimization to automatically find the optimal
learning rate and uses the PTB to gradually adjust train-ing task distribution
toward testing task distribution over the course of training. Experimental
results on meta reinforcement learning environments demonstrate a substantial
performance gain as well as being less sensitive to hyper-parameter choice and
robust to distribution mismatch.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Feature Replacement and Combination for Hybrid ASR Systems. (arXiv:2104.04298v2 [eess.AS] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Vieting_P/0/1/0/all/0/1">Peter Vieting</a>, <a href="http://arxiv.org/find/eess/1/au:+Luscher_C/0/1/0/all/0/1">Christoph L&#xfc;scher</a>, <a href="http://arxiv.org/find/eess/1/au:+Michel_W/0/1/0/all/0/1">Wilfried Michel</a>, <a href="http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1">Ralf Schl&#xfc;ter</a>, <a href="http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1">Hermann Ney</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04298">
                                    <div class="article-summary-box-inner">
                                        <span>Acoustic modeling of raw waveform and learning feature extractors as part of
the neural network classifier has been the goal of many studies in the area of
automatic speech recognition (ASR). Recently, one line of research has focused
on frameworks that can be pre-trained on audio-only data in an unsupervised
fashion and aim at improving downstream ASR tasks. In this work, we investigate
the usefulness of one of these front-end frameworks, namely wav2vec, for hybrid
ASR systems. In addition to deploying a pre-trained feature extractor, we
explore how to make use of an existing acoustic model (AM) trained on the same
task with different features as well. Another neural front-end which is only
trained together with the supervised ASR loss as well as traditional Gammatone
features are applied for comparison. Moreover, it is shown that the AM can be
retrofitted with i-vectors for speaker adaptation. Finally, the described
features are combined in order to further advance the performance. With the
final best system, we obtain a relative improvement of 4% and 6% over our
previous best model on the LibriSpeech test-clean and test-other sets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Toward Deep Supervised Anomaly Detection: Reinforcement Learning from Partially Labeled Anomaly Data. (arXiv:2009.06847v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1">Guansong Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1">Anton van den Hengel</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1">Longbing Cao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06847">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of anomaly detection with a small set of partially
labeled anomaly examples and a large-scale unlabeled dataset. This is a common
scenario in many important applications. Existing related methods either
exclusively fit the limited anomaly examples that typically do not span the
entire set of anomalies, or proceed with unsupervised learning from the
unlabeled data. We propose here instead a deep reinforcement learning-based
approach that enables an end-to-end optimization of the detection of both
labeled and unlabeled anomalies. This approach learns the known abnormality by
automatically interacting with an anomaly-biased simulation environment, while
continuously extending the learned abnormality to novel classes of anomaly
(i.e., unknown anomalies) by actively exploring possible anomalies in the
unlabeled data. This is achieved by jointly optimizing the exploitation of the
small labeled anomaly data and the exploration of the rare unlabeled anomalies.
Extensive experiments on 48 real-world datasets show that our model
significantly outperforms five state-of-the-art competing methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Align, then memorise: the dynamics of learning with feedback alignment. (arXiv:2011.12428v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Refinetti_M/0/1/0/all/0/1">Maria Refinetti</a>, <a href="http://arxiv.org/find/stat/1/au:+dAscoli_S/0/1/0/all/0/1">St&#xe9;phane d&#x27;Ascoli</a>, <a href="http://arxiv.org/find/stat/1/au:+Ohana_R/0/1/0/all/0/1">Ruben Ohana</a>, <a href="http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1">Sebastian Goldt</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12428">
                                    <div class="article-summary-box-inner">
                                        <span>Direct Feedback Alignment (DFA) is emerging as an efficient and biologically
plausible alternative to the ubiquitous backpropagation algorithm for training
deep neural networks. Despite relying on random feedback weights for the
backward pass, DFA successfully trains state-of-the-art models such as
Transformers. On the other hand, it notoriously fails to train convolutional
networks. An understanding of the inner workings of DFA to explain these
diverging results remains elusive. Here, we propose a theory for the success of
DFA. We first show that learning in shallow networks proceeds in two steps: an
alignment phase, where the model adapts its weights to align the approximate
gradient with the true gradient of the loss function, is followed by a
memorisation phase, where the model focuses on fitting the data. This two-step
process has a degeneracy breaking effect: out of all the low-loss solutions in
the landscape, a network trained with DFA naturally converges to the solution
which maximises gradient alignment. We also identify a key quantity underlying
alignment in deep linear networks: the conditioning of the alignment matrices.
The latter enables a detailed understanding of the impact of data structure on
alignment, and suggests a simple explanation for the well-known failure of DFA
to train convolutional neural networks. Numerical experiments on MNIST and
CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and
show that the align-then-memorise process occurs sequentially from the bottom
layers of the network to the top.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation. (arXiv:2106.05969v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhengyi Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1">Ryo Hachiuma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Ye Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1">Kris Kitani</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05969">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a method for object-aware 3D egocentric pose estimation that
tightly integrates kinematics modeling, dynamics modeling, and scene object
information. Unlike prior kinematics or dynamics-based approaches where the two
components are used disjointly, we synergize the two approaches via
dynamics-regulated training. At each timestep, a kinematic model is used to
provide a target pose using video evidence and simulation state. Then, a
prelearned dynamics model attempts to mimic the kinematic pose in a physics
simulator. By comparing the pose instructed by the kinematic model against the
pose generated by the dynamics model, we can use their misalignment to further
improve the kinematic model. By factoring in the 6DoF pose of objects (e.g.,
chairs, boxes) in the scene, we demonstrate for the first time, the ability to
estimate physically-plausible 3D human-object interactions using a single
wearable camera. We evaluate our egocentric pose estimation method in both
controlled laboratory settings and real-world scenarios.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Interferometric Graph Transform for Community Labeling. (arXiv:2106.05875v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Grinsztajn_N/0/1/0/all/0/1">Nathan Grinsztajn</a> (Scool), <a href="http://arxiv.org/find/cs/1/au:+Leconte_L/0/1/0/all/0/1">Louis Leconte</a> (MLIA, CMAP), <a href="http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1">Philippe Preux</a> (Scool), <a href="http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1">Edouard Oyallon</a> (MLIA)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05875">
                                    <div class="article-summary-box-inner">
                                        <span>We present a new approach for learning unsupervised node representations in
community graphs. We significantly extend the Interferometric Graph Transform
(IGT) to community labeling: this non-linear operator iteratively extracts
features that take advantage of the graph topology through demodulation
operations. An unsupervised feature extraction step cascades modulus
non-linearity with linear operators that aim at building relevant invariants
for community labeling. Via a simplified model, we show that the IGT
concentrates around the E-IGT: those two representations are related through
some ergodicity properties. Experiments on community labeling tasks show that
this unsupervised representation achieves performances at the level of the
state of the art on the standard and challenging datasets Cora, Citeseer,
Pubmed and WikiCS.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Leveraged Weighted Loss for Partial Label Learning. (arXiv:2106.05731v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hongwei Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jingyi Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Hang_H/0/1/0/all/0/1">Hanyuan Hang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiabin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yisen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhouchen Lin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05731">
                                    <div class="article-summary-box-inner">
                                        <span>As an important branch of weakly supervised learning, partial label learning
deals with data where each instance is assigned with a set of candidate labels,
whereas only one of them is true. Despite many methodology studies on learning
from partial labels, there still lacks theoretical understandings of their risk
consistent properties under relatively weak assumptions, especially on the link
between theoretical results and the empirical choice of parameters. In this
paper, we propose a family of loss functions named \textit{Leveraged Weighted}
(LW) loss, which for the first time introduces the leverage parameter $\beta$
to consider the trade-off between losses on partial labels and non-partial
ones. From the theoretical side, we derive a generalized result of risk
consistency for the LW loss in learning from partial labels, based on which we
provide guidance to the choice of the leverage parameter $\beta$. In
experiments, we verify the theoretical guidance, and show the high
effectiveness of our proposed LW loss on both benchmark and real datasets
compared with other state-of-the-art partial label learning algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Mathematical Foundation for Robust Machine Learning based on Bias-Variance Trade-off. (arXiv:2106.05522v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_O/0/1/0/all/0/1">Ou Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Weiyao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yingjun Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haixiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1">Qinghu Hou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05522">
                                    <div class="article-summary-box-inner">
                                        <span>A common assumption in machine learning is that samples are independently and
identically distributed (i.i.d). However, the contributions of different
samples are not identical in training. Some samples are difficult to learn and
some samples are noisy. The unequal contributions of samples has a considerable
effect on training performances. Studies focusing on unequal sample
contributions (e.g., easy, hard, noisy) in learning usually refer to these
contributions as robust machine learning (RML). Weighing and regularization are
two common techniques in RML. Numerous learning algorithms have been proposed
but the strategies for dealing with easy/hard/noisy samples differ or even
contradict with different learning algorithms. For example, some strategies
take the hard samples first, whereas some strategies take easy first.
Conducting a clear comparison for existing RML algorithms in dealing with
different samples is difficult due to lack of a unified theoretical framework
for RML. This study attempts to construct a mathematical foundation for RML
based on the bias-variance trade-off theory. A series of definitions and
properties are presented and proved. Several classical learning algorithms are
also explained and compared. Improvements of existing methods are obtained
based on the comparison. A unified method that combines two classical learning
strategies is proposed.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Medical Segmentation Decathlon. (arXiv:2106.05735v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Antonelli_M/0/1/0/all/0/1">Michela Antonelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Reinke_A/0/1/0/all/0/1">Annika Reinke</a>, <a href="http://arxiv.org/find/cs/1/au:+Bakas_S/0/1/0/all/0/1">Spyridon Bakas</a>, <a href="http://arxiv.org/find/cs/1/au:+Farahani_K/0/1/0/all/0/1">Keyvan Farahani</a>, <a href="http://arxiv.org/find/cs/1/au:+AnnetteKopp-Schneider/0/1/0/all/0/1">AnnetteKopp-Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1">Bennett A. Landman</a>, <a href="http://arxiv.org/find/cs/1/au:+Litjens_G/0/1/0/all/0/1">Geert Litjens</a>, <a href="http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1">Bjoern Menze</a>, <a href="http://arxiv.org/find/cs/1/au:+Ronneberger_O/0/1/0/all/0/1">Olaf Ronneberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Summers_R/0/1/0/all/0/1">Ronald M.Summers</a>, <a href="http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1">Bram van Ginneken</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilello_M/0/1/0/all/0/1">Michel Bilello</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilic_P/0/1/0/all/0/1">Patrick Bilic</a>, <a href="http://arxiv.org/find/cs/1/au:+Christ_P/0/1/0/all/0/1">Patrick F. Christ</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_R/0/1/0/all/0/1">Richard K. G. Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Gollub_M/0/1/0/all/0/1">Marc J. Gollub</a>, <a href="http://arxiv.org/find/cs/1/au:+Heckers_S/0/1/0/all/0/1">Stephan H. Heckers</a>, <a href="http://arxiv.org/find/cs/1/au:+Huisman_H/0/1/0/all/0/1">Henkjan Huisman</a>, <a href="http://arxiv.org/find/cs/1/au:+Jarnagin_W/0/1/0/all/0/1">William R. Jarnagin</a>, <a href="http://arxiv.org/find/cs/1/au:+McHugo_M/0/1/0/all/0/1">Maureen K. McHugo</a>, <a href="http://arxiv.org/find/cs/1/au:+Napel_S/0/1/0/all/0/1">Sandy Napel</a>, <a href="http://arxiv.org/find/cs/1/au:+Pernicka_J/0/1/0/all/0/1">Jennifer S. Goli Pernicka</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhode_K/0/1/0/all/0/1">Kawal Rhode</a>, <a href="http://arxiv.org/find/cs/1/au:+Tobon_Gomez_C/0/1/0/all/0/1">Catalina Tobon-Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Vorontsov_E/0/1/0/all/0/1">Eugene Vorontsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Huisman_H/0/1/0/all/0/1">Henkjan Huisman</a>, <a href="http://arxiv.org/find/cs/1/au:+Meakin_J/0/1/0/all/0/1">James A. Meakin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1">Sebastien Ourselin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiesenfarth_M/0/1/0/all/0/1">Manuel Wiesenfarth</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1">Pablo Arbelaez</a>, <a href="http://arxiv.org/find/cs/1/au:+Bae_B/0/1/0/all/0/1">Byeonguk Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sihong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Daza_L/0/1/0/all/0/1">Laura Daza</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianjiang Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Baochun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1">Fabian Isensee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1">Yuanfeng Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1">Fucang Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1">Namkug Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1">Ildoo Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1">Dorit Merhof</a>, <a href="http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1">Akshay Pai</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1">Beomhee Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Perslev_M/0/1/0/all/0/1">Mathias Perslev</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezaiifar_R/0/1/0/all/0/1">Ramin Rezaiifar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rippel_O/0/1/0/all/0/1">Oliver Rippel</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarasua_I/0/1/0/all/0/1">Ignacio Sarasua</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Wei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1">Jaemin Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1">Christian Wachinger</a>, et al. (9 additional authors not shown)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05735">
                                    <div class="article-summary-box-inner">
                                        <span>International challenges have become the de facto standard for comparative
assessment of image analysis algorithms given a specific task. Segmentation is
so far the most widely investigated medical image processing task, but the
various segmentation challenges have typically been organized in isolation,
such that algorithm development was driven by the need to tackle a single
specific clinical problem. We hypothesized that a method capable of performing
well on multiple tasks will generalize well to a previously unseen task and
potentially outperform a custom-designed solution. To investigate the
hypothesis, we organized the Medical Segmentation Decathlon (MSD) - a
biomedical image analysis challenge, in which algorithms compete in a multitude
of both tasks and modalities. The underlying data set was designed to explore
the axis of difficulties typically encountered when dealing with medical
images, such as small data sets, unbalanced labels, multi-site data and small
objects. The MSD challenge confirmed that algorithms with a consistent good
performance on a set of tasks preserved their good average performance on a
different set of previously unseen tasks. Moreover, by monitoring the MSD
winner for two years, we found that this algorithm continued generalizing well
to a wide range of other clinical problems, further confirming our hypothesis.
Three main conclusions can be drawn from this study: (1) state-of-the-art image
segmentation algorithms are mature, accurate, and generalize well when
retrained on unseen tasks; (2) consistent algorithmic performance across
multiple tasks is a strong surrogate of algorithmic generalizability; (3) the
training of accurate AI segmentation models is now commoditized to non AI
experts.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Simplifying Deep Reinforcement Learning via Self-Supervision. (arXiv:2106.05526v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1">Kwei-Herng Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kaixiong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xia Hu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05526">
                                    <div class="article-summary-box-inner">
                                        <span>Supervised regression to demonstrations has been demonstrated to be a stable
way to train deep policy networks. We are motivated to study how we can take
full advantage of supervised loss functions for stably training deep
reinforcement learning agents. This is a challenging task because it is unclear
how the training data could be collected to enable policy improvement. In this
work, we propose Self-Supervised Reinforcement Learning (SSRL), a simple
algorithm that optimizes policies with purely supervised losses. We demonstrate
that, without policy gradient or value estimation, an iterative procedure of
&#x60;&#x60;labeling&quot; data and supervised regression is sufficient to drive stable policy
improvement. By selecting and imitating trajectories with high episodic
rewards, SSRL is surprisingly competitive to contemporary algorithms with more
stable performance and less running time, showing the potential of solving
reinforcement learning with supervised learning techniques. The code is
available at https://github.com/daochenzha/SSRL</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Mode recovery in neural autoregressive sequence modeling. (arXiv:2106.05459v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1">Ilia Kulikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1">Sean Welleck</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1">Kyunghyun Cho</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05459">
                                    <div class="article-summary-box-inner">
                                        <span>Despite its wide use, recent studies have revealed unexpected and undesirable
properties of neural autoregressive sequence models trained with maximum
likelihood, such as an unreasonably high affinity to short sequences after
training and to infinitely long sequences at decoding time. We propose to study
these phenomena by investigating how the modes, or local maxima, of a
distribution are maintained throughout the full learning chain of the
ground-truth, empirical, learned and decoding-induced distributions, via the
newly proposed mode recovery cost. We design a tractable testbed where we build
three types of ground-truth distributions: (1) an LSTM based structured
distribution, (2) an unstructured distribution where probability of a sequence
does not depend on its content, and (3) a product of these two which we call a
semi-structured distribution. Our study reveals both expected and unexpected
findings. First, starting with data collection, mode recovery cost strongly
relies on the ground-truth distribution and is most costly with the
semi-structured distribution. Second, after learning, mode recovery cost from
the ground-truth distribution may increase or decrease compared to data
collection, with the largest cost degradation occurring with the
semi-structured ground-truth distribution. Finally, the ability of the
decoding-induced distribution to recover modes from the learned distribution is
highly impacted by the choices made earlier in the learning chain. We conclude
that future research must consider the entire learning chain in order to fully
understand the potentials and perils and to further improve neural
autoregressive sequence models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Joint Landmark and Structure Learning for Automatic Evaluation of Developmental Dysplasia of the Hip. (arXiv:2106.05458v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xindi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Limin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1">Wufeng Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shengfeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shuangping Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_N/0/1/0/all/0/1">Ning Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1">Dong Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1">Ning Gu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05458">
                                    <div class="article-summary-box-inner">
                                        <span>The ultrasound (US) screening of the infant hip is vital for the early
diagnosis of developmental dysplasia of the hip (DDH). The US diagnosis of DDH
refers to measuring alpha and beta angles that quantify hip joint development.
These two angles are calculated from key anatomical landmarks and structures of
the hip. However, this measurement process is not trivial for sonographers and
usually requires a thorough understanding of complex anatomical structures. In
this study, we propose a multi-task framework to learn the relationships among
landmarks and structures jointly and automatically evaluate DDH. Our multi-task
networks are equipped with three novel modules. Firstly, we adopt Mask R-CNN as
the basic framework to detect and segment key anatomical structures and add one
landmark detection branch to form a new multi-task framework. Secondly, we
propose a novel shape similarity loss to refine the incomplete anatomical
structure prediction robustly and accurately. Thirdly, we further incorporate
the landmark-structure consistent prior to ensure the consistency of the bony
rim estimated from the segmented structure and the detected landmark. In our
experiments, 1,231 US images of the infant hip from 632 patients are collected,
of which 247 images from 126 patients are tested. The average errors in alpha
and beta angles are 2.221 degrees and 2.899 degrees. About 93% and 85%
estimates of alpha and beta angles have errors less than 5 degrees,
respectively. Experimental results demonstrate that the proposed method can
accurately and robustly realize the automatic evaluation of DDH, showing great
potential for clinical application.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DUET: Detection Utilizing Enhancement for Text in Scanned or Captured Documents. (arXiv:2106.05542v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jung_E/0/1/0/all/0/1">Eun-Soo Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1">HyeongGwan Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1">Kyusam Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_Y/0/1/0/all/0/1">Yongkeun Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1">Soonhwan Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Min Soo Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05542">
                                    <div class="article-summary-box-inner">
                                        <span>We present a novel deep neural model for text detection in document images.
For robust text detection in noisy scanned documents, the advantages of
multi-task learning are adopted by adding an auxiliary task of text
enhancement. Namely, our proposed model is designed to perform noise reduction
and text region enhancement as well as text detection. Moreover, we enrich the
training data for the model with synthesized document images that are fully
labeled for text detection and enhancement, thus overcome the insufficiency of
labeled document image data. For the effective exploitation of the synthetic
and real data, the training process is separated in two phases. The first phase
is training only synthetic data in a fully-supervised manner. Then real data
with only detection labels are added in the second phase. The enhancement task
for the real data is weakly-supervised with information from their detection
labels. Our methods are demonstrated in a real document dataset with
performances exceeding those of other text detection methods. Moreover,
ablations are conducted and the results confirm the effectiveness of the
synthetic data, auxiliary task, and weak-supervision. Whereas the existing text
detection studies mostly focus on the text in scenes, our proposed method is
optimized to the applications for the text in scanned documents.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Space-time Mixing Attention for Video Transformer. (arXiv:2106.05968v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1">Adrian Bulat</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Rua_J/0/1/0/all/0/1">Juan-Manuel Perez-Rua</a>, <a href="http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1">Swathikiran Sudhakaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1">Brais Martinez</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1">Georgios Tzimiropoulos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05968">
                                    <div class="article-summary-box-inner">
                                        <span>This paper is on video recognition using Transformers. Very recent attempts
in this area have demonstrated promising results in terms of recognition
accuracy, yet they have been also shown to induce, in many cases, significant
computational overheads due to the additional modelling of the temporal
information. In this work, we propose a Video Transformer model the complexity
of which scales linearly with the number of frames in the video sequence and
hence induces \textit{no overhead} compared to an image-based Transformer
model. To achieve this, our model makes two approximations to the full
space-time attention used in Video Transformers: (a) It restricts time
attention to a local temporal window and capitalizes on the Transformer&#x27;s depth
to obtain full temporal coverage of the video sequence. (b) It uses efficient
space-time mixing to attend \textit{jointly} spatial and temporal locations
without inducing any additional cost on top of a spatial-only attention model.
We also show how to integrate 2 very lightweight mechanisms for global
temporal-only attention which provide additional accuracy improvements at
minimal computational cost. We demonstrate that our model produces very high
recognition accuracy on the most popular video recognition datasets while at
the same time being significantly more efficient than other Video Transformer
models. Code will be made available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An Interpretable Neural Network for Parameter Inference. (arXiv:2106.05536v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Pfitzinger_J/0/1/0/all/0/1">Johann Pfitzinger</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05536">
                                    <div class="article-summary-box-inner">
                                        <span>Adoption of deep neural networks in fields such as economics or finance has
been constrained by the lack of interpretability of model outcomes. This paper
proposes a generative neural network architecture - the parameter encoder
neural network (PENN) - capable of estimating local posterior distributions for
the parameters of a regression model. The parameters fully explain predictions
in terms of the inputs and permit visualization, interpretation and inference
in the presence of complex heterogeneous effects and feature dependencies. The
use of Bayesian inference techniques offers an intuitive mechanism to
regularize local parameter estimates towards a stable solution, and to reduce
noise-fitting in settings of limited data availability. The proposed neural
network is particularly well-suited to applications in economics and finance,
where parameter inference plays an important role. An application to an asset
pricing problem demonstrates how the PENN can be used to explore nonlinear risk
dynamics in financial markets, and to compare empirical nonlinear effects to
behavior posited by financial theory.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Distributionally Robust Prescriptive Analytics with Wasserstein Distance. (arXiv:2106.05724v1 [math.OC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Wang_T/0/1/0/all/0/1">Tianyu Wang</a>, <a href="http://arxiv.org/find/math/1/au:+Chen_N/0/1/0/all/0/1">Ningyuan Chen</a>, <a href="http://arxiv.org/find/math/1/au:+Wang_C/0/1/0/all/0/1">Chun Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05724">
                                    <div class="article-summary-box-inner">
                                        <span>In prescriptive analytics, the decision-maker observes historical samples of
$(X, Y)$, where $Y$ is the uncertain problem parameter and $X$ is the
concurrent covariate, without knowing the joint distribution. Given an
additional covariate observation $x$, the goal is to choose a decision $z$
conditional on this observation to minimize the cost $\mathbb{E}[c(z,Y)|X&#x3D;x]$.
This paper proposes a new distributionally robust approach under Wasserstein
ambiguity sets, in which the nominal distribution of $Y|X&#x3D;x$ is constructed
based on the Nadaraya-Watson kernel estimator concerning the historical data.
We show that the nominal distribution converges to the actual conditional
distribution under the Wasserstein distance. We establish the out-of-sample
guarantees and the computational tractability of the framework. Through
synthetic and empirical experiments about the newsvendor problem and portfolio
optimization, we demonstrate the strong performance and practical value of the
proposed framework.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Score Matching Model for Unbounded Data Score. (arXiv:2106.05527v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dongjun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1">Seungjae Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1">Kyungwoo Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1">Wanmo Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Moon_I/0/1/0/all/0/1">Il-Chul Moon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05527">
                                    <div class="article-summary-box-inner">
                                        <span>Recent advance in score-based models incorporates the stochastic differential
equation (SDE), which brings the state-of-the art performance on image
generation tasks. This paper improves such score-based models by analyzing the
model at the zero perturbation noise. In real datasets, the score function
diverges as the perturbation noise ($\sigma$) decreases to zero, and this
observation leads an argument that the score estimation fails at $\sigma&#x3D;0$
with any neural network structure. Subsequently, we introduce Unbounded Noise
Conditional Score Network (UNCSN) that resolves the score diverging problem
with an easily applicable modification to any noise conditional score-based
models. Additionally, we introduce a new type of SDE, so the exact log
likelihood can be calculated from the newly suggested SDE. On top of that, the
associated loss function mitigates the loss imbalance issue in a mini-batch,
and we present a theoretic analysis on the proposed loss to uncover the behind
mechanism of the data distribution modeling by the score-based models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Deep Variational Approach to Clustering Survival Data. (arXiv:2106.05763v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Manduchi_L/0/1/0/all/0/1">Laura Manduchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Marcinkevics_R/0/1/0/all/0/1">Ri&#x10d;ards Marcinkevi&#x10d;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Massi_M/0/1/0/all/0/1">Michela C. Massi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gotta_V/0/1/0/all/0/1">Verena Gotta</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1">Timothy M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Vasella_F/0/1/0/all/0/1">Flavio Vasella</a>, <a href="http://arxiv.org/find/cs/1/au:+Neidert_M/0/1/0/all/0/1">Marian C. Neidert</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfister_M/0/1/0/all/0/1">Marc Pfister</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1">Julia E. Vogt</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05763">
                                    <div class="article-summary-box-inner">
                                        <span>Survival analysis has gained significant attention in the medical domain and
has many far-reaching applications. Although a variety of machine learning
methods have been introduced for tackling time-to-event prediction in
unstructured data with complex dependencies, clustering of survival data
remains an under-explored problem. The latter is particularly helpful in
discovering patient subpopulations whose survival is regulated by different
generative mechanisms, a critical problem in precision medicine. To this end,
we introduce a novel probabilistic approach to cluster survival data in a
variational deep clustering setting. Our proposed method employs a deep
generative model to uncover the underlying distribution of both the explanatory
variables and the potentially censored survival times. We compare our model to
the related work on survival clustering in comprehensive experiments on a range
of synthetic, semi-synthetic, and real-world datasets. Our proposed method
performs better at identifying clusters and is competitive at predicting
survival times in terms of the concordance index and relative absolute error.
To further demonstrate the usefulness of our approach, we show that our method
identifies meaningful clusters from an observational cohort of hemodialysis
patients that are consistent with previous clinical findings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial Option-Aware Hierarchical Imitation Learning. (arXiv:2106.05530v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jing_M/0/1/0/all/0/1">Mingxuan Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenbing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1">Fuchun Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaojian Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1">Tao Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chuang Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05530">
                                    <div class="article-summary-box-inner">
                                        <span>It has been a challenge to learning skills for an agent from long-horizon
unannotated demonstrations. Existing approaches like Hierarchical Imitation
Learning(HIL) are prone to compounding errors or suboptimal solutions. In this
paper, we propose Option-GAIL, a novel method to learn skills at long horizon.
The key idea of Option-GAIL is modeling the task hierarchy by options and train
the policy via generative adversarial optimization. In particular, we propose
an Expectation-Maximization(EM)-style algorithm: an E-step that samples the
options of expert conditioned on the current learned policy, and an M-step that
updates the low- and high-level policies of agent simultaneously to minimize
the newly proposed option-occupancy measurement between the expert and the
agent. We theoretically prove the convergence of the proposed algorithm.
Experiments show that Option-GAIL outperforms other counterparts consistently
across a variety of tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards an Automated Pipeline for Detecting and Classifying Malware through Machine Learning. (arXiv:2106.05625v1 [cs.CR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Loi_N/0/1/0/all/0/1">Nicola Loi</a>, <a href="http://arxiv.org/find/cs/1/au:+Borile_C/0/1/0/all/0/1">Claudio Borile</a>, <a href="http://arxiv.org/find/cs/1/au:+Ucci_D/0/1/0/all/0/1">Daniele Ucci</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05625">
                                    <div class="article-summary-box-inner">
                                        <span>The constant growth in the number of malware - software or code fragment
potentially harmful for computers and information networks - and the use of
sophisticated evasion and obfuscation techniques have seriously hindered
classic signature-based approaches. On the other hand, malware detection
systems based on machine learning techniques started offering a promising
alternative to standard approaches, drastically reducing analysis time and
turning out to be more robust against evasion and obfuscation techniques. In
this paper, we propose a malware taxonomic classification pipeline able to
classify Windows Portable Executable files (PEs). Given an input PE sample, it
is first classified as either malicious or benign. If malicious, the pipeline
further analyzes it in order to establish its threat type, family, and
behavior(s). We tested the proposed pipeline on the open source dataset EMBER,
containing approximately 1 million PE samples, analyzed through static
analysis. Obtained malware detection results are comparable to other academic
works in the current state of art and, in addition, we provide an in-depth
classification of malicious samples. Models used in the pipeline provides
interpretable results which can help security analysts in better understanding
decisions taken by the automated pipeline.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses. (arXiv:2106.05426v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1">Richard Antonello</a>, <a href="http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1">Javier Turek</a>, <a href="http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1">Vy Vo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1">Alexander Huth</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05426">
                                    <div class="article-summary-box-inner">
                                        <span>How related are the representations learned by neural language models,
translation models, and language tagging tasks? We answer this question by
adapting an encoder-decoder transfer learning method from computer vision to
investigate the structure among 100 different feature spaces extracted from
hidden representations of various networks trained on language tasks. This
method reveals a low-dimensional structure where language models and
translation models smoothly interpolate between word embeddings, syntactic and
semantic tasks, and future word embeddings. We call this low-dimensional
structure a language representation embedding because it encodes the
relationships between representations needed to process language for a variety
of NLP tasks. We find that this representation embedding can predict how well
each individual feature space maps to human brain responses to natural language
stimuli recorded using fMRI. Additionally, we find that the principal dimension
of this structure can be used to create a metric which highlights the brain&#x27;s
natural language processing hierarchy. This suggests that the embedding
captures some part of the brain&#x27;s natural language representation structure.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Feature Extraction for Novelty Detection in Network Traffic. (arXiv:2006.16993v2 [cs.NI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kpotufe_S/0/1/0/all/0/1">Samory Kpotufe</a>, <a href="http://arxiv.org/find/cs/1/au:+Feamster_N/0/1/0/all/0/1">Nick Feamster</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.16993">
                                    <div class="article-summary-box-inner">
                                        <span>Data representation plays a critical role in the performance of novelty
detection (or &#x60;&#x60;anomaly detection&#x27;&#x27;) methods in machine learning. The data
representation of network traffic often determines the effectiveness of these
models as much as the model itself. The wide range of novel events that network
operators need to detect (e.g., attacks, malware, new applications, changes in
traffic demands) introduces the possibility for a broad range of possible
models and data representations. In each scenario, practitioners must spend
significant effort extracting and engineering features that are most predictive
for that situation or application. While anomaly detection is well-studied in
computer networking, much existing work develops specific models that presume a
particular representation -- often IPFIX/NetFlow. Yet, other representations
may result in higher model accuracy, and the rise of programmable networks now
makes it more practical to explore a broader range of representations. To
facilitate such exploration, we develop a systematic framework, open-source
toolkit, and public Python library that makes it both possible and easy to
extract and generate features from network traffic and perform and end-to-end
evaluation of these representations across most prevalent modern novelty
detection models. We first develop and publicly release an open-source tool, an
accompanying Python library (NetML), and end-to-end pipeline for novelty
detection in network traffic. Second, we apply this tool to five different
novelty detection problems in networking, across a range of scenarios from
attack detection to novel device detection. Our findings general insights and
guidelines concerning which features appear to be more appropriate for
particular situations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Meta-Learning for Symbolic Hyperparameter Defaults. (arXiv:2106.05767v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Gijsbers_P/0/1/0/all/0/1">Pieter Gijsbers</a>, <a href="http://arxiv.org/find/stat/1/au:+Pfisterer_F/0/1/0/all/0/1">Florian Pfisterer</a>, <a href="http://arxiv.org/find/stat/1/au:+Rijn_J/0/1/0/all/0/1">Jan N. van Rijn</a>, <a href="http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1">Bernd Bischl</a>, <a href="http://arxiv.org/find/stat/1/au:+Vanschoren_J/0/1/0/all/0/1">Joaquin Vanschoren</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05767">
                                    <div class="article-summary-box-inner">
                                        <span>Hyperparameter optimization in machine learning (ML) deals with the problem
of empirically learning an optimal algorithm configuration from data, usually
formulated as a black-box optimization problem. In this work, we propose a
zero-shot method to meta-learn symbolic default hyperparameter configurations
that are expressed in terms of the properties of the dataset. This enables a
much faster, but still data-dependent, configuration of the ML algorithm,
compared to standard hyperparameter optimization approaches. In the past,
symbolic and static default values have usually been obtained as hand-crafted
heuristics. We propose an approach of learning such symbolic configurations as
formulas of dataset properties from a large set of prior evaluations on
multiple datasets by optimizing over a grammar of expressions using an
evolutionary algorithm. We evaluate our method on surrogate empirical
performance models as well as on real data across 6 ML algorithms on more than
100 datasets and demonstrate that our method indeed finds viable symbolic
defaults.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Unified Framework for Task-Driven Data Quality Management. (arXiv:2106.05484v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1">Yi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1">Ming Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05484">
                                    <div class="article-summary-box-inner">
                                        <span>High-quality data is critical to train performant Machine Learning (ML)
models, highlighting the importance of Data Quality Management (DQM). Existing
DQM schemes often cannot satisfactorily improve ML performance because, by
design, they are oblivious to downstream ML tasks. Besides, they cannot handle
various data quality issues (especially those caused by adversarial attacks)
and have limited applications to only certain types of ML models. Recently,
data valuation approaches (e.g., based on the Shapley value) have been
leveraged to perform DQM; yet, empirical studies have observed that their
performance varies considerably based on the underlying data and training
process. In this paper, we propose a task-driven, multi-purpose, model-agnostic
DQM framework, DataSifter, which is optimized towards a given downstream ML
task, capable of effectively removing data points with various defects, and
applicable to diverse models. Specifically, we formulate DQM as an optimization
problem and devise a scalable algorithm to solve it. Furthermore, we propose a
theoretical framework for comparing the worst-case performance of different DQM
strategies. Remarkably, our results show that the popular strategy based on the
Shapley value may end up choosing the worst data subset in certain practical
scenarios. Our evaluation shows that DataSifter achieves and most often
significantly improves the state-of-the-art performance over a wide range of
DQM tasks, including backdoor, poison, noisy/mislabel data detection, data
summarization, and data debiasing.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Nonparametric Volterra Kernels with Gaussian Processes. (arXiv:2106.05582v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Ross_M/0/1/0/all/0/1">Magnus Ross</a>, <a href="http://arxiv.org/find/stat/1/au:+Smith_M/0/1/0/all/0/1">Michael T. Smith</a>, <a href="http://arxiv.org/find/stat/1/au:+Alvarez_M/0/1/0/all/0/1">Mauricio A. &#xc1;lvarez</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05582">
                                    <div class="article-summary-box-inner">
                                        <span>This paper introduces a method for the nonparametric Bayesian learning of
nonlinear operators, through the use of the Volterra series with kernels
represented using Gaussian processes (GPs), which we term the nonparametric
Volterra kernels model (NVKM). When the input function to the operator is
unobserved and has a GP prior, the NVKM constitutes a powerful method for both
single and multiple output regression, and can be viewed as a nonlinear and
nonparametric latent force model. When the input function is observed, the NVKM
can be used to perform Bayesian system identification. We use recent advances
in efficient sampling of explicit functions from GPs to map process
realisations through the Volterra series without resorting to numerical
integration, allowing scalability through doubly stochastic variational
inference, and avoiding the need for Gaussian approximations of the output
processes. We demonstrate the performance of the model for both multiple output
regression and system identification using standard benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Neural Tangent Kernel Perspective of GANs. (arXiv:2106.05566v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Franceschi_J/0/1/0/all/0/1">Jean-Yves Franceschi</a> (MLIA), <a href="http://arxiv.org/find/cs/1/au:+Bezenac_E/0/1/0/all/0/1">Emmanuel de B&#xe9;zenac</a> (MLIA), <a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1">Ibrahim Ayed</a> (MLIA), <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Micka&#xeb;l Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1">Sylvain Lamprier</a> (MLIA), <a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1">Patrick Gallinari</a> (MLIA)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05566">
                                    <div class="article-summary-box-inner">
                                        <span>Theoretical analyses for Generative Adversarial Networks (GANs) generally
assume an arbitrarily large family of discriminators and do not consider the
characteristics of the architectures used in practice. We show that this
framework of analysis is too simplistic to properly analyze GAN training. To
tackle this issue, we leverage the theory of infinite-width neural networks to
model neural discriminator training for a wide range of adversarial losses via
its Neural Tangent Kernel (NTK). Our analytical results show that GAN
trainability primarily depends on the discriminator&#x27;s architecture. We further
study the discriminator for specific architectures and losses, and highlight
properties providing a new understanding of GAN training. For example, we find
that GANs trained with the integral probability metric loss minimize the
maximum mean discrepancy with the NTK as kernel. Our conclusions demonstrate
the analysis opportunities provided by the proposed framework, which paves the
way for better and more principled GAN models. We release a generic GAN
analysis toolkit based on our framework that supports the empirical part of our
study.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Automated Self-Supervised Learning for Graphs. (arXiv:2106.05470v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1">Wei Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaorui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiangyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1">Neil Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05470">
                                    <div class="article-summary-box-inner">
                                        <span>Graph self-supervised learning has gained increasing attention due to its
capacity to learn expressive node representations. Many pretext tasks, or loss
functions have been designed from distinct perspectives. However, we observe
that different pretext tasks affect downstream tasks differently cross
datasets, which suggests that searching pretext tasks is crucial for graph
self-supervised learning. Different from existing works focusing on designing
single pretext tasks, this work aims to investigate how to automatically
leverage multiple pretext tasks effectively. Nevertheless, evaluating
representations derived from multiple pretext tasks without direct access to
ground truth labels makes this problem challenging. To address this obstacle,
we make use of a key principle of many real-world graphs, i.e., homophily, or
the principle that &#x60;&#x60;like attracts like,&#x27;&#x27; as the guidance to effectively
search various self-supervised pretext tasks. We provide theoretical
understanding and empirical evidence to justify the flexibility of homophily in
this search task. Then we propose the AutoSSL framework which can automatically
search over combinations of various self-supervised tasks. By evaluating the
framework on 7 real-world datasets, our experimental results show that AutoSSL
can significantly boost the performance on downstream tasks including node
clustering and node classification compared with training under individual
tasks. Code will be released at https://github.com/ChandlerBang/AutoSSL.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hyperspace Neighbor Penetration Approach to Dynamic Programming for Model-Based Reinforcement Learning Problems with Slowly Changing Variables in A Continuous State Space. (arXiv:2106.05497v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zha_V/0/1/0/all/0/1">Vincent Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiu_I/0/1/0/all/0/1">Ivey Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guilbault_A/0/1/0/all/0/1">Alexandre Guilbault</a>, <a href="http://arxiv.org/find/cs/1/au:+Tatis_J/0/1/0/all/0/1">Jaime Tatis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05497">
                                    <div class="article-summary-box-inner">
                                        <span>Slowly changing variables in a continuous state space constitute an important
category of reinforcement learning and see its application in many domains,
such as modeling a climate control system where temperature, humidity, etc.
change slowly over time. However, this subject is less addressed in recent
studies. Classical methods with certain variants, such as Dynamic Programming
with Tile Coding which discretizes the state space, fail to handle slowly
changing variables because those methods cannot capture the tiny changes in
each transition step, as it is computationally expensive or impossible to
establish an extremely granular grid system. In this paper, we introduce a
Hyperspace Neighbor Penetration (HNP) approach that solves the problem. HNP
captures in each transition step the state&#x27;s partial &quot;penetration&quot; into its
neighboring hyper-tiles in the gridded hyperspace, thus does not require the
transition to be inter-tile in order for the change to be captured. Therefore,
HNP allows for a very coarse grid system, which makes the computation feasible.
HNP assumes near linearity of the transition function in a local space, which
is commonly satisfied. In summary, HNP can be orders of magnitude more
efficient than classical method in handling slowly changing variables in
reinforcement learning. We have made an industrial implementation of NHP with a
great success.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">End-to-end lung nodule detection framework with model-based feature projection block. (arXiv:2106.05741v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Drokin_I/0/1/0/all/0/1">Ivan Drokin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ericheva_E/0/1/0/all/0/1">Elena Ericheva</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05741">
                                    <div class="article-summary-box-inner">
                                        <span>This paper proposes novel end-to-end framework for detecting suspicious
pulmonary nodules in chest CT scans. The method core idea is a new nodule
segmentation architecture with a model-based feature projection block on
three-dimensional convolutions. This block acts as a preliminary feature
extractor for a two-dimensional U-Net-like convolutional network. Using the
proposed approach along with an axial, coronal, and sagittal projection
analysis makes it possible to abandon the widely used false positives reduction
step. The proposed method achieves SOTA on LUNA2016 with 0.959 average
sensitivity, and 0.936 sensitivity if the false-positive level per scan is
0.25. The paper describes the proposed approach and represents the experimental
results on LUNA2016 as well as ablation studies.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Graph Symbiosis Learning. (arXiv:2106.05455v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1">Liang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zijun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yanqiao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05455">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a framework for learning from multiple generated graph views,
named graph symbiosis learning (GraphSym). In GraphSym, graph neural networks
(GNN) developed in multiple generated graph views can adaptively exchange
parameters with each other and fuse information stored in linkage structures
and node features. Specifically, we propose a novel adaptive exchange method to
iteratively substitute redundant channels in the weight matrix of one GNN with
informative channels of another GNN in a layer-by-layer manner. GraphSym does
not rely on specific methods to generate multiple graph views and GNN
architectures. Thus, existing GNNs can be seamlessly integrated into our
framework. On 3 semi-supervised node classification datasets, GraphSym
outperforms previous single-graph and multiple-graph GNNs without knowledge
distillation, and achieves new state-of-the-art results. We also conduct a
series of experiments on 15 public benchmarks, 8 popular GNN models, and 3
graph tasks -- node classification, graph classification, and edge prediction
-- and show that GraphSym consistently achieves better performance than
existing popular GNNs by 1.9\%$\sim$3.9\% on average and their ensembles.
Extensive ablation studies and experiments on the few-shot setting also
demonstrate the effectiveness of GraphSym.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bayesian Quadrature on Riemannian Data Manifolds. (arXiv:2102.06645v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Frohlich_C/0/1/0/all/0/1">Christian Fr&#xf6;hlich</a>, <a href="http://arxiv.org/find/cs/1/au:+Gessner_A/0/1/0/all/0/1">Alexandra Gessner</a>, <a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1">Philipp Hennig</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Arvanitidis_G/0/1/0/all/0/1">Georgios Arvanitidis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06645">
                                    <div class="article-summary-box-inner">
                                        <span>Riemannian manifolds provide a principled way to model nonlinear geometric
structure inherent in data. A Riemannian metric on said manifolds determines
geometry-aware shortest paths and provides the means to define statistical
models accordingly. However, these operations are typically computationally
demanding. To ease this computational burden, we advocate probabilistic
numerical methods for Riemannian statistics. In particular, we focus on
Bayesian quadrature (BQ) to numerically compute integrals over normal laws on
Riemannian manifolds learned from data. In this task, each function evaluation
relies on the solution of an expensive initial value problem. We show that by
leveraging both prior knowledge and an active exploration scheme, BQ
significantly reduces the number of required evaluations and thus outperforms
Monte Carlo methods on a wide range of integration problems. As a concrete
application, we highlight the merits of adopting Riemannian geometry with our
proposed framework on a nonlinear dataset from molecular dynamics.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Front Contribution instead of Back Propagation. (arXiv:2106.05569v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Swaroop Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1">Anjana Arunkumar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05569">
                                    <div class="article-summary-box-inner">
                                        <span>Deep Learning&#x27;s outstanding track record across several domains has stemmed
from the use of error backpropagation (BP). Several studies, however, have
shown that it is impossible to execute BP in a real brain. Also, BP still
serves as an important and unsolved bottleneck for memory usage and speed. We
propose a simple, novel algorithm, the Front-Contribution algorithm, as a
compact alternative to BP. The contributions of all weights with respect to the
final layer weights are calculated before training commences and all the
contributions are appended to weights of the final layer, i.e., the effective
final layer weights are a non-linear function of themselves. Our algorithm then
essentially collapses the network, precluding the necessity for weight updation
of all weights not in the final layer. This reduction in parameters results in
lower memory usage and higher training speed. We show that our algorithm
produces the exact same output as BP, in contrast to several recently proposed
algorithms approximating BP. Our preliminary experiments demonstrate the
efficacy of the proposed algorithm. Our work provides a foundation to
effectively utilize these presently under-explored &quot;front contributions&quot;, and
serves to inspire the next generation of training algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Brittle AI, Causal Confusion, and Bad Mental Models: Challenges and Successes in the XAI Program. (arXiv:2106.05506v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Druce_J/0/1/0/all/0/1">Jeff Druce</a>, <a href="http://arxiv.org/find/cs/1/au:+Niehaus_J/0/1/0/all/0/1">James Niehaus</a>, <a href="http://arxiv.org/find/cs/1/au:+Moody_V/0/1/0/all/0/1">Vanessa Moody</a>, <a href="http://arxiv.org/find/cs/1/au:+Jensen_D/0/1/0/all/0/1">David Jensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Littman_M/0/1/0/all/0/1">Michael L. Littman</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05506">
                                    <div class="article-summary-box-inner">
                                        <span>The advances in artificial intelligence enabled by deep learning
architectures are undeniable. In several cases, deep neural network driven
models have surpassed human level performance in benchmark autonomy tasks. The
underlying policies for these agents, however, are not easily interpretable. In
fact, given their underlying deep models, it is impossible to directly
understand the mapping from observations to actions for any reasonably complex
agent. Producing this supporting technology to &quot;open the black box&quot; of these AI
systems, while not sacrificing performance, was the fundamental goal of the
DARPA XAI program. In our journey through this program, we have several &quot;big
picture&quot; takeaways: 1) Explanations need to be highly tailored to their
scenario; 2) many seemingly high performing RL agents are extremely brittle and
are not amendable to explanation; 3) causal models allow for rich explanations,
but how to present them isn&#x27;t always straightforward; and 4) human subjects
conjure fantastically wrong mental models for AIs, and these models are often
hard to break. This paper discusses the origins of these takeaways, provides
amplifying information, and suggestions for future work.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Vertical Federated Learning without Revealing Intersection Membership. (arXiv:2106.05508v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiankai Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuanshun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Aonan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1">Weihao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Junyuan Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chong Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05508">
                                    <div class="article-summary-box-inner">
                                        <span>Vertical Federated Learning (vFL) allows multiple parties that own different
attributes (e.g. features and labels) of the same data entity (e.g. a person)
to jointly train a model. To prepare the training data, vFL needs to identify
the common data entities shared by all parties. It is usually achieved by
Private Set Intersection (PSI) which identifies the intersection of training
samples from all parties by using personal identifiable information (e.g.
email) as sample IDs to align data instances. As a result, PSI would make
sample IDs of the intersection visible to all parties, and therefore each party
can know that the data entities shown in the intersection also appear in the
other parties, i.e. intersection membership. However, in many real-world
privacy-sensitive organizations, e.g. banks and hospitals, revealing membership
of their data entities is prohibited. In this paper, we propose a vFL framework
based on Private Set Union (PSU) that allows each party to keep sensitive
membership information to itself. Instead of identifying the intersection of
all training samples, our PSU protocol generates the union of samples as
training instances. In addition, we propose strategies to generate synthetic
features and labels to handle samples that belong to the union but not the
intersection. Through extensive experiments on two real-world datasets, we show
our framework can protect the privacy of the intersection membership while
maintaining the model utility.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ERMAS: Becoming Robust to Reward Function Sim-to-Real Gaps in Multi-Agent Simulations. (arXiv:2106.05492v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhao_E/0/1/0/all/0/1">Eric Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1">Alexander R. Trott</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Stephan Zheng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05492">
                                    <div class="article-summary-box-inner">
                                        <span>Multi-agent simulations provide a scalable environment for learning policies
that interact with rational agents. However, such policies may fail to
generalize to the real-world where agents may differ from simulated
counterparts due to unmodeled irrationality and misspecified reward functions.
We introduce Epsilon-Robust Multi-Agent Simulation (ERMAS), a robust
optimization framework for learning AI policies that are robust to such
multiagent sim-to-real gaps. While existing notions of multi-agent robustness
concern perturbations in the actions of agents, we address a novel robustness
objective concerning perturbations in the reward functions of agents. ERMAS
provides this robustness by anticipating suboptimal behaviors from other
agents, formalized as the worst-case epsilon-equilibrium. We show empirically
that ERMAS yields robust policies for repeated bimatrix games and optimal
taxation problems in economic simulations. In particular, in the two-level RL
problem posed by the AI Economist (Zheng et al., 2020) ERMAS learns tax
policies that are robust to changes in agent risk aversion, improving social
welfare by up to 15% in complex spatiotemporal simulations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Long-time integration of parametric evolution equations with physics-informed DeepONets. (arXiv:2106.05384v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sifan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Perdikaris_P/0/1/0/all/0/1">Paris Perdikaris</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05384">
                                    <div class="article-summary-box-inner">
                                        <span>Ordinary and partial differential equations (ODEs/PDEs) play a paramount role
in analyzing and simulating complex dynamic processes across all corners of
science and engineering. In recent years machine learning tools are aspiring to
introduce new effective ways of simulating PDEs, however existing approaches
are not able to reliably return stable and accurate predictions across long
temporal horizons. We aim to address this challenge by introducing an effective
framework for learning infinite-dimensional operators that map random initial
conditions to associated PDE solutions within a short time interval. Such
latent operators can be parametrized by deep neural networks that are trained
in an entirely self-supervised manner without requiring any paired input-output
observations. Global long-time predictions across a range of initial conditions
can be then obtained by iteratively evaluating the trained model using each
prediction as the initial condition for the next evaluation step. This
introduces a new approach to temporal domain decomposition that is shown to be
effective in performing accurate long-time simulations for a wide range of
parametric ODE and PDE systems, from wave propagation, to reaction-diffusion
dynamics and stiff chemical kinetics, all at a fraction of the computational
cost needed by classical numerical solvers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Next-Gen Machine Learning Supported Diagnostic Systems for Spacecraft. (arXiv:2106.05659v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vlontzos_A/0/1/0/all/0/1">Athanasios Vlontzos</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutherland_G/0/1/0/all/0/1">Gabriel Sutherland</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1">Siddha Ganju</a>, <a href="http://arxiv.org/find/cs/1/au:+Soboczenski_F/0/1/0/all/0/1">Frank Soboczenski</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05659">
                                    <div class="article-summary-box-inner">
                                        <span>Future short or long-term space missions require a new generation of
monitoring and diagnostic systems due to communication impasses as well as
limitations in specialized crew and equipment. Machine learning supported
diagnostic systems present a viable solution for medical and technical
applications. We discuss challenges and applicability of such systems in light
of upcoming missions and outline an example use case for a next-generation
medical diagnostic system for future space operations. Additionally, we present
approach recommendations and constraints for the successful generation and use
of machine learning models aboard a spacecraft.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Differentiable Robust LQR Layers. (arXiv:2106.05535v1 [cs.RO])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1">Ngo Anh Vien</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1">Gerhard Neumann</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05535">
                                    <div class="article-summary-box-inner">
                                        <span>This paper proposes a differentiable robust LQR layer for reinforcement
learning and imitation learning under model uncertainty and stochastic
dynamics. The robust LQR layer can exploit the advantages of robust optimal
control and model-free learning. It provides a new type of inductive bias for
stochasticity and uncertainty modeling in control systems. In particular, we
propose an efficient way to differentiate through a robust LQR optimization
program by rewriting it as a convex program (i.e. semi-definite program) of the
worst-case cost. Based on recent work on using convex optimization inside
neural network layers, we develop a fully differentiable layer for optimizing
this worst-case cost, i.e. we compute the derivative of a performance measure
w.r.t the model&#x27;s unknown parameters, model uncertainty and stochasticity
parameters. We demonstrate the proposed method on imitation learning and
approximate dynamic programming on stochastic and uncertain domains. The
experiment results show that the proposed method can optimize robust policies
under uncertain situations, and are able to achieve a significantly better
performance than existing methods that do not model uncertainty directly.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A New Notion of Individually Fair Clustering: $\alpha$-Equitable $k$-Center. (arXiv:2106.05423v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_D/0/1/0/all/0/1">Darshan Chakrabarti</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John P. Dickerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Esmaeili_S/0/1/0/all/0/1">Seyed A. Esmaeili</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1">Aravind Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsepenekas_L/0/1/0/all/0/1">Leonidas Tsepenekas</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05423">
                                    <div class="article-summary-box-inner">
                                        <span>Clustering is a fundamental problem in unsupervised machine learning, and
fair variants of it have recently received significant attention. In this work
we introduce a novel definition of fairness for clustering problems.
Specifically, in our model each point $j$ has a set of other points
$\mathcal{S}_j$ that it perceives as similar to itself, and it feels that it is
fairly treated, if the quality of service it receives in the solution is
$\alpha$-close to that of the points in $\mathcal{S}_j$. We begin our study by
answering questions regarding the structure of the problem, namely for what
values of $\alpha$ the problem is well-defined, and what the behavior of the
Price of Fairness (PoF) for it is. For the well-defined region of $\alpha$, we
provide efficient and easily implementable approximation algorithms for the
$k$-center objective, which in certain cases also enjoy bounded PoF guarantees.
We finally complement our analysis by an extensive suite of experiments that
validates the effectiveness of our theoretical results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Investigating Alternatives to the Root Mean Square for Adaptive Gradient Methods. (arXiv:2106.05449v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Daley_B/0/1/0/all/0/1">Brett Daley</a>, <a href="http://arxiv.org/find/cs/1/au:+Amato_C/0/1/0/all/0/1">Christopher Amato</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05449">
                                    <div class="article-summary-box-inner">
                                        <span>Adam is an adaptive gradient method that has experienced widespread adoption
due to its fast and reliable training performance. Recent approaches have not
offered significant improvement over Adam, often because they do not innovate
upon one of its core features: normalization by the root mean square (RMS) of
recent gradients. However, as noted by Kingma and Ba (2015), any number of
$L^p$ normalizations are possible, with the RMS corresponding to the specific
case of $p&#x3D;2$. In our work, we theoretically and empirically characterize the
influence of different $L^p$ norms on adaptive gradient methods for the first
time. We show mathematically how the choice of $p$ influences the size of the
steps taken, while leaving other desirable properties unaffected. We evaluate
Adam with various $L^p$ norms on a suite of deep learning benchmarks, and find
that $p &gt; 2$ consistently leads to improved learning speed and final
performance. The choices of $p&#x3D;3$ or $p&#x3D;6$ also match or outperform
state-of-the-art methods in all of our experiments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploiting Local Convergence of Quasi-Newton Methods Globally: Adaptive Sample Size Approach. (arXiv:2106.05445v1 [math.OC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Jin_Q/0/1/0/all/0/1">Qiujiang Jin</a>, <a href="http://arxiv.org/find/math/1/au:+Mokhtari_A/0/1/0/all/0/1">Aryan Mokhtari</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05445">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we study the application of quasi-Newton methods for solving
empirical risk minimization (ERM) problems defined over a large dataset.
Traditional deterministic and stochastic quasi-Newton methods can be executed
to solve such problems; however, it is known that their global convergence rate
may not be better than first-order methods, and their local superlinear
convergence only appears towards the end of the learning process. In this
paper, we use an adaptive sample size scheme that exploits the superlinear
convergence of quasi-Newton methods globally and throughout the entire learning
process. The main idea of the proposed adaptive sample size algorithms is to
start with a small subset of data points and solve their corresponding ERM
problem within its statistical accuracy, and then enlarge the sample size
geometrically and use the optimal solution of the problem corresponding to the
smaller set as an initial point for solving the subsequent ERM problem with
more samples. We show that if the initial sample size is sufficiently large and
we use quasi-Newton methods to solve each subproblem, the subproblems can be
solved superlinearly fast (after at most three iterations), as we guarantee
that the iterates always stay within a neighborhood that quasi-Newton methods
converge superlinearly. Numerical experiments on various datasets confirm our
theoretical results and demonstrate the computational advantages of our method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-VFL: A Vertical Federated Learning System for Multiple Data and Label Owners. (arXiv:2106.05468v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mugunthan_V/0/1/0/all/0/1">Vaikkunth Mugunthan</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1">Pawan Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Kagal_L/0/1/0/all/0/1">Lalana Kagal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05468">
                                    <div class="article-summary-box-inner">
                                        <span>Vertical Federated Learning (VFL) refers to the collaborative training of a
model on a dataset where the features of the dataset are split among multiple
data owners, while label information is owned by a single data owner. In this
paper, we propose a novel method, Multi Vertical Federated Learning
(Multi-VFL), to train VFL models when there are multiple data and label owners.
Our approach is the first to consider the setting where $D$-data owners (across
which features are distributed) and $K$-label owners (across which labels are
distributed) exist. This proposed configuration allows different entities to
train and learn optimal models without having to share their data. Our
framework makes use of split learning and adaptive federated optimizers to
solve this problem. For empirical evaluation, we run experiments on the MNIST
and FashionMNIST datasets. Our results show that using adaptive optimizers for
model aggregation fastens convergence and improves accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-domain Contrastive Learning for Unsupervised Domain Adaptation. (arXiv:2106.05528v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zuxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1">Zejia Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jingjing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1">Guo-Jun Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05528">
                                    <div class="article-summary-box-inner">
                                        <span>Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a fully-labeled source domain to a different unlabeled target domain. Most
existing UDA methods learn domain-invariant feature representations by
minimizing feature distances across domains. In this work, we build upon
contrastive self-supervised learning to align features so as to reduce the
domain discrepancy between training and testing sets. Exploring the same set of
categories shared by both domains, we introduce a simple yet effective
framework CDCL, for domain alignment. In particular, given an anchor image from
one domain, we minimize its distances to cross-domain samples from the same
class relative to those from different categories. Since target labels are
unavailable, we use a clustering-based approach with carefully initialized
centers to produce pseudo labels. In addition, we demonstrate that CDCL is a
general framework and can be adapted to the data-free setting, where the source
data are unavailable during training, with minimal modification. We conduct
experiments on two widely used domain adaptation benchmarks, i.e., Office-31
and VisDA-2017, and demonstrate that CDCL achieves state-of-the-art performance
on both datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scaling Properties of Deep Residual Networks. (arXiv:2105.12245v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1">Alain-Sam Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cont_R/0/1/0/all/0/1">Rama Cont</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossier_A/0/1/0/all/0/1">Alain Rossier</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Renyuan Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12245">
                                    <div class="article-summary-box-inner">
                                        <span>Residual networks (ResNets) have displayed impressive results in pattern
recognition and, recently, have garnered considerable theoretical interest due
to a perceived link with neural ordinary differential equations (neural ODEs).
This link relies on the convergence of network weights to a smooth function as
the number of layers increases. We investigate the properties of weights
trained by stochastic gradient descent and their scaling with network depth
through detailed numerical experiments. We observe the existence of scaling
regimes markedly different from those assumed in neural ODE literature.
Depending on certain features of the network architecture, such as the
smoothness of the activation function, one may obtain an alternative ODE limit,
a stochastic differential equation or neither of these. These findings cast
doubts on the validity of the neural ODE model as an adequate asymptotic
description of deep ResNets and point to an alternative class of differential
equations as a better description of the deep network limit.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Local Post-Hoc Explanations for Predictive Process Monitoring in Manufacturing. (arXiv:2009.10513v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mehdiyev_N/0/1/0/all/0/1">Nijat Mehdiyev</a>, <a href="http://arxiv.org/find/cs/1/au:+Fettke_P/0/1/0/all/0/1">Peter Fettke</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.10513">
                                    <div class="article-summary-box-inner">
                                        <span>This study proposes an innovative explainable predictive quality analytics
solution to facilitate data-driven decision-making for process planning in
manufacturing by combining process mining, machine learning, and explainable
artificial intelligence (XAI) methods. For this purpose, after integrating the
top-floor and shop-floor data obtained from various enterprise information
systems, a deep learning model was applied to predict the process outcomes.
Since this study aims to operationalize the delivered predictive insights by
embedding them into decision-making processes, it is essential to generate
relevant explanations for domain experts. To this end, two complementary local
post-hoc explanation approaches, Shapley values and Individual Conditional
Expectation (ICE) plots are adopted, which are expected to enhance the
decision-making capabilities by enabling experts to examine explanations from
different perspectives. After assessing the predictive strength of the applied
deep neural network with relevant binary classification evaluation measures, a
discussion of the generated explanations is provided.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">From inexact optimization to learning via gradient concentration. (arXiv:2106.05397v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Stankewitz_B/0/1/0/all/0/1">Bernhard Stankewitz</a>, <a href="http://arxiv.org/find/stat/1/au:+Mucke_N/0/1/0/all/0/1">Nicole M&#xfc;cke</a>, <a href="http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1">Lorenzo Rosasco</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05397">
                                    <div class="article-summary-box-inner">
                                        <span>Optimization was recently shown to control the inductive bias in a learning
process, a property referred to as implicit, or iterative regularization. The
estimator obtained iteratively minimizing the training error can generalise
well with no need of further penalties or constraints. In this paper, we
investigate this phenomenon in the context of linear models with smooth loss
functions. In particular, we investigate and propose a proof technique
combining ideas from inexact optimization and probability theory, specifically
gradient concentration. The proof is easy to follow and allows to obtain sharp
learning bounds. More generally, it highlights a way to develop optimization
results into learning guarantees.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Clairvoyant Prefetching for Distributed Machine Learning I/O. (arXiv:2101.08734v2 [cs.DC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dryden_N/0/1/0/all/0/1">Nikoli Dryden</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohringer_R/0/1/0/all/0/1">Roman B&#xf6;hringer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_Nun_T/0/1/0/all/0/1">Tal Ben-Nun</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1">Torsten Hoefler</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08734">
                                    <div class="article-summary-box-inner">
                                        <span>I/O is emerging as a major bottleneck for machine learning training,
especially in distributed environments. Indeed, at large scale, I/O takes as
much as 85% of training time. Addressing this I/O bottleneck necessitates
careful optimization, as optimal data ingestion pipelines differ between
systems, and require a delicate balance between access to local storage,
external filesystems, and remote nodes. We introduce NoPFS, a machine learning
I/O middleware, which provides a scalable, flexible, and easy-to-use solution
to the I/O bottleneck. NoPFS uses clairvoyance: Given the seed generating the
random access pattern for training with SGD, it can exactly predict when and
where a sample will be accessed. We combine this with an analysis of access
patterns and a performance model to provide distributed caching policies that
adapt to different datasets and storage hierarchies. NoPFS reduces I/O times
and improves end-to-end training by up to 5.4x on the ImageNet-1k,
ImageNet-22k, and CosmoFlow datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Linear Classifiers Under Infinite Imbalance. (arXiv:2106.05797v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Glasserman_P/0/1/0/all/0/1">Paul Glasserman</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_M/0/1/0/all/0/1">Mike Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05797">
                                    <div class="article-summary-box-inner">
                                        <span>We study the behavior of linear discriminant functions for binary
classification in the infinite-imbalance limit, where the sample size of one
class grows without bound while the sample size of the other remains fixed. The
coefficients of the classifier minimize an expected loss specified through a
weight function. We show that for a broad class of weight functions, the
intercept diverges but the rest of the coefficient vector has a finite limit
under infinite imbalance, extending prior work on logistic regression. The
limit depends on the left tail of the weight function, for which we distinguish
three cases: bounded, asymptotically polynomial, and asymptotically
exponential. The limiting coefficient vectors reflect robustness or
conservatism properties in the sense that they optimize against certain
worst-case alternatives. In the bounded and polynomial cases, the limit is
equivalent to an implicit choice of upsampling distribution for the minority
class. We apply these ideas in a credit risk setting, with particular emphasis
on performance in the high-sensitivity and high-specificity regions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fair Classification with Adversarial Perturbations. (arXiv:2106.05964v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1">L. Elisa Celis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehrotra_A/0/1/0/all/0/1">Anay Mehrotra</a>, <a href="http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1">Nisheeth K. Vishnoi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05964">
                                    <div class="article-summary-box-inner">
                                        <span>We study fair classification in the presence of an omniscient adversary that,
given an $\eta$, is allowed to choose an arbitrary $\eta$-fraction of the
training samples and arbitrarily perturb their protected attributes. The
motivation comes from settings in which protected attributes can be incorrect
due to strategic misreporting, malicious actors, or errors in imputation; and
prior approaches that make stochastic or independence assumptions on errors may
not satisfy their guarantees in this adversarial setting. Our main contribution
is an optimization framework to learn fair classifiers in this adversarial
setting that comes with provable guarantees on accuracy and fairness. Our
framework works with multiple and non-binary protected attributes, is designed
for the large class of linear-fractional fairness metrics, and can also handle
perturbations besides protected attributes. We prove near-tightness of our
framework&#x27;s guarantees for natural hypothesis classes: no algorithm can have
significantly better accuracy and any algorithm with better fairness must have
lower accuracy. Empirically, we evaluate the classifiers produced by our
framework for statistical rate on real-world and synthetic datasets for a
family of adversaries.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Compositional Modeling of Nonlinear Dynamical Systems with ODE-based Random Features. (arXiv:2106.05960v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+McDonald_T/0/1/0/all/0/1">Thomas M. McDonald</a>, <a href="http://arxiv.org/find/stat/1/au:+Alvarez_M/0/1/0/all/0/1">Mauricio A. &#xc1;lvarez</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05960">
                                    <div class="article-summary-box-inner">
                                        <span>Effectively modeling phenomena present in highly nonlinear dynamical systems
whilst also accurately quantifying uncertainty is a challenging task, which
often requires problem-specific techniques. We present a novel, domain-agnostic
approach to tackling this problem, using compositions of physics-informed
random features, derived from ordinary differential equations. The architecture
of our model leverages recent advances in approximate inference for deep
Gaussian processes, such as layer-wise weight-space approximations which allow
us to incorporate random Fourier features, and stochastic variational inference
for approximate Bayesian inference. We provide evidence that our model is
capable of capturing highly nonlinear behaviour in real-world multivariate time
series data. In addition, we find that our approach achieves comparable
performance to a number of other probabilistic models on benchmark regression
tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Support Recovery of Sparse Signals from a Mixture of Linear Measurements. (arXiv:2106.05951v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Gandikota_V/0/1/0/all/0/1">Venkata Gandikota</a>, <a href="http://arxiv.org/find/stat/1/au:+Mazumdar_A/0/1/0/all/0/1">Arya Mazumdar</a>, <a href="http://arxiv.org/find/stat/1/au:+Pal_S/0/1/0/all/0/1">Soumyabrata Pal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05951">
                                    <div class="article-summary-box-inner">
                                        <span>Recovery of support of a sparse vector from simple measurements is a widely
studied problem, considered under the frameworks of compressed sensing, 1-bit
compressed sensing, and more general single index models. We consider
generalizations of this problem: mixtures of linear regressions, and mixtures
of linear classifiers, where the goal is to recover supports of multiple sparse
vectors using only a small number of possibly noisy linear, and 1-bit
measurements respectively. The key challenge is that the measurements from
different vectors are randomly mixed. Both of these problems were also
extensively studied recently. In mixtures of linear classifiers, the
observations correspond to the side of queried hyperplane a random unknown
vector lies in, whereas in mixtures of linear regressions we observe the
projection of a random unknown vector on the queried hyperplane. The primary
step in recovering the unknown vectors from the mixture is to first identify
the support of all the individual component vectors. In this work, we study the
number of measurements sufficient for recovering the supports of all the
component vectors in a mixture in both these models. We provide algorithms that
use a number of measurements polynomial in $k, \log n$ and quasi-polynomial in
$\ell$, to recover the support of all the $\ell$ unknown vectors in the mixture
with high probability when each individual component is a $k$-sparse
$n$-dimensional vector.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Temporal and Object Quantification Networks. (arXiv:2106.05891v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Jiayuan Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhezheng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chuang Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1">Joshua B. Tenenbaum</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1">Leslie Pack Kaelbling</a>, <a href="http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1">Tomer D. Ullman</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05891">
                                    <div class="article-summary-box-inner">
                                        <span>We present Temporal and Object Quantification Networks (TOQ-Nets), a new
class of neuro-symbolic networks with a structural bias that enables them to
learn to recognize complex relational-temporal events. This is done by
including reasoning layers that implement finite-domain quantification over
objects and time. The structure allows them to generalize directly to input
instances with varying numbers of objects in temporal sequences of varying
lengths. We evaluate TOQ-Nets on input domains that require recognizing
event-types in terms of complex temporal relational patterns. We demonstrate
that TOQ-Nets can generalize from small amounts of data to scenarios containing
more objects than were present during training and to temporal warpings of
input sequences.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hybrid gene selection approach using XGBoost and multi-objective genetic algorithm for cancer classification. (arXiv:2106.05841v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xiongshi Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Min Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shaobo Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lei Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05841">
                                    <div class="article-summary-box-inner">
                                        <span>Microarray gene expression data are often accompanied by a large number of
genes and a small number of samples. However, only a few of these genes are
relevant to cancer, resulting in signigicant gene selection challenges. Hence,
we propose a two-stage gene selection approach by combining extreme gradient
boosting (XGBoost) and a multi-objective optimization genetic algorithm
(XGBoost-MOGA) for cancer classification in microarray datasets. In the first
stage, the genes are ranked use an ensemble-based feature selection using
XGBoost. This stage can effectively remove irrelevant genes and yield a group
comprising the most relevant genes related to the class. In the second stage,
XGBoost-MOGA searches for an optimal gene subset based on the most relevant
genes&#x27;s group using a multi-objective optimization genetic algorithm. We
performed comprehensive experiments to compare XGBoost-MOGA with other
state-of-the-art feature selection methods using two well-known learning
classifiers on 13 publicly available microarray expression datasets. The
experimental results show that XGBoost-MOGA yields significantly better results
than previous state-of-the-art algorithms in terms of various evaluation
criteria, such as accuracy, F-score, precision, and recall.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Bagging and Boosting Based Convexly Combined Optimum Mixture Probabilistic Model. (arXiv:2106.05840v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Adnan_M/0/1/0/all/0/1">Mian Arif Shams Adnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1">H. M. Miraz Mahmud</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05840">
                                    <div class="article-summary-box-inner">
                                        <span>Unlike previous studies on mixture distributions, a bagging and boosting
based convexly combined mixture probabilistic model has been suggested. This
model is a result of iteratively searching for obtaining the optimum
probabilistic model that provides the maximum p value.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (arXiv:2106.05825v1 [cs.CR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1">Mohammad Hossein Samavatian</a>, <a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1">Saikat Majumdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1">Kristin Barber</a>, <a href="http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1">Radu Teodorescu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05825">
                                    <div class="article-summary-box-inner">
                                        <span>DNNs are known to be vulnerable to so-called adversarial attacks, in which
inputs are carefully manipulated to induce misclassification. Existing defenses
are mostly software-based and come with high overheads or other limitations.
This paper presents HASI, a hardware-accelerated defense that uses a process we
call stochastic inference to detect adversarial inputs. HASI carefully injects
noise into the model at inference time and used the model&#x27;s response to
differentiate adversarial inputs from benign ones. We show an adversarial
detection rate of average 87% which exceeds the detection rate of the
state-of-the-art approaches, with a much lower overhead. We demonstrate a
software/hardware-accelerated co-design, which reduces the performance impact
of stochastic inference to 1.58X-2X relative to the unprotected baseline,
compared to 14X-20X overhead for a software-only GPU implementation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Knowing when we do not know: Bayesian continual learning for sensing-based analysis tasks. (arXiv:2106.05872v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Servia_Rodriguez_S/0/1/0/all/0/1">Sandra Servia-Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1">Cecilia Mascolo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1">Young D. Kwon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05872">
                                    <div class="article-summary-box-inner">
                                        <span>Despite much research targeted at enabling conventional machine learning
models to continually learn tasks and data distributions sequentially without
forgetting the knowledge acquired, little effort has been devoted to account
for more realistic situations where learning some tasks accurately might be
more critical than forgetting previous ones. In this paper we propose a
Bayesian inference based framework to continually learn a set of real-world,
sensing-based analysis tasks that can be tuned to prioritize the remembering of
previously learned tasks or the learning of new ones. Our experiments prove the
robustness and reliability of the learned models to adapt to the changing
sensing environment, and show the suitability of using uncertainty of the
predictions to assess their reliability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Lower Bounds on Metropolized Sampling Methods for Well-Conditioned Distributions. (arXiv:2106.05480v1 [cs.DS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yin Tat Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1">Ruoqi Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1">Kevin Tian</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05480">
                                    <div class="article-summary-box-inner">
                                        <span>We give lower bounds on the performance of two of the most popular sampling
methods in practice, the Metropolis-adjusted Langevin algorithm (MALA) and
multi-step Hamiltonian Monte Carlo (HMC) with a leapfrog integrator, when
applied to well-conditioned distributions. Our main result is a nearly-tight
lower bound of $\widetilde{\Omega}(\kappa d)$ on the mixing time of MALA from
an exponentially warm start, matching a line of algorithmic results up to
logarithmic factors and answering an open question of Chewi et. al. We also
show that a polynomial dependence on dimension is necessary for the relaxation
time of HMC under any number of leapfrog steps, and bound the gains achievable
by changing the step count. Our HMC analysis draws upon a novel connection
between leapfrog integration and Chebyshev polynomials, which may be of
independent interest.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hierarchical Agglomerative Graph Clustering in Nearly-Linear Time. (arXiv:2106.05610v1 [cs.DS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dhulipala_L/0/1/0/all/0/1">Laxman Dhulipala</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisenstat_D/0/1/0/all/0/1">David Eisenstat</a>, <a href="http://arxiv.org/find/cs/1/au:+Lacki_J/0/1/0/all/0/1">Jakub &#x141;&#x105;cki</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1">Vahab Mirrokni</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jessica Shi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05610">
                                    <div class="article-summary-box-inner">
                                        <span>We study the widely used hierarchical agglomerative clustering (HAC)
algorithm on edge-weighted graphs. We define an algorithmic framework for
hierarchical agglomerative graph clustering that provides the first efficient
$\tilde{O}(m)$ time exact algorithms for classic linkage measures, such as
complete- and WPGMA-linkage, as well as other measures. Furthermore, for
average-linkage, arguably the most popular variant of HAC, we provide an
algorithm that runs in $\tilde{O}(n\sqrt{m})$ time. For this variant, this is
the first exact algorithm that runs in subquadratic time, as long as
$m&#x3D;n^{2-\epsilon}$ for some constant $\epsilon &gt; 0$. We complement this result
with a simple $\epsilon$-close approximation algorithm for average-linkage in
our framework that runs in $\tilde{O}(m)$ time. As an application of our
algorithms, we consider clustering points in a metric space by first using
$k$-NN to generate a graph from the point set, and then running our algorithms
on the resulting weighted graph. We validate the performance of our algorithms
on publicly available datasets, and show that our approach can speed up
clustering of point datasets by a factor of 20.7--76.5x.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Semantic-aware Binary Code Representation with BERT. (arXiv:2106.05478v1 [cs.CR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1">Hyungjoon Koo</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Soyeon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1">Daejin Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Taesoo Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05478">
                                    <div class="article-summary-box-inner">
                                        <span>A wide range of binary analysis applications, such as bug discovery, malware
analysis and code clone detection, require recovery of contextual meanings on a
binary code. Recently, binary analysis techniques based on machine learning
have been proposed to automatically reconstruct the code representation of a
binary instead of manually crafting specifics of the analysis algorithm.
However, the existing approaches utilizing machine learning are still
specialized to solve one domain of problems, rendering recreation of models for
different types of binary analysis. In this paper, we propose DeepSemantic
utilizing BERT in producing the semantic-aware code representation of a binary
code.

To this end, we introduce well-balanced instruction normalization that holds
rich information for each of instructions yet minimizing an out-of-vocabulary
(OOV) problem. DeepSemantic has been carefully designed based on our study with
large swaths of binaries. Besides, DeepSemantic leverages the essence of the
BERT architecture into re-purposing a pre-trained generic model that is readily
available as a one-time processing, followed by quickly applying specific
downstream tasks with a fine-tuning process. We demonstrate DeepSemantic with
two downstream tasks, namely, binary similarity comparison and compiler
provenance (i.e., compiler and optimization level) prediction. Our experimental
results show that the binary similarity model outperforms two state-of-the-art
binary similarity tools, DeepBinDiff and SAFE, 49.84% and 15.83% on average,
respectively.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unsupervised Behaviour Discovery with Quality-Diversity Optimisation. (arXiv:2106.05648v1 [cs.NE])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Grillotti_L/0/1/0/all/0/1">Luca Grillotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Cully_A/0/1/0/all/0/1">Antoine Cully</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05648">
                                    <div class="article-summary-box-inner">
                                        <span>Quality-Diversity algorithms refer to a class of evolutionary algorithms
designed to find a collection of diverse and high-performing solutions to a
given problem. In robotics, such algorithms can be used for generating a
collection of controllers covering most of the possible behaviours of a robot.
To do so, these algorithms associate a behavioural descriptor to each of these
behaviours. Each behavioural descriptor is used for estimating the novelty of
one behaviour compared to the others. In most existing algorithms, the
behavioural descriptor needs to be hand-coded, thus requiring prior knowledge
about the task to solve. In this paper, we introduce: Autonomous Robots
Realising their Abilities, an algorithm that uses a dimensionality reduction
technique to automatically learn behavioural descriptors based on raw sensory
data. The performance of this algorithm is assessed on three robotic tasks in
simulation. The experimental results show that it performs similarly to
traditional hand-coded approaches without the requirement to provide any
hand-coded behavioural descriptor. In the collection of diverse and
high-performing solutions, it also manages to find behaviours that are novel
with respect to more features than its hand-coded baselines. Finally, we
introduce a variant of the algorithm which is robust to the dimensionality of
the behavioural descriptor space.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Audiovisual transfer learning for audio tagging and sound event detection. (arXiv:2106.05408v1 [eess.AS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Boes_W/0/1/0/all/0/1">Wim Boes</a>, <a href="http://arxiv.org/find/eess/1/au:+hamme_H/0/1/0/all/0/1">Hugo Van hamme</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05408">
                                    <div class="article-summary-box-inner">
                                        <span>We study the merit of transfer learning for two sound recognition problems,
i.e., audio tagging and sound event detection. Employing feature fusion, we
adapt a baseline system utilizing only spectral acoustic inputs to also make
use of pretrained auditory and visual features, extracted from networks built
for different tasks and trained with external data. We perform experiments with
these modified models on an audiovisual multi-label data set, of which the
training partition contains a large number of unlabeled samples and a smaller
amount of clips with weak annotations, indicating the clip-level presence of 10
sound categories without specifying the temporal boundaries of the active
auditory events. For clip-based audio tagging, this transfer learning method
grants marked improvements. Addition of the visual modality on top of audio
also proves to be advantageous in this context. When it comes to generating
transcriptions of audio recordings, the benefit of pretrained features depends
on the requested temporal resolution: for coarse-grained sound event detection,
their utility remains notable. But when more fine-grained predictions are
required, performance gains are strongly reduced due to a mismatch between the
problem at hand and the goals of the models from which the pretrained vectors
were obtained.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Quantized Conditional COT-GAN for Video Prediction. (arXiv:2106.05658v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Xu_T/0/1/0/all/0/1">Tianlin Xu</a>, <a href="http://arxiv.org/find/stat/1/au:+Acciaio_B/0/1/0/all/0/1">Beatrice Acciaio</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05658">
                                    <div class="article-summary-box-inner">
                                        <span>Causal Optimal Transport (COT) results from imposing a temporal causality
constraint on classic optimal transport problems, which naturally generates a
new concept of distances between distributions on path spaces. The first
application of the COT theory for sequential learning was given in Xu et al.
(2020), where COT-GAN was introduced as an adversarial algorithm to train
implicit generative models optimized for producing sequential data. Relying on
Xu et al. (2020), the contribution of the present paper is twofold. First, we
develop a conditional version of COT-GAN suitable for sequence prediction. This
means that the dataset is now used in order to learn how a sequence will evolve
given the observation of its past evolution. Second, we improve on the
convergence results by working with modifications of the empirical measures via
a specific type of quantization due to Backhoff et al. (2020). The resulting
quantized conditional COT-GAN algorithm is illustrated with an application for
video prediction.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Understanding the Under-Coverage Bias in Uncertainty Estimation. (arXiv:2106.05515v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yu Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1">Song Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05515">
                                    <div class="article-summary-box-inner">
                                        <span>Estimating the data uncertainty in regression tasks is often done by learning
a quantile function or a prediction interval of the true label conditioned on
the input. It is frequently observed that quantile regression -- a vanilla
algorithm for learning quantiles with asymptotic guarantees -- tends to
\emph{under-cover} than the desired coverage level in reality. While various
fixes have been proposed, a more fundamental understanding of why this
under-coverage bias happens in the first place remains elusive.

In this paper, we present a rigorous theoretical study on the coverage of
uncertainty estimation algorithms in learning quantiles. We prove that quantile
regression suffers from an inherent under-coverage bias, in a vanilla setting
where we learn a realizable linear quantile function and there is more data
than parameters. More quantitatively, for $\alpha&gt;0.5$ and small $d/n$, the
$\alpha$-quantile learned by quantile regression roughly achieves coverage
$\alpha - (\alpha-1/2)\cdot d/n$ regardless of the noise distribution, where
$d$ is the input dimension and $n$ is the number of training data. Our theory
reveals that this under-coverage bias stems from a certain high-dimensional
parameter estimation error that is not implied by existing theories on quantile
regression. Experiments on simulated and real data verify our theory and
further illustrate the effect of various factors such as sample size and model
capacity on the under-coverage bias in more practical setups.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GraphiT: Encoding Graph Structure in Transformers. (arXiv:2106.05667v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mialon_G/0/1/0/all/0/1">Gr&#xe9;goire Mialon</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dexiong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Selosse_M/0/1/0/all/0/1">Margot Selosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1">Julien Mairal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05667">
                                    <div class="article-summary-box-inner">
                                        <span>We show that viewing graphs as sets of node features and incorporating
structural and positional information into a transformer architecture is able
to outperform representations learned with classical graph neural networks
(GNNs). Our model, GraphiT, encodes such information by (i) leveraging relative
positional encoding strategies in self-attention scores based on positive
definite kernels on graphs, and (ii) enumerating and encoding local
sub-structures such as paths of short length. We thoroughly evaluate these two
ideas on many classification and regression tasks, demonstrating the
effectiveness of each of them independently, as well as their combination. In
addition to performing well on standard benchmarks, our model also admits
natural visualization mechanisms for interpreting graph motifs explaining the
predictions, making it a potentially strong candidate for scientific
applications where interpretation is important. Code available at
https://github.com/inria-thoth/GraphiT.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Discontinuity Capturing Shallow Neural Network for Elliptic Interface Problems. (arXiv:2106.05587v1 [math.NA])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Hu_W/0/1/0/all/0/1">Wei-Fan Hu</a>, <a href="http://arxiv.org/find/math/1/au:+Lin_T/0/1/0/all/0/1">Te-Sheng Lin</a>, <a href="http://arxiv.org/find/math/1/au:+Lai_M/0/1/0/all/0/1">Ming-Chih Lai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05587">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, a new Discontinuity Capturing Shallow Neural Network (DCSNN)
for approximating $d$-dimensional piecewise continuous functions and for
solving elliptic interface problems is developed. There are three novel
features in the present network; namely, (i) jump discontinuity is captured
sharply, (ii) it is completely shallow consisting of only one hidden layer,
(iii) it is completely mesh-free for solving partial differential equations
(PDEs). We first continuously extend the $d$-dimensional piecewise continuous
function in $(d+1)$-dimensional space by augmenting one coordinate variable to
label the pieces of discontinuous function, and then construct a shallow neural
network to express this new augmented function. Since only one hidden layer is
employed, the number of training parameters (weights and biases) scales
linearly with the dimension and the neurons used in the hidden layer. For
solving elliptic interface equations, the network is trained by minimizing the
mean squared error loss that consists of the residual of governing equation,
boundary condition, and the interface jump conditions. We perform a series of
numerical tests to compare the accuracy and efficiency of the present network.
Our DCSNN model is comparably efficient due to only moderate number of
parameters needed to be trained (a few hundreds of parameters used throughout
all numerical examples here), and the result shows better accuracy (and less
parameters) than other method using piecewise deep neural network in
literature. We also compare the results obtained by the traditional grid-based
immersed interface method (IIM) which is designed particularly for elliptic
interface problems. Again, the present results show better accuracy than the
ones obtained by IIM. We conclude by solving a six-dimensional problem to show
the capability of the present network for high-dimensional applications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">StreamBrain: An HPC Framework for Brain-like Neural Networks on CPUs, GPUs and FPGAs. (arXiv:2106.05373v1 [cs.DC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Podobas_A/0/1/0/all/0/1">Artur Podobas</a>, <a href="http://arxiv.org/find/cs/1/au:+Svedin_M/0/1/0/all/0/1">Martin Svedin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1">Steven W. D. Chien</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_I/0/1/0/all/0/1">Ivy B. Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravichandran_N/0/1/0/all/0/1">Naresh Balaji Ravichandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Herman_P/0/1/0/all/0/1">Pawel Herman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lansner_A/0/1/0/all/0/1">Anders Lansner</a>, <a href="http://arxiv.org/find/cs/1/au:+Markidis_S/0/1/0/all/0/1">Stefano Markidis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05373">
                                    <div class="article-summary-box-inner">
                                        <span>The modern deep learning method based on backpropagation has surged in
popularity and has been used in multiple domains and application areas. At the
same time, there are other -- less-known -- machine learning algorithms with a
mature and solid theoretical foundation whose performance remains unexplored.
One such example is the brain-like Bayesian Confidence Propagation Neural
Network (BCPNN). In this paper, we introduce StreamBrain -- a framework that
allows neural networks based on BCPNN to be practically deployed in
High-Performance Computing systems. StreamBrain is a domain-specific language
(DSL), similar in concept to existing machine learning (ML) frameworks, and
supports backends for CPUs, GPUs, and even FPGAs. We empirically demonstrate
that StreamBrain can train the well-known ML benchmark dataset MNIST within
seconds, and we are the first to demonstrate BCPNN on STL-10 size networks. We
also show how StreamBrain can be used to train with custom floating-point
formats and illustrate the impact of using different bfloat variations on BCPNN
using FPGAs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Reinforcement Learning for Industrial Control Network Cyber Security Orchestration. (arXiv:2106.05332v1 [cs.CR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mern_J/0/1/0/all/0/1">John Mern</a>, <a href="http://arxiv.org/find/cs/1/au:+Hatch_K/0/1/0/all/0/1">Kyle Hatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1">Ryan Silva</a>, <a href="http://arxiv.org/find/cs/1/au:+Brush_J/0/1/0/all/0/1">Jeff Brush</a>, <a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1">Mykel J. Kochenderfer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05332">
                                    <div class="article-summary-box-inner">
                                        <span>Defending computer networks from cyber attack requires coordinating actions
across multiple nodes based on imperfect indicators of compromise while
minimizing disruptions to network operations. Advanced attacks can progress
with few observable signals over several months before execution. The resulting
sequential decision problem has large observation and action spaces and a long
time-horizon, making it difficult to solve with existing methods. In this work,
we present techniques to scale deep reinforcement learning to solve the cyber
security orchestration problem for large industrial control networks. We
propose a novel attention-based neural architecture with size complexity that
is invariant to the size of the network under protection. A pre-training
curriculum is presented to overcome early exploration difficulty. Experiments
show in that the proposed approaches greatly improve both the learning sample
complexity and converged policy performance over baseline methods in
simulation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online Learning for Stochastic Shortest Path Model via Posterior Sampling. (arXiv:2106.05335v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jafarnia_Jahromi_M/0/1/0/all/0/1">Mehdi Jafarnia-Jahromi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1">Rahul Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1">Haipeng Luo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05335">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of online reinforcement learning for the Stochastic
Shortest Path (SSP) problem modeled as an unknown MDP with an absorbing state.
We propose PSRL-SSP, a simple posterior sampling-based reinforcement learning
algorithm for the SSP problem. The algorithm operates in epochs. At the
beginning of each epoch, a sample is drawn from the posterior distribution on
the unknown model dynamics, and the optimal policy with respect to the drawn
sample is followed during that epoch. An epoch completes if either the number
of visits to the goal state in the current epoch exceeds that of the previous
epoch, or the number of visits to any of the state-action pairs is doubled. We
establish a Bayesian regret bound of $O(B_\star S\sqrt{AK})$, where $B_\star$
is an upper bound on the expected cost of the optimal policy, $S$ is the size
of the state space, $A$ is the size of the action space, and $K$ is the number
of episodes. The algorithm only requires the knowledge of the prior
distribution, and has no hyper-parameters to tune. It is the first such
posterior sampling algorithm and outperforms numerically previously proposed
optimism-based algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adaptive Streaming Perception using Deep Reinforcement Learning. (arXiv:2106.05665v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1">Anurag Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Nambi_A/0/1/0/all/0/1">Akshay Nambi</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Aditya Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+YVS_H/0/1/0/all/0/1">Harish YVS</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganu_T/0/1/0/all/0/1">Tanuja Ganu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05665">
                                    <div class="article-summary-box-inner">
                                        <span>Executing computer vision models on streaming visual data, or streaming
perception is an emerging problem, with applications in self-driving, embodied
agents, and augmented/virtual reality. The development of such systems is
largely governed by the accuracy and latency of the processing pipeline. While
past work has proposed numerous approximate execution frameworks, their
decision functions solely focus on optimizing latency, accuracy, or energy,
etc. This results in sub-optimum decisions, affecting the overall system
performance. We argue that the streaming perception systems should holistically
maximize the overall system performance (i.e., considering both accuracy and
latency simultaneously). To this end, we describe a new approach based on deep
reinforcement learning to learn these tradeoffs at runtime for streaming
perception. This tradeoff optimization is formulated as a novel deep contextual
bandit problem and we design a new reward function that holistically integrates
latency and accuracy into a single metric. We show that our agent can learn a
competitive policy across multiple decision dimensions, which outperforms
state-of-the-art policies on public datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DASVDD: Deep Autoencoding Support Vector Data Descriptor for Anomaly Detection. (arXiv:2106.05410v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hojjati_H/0/1/0/all/0/1">Hadi Hojjati</a>, <a href="http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1">Narges Armanfard</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05410">
                                    <div class="article-summary-box-inner">
                                        <span>Semi-supervised anomaly detection, which aims to detect anomalies from normal
samples using a model that is solely trained on normal data, has been an active
field of research in the past decade. With recent advancements in deep
learning, particularly generative adversarial networks and autoencoders,
researchers have designed efficient deep anomaly detection methods. Existing
works commonly use neural networks such as an autoencoder to map the data into
a new representation that is easier to work with and then apply an anomaly
detection algorithm. In this paper, we propose a method, DASVDD, that jointly
learns the parameters of an autoencoder while minimizing the volume of an
enclosing hyper-sphere on its latent representation. We propose a customized
anomaly score which is a combination of autoencoder&#x27;s reconstruction error and
distance of the lower-dimensional representation of a sample from the center of
the enclosing hyper-sphere. Minimizing this anomaly score on the normal data
during training aids us in learning the underlying distribution of normal data.
Including the reconstruction error in the anomaly score ensures that DASVDD
does not suffer from the common hyper-sphere collapse issue since the proposed
DASVDD model does not converge to the trivial solution of mapping all inputs to
a constant point in the latent representation. Experimental evaluations on
several benchmark datasets from different domains show that the proposed method
outperforms most of the commonly used state-of-the-art anomaly detection
algorithms while maintaining robust and accurate performance across different
anomaly classes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Parameter and Feature Selection in Stochastic Linear Bandits. (arXiv:2106.05378v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Moradipari_A/0/1/0/all/0/1">Ahmadreza Moradipari</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1">Yasin Abbasi-Yadkori</a>, <a href="http://arxiv.org/find/cs/1/au:+Alizadeh_M/0/1/0/all/0/1">Mahnoosh Alizadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1">Mohammad Ghavamzadeh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05378">
                                    <div class="article-summary-box-inner">
                                        <span>We study two model selection settings in stochastic linear bandits (LB). In
the first setting, the reward parameter of the LB problem is arbitrarily
selected from $M$ models represented as (possibly) overlapping balls in
$\mathbb R^d$. However, the agent only has access to misspecified models, i.e.,
estimates of the centers and radii of the balls. We refer to this setting as
parameter selection. In the second setting, which we refer to as feature
selection, the expected reward of the LB problem is in the linear span of at
least one of $M$ feature maps (models). For each setting, we develop and
analyze an algorithm that is based on a reduction from bandits to
full-information problems. This allows us to obtain regret bounds that are not
worse (up to a $\sqrt{\log M}$ factor) than the case where the true model is
known. Our parameter selection algorithm is OFUL-style and the one for feature
selection is based on the SquareCB algorithm. We also show that the regret of
our parameter selection algorithm scales logarithmically with model
misspecification.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Pulling back information geometry. (arXiv:2106.05367v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Arvanitidis_G/0/1/0/all/0/1">Georgios Arvanitidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Duque_M/0/1/0/all/0/1">Miguel Gonz&#xe1;lez-Duque</a>, <a href="http://arxiv.org/find/cs/1/au:+Pouplin_A/0/1/0/all/0/1">Alison Pouplin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalatzis_D/0/1/0/all/0/1">Dimitris Kalatzis</a>, <a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1">S&#xf8;ren Hauberg</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05367">
                                    <div class="article-summary-box-inner">
                                        <span>Latent space geometry has shown itself to provide a rich and rigorous
framework for interacting with the latent variables of deep generative models.
The existing theory, however, relies on the decoder being a Gaussian
distribution as its simple reparametrization allows us to interpret the
generating process as a random projection of a deterministic manifold.
Consequently, this approach breaks down when applied to decoders that are not
as easily reparametrized. We here propose to use the Fisher-Rao metric
associated with the space of decoder distributions as a reference metric, which
we pull back to the latent space. We show that we can achieve meaningful latent
geometries for a wide range of decoder distributions for which the previous
theory was not applicable, opening the door to &#x60;black box&#x27; latent geometries.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Certified Defenses: Why Tighter Relaxations May Hurt Training. (arXiv:2102.06700v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jovanovic_N/0/1/0/all/0/1">Nikola Jovanovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1">Mislav Balunovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Baader_M/0/1/0/all/0/1">Maximilian Baader</a>, <a href="http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1">Martin Vechev</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06700">
                                    <div class="article-summary-box-inner">
                                        <span>Certified defenses based on convex relaxations are an established technique
for training provably robust models. The key component is the choice of
relaxation, varying from simple intervals to tight polyhedra. Paradoxically,
however, training with tighter relaxations can often lead to worse certified
robustness. The poor understanding of this paradox has forced recent
state-of-the-art certified defenses to focus on designing various heuristics in
order to mitigate its effects. In contrast, in this paper we study the
underlying causes and show that tightness alone may not be the determining
factor. Concretely, we identify two key properties of relaxations that impact
training dynamics: continuity and sensitivity. Our extensive experimental
evaluation demonstrates that these two factors, observed alongside tightness,
explain the drop in certified robustness for popular relaxations. Further, we
investigate the possibility of designing and training with relaxations that are
tight, continuous and not sensitive. We believe the insights of this work can
help drive the principled discovery of new and effective certified defense
mechanisms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Public Transit for Special Events: Ridership Prediction and Train Optimization. (arXiv:2106.05359v1 [math.OC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Santanam_T/0/1/0/all/0/1">Tejas Santanam</a>, <a href="http://arxiv.org/find/math/1/au:+Trasatti_A/0/1/0/all/0/1">Anthony Trasatti</a>, <a href="http://arxiv.org/find/math/1/au:+Hentenryck_P/0/1/0/all/0/1">Pascal Van Hentenryck</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1">Hanyu Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05359">
                                    <div class="article-summary-box-inner">
                                        <span>Many special events, including sport games and concerts, often cause surges
in demand and congestion for transit systems. Therefore, it is important for
transit providers to understand their impact on disruptions, delays, and fare
revenues. This paper proposes a suite of data-driven techniques that exploit
Automated Fare Collection (AFC) data for evaluating, anticipating, and managing
the performance of transit systems during recurring congestion peaks due to
special events. This includes an extensive analysis of ridership of the two
major stadiums in downtown Atlanta using rail data from the Metropolitan
Atlanta Rapid Transit Authority (MARTA). The paper first highlights the
ridership predictability at the aggregate level for each station on both event
and non-event days. It then presents an unsupervised machine-learning model to
cluster passengers and identify which train they are boarding. The model makes
it possible to evaluate system performance in terms of fundamental metrics such
as the passenger load per train and the wait times of riders. The paper also
presents linear regression and random forest models for predicting ridership
that are used in combination with historical throughput analysis to forecast
demand. Finally, simulations are performed that showcase the potential
improvements to wait times and demand matching by leveraging proposed
techniques to optimize train frequencies based on forecasted demand.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ZoPE: A Fast Optimizer for ReLU Networks with Low-Dimensional Inputs. (arXiv:2106.05325v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Strong_C/0/1/0/all/0/1">Christopher A. Strong</a>, <a href="http://arxiv.org/find/cs/1/au:+Katz_S/0/1/0/all/0/1">Sydney M. Katz</a>, <a href="http://arxiv.org/find/cs/1/au:+Corso_A/0/1/0/all/0/1">Anthony L. Corso</a>, <a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1">Mykel J. Kochenderfer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05325">
                                    <div class="article-summary-box-inner">
                                        <span>Deep neural networks often lack the safety and robustness guarantees needed
to be deployed in safety critical systems. Formal verification techniques can
be used to prove input-output safety properties of networks, but when
properties are difficult to specify, we rely on the solution to various
optimization problems. In this work, we present an algorithm called ZoPE that
solves optimization problems over the output of feedforward ReLU networks with
low-dimensional inputs. The algorithm eagerly splits the input space, bounding
the objective using zonotope propagation at each step, and improves
computational efficiency compared to existing mixed integer programming
approaches. We demonstrate how to formulate and solve three types of
optimization problems: (i) minimization of any convex function over the output
space, (ii) minimization of a convex function over the output of two networks
in series with an adversarial perturbation in the layer between them, and (iii)
maximization of the difference in output between two networks. Using ZoPE, we
observe a $25\times$ speedup on property 1 of the ACAS Xu neural network
verification benchmark and an $85\times$ speedup on a set of linear
optimization problems. We demonstrate the versatility of the optimizer in
analyzing networks by projecting onto the range of a generative adversarial
network and visualizing the differences between a compressed and uncompressed
network.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">How Robust are Model Rankings: A Leaderboard Customization Approach for Equitable Evaluation. (arXiv:2106.05532v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Swaroop Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1">Anjana Arunkumar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05532">
                                    <div class="article-summary-box-inner">
                                        <span>Models that top leaderboards often perform unsatisfactorily when deployed in
real world applications; this has necessitated rigorous and expensive
pre-deployment model testing. A hitherto unexplored facet of model performance
is: Are our leaderboards doing equitable evaluation? In this paper, we
introduce a task-agnostic method to probe leaderboards by weighting samples
based on their &#x60;difficulty&#x27; level. We find that leaderboards can be
adversarially attacked and top performing models may not always be the best
models. We subsequently propose alternate evaluation metrics. Our experiments
on 10 models show changes in model ranking and an overall reduction in
previously reported performance -- thus rectifying the overestimation of AI
systems&#x27; capabilities. Inspired by behavioral testing principles, we further
develop a prototype of a visual analytics tool that enables leaderboard
revamping through customization, based on an end user&#x27;s focus area. This helps
users analyze models&#x27; strengths and weaknesses, and guides them in the
selection of a model best suited for their application scenario. In a user
study, members of various commercial product development teams, covering 5
focus areas, find that our prototype reduces pre-deployment development and
testing effort by 41% on average.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fine-Grained System Identification of Nonlinear Neural Circuits. (arXiv:2106.05400v1 [q-bio.QM])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/q-bio/1/au:+Bagherian_D/0/1/0/all/0/1">Dawna Bagherian</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gornet_J/0/1/0/all/0/1">James Gornet</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Bernstein_J/0/1/0/all/0/1">Jeremy Bernstein</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ni_Y/0/1/0/all/0/1">Yu-Li Ni</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yue_Y/0/1/0/all/0/1">Yisong Yue</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Meister_M/0/1/0/all/0/1">Markus Meister</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05400">
                                    <div class="article-summary-box-inner">
                                        <span>We study the problem of sparse nonlinear model recovery of high dimensional
compositional functions. Our study is motivated by emerging opportunities in
neuroscience to recover fine-grained models of biological neural circuits using
collected measurement data. Guided by available domain knowledge in
neuroscience, we explore conditions under which one can recover the underlying
biological circuit that generated the training data. Our results suggest
insights of both theoretical and practical interests. Most notably, we find
that a sign constraint on the weights is a necessary condition for system
recovery, which we establish both theoretically with an identifiability
guarantee and empirically on simulated biological circuits. We conclude with a
case study on retinal ganglion cell circuits using data collected from mouse
retina, showcasing the practical potential of this approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Direct Volume Rendering: Learning Visual Feature Mappings From Exemplary Images. (arXiv:2106.05429v1 [cs.GR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Weiss_J/0/1/0/all/0/1">Jakob Weiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05429">
                                    <div class="article-summary-box-inner">
                                        <span>Volume Rendering is an important technique for visualizing three-dimensional
scalar data grids and is commonly employed for scientific and medical image
data. Direct Volume Rendering (DVR) is a well established and efficient
rendering algorithm for volumetric data. Neural rendering uses deep neural
networks to solve inverse rendering tasks and applies techniques similar to
DVR. However, it has not been demonstrated successfully for the rendering of
scientific volume data.

In this work, we introduce Deep Direct Volume Rendering (DeepDVR), a
generalization of DVR that allows for the integration of deep neural networks
into the DVR algorithm. We conceptualize the rendering in a latent color space,
thus enabling the use of deep architectures to learn implicit mappings for
feature extraction and classification, replacing explicit feature design and
hand-crafted transfer functions. Our generalization serves to derive novel
volume rendering architectures that can be trained end-to-end directly from
examples in image space, obviating the need to manually define and fine-tune
multidimensional transfer functions while providing superior classification
strength. We further introduce a novel stepsize annealing scheme to accelerate
the training of DeepDVR models and validate its effectiveness in a set of
experiments. We validate our architectures on two example use cases: (1)
learning an optimized rendering from manually adjusted reference images for a
single volume and (2) learning advanced visualization concepts like shading and
semantic colorization that generalize to unseen volume data.

We find that deep volume rendering architectures with explicit modeling of
the DVR pipeline effectively enable end-to-end learning of scientific volume
rendering tasks from target images.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Eye of the Beholder: Improved Relation Generalization for Text-based Reinforcement Learning Agents. (arXiv:2106.05387v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1">Keerthiram Murugesan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1">Subhajit Chaudhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Talamadupula_K/0/1/0/all/0/1">Kartik Talamadupula</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05387">
                                    <div class="article-summary-box-inner">
                                        <span>Text-based games (TBGs) have become a popular proving ground for the
demonstration of learning-based agents that make decisions in quasi real-world
settings. The crux of the problem for a reinforcement learning agent in such
TBGs is identifying the objects in the world, and those objects&#x27; relations with
that world. While the recent use of text-based resources for increasing an
agent&#x27;s knowledge and improving its generalization have shown promise, we posit
in this paper that there is much yet to be learned from visual representations
of these same worlds. Specifically, we propose to retrieve images that
represent specific instances of text observations from the world and train our
agents on such images. This improves the agent&#x27;s overall understanding of the
game &#x27;scene&#x27; and objects&#x27; relationships to the world around them, and the
variety of visual representations on offer allow the agent to generate a better
generalization of a relationship. We show that incorporating such images
improves the performance of agents in various TBG settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MLP-Mixer: An all-MLP Architecture for Vision. (arXiv:2105.01601v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1">Ilya Tolstikhin</a>, <a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1">Neil Houlsby</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1">Alexander Kolesnikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1">Lucas Beyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaohua Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1">Thomas Unterthiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Yung_J/0/1/0/all/0/1">Jessica Yung</a>, <a href="http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1">Andreas Steiner</a>, <a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1">Daniel Keysers</a>, <a href="http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1">Jakob Uszkoreit</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1">Mario Lucic</a>, <a href="http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1">Alexey Dosovitskiy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01601">
                                    <div class="article-summary-box-inner">
                                        <span>Convolutional Neural Networks (CNNs) are the go-to model for computer vision.
Recently, attention-based networks, such as the Vision Transformer, have also
become popular. In this paper we show that while convolutions and attention are
both sufficient for good performance, neither of them are necessary. We present
MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs).
MLP-Mixer contains two types of layers: one with MLPs applied independently to
image patches (i.e. &quot;mixing&quot; the per-location features), and one with MLPs
applied across patches (i.e. &quot;mixing&quot; spatial information). When trained on
large datasets, or with modern regularization schemes, MLP-Mixer attains
competitive scores on image classification benchmarks, with pre-training and
inference cost comparable to state-of-the-art models. We hope that these
results spark further research beyond the realms of well established CNNs and
Transformers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Probing transfer learning with a model of synthetic correlated datasets. (arXiv:2106.05418v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gerace_F/0/1/0/all/0/1">Federica Gerace</a>, <a href="http://arxiv.org/find/cs/1/au:+Saglietti_L/0/1/0/all/0/1">Luca Saglietti</a>, <a href="http://arxiv.org/find/cs/1/au:+Mannelli_S/0/1/0/all/0/1">Stefano Sarao Mannelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1">Andrew Saxe</a>, <a href="http://arxiv.org/find/cs/1/au:+Zdeborova_L/0/1/0/all/0/1">Lenka Zdeborov&#xe1;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05418">
                                    <div class="article-summary-box-inner">
                                        <span>Transfer learning can significantly improve the sample efficiency of neural
networks, by exploiting the relatedness between a data-scarce target task and a
data-abundant source task. Despite years of successful applications, transfer
learning practice often relies on ad-hoc solutions, while theoretical
understanding of these procedures is still limited. In the present work, we
re-think a solvable model of synthetic data as a framework for modeling
correlation between data-sets. This setup allows for an analytic
characterization of the generalization performance obtained when transferring
the learned feature map from the source to the target task. Focusing on the
problem of training two-layer networks in a binary classification setting, we
show that our model can capture a range of salient features of transfer
learning with real data. Moreover, by exploiting parametric control over the
correlation between the two data-sets, we systematically investigate under
which conditions the transfer of features is beneficial for generalization.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Distance Metric Learning through Minimization of the Free Energy. (arXiv:2106.05495v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Stosic_D/0/1/0/all/0/1">Dusan Stosic</a>, <a href="http://arxiv.org/find/cs/1/au:+Stosic_D/0/1/0/all/0/1">Darko Stosic</a>, <a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1">Teresa B. Ludermir</a>, <a href="http://arxiv.org/find/cs/1/au:+Stosic_B/0/1/0/all/0/1">Borko Stosic</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05495">
                                    <div class="article-summary-box-inner">
                                        <span>Distance metric learning has attracted a lot of interest for solving machine
learning and pattern recognition problems over the last decades. In this work
we present a simple approach based on concepts from statistical physics to
learn optimal distance metric for a given problem. We formulate the task as a
typical statistical physics problem: distances between patterns represent
constituents of a physical system and the objective function corresponds to
energy. Then we express the problem as a minimization of the free energy of a
complex system, which is equivalent to distance metric learning. Much like for
many problems in physics, we propose an approach based on Metropolis Monte
Carlo to find the best distance metric. This provides a natural way to learn
the distance metric, where the learning process can be intuitively seen as
stretching and rotating the metric space until some heuristic is satisfied. Our
proposed method can handle a wide variety of constraints including those with
spurious local minima. The approach works surprisingly well with stochastic
nearest neighbors from neighborhood component analysis (NCA). Experimental
results on artificial and real-world data sets reveal a clear superiority over
a number of state-of-the-art distance metric learning methods for nearest
neighbors classification.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Artificial Intelligence in Drug Discovery:Applications and Techniques. (arXiv:2106.05386v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jianyuan Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhibo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1">Dimitris Samaras</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fusheng Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05386">
                                    <div class="article-summary-box-inner">
                                        <span>Artificial intelligence has transformed the practice of drug discovery in the
past decade. Various artificial intelligence techniques have been used in a
wide range of applications. In this perspective, we present major applications
of AI in drug discovery and discuss the relevant AI techniques, covering most
recent progress in AI-driven drug discovery. We expect that the perspective
will serve as a guide for researchers who are interested in working at this
intersected area of artificial intelligence and drug discovery. We also provide
a GitHub repository summarizing the surveyed papers as a learning resource,
which will be regularly updated.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision. (arXiv:2102.03334v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Kim_W/0/1/0/all/0/1">Wonjae Kim</a>, <a href="http://arxiv.org/find/stat/1/au:+Son_B/0/1/0/all/0/1">Bokyung Son</a>, <a href="http://arxiv.org/find/stat/1/au:+Kim_I/0/1/0/all/0/1">Ildoo Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03334">
                                    <div class="article-summary-box-inner">
                                        <span>Vision-and-Language Pre-training (VLP) has improved performance on various
joint vision-and-language downstream tasks. Current approaches to VLP heavily
rely on image feature extraction processes, most of which involve region
supervision (e.g., object detection) and the convolutional architecture (e.g.,
ResNet). Although disregarded in the literature, we find it problematic in
terms of both (1) efficiency/speed, that simply extracting input features
requires much more computation than the multimodal interaction steps; and (2)
expressive power, as it is upper bounded to the expressive power of the visual
embedder and its predefined visual vocabulary. In this paper, we present a
minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the
sense that the processing of visual inputs is drastically simplified to just
the same convolution-free manner that we process textual inputs. We show that
ViLT is up to tens of times faster than previous VLP models, yet with
competitive or better downstream task performance. Our code and pre-trained
weights are available at https://github.com/dandelin/vilt.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A step towards a reinforcement learning de novo genome assembler. (arXiv:2102.02649v2 [q-bio.GN] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/q-bio/1/au:+Padovani_K/0/1/0/all/0/1">Kleber Padovani</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Xavier_R/0/1/0/all/0/1">Roberto Xavier</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Carvalho_A/0/1/0/all/0/1">Andre Carvalho</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Reali_A/0/1/0/all/0/1">Anna Reali</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Chateau_A/0/1/0/all/0/1">Annie Chateau</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Alves_R/0/1/0/all/0/1">Ronnie Alves</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02649">
                                    <div class="article-summary-box-inner">
                                        <span>The use of reinforcement learning has proven to be very promising for solving
complex activities without human supervision during their learning process.
However, their successful applications are predominantly focused on fictional
and entertainment problems - such as games. Based on the above, this work aims
to shed light on the application of reinforcement learning to solve this
relevant real-world problem, the genome assembly. By expanding the only
approach found in the literature that addresses this problem, we carefully
explored the aspects of intelligent agent learning, performed by the Q-learning
algorithm, to understand its suitability to be applied in scenarios whose
characteristics are more similar to those faced by real genome projects. The
improvements proposed here include changing the previously proposed reward
system and including state space exploration optimization strategies based on
dynamic pruning and mutual collaboration with evolutionary computing. These
investigations were tried on 23 new environments with larger inputs than those
used previously. All these environments are freely available on the internet
for the evolution of this research by the scientific community. The results
suggest consistent performance progress using the proposed improvements,
however, they also demonstrate the limitations of them, especially related to
the high dimensionality of state and action spaces. We also present, later, the
paths that can be traced to tackle genome assembly efficiently in real
scenarios considering recent, successfully reinforcement learning applications
- including deep reinforcement learning - from other domains dealing with
high-dimensional inputs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Synthetic Data -- Anonymisation Groundhog Day. (arXiv:2011.07018v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Stadler_T/0/1/0/all/0/1">Theresa Stadler</a>, <a href="http://arxiv.org/find/cs/1/au:+Oprisanu_B/0/1/0/all/0/1">Bristena Oprisanu</a>, <a href="http://arxiv.org/find/cs/1/au:+Troncoso_C/0/1/0/all/0/1">Carmela Troncoso</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07018">
                                    <div class="article-summary-box-inner">
                                        <span>Synthetic data has been advertised as a silver-bullet solution to
privacy-preserving data publishing that addresses the shortcomings of
traditional anonymisation techniques. The promise is that synthetic data drawn
from generative models preserves the statistical properties of the original
dataset but, at the same time, provides perfect protection against privacy
attacks. In this work, we present the first quantitative evaluation of the
privacy gain of synthetic data publishing and compare it to that of previous
anonymisation techniques.

Our evaluation of a wide range of state-of-the-art generative models
demonstrates that synthetic data either does not prevent inference attacks or
does not retain data utility. In other words, we empirically show that
synthetic data suffers from the same limitations as traditional anonymisation
techniques.

Furthermore, we find that, in contrast to traditional anonymisation, the
privacy-utility tradeoff of synthetic data publishing is hard to predict.
Because it is impossible to predict what signals a synthetic dataset will
preserve and what information will be lost, synthetic data leads to a highly
variable privacy gain and unpredictable utility loss. In summary, we find that
synthetic data is far from the holy grail of privacy-preserving data
publishing.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Characterizing Residential Load Patterns by Household Demographic and Socioeconomic Factors. (arXiv:2106.05858v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhuo Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05858">
                                    <div class="article-summary-box-inner">
                                        <span>The wide adoption of smart meters makes residential load data available and
thus improves the understanding of the energy consumption behavior. Many
existing studies have focused on smart-meter data analysis, but the drivers of
energy consumption behaviors are not well understood. This paper aims to
characterize and estimate users&#x27; load patterns based on their demographic and
socioeconomic information. We adopt the symbolic aggregate approximation (SAX)
method to process the load data and use the K-Means method to extract key load
patterns. We develop a deep neural network (DNN) to analyze the relationship
between users&#x27; load patterns and their demographic and socioeconomic features.
Using real-world load data, we validate our framework and demonstrate the
connections between load patterns and household demographic and socioeconomic
features. We also take two regression models as benchmarks for comparisons.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Score-based Generative Modeling in Latent Space. (arXiv:2106.05931v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Vahdat_A/0/1/0/all/0/1">Arash Vahdat</a>, <a href="http://arxiv.org/find/stat/1/au:+Kreis_K/0/1/0/all/0/1">Karsten Kreis</a>, <a href="http://arxiv.org/find/stat/1/au:+Kautz_J/0/1/0/all/0/1">Jan Kautz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05931">
                                    <div class="article-summary-box-inner">
                                        <span>Score-based generative models (SGMs) have recently demonstrated impressive
results in terms of both sample quality and distribution coverage. However,
they are usually applied directly in data space and often require thousands of
network evaluations for sampling. Here, we propose the Latent Score-based
Generative Model (LSGM), a novel approach that trains SGMs in a latent space,
relying on the variational autoencoder framework. Moving from data to latent
space allows us to train more expressive generative models, apply SGMs to
non-continuous data, and learn smoother SGMs in a smaller space, resulting in
fewer network evaluations and faster sampling. To enable training LSGMs
end-to-end in a scalable and stable manner, we (i) introduce a new
score-matching objective suitable to the LSGM setting, (ii) propose a novel
parameterization of the score function that allows SGM to focus on the mismatch
of the target distribution with respect to a simple Normal one, and (iii)
analytically derive multiple techniques for variance reduction of the training
objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10,
outperforming all existing generative results on this dataset. On
CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while
outperforming them in sampling time by two orders of magnitude. In modeling
binary images, LSGM achieves state-of-the-art likelihood on the binarized
OMNIGLOT dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Matrix Completion with Model-free Weighting. (arXiv:2106.05850v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1">Jiayi Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Wong_R/0/1/0/all/0/1">Raymond K. W. Wong</a>, <a href="http://arxiv.org/find/stat/1/au:+Mao_X/0/1/0/all/0/1">Xiaojun Mao</a>, <a href="http://arxiv.org/find/stat/1/au:+Chan_K/0/1/0/all/0/1">Kwun Chuen Gary Chan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05850">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose a novel method for matrix completion under general
non-uniform missing structures. By controlling an upper bound of a novel
balancing error, we construct weights that can actively adjust for the
non-uniformity in the empirical risk without explicitly modeling the
observation probabilities, and can be computed efficiently via convex
optimization. The recovered matrix based on the proposed weighted empirical
risk enjoys appealing theoretical guarantees. In particular, the proposed
method achieves a stronger guarantee than existing work in terms of the scaling
with respect to the observation probabilities, under asymptotically
heterogeneous missing settings (where entry-wise observation probabilities can
be of different orders). These settings can be regarded as a better theoretical
model of missing patterns with highly varying probabilities. We also provide a
new minimax lower bound under a class of heterogeneous settings. Numerical
experiments are also provided to demonstrate the effectiveness of the proposed
method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Functional Priors and Posteriors from Data and Physics. (arXiv:2106.05863v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1">Xuhui Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Liu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1">Zhiping Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrandis_J/0/1/0/all/0/1">Jose del Aguila Ferrandis</a>, <a href="http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05863">
                                    <div class="article-summary-box-inner">
                                        <span>We develop a new Bayesian framework based on deep neural networks to be able
to extrapolate in space-time using historical data and to quantify
uncertainties arising from both noisy and gappy data in physical problems.
Specifically, the proposed approach has two stages: (1) prior learning and (2)
posterior estimation. At the first stage, we employ the physics-informed
Generative Adversarial Networks (PI-GAN) to learn a functional prior either
from a prescribed function distribution, e.g., Gaussian process, or from
historical data and physics. At the second stage, we employ the Hamiltonian
Monte Carlo (HMC) method to estimate the posterior in the latent space of
PI-GANs. In addition, we use two different approaches to encode the physics:
(1) automatic differentiation, used in the physics-informed neural networks
(PINNs) for scenarios with explicitly known partial differential equations
(PDEs), and (2) operator regression using the deep operator network (DeepONet)
for PDE-agnostic scenarios. We then test the proposed method for (1)
meta-learning for one-dimensional regression, and forward/inverse PDE problems
(combined with PINNs); (2) PDE-agnostic physical problems (combined with
DeepONet), e.g., fractional diffusion as well as saturated stochastic
(100-dimensional) flows in heterogeneous porous media; and (3) spatial-temporal
regression problems, i.e., inference of a marine riser displacement field. The
results demonstrate that the proposed approach can provide accurate predictions
as well as uncertainty quantification given very limited scattered and noisy
data, since historical data could be available to provide informative priors.
In summary, the proposed method is capable of learning flexible functional
priors, and can be extended to big data problems using stochastic HMC or
normalizing flows since the latent space is generally characterized as low
dimensional.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MLDemon: Deployment Monitoring for Machine Learning Systems. (arXiv:2104.13621v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ginart_A/0/1/0/all/0/1">Antonio Ginart</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Martin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1">James Zou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13621">
                                    <div class="article-summary-box-inner">
                                        <span>Post-deployment monitoring of the performance of ML systems is critical for
ensuring reliability, especially as new user inputs can differ from the
training distribution. Here we propose a novel approach, MLDemon, for ML
DEployment MONitoring. MLDemon integrates both unlabeled features and a small
amount of on-demand labeled examples over time to produce a real-time estimate
of the ML model&#x27;s current performance on a given data stream. Subject to budget
constraints, MLDemon decides when to acquire additional, potentially costly,
supervised labels to verify the model. On temporal datasets with diverse
distribution drifts and models, MLDemon substantially outperforms existing
monitoring approaches. Moreover, we provide theoretical analysis to show that
MLDemon is minimax rate optimal up to logarithmic factors and is provably
robust against broad distribution drifts whereas prior approaches are not.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learnable Hypergraph Laplacian for Hypergraph Learning. (arXiv:2106.05701v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiying Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuzhao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1">Xi Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1">Runiu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1">Shu-Tao Xia</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05701">
                                    <div class="article-summary-box-inner">
                                        <span>HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their
potential in modeling high-order relations preserved in graph structured data.
However, most existing convolution filters are localized and determined by the
pre-defined initial hypergraph topology, neglecting to explore implicit and
long-ange relations in real-world data. In this paper, we propose the first
learning-based method tailored for constructing adaptive hypergraph structure,
termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic
plug-in-play module for improving the representational power of HGCNNs.
Specifically, HERALD adaptively optimizes the adjacency relationship between
hypernodes and hyperedges in an end-to-end manner and thus the task-aware
hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism
to capture the non-local paired-nodes relation. Extensive experiments on
various popular hypergraph datasets for node classification and graph
classification tasks demonstrate that our approach obtains consistent and
considerable performance enhancement, proving its effectiveness and
generalization ability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Large-scale optimal transport map estimation using projection pursuit. (arXiv:2106.05838v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Meng_C/0/1/0/all/0/1">Cheng Meng</a>, <a href="http://arxiv.org/find/stat/1/au:+Ke_Y/0/1/0/all/0/1">Yuan Ke</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1">Jingyi Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1">Mengrui Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhong_W/0/1/0/all/0/1">Wenxuan Zhong</a>, <a href="http://arxiv.org/find/stat/1/au:+Ma_P/0/1/0/all/0/1">Ping Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05838">
                                    <div class="article-summary-box-inner">
                                        <span>This paper studies the estimation of large-scale optimal transport maps
(OTM), which is a well-known challenging problem owing to the curse of
dimensionality. Existing literature approximates the large-scale OTM by a
series of one-dimensional OTM problems through iterative random projection.
Such methods, however, suffer from slow or none convergence in practice due to
the nature of randomly selected projection directions. Instead, we propose an
estimation method of large-scale OTM by combining the idea of projection
pursuit regression and sufficient dimension reduction. The proposed method,
named projection pursuit Monge map (PPMM), adaptively selects the most
&#x60;&#x60;informative&#x27;&#x27; projection direction in each iteration. We theoretically show
the proposed dimension reduction method can consistently estimate the most
&#x60;&#x60;informative&#x27;&#x27; projection direction in each iteration. Furthermore, the PPMM
algorithm weakly convergences to the target large-scale OTM in a reasonable
number of steps. Empirically, PPMM is computationally easy and converges fast.
We assess its finite sample performance through the applications of Wasserstein
distance estimation and generative models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Efficient Quantum State Sample Tomography with Basis-dependent Neural-networks. (arXiv:2009.07601v3 [quant-ph] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/quant-ph/1/au:+Smith_A/0/1/0/all/0/1">Alistair W. R. Smith</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gray_J/0/1/0/all/0/1">Johnnie Gray</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Kim_M/0/1/0/all/0/1">M. S. Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.07601">
                                    <div class="article-summary-box-inner">
                                        <span>We use a meta-learning neural-network approach to analyse data from a
measured quantum state. Once our neural network has been trained it can be used
to efficiently sample measurements of the state in measurement bases not
contained in the training data. These samples can be used calculate expectation
values and other useful quantities. We refer to this process as &quot;state sample
tomography&quot;. We encode the state&#x27;s measurement outcome distributions using an
efficiently parameterized generative neural network. This allows each stage in
the tomography process to be performed efficiently even for large systems. Our
scheme is demonstrated on recent IBM Quantum devices, producing a model for a
6-qubit state&#x27;s measurement outcomes with a predictive accuracy (classical
fidelity) &gt; 95% for all test cases using only 100 random measurement settings
as opposed to the 729 settings required for standard full tomography using
local measurements. This reduction in the required number of measurements
scales favourably, with training data in 200 measurement settings yielding a
predictive accuracy &gt; 92% for a 10 qubit state where 59,049 settings are
typically required for full local measurement-based quantum state tomography. A
reduction in number of measurements by a factor, in this case, of almost 600
could allow for estimations of expectation values and state fidelities in
practicable times on current quantum devices.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">How to Train Your Differentiable Filter. (arXiv:2012.14313v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kloss_A/0/1/0/all/0/1">Alina Kloss</a>, <a href="http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1">Georg Martius</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1">Jeannette Bohg</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14313">
                                    <div class="article-summary-box-inner">
                                        <span>In many robotic applications, it is crucial to maintain a belief about the
state of a system, which serves as input for planning and decision making and
provides feedback during task execution. Bayesian Filtering algorithms address
this state estimation problem, but they require models of process dynamics and
sensory observations and the respective noise characteristics of these models.
Recently, multiple works have demonstrated that these models can be learned by
end-to-end training through differentiable versions of recursive filtering
algorithms. In this work, we investigate the advantages of differentiable
filters (DFs) over both unstructured learning approaches and manually-tuned
filtering algorithms, and provide practical guidance to researchers interested
in applying such differentiable filters. For this, we implement DFs with four
different underlying filtering algorithms and compare them in extensive
experiments. Specifically, we (i) evaluate different implementation choices and
training approaches, (ii) investigate how well complex models of uncertainty
can be learned in DFs, (iii) evaluate the effect of end-to-end training through
DFs and (iv) compare the DFs among each other and to unstructured LSTM models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Meta Learning Approach to Discerning Causal Graph Structure. (arXiv:2106.05859v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1">Justin Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Damjakob_D/0/1/0/all/0/1">Dominik Damjakob</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05859">
                                    <div class="article-summary-box-inner">
                                        <span>We explore the usage of meta-learning to derive the causal direction between
variables by optimizing over a measure of distribution simplicity. We
incorporate a stochastic graph representation which includes latent variables
and allows for more generalizability and graph structure expression. Our model
is able to learn causal direction indicators for complex graph structures
despite effects of latent confounders. Further, we explore robustness of our
method with respect to violations of our distributional assumptions and data
scarcity. Our model is particularly robust to modest data scarcity, but is less
robust to distributional changes. By interpreting the model predictions as
stochastic events, we propose a simple ensemble method classifier to reduce the
outcome variability as an average of biased events. This methodology
demonstrates ability to infer the existence as well as the direction of a
causal relationship between data distributions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Optimal Cost Design for Model Predictive Control. (arXiv:2104.11353v2 [cs.RO] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Avik Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1">Lawrence Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Daniel S. Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1">Anca D. Dragan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11353">
                                    <div class="article-summary-box-inner">
                                        <span>Many robotics domains use some form of nonconvex model predictive control
(MPC) for planning, which sets a reduced time horizon, performs trajectory
optimization, and replans at every step. The actual task typically requires a
much longer horizon than is computationally tractable, and is specified via a
cost function that cumulates over that full horizon. For instance, an
autonomous car may have a cost function that makes a desired trade-off between
efficiency, safety, and obeying traffic laws. In this work, we challenge the
common assumption that the cost we optimize using MPC should be the same as the
ground truth cost for the task (plus a terminal cost). MPC solvers can suffer
from short planning horizons, local optima, incorrect dynamics models, and,
importantly, fail to account for future replanning ability. Thus, we propose
that in many tasks it could be beneficial to purposefully choose a different
cost function for MPC to optimize: one that results in the MPC rollout having
low ground truth cost, rather than the MPC planned trajectory. We formalize
this as an optimal cost design problem, and propose a zeroth-order
optimization-based approach that enables us to design optimal costs for an MPC
planning robot in continuous MDPs. We test our approach in an autonomous
driving domain where we find costs different from the ground truth that
implicitly compensate for replanning, short horizon, incorrect dynamics models,
and local minima issues. As an example, the learned cost incentivizes MPC to
delay its decision until later, implicitly accounting for the fact that it will
get more information in the future and be able to make a better decision. Code
and videos available at https://sites.google.com/berkeley.edu/ocd-mpc/.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Informative Policy Representations in Multi-Agent Reinforcement Learning via Joint-Action Distributions. (arXiv:2106.05802v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yifan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Haobin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zongqing Lu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05802">
                                    <div class="article-summary-box-inner">
                                        <span>In multi-agent reinforcement learning, the inherent non-stationarity of the
environment caused by other agents&#x27; actions posed significant difficulties for
an agent to learn a good policy independently. One way to deal with
non-stationarity is agent modeling, by which the agent takes into consideration
the influence of other agents&#x27; policies. Most existing work relies on
predicting other agents&#x27; actions or goals, or discriminating between their
policies. However, such modeling fails to capture the similarities and
differences between policies simultaneously and thus cannot provide useful
information when generalizing to unseen policies. To address this, we propose a
general method to learn representations of other agents&#x27; policies via the
joint-action distributions sampled in interactions. The similarities and
differences between policies are naturally captured by the policy distance
inferred from the joint-action distributions and deliberately reflected in the
learned representations. Agents conditioned on the policy representations can
well generalize to unseen agents. We empirically demonstrate that our method
outperforms existing work in multi-agent tasks when facing unseen agents.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Segmenting Hybrid Trajectories using Latent ODEs. (arXiv:2105.03835v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1">Ruian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Morris_Q/0/1/0/all/0/1">Quaid Morris</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03835">
                                    <div class="article-summary-box-inner">
                                        <span>Smooth dynamics interrupted by discontinuities are known as hybrid systems
and arise commonly in nature. Latent ODEs allow for powerful representation of
irregularly sampled time series but are not designed to capture trajectories
arising from hybrid systems. Here, we propose the Latent Segmented ODE
(LatSegODE), which uses Latent ODEs to perform reconstruction and changepoint
detection within hybrid trajectories featuring jump discontinuities and
switching dynamical modes. Where it is possible to train a Latent ODE on the
smooth dynamical flows between discontinuities, we apply the pruned exact
linear time (PELT) algorithm to detect changepoints where latent dynamics
restart, thereby maximizing the joint probability of a piece-wise continuous
latent dynamical representation. We propose usage of the marginal likelihood as
a score function for PELT, circumventing the need for model complexity-based
penalization. The LatSegODE outperforms baselines in reconstructive and
segmentation tasks including synthetic data sets of sine waves, Lotka Volterra
dynamics, and UCI Character Trajectories.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Transformed CNNs: recasting pre-trained convolutional layers with self-attention. (arXiv:2106.05795v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+dAscoli_S/0/1/0/all/0/1">St&#xe9;phane d&#x27;Ascoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Sagun_L/0/1/0/all/0/1">Levent Sagun</a>, <a href="http://arxiv.org/find/cs/1/au:+Biroli_G/0/1/0/all/0/1">Giulio Biroli</a>, <a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1">Ari Morcos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05795">
                                    <div class="article-summary-box-inner">
                                        <span>Vision Transformers (ViT) have recently emerged as a powerful alternative to
convolutional networks (CNNs). Although hybrid models attempt to bridge the gap
between these two architectures, the self-attention layers they rely on induce
a strong computational bottleneck, especially at large spatial resolutions. In
this work, we explore the idea of reducing the time spent training these layers
by initializing them as convolutional layers. This enables us to transition
smoothly from any pre-trained CNN to its functionally identical hybrid model,
called Transformed CNN (T-CNN). With only 50 epochs of fine-tuning, the
resulting T-CNNs demonstrate significant performance gains over the CNN (+2.2%
top-1 on ImageNet-1k for a ResNet50-RS) as well as substantially improved
robustness (+11% top-1 on ImageNet-C). We analyze the representations learnt by
the T-CNN, providing deeper insights into the fruitful interplay between
convolutions and self-attention. Finally, we experiment initializing the T-CNN
from a partially trained CNN, and find that it reaches better performance than
the corresponding hybrid model trained from scratch, while reducing training
time.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online Bayesian inference for multiple changepoints and risk assessment. (arXiv:2106.05834v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sorba_O/0/1/0/all/0/1">Olivier Sorba</a>, <a href="http://arxiv.org/find/cs/1/au:+Geissler_C/0/1/0/all/0/1">C Geissler</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05834">
                                    <div class="article-summary-box-inner">
                                        <span>The aim of the present study is to detect abrupt trend changes in the mean of
a multidimensional sequential signal. Directly inspired by papers of Fernhead
and Liu ([4] and [5]), this work describes the signal in a hierarchical manner
: the change dates of a time segmentation process trigger the renewal of a
piece-wise constant emission law. Bayesian posterior information on the change
dates and emission parameters is obtained. These estimations can be revised
online, i.e. as new data arrive. This paper proposes explicit formulations
corresponding to various emission laws, as well as a generalization to the case
where only partially observed data are available. Practical applications
include the returns of partially observed multi-asset investment strategies,
when only scant prior knowledge of the movers of the returns is at hand,
limited to some statistical assumptions. This situation is different from the
study of trend changes in the returns of individual assets, where fundamental
exogenous information (news, earnings announcements, controversies, etc.) can
be used.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dataset Condensation with Differentiable Siamese Augmentation. (arXiv:2102.08259v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bo Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1">Hakan Bilen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08259">
                                    <div class="article-summary-box-inner">
                                        <span>In many machine learning problems, large-scale datasets have become the
de-facto standard to train state-of-the-art deep networks at the price of heavy
computation load. In this paper, we focus on condensing large training sets
into significantly smaller synthetic sets which can be used to train deep
neural networks from scratch with minimum drop in performance. Inspired from
the recent training set synthesis methods, we propose Differentiable Siamese
Augmentation that enables effective use of data augmentation to synthesize more
informative synthetic images and thus achieves better performance when training
networks with augmentations. Experiments on multiple image classification
benchmarks demonstrate that the proposed method obtains substantial gains over
the state-of-the-art, 7% improvements on CIFAR10 and CIFAR100 datasets. We show
with only less than 1% data that our method achieves 99.6%, 94.9%, 88.5%, 71.5%
relative performance on MNIST, FashionMNIST, SVHN, CIFAR10 respectively. We
also explore the use of our method in continual learning and neural
architecture search, and show promising results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Off-Policy Evaluation via Adaptive Weighting with Data from Contextual Bandits. (arXiv:2106.02029v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Zhan_R/0/1/0/all/0/1">Ruohan Zhan</a>, <a href="http://arxiv.org/find/stat/1/au:+Hadad_V/0/1/0/all/0/1">Vitor Hadad</a>, <a href="http://arxiv.org/find/stat/1/au:+Hirshberg_D/0/1/0/all/0/1">David A. Hirshberg</a>, <a href="http://arxiv.org/find/stat/1/au:+Athey_S/0/1/0/all/0/1">Susan Athey</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02029">
                                    <div class="article-summary-box-inner">
                                        <span>It has become increasingly common for data to be collected adaptively, for
example using contextual bandits. Historical data of this type can be used to
evaluate other treatment assignment policies to guide future innovation or
experiments. However, policy evaluation is challenging if the target policy
differs from the one used to collect data, and popular estimators, including
doubly robust (DR) estimators, can be plagued by bias, excessive variance, or
both. In particular, when the pattern of treatment assignment in the collected
data looks little like the pattern generated by the policy to be evaluated, the
importance weights used in DR estimators explode, leading to excessive
variance.

In this paper, we improve the DR estimator by adaptively weighting
observations to control its variance. We show that a t-statistic based on our
improved estimator is asymptotically normal under certain conditions, allowing
us to form confidence intervals and test hypotheses. Using synthetic data and
public benchmarks, we provide empirical evidence for our estimator&#x27;s improved
accuracy and inferential properties relative to existing alternatives.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-resolution Outlier Pooling for Sorghum Classification. (arXiv:2106.05748v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1">Chao Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Dulay_J/0/1/0/all/0/1">Justin Dulay</a>, <a href="http://arxiv.org/find/cs/1/au:+Rolwes_G/0/1/0/all/0/1">Gregory Rolwes</a>, <a href="http://arxiv.org/find/cs/1/au:+Pauli_D/0/1/0/all/0/1">Duke Pauli</a>, <a href="http://arxiv.org/find/cs/1/au:+Shakoor_N/0/1/0/all/0/1">Nadia Shakoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1">Abby Stylianou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05748">
                                    <div class="article-summary-box-inner">
                                        <span>Automated high throughput plant phenotyping involves leveraging sensors, such
as RGB, thermal and hyperspectral cameras (among others), to make large scale
and rapid measurements of the physical properties of plants for the purpose of
better understanding the difference between crops and facilitating rapid plant
breeding programs. One of the most basic phenotyping tasks is to determine the
cultivar, or species, in a particular sensor product. This simple phenotype can
be used to detect errors in planting and to learn the most differentiating
features between cultivars. It is also a challenging visual recognition task,
as a large number of highly related crops are grown simultaneously, leading to
a classification problem with low inter-class variance. In this paper, we
introduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum
captured by a state-of-the-art gantry system, a multi-resolution network
architecture that learns both global and fine-grained features on the crops,
and a new global pooling strategy called Dynamic Outlier Pooling which
outperforms standard global pooling strategies on this task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Real-time simulation of parameter-dependent fluid flows through deep learning-based reduced order models. (arXiv:2106.05722v1 [physics.flu-dyn])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/physics/1/au:+Fresca_S/0/1/0/all/0/1">Stefania Fresca</a>, <a href="http://arxiv.org/find/physics/1/au:+Manzoni_A/0/1/0/all/0/1">Andrea Manzoni</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05722">
                                    <div class="article-summary-box-inner">
                                        <span>Simulating fluid flows in different virtual scenarios is of key importance in
engineering applications. However, high-fidelity, full-order models relying,
e.g., on the finite element method, are unaffordable whenever fluid flows must
be simulated in almost real-time. Reduced order models (ROMs) relying, e.g., on
proper orthogonal decomposition (POD) provide reliable approximations to
parameter-dependent fluid dynamics problems in rapid times. However, they might
require expensive hyper-reduction strategies for handling parameterized
nonlinear terms, and enriched reduced spaces (or Petrov-Galerkin projections)
if a mixed velocity-pressure formulation is considered, possibly hampering the
evaluation of reliable solutions in real-time. Dealing with fluid-structure
interactions entails even higher difficulties. The proposed deep learning
(DL)-based ROMs overcome all these limitations by learning in a non-intrusive
way both the nonlinear trial manifold and the reduced dynamics. To do so, they
rely on deep neural networks, after performing a former dimensionality
reduction through POD enhancing their training times substantially. The
resulting POD-DL-ROMs are shown to provide accurate results in almost real-time
for the flow around a cylinder benchmark, the fluid-structure interaction
between an elastic beam attached to a fixed, rigid block and a laminar
incompressible flow, and the blood flow in a cerebral aneurysm.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial Reinforcement Learning for Procedural Content Generation. (arXiv:2103.04847v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gisslen_L/0/1/0/all/0/1">Linus Gissl&#xe9;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Eakins_A/0/1/0/all/0/1">Andy Eakins</a>, <a href="http://arxiv.org/find/cs/1/au:+Gordillo_C/0/1/0/all/0/1">Camilo Gordillo</a>, <a href="http://arxiv.org/find/cs/1/au:+Bergdahl_J/0/1/0/all/0/1">Joakim Bergdahl</a>, <a href="http://arxiv.org/find/cs/1/au:+Tollmar_K/0/1/0/all/0/1">Konrad Tollmar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04847">
                                    <div class="article-summary-box-inner">
                                        <span>We present a new approach ARLPCG: Adversarial Reinforcement Learning for
Procedural Content Generation, which procedurally generates and tests
previously unseen environments with an auxiliary input as a control variable.
Training RL agents over novel environments is a notoriously difficult task. One
popular approach is to procedurally generate different environments to increase
the generalizability of the trained agents. ARLPCG instead deploys an
adversarial model with one PCG RL agent (called Generator) and one solving RL
agent (called Solver). The Generator receives a reward signal based on the
Solver&#x27;s performance, which encourages the environment design to be challenging
but not impossible. To further drive diversity and control of the environment
generation, we propose using auxiliary inputs for the Generator. The benefit is
two-fold: Firstly, the Solver achieves better generalization through the
Generator&#x27;s generated challenges. Secondly, the trained Generator can be used
as a creator of novel environments that, together with the Solver, can be shown
to be solvable. We create two types of 3D environments to validate our model,
representing two popular game genres: a third-person platformer and a racing
game. In these cases, we shows that ARLPCG has a significantly better solve
ratio, and that the auxiliary inputs renders the levels creation controllable
to a certain degree. For a video compilation of the results please visit
https://youtu.be/z7q2PtVsT0I.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">dFDA-VeD: A Dynamic Future Demand Aware Vehicle Dispatching System. (arXiv:2106.05737v1 [math.OC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Guo_Y/0/1/0/all/0/1">Yang Guo</a>, <a href="http://arxiv.org/find/math/1/au:+Anwar_T/0/1/0/all/0/1">Tarique Anwar</a>, <a href="http://arxiv.org/find/math/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a>, <a href="http://arxiv.org/find/math/1/au:+Wu_J/0/1/0/all/0/1">Jia Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05737">
                                    <div class="article-summary-box-inner">
                                        <span>With the rising demand of smart mobility, ride-hailing service is getting
popular in the urban regions. These services maintain a system for serving the
incoming trip requests by dispatching available vehicles to the pickup points.
As the process should be socially and economically profitable, the task of
vehicle dispatching is highly challenging, specially due to the time-varying
travel demands and traffic conditions. Due to the uneven distribution of travel
demands, many idle vehicles could be generated during the operation in
different subareas. Most of the existing works on vehicle dispatching system,
designed static relocation centers to relocate idle vehicles. However, as
traffic conditions and demand distribution dynamically change over time, the
static solution can not fit the evolving situations. In this paper, we propose
a dynamic future demand aware vehicle dispatching system. It can dynamically
search the relocation centers considering both travel demand and traffic
conditions. We evaluate the system on real-world dataset, and compare with the
existing state-of-the-art methods in our experiments in terms of several
standard evaluation metrics and operation time. Through our experiments, we
demonstrate that the proposed system significantly improves the serving ratio
and with a very small increase in operation cost.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data augmentation in Bayesian neural networks and the cold posterior effect. (arXiv:2106.05586v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Nabarro_S/0/1/0/all/0/1">Seth Nabarro</a>, <a href="http://arxiv.org/find/stat/1/au:+Ganev_S/0/1/0/all/0/1">Stoil Ganev</a>, <a href="http://arxiv.org/find/stat/1/au:+Garriga_Alonso_A/0/1/0/all/0/1">Adri&#xe0; Garriga-Alonso</a>, <a href="http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1">Vincent Fortuin</a>, <a href="http://arxiv.org/find/stat/1/au:+Wilk_M/0/1/0/all/0/1">Mark van der Wilk</a>, <a href="http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1">Laurence Aitchison</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05586">
                                    <div class="article-summary-box-inner">
                                        <span>Data augmentation is a highly effective approach for improving performance in
deep neural networks. The standard view is that it creates an enlarged dataset
by adding synthetic data, which raises a problem when combining it with
Bayesian inference: how much data are we really conditioning on? This question
is particularly relevant to recent observations linking data augmentation to
the cold posterior effect. We investigate various principled ways of finding a
log-likelihood for augmented datasets. Our approach prescribes augmenting the
same underlying image multiple times, both at test and train-time, and
averaging either the logits or the predictive probabilities. Empirically, we
observe the best performance with averaging probabilities. While there are
interactions with the cold posterior effect, neither averaging logits or
averaging probabilities eliminates it.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Identifiability of interaction kernels in mean-field equations of interacting particles. (arXiv:2106.05565v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Lang_Q/0/1/0/all/0/1">Quanjun Lang</a>, <a href="http://arxiv.org/find/stat/1/au:+Lu_F/0/1/0/all/0/1">Fei Lu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05565">
                                    <div class="article-summary-box-inner">
                                        <span>We study the identifiability of the interaction kernels in mean-field
equations for intreacting particle systems. The key is to identify function
spaces on which a probabilistic loss functional has a unique minimizer. We
prove that identifiability holds on any subspace of two reproducing kernel
Hilbert spaces (RKHS), whose reproducing kernels are intrinsic to the system
and are data-adaptive. Furthermore, identifiability holds on two ambient L2
spaces if and only if the integral operators associated with the reproducing
kernels are strictly positive. Thus, the inverse problem is ill-posed in
general. We also discuss the implications of identifiability in computational
practice.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Predictive Factors of Kinematics in Traumatic Brain Injury from Head Impacts Based on Statistical Interpretation. (arXiv:2102.05020v3 [physics.bio-ph] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/physics/1/au:+Zhan_X/0/1/0/all/0/1">Xianghao Zhan</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_Y/0/1/0/all/0/1">Yiheng Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1">Yuzhe Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Domel_A/0/1/0/all/0/1">August G. Domel</a>, <a href="http://arxiv.org/find/physics/1/au:+Alizadeh_H/0/1/0/all/0/1">Hossein Vahid Alizadeh</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhou_Z/0/1/0/all/0/1">Zhou Zhou</a>, <a href="http://arxiv.org/find/physics/1/au:+Cecchi_N/0/1/0/all/0/1">Nicholas J. Cecchi</a>, <a href="http://arxiv.org/find/physics/1/au:+Raymond_S/0/1/0/all/0/1">Samuel J. Raymond</a>, <a href="http://arxiv.org/find/physics/1/au:+Tiernan_S/0/1/0/all/0/1">Stephen Tiernan</a>, <a href="http://arxiv.org/find/physics/1/au:+Ruan_J/0/1/0/all/0/1">Jesse Ruan</a>, <a href="http://arxiv.org/find/physics/1/au:+Barbat_S/0/1/0/all/0/1">Saeed Barbat</a>, <a href="http://arxiv.org/find/physics/1/au:+Gevaert_O/0/1/0/all/0/1">Olivier Gevaert</a>, <a href="http://arxiv.org/find/physics/1/au:+Zeineh_M/0/1/0/all/0/1">Michael M. Zeineh</a>, <a href="http://arxiv.org/find/physics/1/au:+Grant_G/0/1/0/all/0/1">Gerald A. Grant</a>, <a href="http://arxiv.org/find/physics/1/au:+Camarillo_D/0/1/0/all/0/1">David B. Camarillo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05020">
                                    <div class="article-summary-box-inner">
                                        <span>Brain tissue deformation resulting from head impacts is primarily caused by
rotation and can lead to traumatic brain injury. To quantify brain injury risk
based on measurements of kinematics on the head, finite element (FE) models and
various brain injury criteria based on different factors of these kinematics
have been developed, but the contribution of different kinematic factors has
not been comprehensively analyzed across different types of head impacts in a
data-driven manner. To better design brain injury criteria, the predictive
power of rotational kinematics factors, which are different in 1) the
derivative order (angular velocity, angular acceleration, angular jerk), 2) the
direction and 3) the power (e.g., square-rooted, squared, cubic) of the angular
velocity, were analyzed based on different datasets including laboratory
impacts, American football, mixed martial arts (MMA), NHTSA automobile
crashworthiness tests and NASCAR crash events. Ordinary least squares
regressions were built from kinematics factors to the 95\% maximum principal
strain (MPS95), and we compared zero-order correlation coefficients, structure
coefficients, commonality analysis, and dominance analysis. The angular
acceleration, the magnitude, and the first power factors showed the highest
predictive power for the majority of impacts including laboratory impacts,
American football impacts, with few exceptions (angular velocity for MMA and
NASCAR impacts). The predictive power of rotational kinematics in three
directions (x: posterior-to-anterior, y: left-to-right, z:
superior-to-inferior) of kinematics varied with different sports and types of
head impacts.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Code Generation from Natural Language with Less Prior and More Monolingual Data. (arXiv:2101.00259v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Norouzi_S/0/1/0/all/0/1">Sajad Norouzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1">Keyi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yanshuai Cao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00259">
                                    <div class="article-summary-box-inner">
                                        <span>Training datasets for semantic parsing are typically small due to the higher
expertise required for annotation than most other NLP tasks. As a result,
models for this application usually need additional prior knowledge to be built
into the architecture or algorithm. The increased dependency on human experts
hinders automation and raises the development and maintenance costs in
practice. This work investigates whether a generic transformer-based seq2seq
model can achieve competitive performance with minimal code-generation-specific
inductive bias design. By exploiting a relatively sizeable monolingual corpus
of the target programming language, which is cheap to mine from the web, we
achieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa.
Both are SOTA to the best of our knowledge. This positive evidence highlights a
potentially easier path toward building accurate semantic parsers in practice.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Based Proximity Matrix Factorization for Node Embedding. (arXiv:2106.05476v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xingyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1">Kun Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sibo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zengfeng Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05476">
                                    <div class="article-summary-box-inner">
                                        <span>Node embedding learns a low-dimensional representation for each node in the
graph. Recent progress on node embedding shows that proximity matrix
factorization methods gain superb performance and scale to large graphs with
millions of nodes. Existing approaches first define a proximity matrix and then
learn the embeddings that fit the proximity by matrix factorization. Most
existing matrix factorization methods adopt the same proximity for different
tasks, while it is observed that different tasks and datasets may require
different proximity, limiting their representation power.

Motivated by this, we propose {\em Lemane}, a framework with trainable
proximity measures, which can be learned to best suit the datasets and tasks at
hand automatically. Our method is end-to-end, which incorporates differentiable
SVD in the pipeline so that the parameters can be trained via backpropagation.
However, this learning process is still expensive on large graphs. To improve
the scalability, we train proximity measures only on carefully subsampled
graphs, and then apply standard proximity matrix factorization on the original
graph using the learned proximity. Note that, computing the learned proximities
for each pair is still expensive for large graphs, and existing techniques for
computing proximities are not applicable to the learned proximities. Thus, we
present generalized push techniques to make our solution scalable to large
graphs with millions of nodes. Extensive experiments show that our proposed
solution outperforms existing solutions on both link prediction and node
classification tasks on almost all datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Architecture Search of SPD Manifold Networks. (arXiv:2010.14535v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1">Rhea Sanjay Sukthanker</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhiwu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Suryansh Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Endsjo_E/0/1/0/all/0/1">Erik Goron Endsjo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.14535">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose a new neural architecture search (NAS) problem of
Symmetric Positive Definite (SPD) manifold networks, aiming to automate the
design of SPD neural architectures. To address this problem, we first introduce
a geometrically rich and diverse SPD neural architecture search space for an
efficient SPD cell design. Further, we model our new NAS problem with a
one-shot training process of a single supernet. Based on the supernet modeling,
we exploit a differentiable NAS algorithm on our relaxed continuous search
space for SPD neural architecture search. Statistical evaluation of our method
on drone, action, and emotion recognition tasks mostly provides better results
than the state-of-the-art SPD networks and traditional NAS algorithms.
Empirical results show that our algorithm excels in discovering better
performing SPD network design and provides models that are more than three
times lighter than searched by the state-of-the-art NAS algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Relative Positional Encoding for Transformers with Linear Complexity. (arXiv:2105.08399v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liutkus_A/0/1/0/all/0/1">Antoine Liutkus</a>, <a href="http://arxiv.org/find/cs/1/au:+Cifka_O/0/1/0/all/0/1">Ond&#x159;ej C&#xed;fka</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shih-Lun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Simsekli_U/0/1/0/all/0/1">Umut &#x15e;im&#x15f;ekli</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi-Hsuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1">Ga&#xeb;l Richard</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08399">
                                    <div class="article-summary-box-inner">
                                        <span>Recent advances in Transformer models allow for unprecedented sequence
lengths, due to linear space and time complexity. In the meantime, relative
positional encoding (RPE) was proposed as beneficial for classical Transformers
and consists in exploiting lags instead of absolute positions for inference.
Still, RPE is not available for the recent linear-variants of the Transformer,
because it requires the explicit computation of the attention matrix, which is
precisely what is avoided by such methods. In this paper, we bridge this gap
and present Stochastic Positional Encoding as a way to generate PE that can be
used as a replacement to the classical additive (sinusoidal) PE and provably
behaves like RPE. The main theoretical contribution is to make a connection
between positional encoding and cross-covariance structures of correlated
Gaussian processes. We illustrate the performance of our approach on the
Long-Range Arena benchmark and on music generation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SignalNet: A Low Resolution Sinusoid Decomposition and Estimation Network. (arXiv:2106.05490v1 [eess.SP])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Dreifuerst_R/0/1/0/all/0/1">Ryan Dreifuerst</a>, <a href="http://arxiv.org/find/eess/1/au:+Heath_R/0/1/0/all/0/1">Robert W. Heath Jr</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05490">
                                    <div class="article-summary-box-inner">
                                        <span>The detection and estimation of sinusoids is a fundamental signal processing
task for many applications related to sensing and communications. While
algorithms have been proposed for this setting, quantization is a critical, but
often ignored modeling effect. In wireless communications, estimation with low
resolution data converters is relevant for reduced power consumption in
wideband receivers. Similarly, low resolution sampling in imaging and spectrum
sensing allows for efficient data collection. In this work, we propose
SignalNet, a neural network architecture that detects the number of sinusoids
and estimates their parameters from quantized in-phase and quadrature samples.
We incorporate signal reconstruction internally as domain knowledge within the
network to enhance learning and surpass traditional algorithms in mean squared
error and Chamfer error. We introduce a worst-case learning threshold for
comparing the results of our network relative to the underlying data
distributions. This threshold provides insight into why neural networks tend to
outperform traditional methods and into the learned relationships between the
input and output distributions. In simulation, we find that our algorithm is
always able to surpass the threshold for three-bit data but often cannot exceed
the threshold for one-bit data. We use the learning threshold to explain, in
the one-bit case, how our estimators learn to minimize the distributional loss,
rather than learn features from the data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Know Your Limits: Uncertainty Estimation with ReLU Classifiers Fails at Reliable OOD Detection. (arXiv:2012.05329v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1">Dennis Ulmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Cina_G/0/1/0/all/0/1">Giovanni Cin&#xe0;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05329">
                                    <div class="article-summary-box-inner">
                                        <span>A crucial requirement for reliable deployment of deep learning models for
safety-critical applications is the ability to identify out-of-distribution
(OOD) data points, samples which differ from the training data and on which a
model might underperform. Previous work has attempted to tackle this problem
using uncertainty estimation techniques. However, there is empirical evidence
that a large family of these techniques do not detect OOD reliably in
classification tasks.

This paper gives a theoretical explanation for said experimental findings and
illustrates it on synthetic data. We prove that such techniques are not able to
reliably identify OOD samples in a classification setting, since their level of
confidence is generalized to unseen areas of the feature space. This result
stems from the interplay between the representation of ReLU networks as
piece-wise affine transformations, the saturating nature of activation
functions like softmax, and the most widely-used uncertainty metrics.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Linear Classifiers that Encourage Constructive Adaptation. (arXiv:2011.00355v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yatong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jialu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00355">
                                    <div class="article-summary-box-inner">
                                        <span>Machine learning systems are often used in settings where individuals adapt
their features to obtain a desired outcome. In such settings, strategic
behavior leads to a sharp loss in model performance in deployment. In this
work, we aim to address this problem by learning classifiers that encourage
decision subjects to change their features in a way that leads to improvement
in both predicted \emph{and} true outcome. We frame the dynamics of prediction
and adaptation as a two-stage game, and characterize optimal strategies for the
model designer and its decision subjects. In benchmarks on simulated and
real-world datasets, we find that classifiers trained using our method maintain
the accuracy of existing approaches while inducing higher levels of improvement
and less manipulation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DNN-Based Topology Optimisation: Spatial Invariance and Neural Tangent Kernel. (arXiv:2106.05710v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Dupuis_B/0/1/0/all/0/1">Benjamin Dupuis</a>, <a href="http://arxiv.org/find/stat/1/au:+Jacot_A/0/1/0/all/0/1">Arthur Jacot</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05710">
                                    <div class="article-summary-box-inner">
                                        <span>We study the SIMP method with a density field generated by a fully-connected
neural network, taking the coordinates as inputs. In the large width limit, we
show that the use of DNNs leads to a filtering effect similar to traditional
filtering techniques for SIMP, with a filter described by the Neural Tangent
Kernel (NTK). This filter is however not invariant under translation, leading
to visual artifacts and non-optimal shapes. We propose two embeddings of the
input coordinates, which lead to (approximate) spatial invariance of the NTK
and of the filter. We empirically confirm our theoretical observations and
study how the filter size is affected by the architecture of the network. Our
solution can easily be applied to any other coordinates-based generation
method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data Fusion for Deep Learning on Transport Mode Detection: A Case Study. (arXiv:2106.05876v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Moreau_H/0/1/0/all/0/1">Hugues Moreau</a>, <a href="http://arxiv.org/find/cs/1/au:+Vassilev_A/0/1/0/all/0/1">Andr&#xe9;a Vassilev</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Liming Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05876">
                                    <div class="article-summary-box-inner">
                                        <span>In Transport Mode Detection, a great diversity of methodologies exist
according to the choice made on sensors, preprocessing, model used, etc. In
this domain, the comparisons between each option are not always complete.
Experiments on a public, real-life dataset are led here to evaluate carefully
each of the choices that were made, with a specific emphasis on data fusion
methods. Our most surprising finding is that none of the methods we implemented
from the literature is better than a simple late fusion. Two important
decisions are the choice of a sensor and the choice of a representation for the
data: we found that using 2D convolutions on spectrograms with a logarithmic
axis for the frequencies was better than 1-dimensional temporal
representations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fairness for Cooperative Multi-Agent Learning with Equivariant Policies. (arXiv:2106.05727v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Grupen_N/0/1/0/all/0/1">Niko A. Grupen</a>, <a href="http://arxiv.org/find/cs/1/au:+Selman_B/0/1/0/all/0/1">Bart Selman</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Daniel D. Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05727">
                                    <div class="article-summary-box-inner">
                                        <span>We study fairness through the lens of cooperative multi-agent learning. Our
work is motivated by empirical evidence that naive maximization of team reward
yields unfair outcomes for individual team members. To address fairness in
multi-agent contexts, we introduce team fairness, a group-based fairness
measure for multi-agent learning. We then incorporate team fairness into policy
optimization -- introducing Fairness through Equivariance (Fair-E), a novel
learning strategy that achieves provably fair reward distributions. We then
introduce Fairness through Equivariance Regularization (Fair-ER) as a
soft-constraint version of Fair-E and show that Fair-ER reaches higher levels
of utility than Fair-E and fairer outcomes than policies with no equivariance.
Finally, we investigate the fairness-utility trade-off in multi-agent settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep neural network loses attention to adversarial images. (arXiv:2106.05657v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1">Shashank Kotyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1">Danilo Vasconcellos Vargas</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05657">
                                    <div class="article-summary-box-inner">
                                        <span>Adversarial algorithms have shown to be effective against neural networks for
a variety of tasks. Some adversarial algorithms perturb all the pixels in the
image minimally for the image classification task in image classification. In
contrast, some algorithms perturb few pixels strongly. However, very little
information is available regarding why these adversarial samples so diverse
from each other exist. Recently, Vargas et al. showed that the existence of
these adversarial samples might be due to conflicting saliency within the
neural network. We test this hypothesis of conflicting saliency by analysing
the Saliency Maps (SM) and Gradient-weighted Class Activation Maps (Grad-CAM)
of original and few different types of adversarial samples. We also analyse how
different adversarial samples distort the attention of the neural network
compared to original samples. We show that in the case of Pixel Attack,
perturbed pixels either calls the network attention to themselves or divert the
attention from them. Simultaneously, the Projected Gradient Descent Attack
perturbs pixels so that intermediate layers inside the neural network lose
attention for the correct class. We also show that both attacks affect the
saliency map and activation maps differently. Thus, shedding light on why some
defences successful against some attacks remain vulnerable against other
attacks. We hope that this analysis will improve understanding of the existence
and the effect of adversarial samples and enable the community to develop more
robust neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GBHT: Gradient Boosting Histogram Transform for Density Estimation. (arXiv:2106.05738v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Cui_J/0/1/0/all/0/1">Jingyi Cui</a>, <a href="http://arxiv.org/find/stat/1/au:+Hang_H/0/1/0/all/0/1">Hanyuan Hang</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1">Yisen Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Lin_Z/0/1/0/all/0/1">Zhouchen Lin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05738">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose a density estimation algorithm called
\textit{Gradient Boosting Histogram Transform} (GBHT), where we adopt the
\textit{Negative Log Likelihood} as the loss function to make the boosting
procedure available for the unsupervised tasks. From a learning theory
viewpoint, we first prove fast convergence rates for GBHT with the smoothness
assumption that the underlying density function lies in the space
$C^{0,\alpha}$. Then when the target density function lies in spaces
$C^{1,\alpha}$, we present an upper bound for GBHT which is smaller than the
lower bound of its corresponding base learner, in the sense of convergence
rates. To the best of our knowledge, we make the first attempt to theoretically
explain why boosting can enhance the performance of its base learners for
density estimation problems. In experiments, we not only conduct performance
comparisons with the widely used KDE, but also apply GBHT to anomaly detection
to showcase a further application of GBHT.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A multi-objective perspective on jointly tuning hardware and hyperparameters. (arXiv:2106.05680v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Salinas_D/0/1/0/all/0/1">David Salinas</a>, <a href="http://arxiv.org/find/cs/1/au:+Perrone_V/0/1/0/all/0/1">Valerio Perrone</a>, <a href="http://arxiv.org/find/cs/1/au:+Cruchant_O/0/1/0/all/0/1">Olivier Cruchant</a>, <a href="http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1">Cedric Archambeau</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05680">
                                    <div class="article-summary-box-inner">
                                        <span>In addition to the best model architecture and hyperparameters, a full AutoML
solution requires selecting appropriate hardware automatically. This can be
framed as a multi-objective optimization problem: there is not a single best
hardware configuration but a set of optimal ones achieving different trade-offs
between cost and runtime. In practice, some choices may be overly costly or
take days to train. To lift this burden, we adopt a multi-objective approach
that selects and adapts the hardware configuration automatically alongside
neural architectures and their hyperparameters. Our method builds on Hyperband
and extends it in two ways. First, we replace the stopping rule used in
Hyperband by a non-dominated sorting rule to preemptively stop unpromising
configurations. Second, we leverage hyperparameter evaluations from related
tasks via transfer learning by building a probabilistic estimate of the Pareto
front that finds promising configurations more efficiently than random search.
We show in extensive NAS and HPO experiments that both ingredients bring
significant speed-ups and cost savings, with little to no impact on accuracy.
In three benchmarks where hardware is selected in addition to hyperparameters,
we obtain runtime and cost reductions of at least 5.8x and 8.8x, respectively.
Furthermore, when applying our multi-objective method to the tuning of
hyperparameters only, we obtain a 10\% improvement in runtime while maintaining
the same accuracy on two popular NAS benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fairness-Aware Node Representation Learning. (arXiv:2106.05391v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kose_O/0/1/0/all/0/1">&#xd6;yk&#xfc; Deniz K&#xf6;se</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yanning Shen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05391">
                                    <div class="article-summary-box-inner">
                                        <span>Node representation learning has demonstrated its effectiveness for various
applications on graphs. Particularly, recent developments in contrastive
learning have led to promising results in unsupervised node representation
learning for a number of tasks. Despite the success of graph contrastive
learning and consequent growing interest, fairness is largely under-explored in
the field. To this end, this study addresses fairness issues in graph
contrastive learning with fairness-aware graph augmentation designs, through
adaptive feature masking and edge deletion. In the study, different fairness
notions on graphs are introduced, which serve as guidelines for the proposed
graph augmentations. Furthermore, theoretical analysis is provided to
quantitatively prove that the proposed feature masking approach can reduce
intrinsic bias. Experimental results on real social networks are presented to
demonstrate that the proposed augmentations can enhance fairness in terms of
statistical parity and equal opportunity, while providing comparable
classification accuracy to state-of-the-art contrastive methods for node
classification.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deception in Social Learning: A Multi-Agent Reinforcement Learning Perspective. (arXiv:2106.05402v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chelarescu_P/0/1/0/all/0/1">Paul Chelarescu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05402">
                                    <div class="article-summary-box-inner">
                                        <span>Within the framework of Multi-Agent Reinforcement Learning, Social Learning
is a new class of algorithms that enables agents to reshape the reward function
of other agents with the goal of promoting cooperation and achieving higher
global rewards in mixed-motive games. However, this new modification allows
agents unprecedented access to each other&#x27;s learning process, which can
drastically increase the risk of manipulation when an agent does not realize it
is being deceived into adopting policies which are not actually in its own best
interest. This research review introduces the problem statement, defines key
concepts, critically evaluates existing evidence and addresses open problems
that should be addressed in future research.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Supervising the Transfer of Reasoning Patterns in VQA. (arXiv:2106.05597v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kervadec_C/0/1/0/all/0/1">Corentin Kervadec</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1">Christian Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Antipov_G/0/1/0/all/0/1">Grigory Antipov</a>, <a href="http://arxiv.org/find/cs/1/au:+Baccouche_M/0/1/0/all/0/1">Moez Baccouche</a>, <a href="http://arxiv.org/find/cs/1/au:+Nadri_M/0/1/0/all/0/1">Madiha Nadri</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05597">
                                    <div class="article-summary-box-inner">
                                        <span>Methods for Visual Question Anwering (VQA) are notorious for leveraging
dataset biases rather than performing reasoning, hindering generalization. It
has been recently shown that better reasoning patterns emerge in attention
layers of a state-of-the-art VQA model when they are trained on perfect
(oracle) visual inputs. This provides evidence that deep neural networks can
learn to reason when training conditions are favorable enough. However,
transferring this learned knowledge to deployable models is a challenge, as
much of it is lost during the transfer. We propose a method for knowledge
transfer based on a regularization term in our loss function, supervising the
sequence of required reasoning operations. We provide a theoretical analysis
based on PAC-learning, showing that such program prediction can lead to
decreased sample complexity under mild hypotheses. We also demonstrate the
effectiveness of this approach experimentally on the GQA dataset and show its
complementarity to BERT-like self-supervised pre-training.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Central Limit Theorem, Loss Aversion and Multi-Armed Bandits. (arXiv:2106.05472v1 [math.PR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Chen_Z/0/1/0/all/0/1">Zengjing Chen</a>, <a href="http://arxiv.org/find/math/1/au:+Epstein_L/0/1/0/all/0/1">Larry G. Epstein</a>, <a href="http://arxiv.org/find/math/1/au:+Zhang_G/0/1/0/all/0/1">Guodong Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05472">
                                    <div class="article-summary-box-inner">
                                        <span>This paper establishes a central limit theorem under the assumption that
conditional variances can vary in a largely unstructured history-dependent way
across experiments subject only to the restriction that they lie in a fixed
interval. Limits take a novel and tractable form, and are expressed in terms of
oscillating Brownian motion. A second contribution is application of this
result to a class of multi-armed bandit problems where the decision-maker is
loss averse.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Thompson Sampling with a Mixture Prior. (arXiv:2106.05608v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1">Joey Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1">Branislav Kveton</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1">Manzil Zaheer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1">Mohammad Ghavamzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Boutilier_C/0/1/0/all/0/1">Craig Boutilier</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05608">
                                    <div class="article-summary-box-inner">
                                        <span>We study Thompson sampling (TS) in online decision-making problems where the
uncertain environment is sampled from a mixture distribution. This is relevant
to multi-task settings, where a learning agent is faced with different classes
of problems. We incorporate this structure in a natural way by initializing TS
with a mixture prior -- dubbed MixTS -- and develop a novel, general technique
for analyzing the regret of TS with such priors. We apply this technique to
derive Bayes regret bounds for MixTS in both linear bandits and tabular Markov
decision processes (MDPs). Our regret bounds reflect the structure of the
problem and depend on the number of components and confidence width of each
component of the prior. Finally, we demonstrate the empirical effectiveness of
MixTS in both synthetic and real-world experiments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cocktail: Leveraging Ensemble Learning for Optimized Model Serving in Public Cloud. (arXiv:2106.05345v1 [cs.DC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gunasekaran_J/0/1/0/all/0/1">Jashwant Raj Gunasekaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_C/0/1/0/all/0/1">Cyan Subhra Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Thinakaran_P/0/1/0/all/0/1">Prashanth Thinakaran</a>, <a href="http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1">Mahmut Taylan Kandemir</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_C/0/1/0/all/0/1">Chita R. Das</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05345">
                                    <div class="article-summary-box-inner">
                                        <span>With a growing demand for adopting ML models for a varietyof application
services, it is vital that the frameworks servingthese models are capable of
delivering highly accurate predic-tions with minimal latency along with reduced
deploymentcosts in a public cloud environment. Despite high latency,prior works
in this domain are crucially limited by the accu-racy offered by individual
models. Intuitively, model ensem-bling can address the accuracy gap by
intelligently combiningdifferent models in parallel. However, selecting the
appro-priate models dynamically at runtime to meet the desiredaccuracy with low
latency at minimal deployment cost is anontrivial problem. Towards this, we
proposeCocktail, a costeffective ensembling-based model serving
framework.Cock-tailcomprises of two key components: (i) a dynamic
modelselection framework, which reduces the number of modelsin the ensemble,
while satisfying the accuracy and latencyrequirements; (ii) an adaptive
resource management (RM)framework that employs a distributed proactive
autoscalingpolicy combined with importance sampling, to efficiently allo-cate
resources for the models. The RM framework leveragestransient virtual machine
(VM) instances to reduce the de-ployment cost in a public cloud. A prototype
implementationofCocktailon the AWS EC2 platform and exhaustive evalua-tions
using a variety of workloads demonstrate thatCocktailcan reduce deployment cost
by 1.45x, while providing 2xreduction in latency and satisfying the target
accuracy for upto 96% of the requests, when compared to
state-of-the-artmodel-serving frameworks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CaloFlow: Fast and Accurate Generation of Calorimeter Showers with Normalizing Flows. (arXiv:2106.05285v1 [physics.ins-det])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/physics/1/au:+Krause_C/0/1/0/all/0/1">Claudius Krause</a>, <a href="http://arxiv.org/find/physics/1/au:+Shih_D/0/1/0/all/0/1">David Shih</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05285">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce CaloFlow, a fast detector simulation framework based on
normalizing flows. For the first time, we demonstrate that normalizing flows
can reproduce many-channel calorimeter showers with extremely high fidelity,
providing a fresh alternative to computationally expensive GEANT4 simulations,
as well as other state-of-the-art fast simulation frameworks based on GANs and
VAEs. Besides the usual histograms of physical features and images of
calorimeter showers, we introduce a new metric for judging the quality of
generative modeling: the performance of a classifier trained to differentiate
real from generated images. We show that GAN-generated images can be identified
by the classifier with 100% accuracy, while images generated from CaloFlow are
able to fool the classifier much of the time. More broadly, normalizing flows
offer several advantages compared to other state-of-the-art approaches (GANs
and VAEs), including: tractable likelihoods; stable and convergent training;
and principled model selection. Normalizing flows also provide a bijective
mapping between data and the latent space, which could have other applications
beyond simulation, for example, to detector unfolding.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Attentional meta-learners are polythetic classifiers. (arXiv:2106.05317v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Day_B/0/1/0/all/0/1">Ben Day</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinas_R/0/1/0/all/0/1">Ramon Vi&#xf1;as</a>, <a href="http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1">Nikola Simidjievski</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05317">
                                    <div class="article-summary-box-inner">
                                        <span>Polythetic classifications, based on shared patterns of features that need
neither be universal nor constant among members of a class, are common in the
natural world and greatly outnumber monothetic classifications over a set of
features. We show that threshold meta-learners require an embedding dimension
that is exponential in the number of features to emulate these functions. In
contrast, attentional classifiers are polythetic by default and able to solve
these problems with a linear embedding dimension. However, we find that in the
presence of task-irrelevant features, inherent to meta-learning problems,
attentional models are susceptible to misclassification. To address this
challenge, we further propose a self-attention feature-selection mechanism that
adaptively dilutes non-discriminative features. We demonstrate the
effectiveness of our approach in meta-learning Boolean functions, and synthetic
and real-world few-shot learning tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Stein Latent Optimization for GANs. (arXiv:2106.05319v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1">Uiwon Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Heeseung Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1">Dahuin Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1">Hyemi Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hyungyu Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sungroh Yoon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05319">
                                    <div class="article-summary-box-inner">
                                        <span>Generative adversarial networks (GANs) with clustered latent spaces can
perform conditional generation in a completely unsupervised manner. However,
the salient attributes of unlabeled data in the real-world are mostly
imbalanced. Existing unsupervised conditional GANs cannot properly cluster the
attributes in their latent spaces because they assume uniform distributions of
the attributes. To address this problem, we theoretically derive Stein latent
optimization that provides reparameterizable gradient estimations of the latent
distribution parameters assuming a Gaussian mixture prior in a continuous
latent space. Structurally, we introduce an encoder network and a novel
contrastive loss to help generated data from a single mixture component to
represent a single attribute. We confirm that the proposed method, named Stein
Latent Optimization for GANs (SLOGAN), successfully learns the balanced or
imbalanced attributes and performs unsupervised tasks such as unsupervised
conditional generation, unconditional generation, and cluster assignment even
in the absence of information of the attributes (e.g. the imbalance ratio).
Moreover, we demonstrate that the attributes to be learned can be manipulated
using a small amount of probe data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Tractable Density Estimation on Learned Manifolds with Conformal Embedding Flows. (arXiv:2106.05275v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Ross_B/0/1/0/all/0/1">Brendan Leigh Ross</a>, <a href="http://arxiv.org/find/stat/1/au:+Cresswell_J/0/1/0/all/0/1">Jesse C. Cresswell</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05275">
                                    <div class="article-summary-box-inner">
                                        <span>Normalizing flows are generative models that provide tractable density
estimation by transforming a simple base distribution into a complex target
distribution. However, this technique cannot directly model data supported on
an unknown low-dimensional manifold, a common occurrence in real-world domains
such as image data. Recent attempts to remedy this limitation have introduced
geometric complications that defeat a central benefit of normalizing flows:
exact density estimation. We recover this benefit with Conformal Embedding
Flows, a framework for designing flows that learn manifolds with tractable
densities. We argue that composing a standard flow with a trainable conformal
embedding is the most natural way to model manifold-supported data. To this
end, we present a series of conformal building blocks and apply them in
experiments with real-world and synthetic data to demonstrate that flows can
model manifold-supported distributions without sacrificing tractable
likelihoods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Physics-Informed Deep Learning Paradigm for Traffic State Estimation and Fundamental Diagram Discovery. (arXiv:2106.03142v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1">Rongye Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1">Zhaobin Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kuang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1">Xuan Di</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1">Qiang Du</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03142">
                                    <div class="article-summary-box-inner">
                                        <span>Traffic state estimation (TSE) bifurcates into two main categories,
model-driven and data-driven (e.g., machine learning, ML) approaches, while
each suffers from either deficient physics or small data. To mitigate these
limitations, recent studies introduced hybrid methods, such as physics-informed
deep learning (PIDL), which contains both model-driven and data-driven
components. This paper contributes an improved paradigm, called
physics-informed deep learning with a fundamental diagram learner (PIDL+FDL),
which integrates ML terms into the model-driven component to learn a functional
form of a fundamental diagram (FD), i.e., a mapping from traffic density to
flow or velocity. The proposed PIDL+FDL has the advantages of performing the
TSE learning, model parameter discovery, and FD discovery simultaneously. This
paper focuses on highway TSE with observed data from loop detectors, using
traffic density or velocity as traffic variables. We demonstrate the use of
PIDL+FDL to solve popular first-order and second-order traffic flow models and
reconstruct the FD relation as well as model parameters that are outside the FD
term. We then evaluate the PIDL+FDL-based TSE using the Next Generation
SIMulation (NGSIM) dataset. The experimental results show the superiority of
the PIDL+FDL in terms of improved estimation accuracy and data efficiency over
advanced baseline TSE methods, and additionally, the capacity to properly learn
the unknown underlying FD relation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces. (arXiv:2103.00349v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Eriksson_D/0/1/0/all/0/1">David Eriksson</a>, <a href="http://arxiv.org/find/cs/1/au:+Jankowiak_M/0/1/0/all/0/1">Martin Jankowiak</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00349">
                                    <div class="article-summary-box-inner">
                                        <span>Bayesian optimization (BO) is a powerful paradigm for efficient optimization
of black-box objective functions. High-dimensional BO presents a particular
challenge, in part because the curse of dimensionality makes it difficult to
define -- as well as do inference over -- a suitable class of surrogate models.
We argue that Gaussian process surrogate models defined on sparse axis-aligned
subspaces offer an attractive compromise between flexibility and parsimony. We
demonstrate that our approach, which relies on Hamiltonian Monte Carlo for
inference, can rapidly identify sparse subspaces relevant to modeling the
unknown objective function, enabling sample-efficient high-dimensional BO. In
an extensive suite of experiments comparing to existing methods for
high-dimensional BO we demonstrate that our algorithm, Sparse Axis-Aligned
Subspace BO (SAASBO), achieves excellent performance on several synthetic and
real-world problems without the need to set problem-specific hyperparameters.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Gi and Pal Scores: Deep Neural Network Generalization Statistics. (arXiv:2104.03469v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Schiff_Y/0/1/0/all/0/1">Yair Schiff</a>, <a href="http://arxiv.org/find/cs/1/au:+Quanz_B/0/1/0/all/0/1">Brian Quanz</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1">Payel Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03469">
                                    <div class="article-summary-box-inner">
                                        <span>The field of Deep Learning is rich with empirical evidence of human-like
performance on a variety of regression, classification, and control tasks.
However, despite these successes, the field lacks strong theoretical error
bounds and consistent measures of network generalization and learned
invariances. In this work, we introduce two new measures, the Gi-score and
Pal-score, that capture a deep neural network&#x27;s generalization capabilities.
Inspired by the Gini coefficient and Palma ratio, measures of income
inequality, our statistics are robust measures of a network&#x27;s invariance to
perturbations that accurately predict generalization gaps, i.e., the difference
between accuracy on training and test sets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Local Explanations via Necessity and Sufficiency: Unifying Theory and Practice. (arXiv:2103.14651v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Watson_D/0/1/0/all/0/1">David Watson</a>, <a href="http://arxiv.org/find/cs/1/au:+Gultchin_L/0/1/0/all/0/1">Limor Gultchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Taly_A/0/1/0/all/0/1">Ankur Taly</a>, <a href="http://arxiv.org/find/cs/1/au:+Floridi_L/0/1/0/all/0/1">Luciano Floridi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14651">
                                    <div class="article-summary-box-inner">
                                        <span>Necessity and sufficiency are the building blocks of all successful
explanations. Yet despite their importance, these notions have been
conceptually underdeveloped and inconsistently applied in explainable
artificial intelligence (XAI), a fast-growing research area that is so far
lacking in firm theoretical foundations. Building on work in logic,
probability, and causality, we establish the central role of necessity and
sufficiency in XAI, unifying seemingly disparate methods in a single formal
framework. We provide a sound and complete algorithm for computing explanatory
factors with respect to a given context, and demonstrate its flexibility and
competitive performance against state of the art alternatives on various tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Latent Space Arc Therapy Optimization. (arXiv:2106.05846v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bice_N/0/1/0/all/0/1">Noah Bice</a>, <a href="http://arxiv.org/find/cs/1/au:+Fakhreddine_M/0/1/0/all/0/1">Mohamad Fakhreddine</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruiqi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Dan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabat_C/0/1/0/all/0/1">Christopher Kabat</a>, <a href="http://arxiv.org/find/cs/1/au:+Myers_P/0/1/0/all/0/1">Pamela Myers</a>, <a href="http://arxiv.org/find/cs/1/au:+Papanikolaou_N/0/1/0/all/0/1">Niko Papanikolaou</a>, <a href="http://arxiv.org/find/cs/1/au:+Kirby_N/0/1/0/all/0/1">Neil Kirby</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05846">
                                    <div class="article-summary-box-inner">
                                        <span>Volumetric modulated arc therapy planning is a challenging problem in
high-dimensional, non-convex optimization. Traditionally, heuristics such as
fluence-map-optimization-informed segment initialization use locally optimal
solutions to begin the search of the full arc therapy plan space from a
reasonable starting point. These routines facilitate arc therapy optimization
such that clinically satisfactory radiation treatment plans can be created in
about 10 minutes. However, current optimization algorithms favor solutions near
their initialization point and are slower than necessary due to plan
overparameterization. In this work, arc therapy overparameterization is
addressed by reducing the effective dimension of treatment plans with
unsupervised deep learning. An optimization engine is then built based on
low-dimensional arc representations which facilitates faster planning times.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">UneVEn: Universal Value Exploration for Multi-Agent Reinforcement Learning. (arXiv:2010.02974v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1">Tarun Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1">Anuj Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Bei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohmer_W/0/1/0/all/0/1">Wendelin B&#xf6;hmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1">Shimon Whiteson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.02974">
                                    <div class="article-summary-box-inner">
                                        <span>VDN and QMIX are two popular value-based algorithms for cooperative MARL that
learn a centralized action value function as a monotonic mixing of per-agent
utilities. While this enables easy decentralization of the learned policy, the
restricted joint action value function can prevent them from solving tasks that
require significant coordination between agents at a given timestep. We show
that this problem can be overcome by improving the joint exploration of all
agents during training. Specifically, we propose a novel MARL approach called
Universal Value Exploration (UneVEn) that learns a set of related tasks
simultaneously with a linear decomposition of universal successor features.
With the policies of already solved related tasks, the joint exploration
process of all agents can be improved to help them achieve better coordination.
Empirical results on a set of exploration games, challenging cooperative
predator-prey tasks requiring significant coordination among agents, and
StarCraft II micromanagement benchmarks show that UneVEn can solve tasks where
other state-of-the-art MARL methods fail.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On Information Plane Analyses of Neural Network Classifiers -- A Review. (arXiv:2003.09671v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Geiger_B/0/1/0/all/0/1">Bernhard C. Geiger</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.09671">
                                    <div class="article-summary-box-inner">
                                        <span>We review the current literature concerned with information plane analyses of
neural network classifiers. While the underlying information bottleneck theory
and the claim that information-theoretic compression is causally linked to
generalization are plausible, empirical evidence was found to be both
supporting and conflicting. We review this evidence together with a detailed
analysis of how the respective information quantities were estimated. Our
survey suggests that compression visualized in information planes is not
necessarily information-theoretic, but is rather often compatible with
geometric compression of the latent representations. This insight gives the
information plane a renewed justification.

Aside from this, we shed light on the problem of estimating mutual
information in deterministic neural networks and its consequences.
Specifically, we argue that even in feed-forward neural networks the data
processing inequality need not hold for estimates of mutual information.
Similarly, while a fitting phase, in which the mutual information between the
latent representation and the target increases, is necessary (but not
sufficient) for good classification performance, depending on the specifics of
mutual information estimation such a fitting phase need not be visible in the
information plane.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Two-stage Textual Knowledge Distillation for End-to-End Spoken Language Understanding. (arXiv:2010.13105v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seongbin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1">Gyuwan Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1">Seongjin Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Sangmin Lee</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13105">
                                    <div class="article-summary-box-inner">
                                        <span>End-to-end approaches open a new way for more accurate and efficient spoken
language understanding (SLU) systems by alleviating the drawbacks of
traditional pipeline systems. Previous works exploit textual information for an
SLU model via pre-training with automatic speech recognition or fine-tuning
with knowledge distillation. To utilize textual information more effectively,
this work proposes a two-stage textual knowledge distillation method that
matches utterance-level representations and predicted logits of two modalities
during pre-training and fine-tuning, sequentially. We use vq-wav2vec BERT as a
speech encoder because it captures general and rich features. Furthermore, we
improve the performance, especially in a low-resource scenario, with data
augmentation methods by randomly masking spans of discrete audio tokens and
contextualized hidden representations. Consequently, we push the
state-of-the-art on the Fluent Speech Commands, achieving 99.7% test accuracy
in the full dataset setting and 99.5% in the 10% subset setting. Throughout the
ablation studies, we empirically verify that all used methods are crucial to
the final performance, providing the best practice for spoken language
understanding. Code is available at https://github.com/clovaai/textual-kd-slu.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On Under-exploration in Bandits with Mean Bounds from Confounded Data. (arXiv:2002.08405v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sharma_N/0/1/0/all/0/1">Nihal Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1">Soumya Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1">Karthikeyan Shanmugam</a>, <a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1">Sanjay Shakkottai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.08405">
                                    <div class="article-summary-box-inner">
                                        <span>We study a variant of the multi-armed bandit problem where side information
in the form of bounds on the mean of each arm is provided. We develop the novel
non-optimistic Global Under-Explore (GLUE) algorithm which uses the provided
mean bounds (across all the arms) to infer pseudo-variances for each arm, which
in turn decide the rate of exploration for the arms. We analyze the regret of
GLUE and prove regret upper bounds that are never worse than that of the
standard UCB algorithm. Furthermore, we show that GLUE improves upon regret
guarantees that exists in literature for structured bandit problems (both
theoretically and empirically). Finally, we study the practical setting of
learning adaptive interventions using prior data that has been confounded by
unrecorded variables that affect rewards. We show that mean bounds can be
inferred naturally from such logs and can thus be used to improve the learning
process. We validate our findings through semi-synthetic experiments on data
derived from real data sets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adaptive machine learning for protein engineering. (arXiv:2106.05466v1 [q-bio.QM])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/q-bio/1/au:+Hie_B/0/1/0/all/0/1">Brian L. Hie</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yang_K/0/1/0/all/0/1">Kevin K. Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05466">
                                    <div class="article-summary-box-inner">
                                        <span>Machine-learning models that learn from data to predict how protein sequence
encodes function are emerging as a useful protein engineering tool. However,
when using these models to suggest new protein designs, one must deal with the
vast combinatorial complexity of protein sequences. Here, we review how to use
a sequence-to-function machine-learning surrogate model to select sequences for
experimental measurement. First, we discuss how to select sequences through a
single round of machine-learning optimization. Then, we discuss sequential
optimization, where the goal is to discover optimized sequences and improve the
model across multiple rounds of training, optimization, and experimental
measurement.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning. (arXiv:2106.05956v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1">Ekdeep Singh Lubana</a>, <a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1">Robert P. Dick</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1">Hidenori Tanaka</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05956">
                                    <div class="article-summary-box-inner">
                                        <span>Inspired by BatchNorm, there has been an explosion of normalization layers in
deep learning. Recent works have identified a multitude of beneficial
properties in BatchNorm to explain its success. However, given the pursuit of
alternative normalization techniques, these properties need to be generalized
so that any given layer&#x27;s success/failure can be accurately predicted. In this
work, we take a first step towards this goal by extending known properties of
BatchNorm in randomly initialized deep neural networks (DNNs) to nine recently
proposed normalization layers. Our primary findings follow: (i) Similar to
BatchNorm, activations-based normalization layers can avoid exploding
activations in ResNets; (ii) Use of GroupNorm ensures rank of activations is at
least $\Omega(\sqrt{\frac{\text{width}}{\text{Group Size}}})$, thus explaining
why LayerNorm witnesses slow optimization speed; (iii) Small group sizes result
in large gradient norm in earlier layers, hence justifying training instability
issues in Instance Normalization and illustrating a speed-stability tradeoff in
GroupNorm. Overall, our analysis reveals several general mechanisms that
explain the success of normalization techniques in deep learning, providing us
with a compass to systematically explore the vast design space of DNN
normalization layers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Verifiable and Compositional Reinforcement Learning Systems. (arXiv:2106.05864v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Neary_C/0/1/0/all/0/1">Cyrus Neary</a>, <a href="http://arxiv.org/find/cs/1/au:+Verginis_C/0/1/0/all/0/1">Christos Verginis</a>, <a href="http://arxiv.org/find/cs/1/au:+Cubuktepe_M/0/1/0/all/0/1">Murat Cubuktepe</a>, <a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1">Ufuk Topcu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05864">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a novel framework for verifiable and compositional reinforcement
learning (RL) in which a collection of RL sub-systems, each of which learns to
accomplish a separate sub-task, are composed to achieve an overall task. The
framework consists of a high-level model, represented as a parametric Markov
decision process (pMDP) which is used to plan and to analyze compositions of
sub-systems, and of the collection of low-level sub-systems themselves. By
defining interfaces between the sub-systems, the framework enables automatic
decompositons of task specifications, e.g., reach a target set of states with a
probability of at least 0.95, into individual sub-task specifications, i.e.
achieve the sub-system&#x27;s exit conditions with at least some minimum
probability, given that its entry conditions are met. This in turn allows for
the independent training and testing of the sub-systems; if they each learn a
policy satisfying the appropriate sub-task specification, then their
composition is guaranteed to satisfy the overall task specification.
Conversely, if the sub-task specifications cannot all be satisfied by the
learned policies, we present a method, formulated as the problem of finding an
optimal set of parameters in the pMDP, to automatically update the sub-task
specifications to account for the observed shortcomings. The result is an
iterative procedure for defining sub-task specifications, and for training the
sub-systems to meet them. As an additional benefit, this procedure allows for
particularly challenging or important components of an overall task to be
determined automatically, and focused on, during training. Experimental results
demonstrate the presented framework&#x27;s novel capabilities.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Programming Puzzles. (arXiv:2106.05784v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1">Tal Schuster</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1">Ashwin Kalyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Polozov_O/0/1/0/all/0/1">Oleksandr Polozov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1">Adam Tauman Kalai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05784">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a new type of programming challenge called programming puzzles,
as an objective and comprehensive evaluation of program synthesis, and release
an open-source dataset of Python Programming Puzzles (P3). Each puzzle is
defined by a short Python program $f$, and the goal is to find an input $x$
which makes $f$ output &quot;True&quot;. The puzzles are objective in that each one is
specified entirely by the source code of its verifier $f$, so evaluating $f(x)$
is all that is needed to test a candidate solution $x$. They do not require an
answer key or input/output examples, nor do they depend on natural language
understanding. The dataset is comprehensive in that it spans problems of a
range of difficulties and domains, ranging from trivial string manipulation
problems that are immediately obvious to human programmers (but not necessarily
to AI), to classic programming puzzles (e.g., Towers of Hanoi), to
interview/competitive-programming problems (e.g., dynamic programming), to
longstanding open problems in algorithms and mathematics (e.g., factoring). The
objective nature of P3 readily supports self-supervised bootstrapping. We
develop baseline enumerative program synthesis and GPT-3 solvers that are
capable of solving easy puzzles -- even without access to any reference
solutions -- by learning from their own past solutions. Based on a small user
study, we find puzzle difficulty to correlate between human programmers and the
baseline AI solvers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fair Disaster Containment via Graph-Cut Problems. (arXiv:2106.05424v1 [cs.DS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Babay_A/0/1/0/all/0/1">Amy Babay</a>, <a href="http://arxiv.org/find/cs/1/au:+Dinitz_M/0/1/0/all/0/1">Michael Dinitz</a>, <a href="http://arxiv.org/find/cs/1/au:+Sambaturu_P/0/1/0/all/0/1">Prathyush Sambaturu</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1">Aravind Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsepenekas_L/0/1/0/all/0/1">Leonidas Tsepenekas</a>, <a href="http://arxiv.org/find/cs/1/au:+Vullikanti_A/0/1/0/all/0/1">Anil Vullikanti</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05424">
                                    <div class="article-summary-box-inner">
                                        <span>Graph cut problems form a fundamental problem type in combinatorial
optimization, and are a central object of study in both theory and practice. In
addition, the study of fairness in Algorithmic Design and Machine Learning has
recently received significant attention, with many different notions proposed
and analyzed in a variety of contexts. In this paper we initiate the study of
fairness for graph cut problems by giving the first fair definitions for them,
and subsequently we demonstrate appropriate algorithmic techniques that yield a
rigorous theoretical analysis. Specifically, we incorporate two different
definitions of fairness, namely demographic and probabilistic individual
fairness, in a particular cut problem modeling disaster containment scenarios.
Our results include a variety of approximation algorithms with provable
theoretical guarantees.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Zero Time Waste: Recycling Predictions in Early Exit Neural Networks. (arXiv:2106.05409v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wolczyk_M/0/1/0/all/0/1">Maciej Wo&#x142;czyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Wojcik_B/0/1/0/all/0/1">Bartosz W&#xf3;jcik</a>, <a href="http://arxiv.org/find/cs/1/au:+Balazy_K/0/1/0/all/0/1">Klaudia Ba&#x142;azy</a>, <a href="http://arxiv.org/find/cs/1/au:+Podolak_I/0/1/0/all/0/1">Igor Podolak</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1">Jacek Tabor</a>, <a href="http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1">Marek &#x15a;mieja</a>, <a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1">Tomasz Trzci&#x144;ski</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05409">
                                    <div class="article-summary-box-inner">
                                        <span>The problem of reducing processing time of large deep learning models is a
fundamental challenge in many real-world applications. Early exit methods
strive towards this goal by attaching additional Internal Classifiers (ICs) to
intermediate layers of a neural network. ICs can quickly return predictions for
easy examples and, as a result, reduce the average inference time of the whole
model. However, if a particular IC does not decide to return an answer early,
its predictions are discarded, with its computations effectively being wasted.
To solve this issue, we introduce Zero Time Waste (ZTW), a novel approach in
which each IC reuses predictions returned by its predecessors by (1) adding
direct connections between ICs and (2) combining previous outputs in an
ensemble-like manner. We conduct extensive experiments across various datasets
and architectures to demonstrate that ZTW achieves a significantly better
accuracy vs. inference time trade-off than other recently proposed early exit
methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings. (arXiv:2106.05609v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fey_M/0/1/0/all/0/1">Matthias Fey</a>, <a href="http://arxiv.org/find/cs/1/au:+Lenssen_J/0/1/0/all/0/1">Jan E. Lenssen</a>, <a href="http://arxiv.org/find/cs/1/au:+Weichert_F/0/1/0/all/0/1">Frank Weichert</a>, <a href="http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1">Jure Leskovec</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05609">
                                    <div class="article-summary-box-inner">
                                        <span>We present GNNAutoScale (GAS), a framework for scaling arbitrary
message-passing GNNs to large graphs. GAS prunes entire sub-trees of the
computation graph by utilizing historical embeddings from prior training
iterations, leading to constant GPU memory consumption in respect to input node
size without dropping any data. While existing solutions weaken the expressive
power of message passing due to sub-sampling of edges or non-trainable
propagations, our approach is provably able to maintain the expressive power of
the original GNN. We achieve this by providing approximation error bounds of
historical embeddings and show how to tighten them in practice. Empirically, we
show that the practical realization of our framework, PyGAS, an easy-to-use
extension for PyTorch Geometric, is both fast and memory-efficient, learns
expressive node representations, closely resembles the performance of their
non-scaling counterparts, and reaches state-of-the-art performance on
large-scale graphs.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Validation of Simulation-Based Testing: Bypassing Domain Shift with Label-to-Image Synthesis. (arXiv:2106.05549v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rosenzweig_J/0/1/0/all/0/1">Julia Rosenzweig</a>, <a href="http://arxiv.org/find/cs/1/au:+Brito_E/0/1/0/all/0/1">Eduardo Brito</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobialka_H/0/1/0/all/0/1">Hans-Ulrich Kobialka</a>, <a href="http://arxiv.org/find/cs/1/au:+Akila_M/0/1/0/all/0/1">Maram Akila</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidt_N/0/1/0/all/0/1">Nico M. Schmidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlicht_P/0/1/0/all/0/1">Peter Schlicht</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Jan David Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Huger_F/0/1/0/all/0/1">Fabian H&#xfc;ger</a>, <a href="http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1">Matthias Rottmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Houben_S/0/1/0/all/0/1">Sebastian Houben</a>, <a href="http://arxiv.org/find/cs/1/au:+Wirtz_T/0/1/0/all/0/1">Tim Wirtz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05549">
                                    <div class="article-summary-box-inner">
                                        <span>Many machine learning applications can benefit from simulated data for
systematic validation - in particular if real-life data is difficult to obtain
or annotate. However, since simulations are prone to domain shift w.r.t.
real-life data, it is crucial to verify the transferability of the obtained
results. We propose a novel framework consisting of a generative label-to-image
synthesis model together with different transferability measures to inspect to
what extent we can transfer testing results of semantic segmentation models
from synthetic data to equivalent real-life data. With slight modifications,
our approach is extendable to, e.g., general multi-class classification tasks.
Grounded on the transferability analysis, our approach additionally allows for
extensive testing by incorporating controlled simulations. We validate our
approach empirically on a semantic segmentation task on driving scenes.
Transferability is tested using correlation analysis of IoU and a learned
discriminator. Although the latter can distinguish between real-life and
synthetic tests, in the former we observe surprisingly strong correlations of
0.7 for both cars and pedestrians.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Separation Results between Fixed-Kernel and Feature-Learning Probability Metrics. (arXiv:2106.05739v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Domingo_Enrich_C/0/1/0/all/0/1">Carles Domingo-Enrich</a>, <a href="http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1">Youssef Mroueh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05739">
                                    <div class="article-summary-box-inner">
                                        <span>Several works in implicit and explicit generative modeling empirically
observed that feature-learning discriminators outperform fixed-kernel
discriminators in terms of the sample quality of the models. We provide
separation results between probability metrics with fixed-kernel and
feature-learning discriminators using the function classes $\mathcal{F}_2$ and
$\mathcal{F}_1$ respectively, which were developed to study overparametrized
two-layer neural networks. In particular, we construct pairs of distributions
over hyper-spheres that can not be discriminated by fixed kernel
$(\mathcal{F}_2)$ integral probability metric (IPM) and Stein discrepancy (SD)
in high dimensions, but that can be discriminated by their feature learning
($\mathcal{F}_1$) counterparts. To further study the separation we provide
links between the $\mathcal{F}_1$ and $\mathcal{F}_2$ IPMs with sliced
Wasserstein distances. Our work suggests that fixed-kernel discriminators
perform worse than their feature learning counterparts because their
corresponding metrics are weaker.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Accurate Learning of Graph Representations with Graph Multiset Pooling. (arXiv:2102.11533v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1">Jinheon Baek</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Minki Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11533">
                                    <div class="article-summary-box-inner">
                                        <span>Graph neural networks have been widely used on modeling graph data, achieving
impressive results on node classification and link prediction tasks. Yet,
obtaining an accurate representation for a graph further requires a pooling
function that maps a set of node representations into a compact form. A simple
sum or average over all node representations considers all node features
equally without consideration of their task relevance, and any structural
dependencies among them. Recently proposed hierarchical graph pooling methods,
on the other hand, may yield the same representation for two different graphs
that are distinguished by the Weisfeiler-Lehman test, as they suboptimally
preserve information from the node features. To tackle these limitations of
existing graph pooling methods, we first formulate the graph pooling problem as
a multiset encoding problem with auxiliary information about the graph
structure, and propose a Graph Multiset Transformer (GMT) which is a multi-head
attention based global pooling layer that captures the interaction between
nodes according to their structural dependencies. We show that GMT satisfies
both injectiveness and permutation invariance, such that it is at most as
powerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods
can be easily extended to the previous node clustering approaches for
hierarchical graph pooling. Our experimental results show that GMT
significantly outperforms state-of-the-art graph pooling methods on graph
classification benchmarks with high memory and time efficiency, and obtains
even larger performance gain on graph reconstruction and generation tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GraphITE: Estimating Individual Effects of Graph-structured Treatments. (arXiv:2009.14061v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Harada_S/0/1/0/all/0/1">Shonosuke Harada</a>, <a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1">Hisashi Kashima</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14061">
                                    <div class="article-summary-box-inner">
                                        <span>Outcome estimation of treatments for target individuals is an important
foundation for decision making based on causal relations. Most existing outcome
estimation methods deal with binary or multiple-choice treatments; however, in
some applications, the number of treatments can be significantly large, while
the treatments themselves have rich information. In this study, we considered
one important instance of such cases: the outcome estimation problem of
graph-structured treatments such as drugs. Owing to the large number of
possible treatments, the counterfactual nature of observational data that
appears in conventional treatment effect estimation becomes more of a concern
for this problem. Our proposed method, GraphITE (pronounced &quot;graphite&quot;) learns
the representations of graph-structured treatments using graph neural networks
while mitigating observation biases using Hilbert-Schmidt Independence
Criterion regularization, which increases the independence of the
representations of the targets and treatments. Experiments on two real-world
datasets show that GraphITE outperforms baselines, especially in cases with a
large number of treatments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced Semi-Supervised Learning. (arXiv:2106.05682v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1">Youngtaek Oh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Dong-Jin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1">In So Kweon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05682">
                                    <div class="article-summary-box-inner">
                                        <span>The capability of the traditional semi-supervised learning (SSL) methods is
far from real-world application since they do not consider (1) class imbalance
and (2) class distribution mismatch between labeled and unlabeled data. This
paper addresses such a relatively under-explored problem, imbalanced
semi-supervised learning, where heavily biased pseudo-labels can harm the model
performance. Interestingly, we find that the semantic pseudo-labels from a
similarity-based classifier in feature space and the traditional pseudo-labels
from the linear classifier show the complementary property. To this end, we
propose a general pseudo-labeling framework to address the bias motivated by
this observation. The key idea is to class-adaptively blend the semantic
pseudo-label to the linear one, depending on the current pseudo-label
distribution. Thereby, the increased semantic pseudo-label component suppresses
the false positives in the majority classes and vice versa. We term the novel
pseudo-labeling framework for imbalanced SSL as Distribution-Aware
Semantics-Oriented (DASO) Pseudo-label. Extensive evaluation on CIFAR10/100-LT
and STL10-LT shows that DASO consistently outperforms both recently proposed
re-balancing methods for label and pseudo-label. Moreover, we demonstrate that
typical SSL algorithms can effectively benefit from unlabeled data with DASO,
especially when (1) class imbalance and (2) class distribution mismatch exist
and even on recent real-world Semi-Aves benchmark.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline. (arXiv:2106.05304v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1">Ankit Goyal</a>, <a href="http://arxiv.org/find/cs/1/au:+Law_H/0/1/0/all/0/1">Hei Law</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bowei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Newell_A/0/1/0/all/0/1">Alejandro Newell</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1">Jia Deng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05304">
                                    <div class="article-summary-box-inner">
                                        <span>Processing point cloud data is an important component of many real-world
systems. As such, a wide variety of point-based approaches have been proposed,
reporting steady benchmark improvements over time. We study the key ingredients
of this progress and uncover two critical results. First, we find that
auxiliary factors like different evaluation schemes, data augmentation
strategies, and loss functions, which are independent of the model
architecture, make a large difference in performance. The differences are large
enough that they obscure the effect of architecture. When these factors are
controlled for, PointNet++, a relatively older network, performs competitively
with recent methods. Second, a very simple projection-based method, which we
refer to as SimpleView, performs surprisingly well. It achieves on par or
better results than sophisticated state-of-the-art methods on ModelNet40 while
being half the size of PointNet++. It also outperforms state-of-the-art methods
on ScanObjectNN, a real-world point cloud benchmark, and demonstrates better
cross-dataset generalization. Code is available at
https://github.com/princeton-vl/SimpleView.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Explaining Time Series Predictions with Dynamic Masks. (arXiv:2106.05303v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Crabbe_J/0/1/0/all/0/1">Jonathan Crabb&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1">Mihaela van der Schaar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05303">
                                    <div class="article-summary-box-inner">
                                        <span>How can we explain the predictions of a machine learning model? When the data
is structured as a multivariate time series, this question induces additional
difficulties such as the necessity for the explanation to embody the time
dependency and the large number of inputs. To address these challenges, we
propose dynamic masks (Dynamask). This method produces instance-wise importance
scores for each feature at each time step by fitting a perturbation mask to the
input sequence. In order to incorporate the time dependency of the data,
Dynamask studies the effects of dynamic perturbation operators. In order to
tackle the large number of inputs, we propose a scheme to make the feature
selection parsimonious (to select no more feature than necessary) and legible
(a notion that we detail by making a parallel with information theory). With
synthetic and real-world data, we demonstrate that the dynamic underpinning of
Dynamask, together with its parsimony, offer a neat improvement in the
identification of feature importance over time. The modularity of Dynamask
makes it ideal as a plug-in to increase the transparency of a wide range of
machine learning models in areas such as medicine and finance, where time
series are abundant.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DiffCloth: Differentiable Cloth Simulation with Dry Frictional Contact. (arXiv:2106.05306v1 [cs.GR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yifei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1">Tao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jie Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1">Wojciech Matusik</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05306">
                                    <div class="article-summary-box-inner">
                                        <span>Cloth simulation has wide applications including computer animation, garment
design, and robot-assisted dressing. In this work, we present a differentiable
cloth simulator whose additional gradient information facilitates cloth-related
applications. Our differentiable simulator extends the state-of-the-art cloth
simulator based on Projective Dynamics and with dry frictional contact governed
by the Signorini-Coulomb law. We derive gradients with contact in this forward
simulation framework and speed up the computation with Jacobi iteration
inspired by previous differentiable simulation work. To our best knowledge, we
present the first differentiable cloth simulator with the Coulomb law of
friction. We demonstrate the efficacy of our simulator in various applications,
including system identification, manipulation, inverse design, and a
real-to-sim task. Many of our applications have not been demonstrated in
previous differentiable cloth simulators. The gradient information from our
simulator enables efficient gradient-based task solvers from which we observe a
substantial speedup over standard gradient-free methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.MM"">cs.MM updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training. (arXiv:2106.05630v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1">Mingliang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ju_Z/0/1/0/all/0/1">Zeqian Ju</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1">Tao Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tie-Yan Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05630">
                                    <div class="article-summary-box-inner">
                                        <span>Symbolic music understanding, which refers to the understanding of music from
the symbolic data (e.g., MIDI format, but not audio), covers many music
applications such as genre classification, emotion classification, and music
pieces matching. While good music representations are beneficial for these
applications, the lack of training data hinders representation learning.
Inspired by the success of pre-training models in natural language processing,
in this paper, we develop MusicBERT, a large-scale pre-trained model for music
understanding. To this end, we construct a large-scale symbolic music corpus
that contains more than 1 million music songs. Since symbolic music contains
more structural (e.g., bar, position) and diverse information (e.g., tempo,
instrument, and pitch), simply adopting the pre-training techniques from NLP to
symbolic music only brings marginal gains. Therefore, we design several
mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to
enhance pre-training with symbolic music data. Experiments demonstrate the
advantages of MusicBERT on four music understanding tasks, including melody
completion, accompaniment suggestion, genre classification, and style
classification. Ablation studies also verify the effectiveness of our designs
of OctupleMIDI encoding and bar-level masking strategy in MusicBERT.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data Clustering-Driven Neural Network for Intra Prediction. (arXiv:2106.05481v1 [cs.MM])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Man_H/0/1/0/all/0/1">Hengyu Man</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xiaopeng Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1">Ruiqin Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Debin Zhao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05481">
                                    <div class="article-summary-box-inner">
                                        <span>As a crucial part of video compression, intra prediction utilizes local
information of images to eliminate the redundancy in spatial domain. In both
H.265/HEVC and H.266/VVC, multiple directional prediction modes are employed to
find the texture trend of each small block and then the prediction is made
based on reference samples in the selected direction. Recently, the intra
prediction schemes based on neural networks have achieved great success. In
these methods, the networks are trained and applied to intra prediction in
addition to the directional prediction modes. In this paper, we propose a novel
data clustering-driven neural network (dubbed DCDNN) for intra prediction,
which can learn deep features of the clustered data. In DCDNN, each network can
be split into two networks by adding or subtracting Gaussian random noise. Then
a data clustering-driven training is applied to train all the derived networks
recursively. In each iteration, the entire training dataset is partitioned
according to the recovery qualities of the derived networks. For the
experiment, DCDNN is implemented into HEVC reference software HM-16.9. The
experimental results demonstrate that DCDNN can reach an average of 4.2%
Bjontegaard distortion rate (BDrate) improvement (up to 7.0%) over HEVC with
all intra configuration. Compared with existing fully connected networkbased
intra prediction methods, the bitrate saving performance is further improved.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">New Design Paradigm of Distortion Cost Function for Efficient JPEG Steganography. (arXiv:1908.01947v3 [cs.MM] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1">Wenkang Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1">Jiangqun Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xianglei Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiwu Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.01947">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, with the introduction of JPEG phase-aware steganalysis features,
e.g., GFR, the design of JPEG steganographic distortion cost function turns to
maintain not only the statistical undetectability in DCT domain but also in
spatial domain. To tackle this issue, this paper presents a novel paradigm for
the design of JPEG steganographic distortion cost function, which calculates
the distortion cost via a generalized Distortion Cost Domain Transformation
(DCDT) function. The proposed function comprises the decompressed pixel block
embedding changes and their corresponding embedding distortion costs for unit
change, where the pixel embedding distortion costs are represented in a more
general exponential model, aiming to flexibly allocate the embedding data. In
this way, the JPEG steganography could be formulated as the optimization
problem of minimizing the overall distortion cost in its decompressed spatial
domain, which is equivalent to maximizing its statistical undetectability
against JPEG phase-aware steganalysis features. Experimental results show that
the proposed DCDT equipped with HiLL (a spatial steganographic distortion cost
function) is superior to other state-of-the-art JPEG steganographic schemes,
e.g., UERD, J-UNIWARD, and GUED in resisting the detection of JPEG phase-aware
feature-based steganalyzers GFR and SCA-GFR, and rivals BET-HiLL with one order
of magnitude lower computational complexity, along with the possibility of
being further improved by considering the mutually dependent embedding
interactions. In addition, the proposed DCDT is also verified to be effective
for different image databases and quality factors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
        </ul>
    </section>
    <section class="daily-content">
        <h2 class="daily-heading"><time datatime="2021-06-10">2021-06-10</time></h2>
        <ul class="sources card">
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.CL"">cs.CL updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations. (arXiv:2106.01978v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1">Dou Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Lingwei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Huai_X/0/1/0/all/0/1">Xiaoyong Huai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01978">
                                    <div class="article-summary-box-inner">
                                        <span>Emotion Recognition in Conversations (ERC) has gained increasing attention
for developing empathetic machines. Recently, many approaches have been devoted
to perceiving conversational context by deep learning models. However, these
approaches are insufficient in understanding the context due to lacking the
ability to extract and integrate emotional clues. In this work, we propose
novel Contextual Reasoning Networks (DialogueCRN) to fully understand the
conversational context from a cognitive perspective. Inspired by the Cognitive
Theory of Emotion, we design multi-turn reasoning modules to extract and
integrate emotional clues. The reasoning module iteratively performs an
intuitive retrieving process and a conscious reasoning process, which imitates
human unique cognitive thinking. Extensive experiments on three public
benchmark datasets demonstrate the effectiveness and superiority of the
proposed model.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MICE: A Crosslinguistic Emotion Corpus in Malay, Indonesian, Chinese and English. (arXiv:2106.04831v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chin_N/0/1/0/all/0/1">Ng Bee Chin</a>, <a href="http://arxiv.org/find/cs/1/au:+Susanto_Y/0/1/0/all/0/1">Yosephine Susanto</a>, <a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1">Erik Cambria</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04831">
                                    <div class="article-summary-box-inner">
                                        <span>MICE is a corpus of emotion words in four languages which is currently
working progress. There are two sections to this study, Part I: Emotion word
corpus and Part II: Emotion word survey. In Part 1, the method of how the
emotion data is culled for each of the four languages will be described and
very preliminary data will be presented. In total, we identified 3,750 emotion
expressions in Malay, 6,657 in Indonesian, 3,347 in Mandarin Chinese and 8,683
in English. We are currently evaluating and double checking the corpus and
doing further analysis on the distribution of these emotion expressions. Part
II Emotion word survey involved an online language survey which collected
information on how speakers assigned the emotion words into basic emotion
categories, the rating for valence and intensity as well as biographical
information of all the respondents.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DeepTileBars: Visualizing Term Distribution for Neural Information Retrieval. (arXiv:1811.00606v3 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhiwen Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Grace Hui Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1811.00606">
                                    <div class="article-summary-box-inner">
                                        <span>Most neural Information Retrieval (Neu-IR) models derive query-to-document
ranking scores based on term-level matching. Inspired by TileBars, a classical
term distribution visualization method, in this paper, we propose a novel
Neu-IR model that handles query-to-document matching at the subtopic and higher
levels. Our system first splits the documents into topical segments,
&quot;visualizes&quot; the matchings between the query and the segments, and then feeds
an interaction matrix into a Neu-IR model, DeepTileBars, to obtain the final
ranking scores. DeepTileBars models the relevance signals occurring at
different granularities in a document&#x27;s topic hierarchy. It better captures the
discourse structure of a document and thus the matching patterns. Although its
design and implementation are light-weight, DeepTileBars outperforms other
state-of-the-art Neu-IR models on benchmark datasets including the Text
REtrieval Conference (TREC) 2010-2012 Web Tracks and LETOR 4.0.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT. (arXiv:2106.05141v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mohiuddin_T/0/1/0/all/0/1">Tasnim Mohiuddin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1">M Saiful Bari</a>, <a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1">Shafiq Joty</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05141">
                                    <div class="article-summary-box-inner">
                                        <span>The success of Neural Machine Translation (NMT) largely depends on the
availability of large bitext training corpora. Due to the lack of such large
corpora in low-resource language pairs, NMT systems often exhibit poor
performance. Extra relevant monolingual data often helps, but acquiring it
could be quite expensive, especially for low-resource languages. Moreover,
domain mismatch between bitext (train/test) and monolingual data might degrade
the performance. To alleviate such issues, we propose AUGVIC, a novel data
augmentation framework for low-resource NMT which exploits the vicinal samples
of the given bitext without using any extra monolingual data explicitly. It can
diversify the in-domain bitext data with finer level control. Through extensive
experiments on four low-resource language pairs comprising data from different
domains, we have shown that our method is comparable to the traditional
back-translation that uses extra in-domain monolingual data. When we combine
the synthetic parallel data generated from AUGVIC with the ones from the extra
monolingual data, we achieve further improvements. We show that AUGVIC helps to
attenuate the discrepancies between relevant and distant-domain monolingual
data in traditional back-translation. To understand the contributions of
different components of AUGVIC, we perform an in-depth framework analysis.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-hop Graph Convolutional Network with High-order Chebyshev Approximation for Text Reasoning. (arXiv:2106.05221v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Shuoran Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qingcai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Baotian Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lisai Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05221">
                                    <div class="article-summary-box-inner">
                                        <span>Graph convolutional network (GCN) has become popular in various natural
language processing (NLP) tasks with its superiority in long-term and
non-consecutive word interactions. However, existing single-hop graph reasoning
in GCN may miss some important non-consecutive dependencies. In this study, we
define the spectral graph convolutional network with the high-order dynamic
Chebyshev approximation (HDGCN), which augments the multi-hop graph reasoning
by fusing messages aggregated from direct and long-term dependencies into one
convolutional layer. To alleviate the over-smoothing in high-order Chebyshev
approximation, a multi-vote-based cross-attention (MVCAttn) with linear
computation complexity is also proposed. The empirical results on four
transductive and inductive NLP tasks and the ablation study verify the efficacy
of the proposed model. Our source code is available at
https://github.com/MathIsAll/HDGCN-pytorch.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages. (arXiv:2012.05628v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Vries_W/0/1/0/all/0/1">Wietse de Vries</a>, <a href="http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1">Malvina Nissim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05628">
                                    <div class="article-summary-box-inner">
                                        <span>Large generative language models have been very successful for English, but
other languages lag behind, in part due to data and computational limitations.
We propose a method that may overcome these problems by adapting existing
pre-trained models to new languages. Specifically, we describe the adaptation
of English GPT-2 to Italian and Dutch by retraining lexical embeddings without
tuning the Transformer layers. As a result, we obtain lexical embeddings for
Italian and Dutch that are aligned with the original English lexical
embeddings. Additionally, we scale up complexity by transforming relearned
lexical embeddings of GPT-2 small to the GPT-2 medium embedding space. This
method minimises the amount of training and prevents losing information during
adaptation that was learned by GPT-2. English GPT-2 models with relearned
lexical embeddings can generate realistic sentences in Italian and Dutch.
Though on average these sentences are still identifiable as artificial by
humans, they are assessed on par with sentences generated by a GPT-2 model
fully trained from scratch.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Crosslingual Embeddings are Essential in UNMT for Distant Languages: An English to IndoAryan Case Study. (arXiv:2106.04995v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1">Tamali Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1">Rudra Murthy V</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1">Pushpak Bhattacharyya</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04995">
                                    <div class="article-summary-box-inner">
                                        <span>Recent advances in Unsupervised Neural Machine Translation (UNMT) have
minimized the gap between supervised and unsupervised machine translation
performance for closely related language pairs. However, the situation is very
different for distant language pairs. Lack of lexical overlap and low syntactic
similarities such as between English and Indo-Aryan languages leads to poor
translation quality in existing UNMT systems. In this paper, we show that
initializing the embedding layer of UNMT models with cross-lingual embeddings
shows significant improvements in BLEU score over existing approaches with
embeddings randomly initialized. Further, static embeddings (freezing the
embedding layer weights) lead to better gains compared to updating the
embedding layer weights during training (non-static). We experimented using
Masked Sequence to Sequence (MASS) and Denoising Autoencoder (DAE) UNMT
approaches for three distant language pairs. The proposed cross-lingual
embedding initialization yields BLEU score improvement of as much as ten times
over the baseline for English-Hindi, English-Bengali, and English-Gujarati. Our
analysis shows the importance of cross-lingual embedding, comparisons between
approaches, and the scope of improvements in these systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data. (arXiv:2106.05006v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hazoom_M/0/1/0/all/0/1">Moshe Hazoom</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_V/0/1/0/all/0/1">Vibhor Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1">Ben Bogin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05006">
                                    <div class="article-summary-box-inner">
                                        <span>Most available semantic parsing datasets, comprising of pairs of natural
utterances and logical forms, were collected solely for the purpose of training
and evaluation of natural language understanding systems. As a result, they do
not contain any of the richness and variety of natural-occurring utterances,
where humans ask about data they need or are curious about. In this work, we
release SEDE, a dataset with 12,023 pairs of utterances and SQL queries
collected from real usage on the Stack Exchange website. We show that these
pairs contain a variety of real-world challenges which were rarely reflected so
far in any other semantic parsing dataset, propose an evaluation metric based
on comparison of partial query clauses that is more suitable for real-world
queries, and conduct experiments with strong baselines, showing a large gap
between the performance on SEDE compared to other common datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">RealTranS: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer. (arXiv:2106.04833v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xingshan Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liangyou Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04833">
                                    <div class="article-summary-box-inner">
                                        <span>End-to-end simultaneous speech translation (SST), which directly translates
speech in one language into text in another language in real-time, is useful in
many scenarios but has not been fully investigated. In this work, we propose
RealTranS, an end-to-end model for SST. To bridge the modality gap between
speech and text, RealTranS gradually downsamples the input speech with
interleaved convolution and unidirectional Transformer layers for acoustic
modeling, and then maps speech features into text space with a
weighted-shrinking operation and a semantic encoder. Besides, to improve the
model performance in simultaneous scenarios, we propose a blank penalty to
enhance the shrinking quality and a Wait-K-Stride-N strategy to allow local
reranking during decoding. Experiments on public and widely-used datasets show
that RealTranS with the Wait-K-Stride-N strategy outperforms prior end-to-end
models as well as cascaded models in diverse latency settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fast Text-Only Domain Adaptation of RNN-Transducer Prediction Network. (arXiv:2104.11127v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pylkkonen_J/0/1/0/all/0/1">Janne Pylkk&#xf6;nen</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Ukkonen_A/0/1/0/all/0/1">Antti Ukkonen</a> (1 and 2), <a href="http://arxiv.org/find/cs/1/au:+Kilpikoski_J/0/1/0/all/0/1">Juho Kilpikoski</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Tamminen_S/0/1/0/all/0/1">Samu Tamminen</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Heikinheimo_H/0/1/0/all/0/1">Hannes Heikinheimo</a> (1) ((1) Speechly, (2) Department of Computer Science, University of Helsinki, Finland)
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11127">
                                    <div class="article-summary-box-inner">
                                        <span>Adaption of end-to-end speech recognition systems to new tasks is known to be
challenging. A number of solutions have been proposed which apply external
language models with various fusion methods, possibly with a combination of
two-pass decoding. Also TTS systems have been used to generate adaptation data
for the end-to-end models. In this paper we show that RNN-transducer models can
be effectively adapted to new domains using only small amounts of textual data.
By taking advantage of model&#x27;s inherent structure, where the prediction network
is interpreted as a language model, we can apply fast adaptation to the model.
Adapting the model avoids the need for complicated decoding time fusions and
external language models. Using appropriate regularization, the prediction
network can be adapted to new domains while still retaining good generalization
capabilities. We show with multiple ASR evaluation tasks how this method can
provide relative gains of 10-45% in target task WER. We also share insights how
RNN-transducer prediction network performs as a language model.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hierarchical Interaction Networks with Rethinking Mechanism for Document-level Sentiment Analysis. (arXiv:2007.08445v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1">Lingwei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1">Dou Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xuehai Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaodan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jizhong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Songlin Hu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.08445">
                                    <div class="article-summary-box-inner">
                                        <span>Document-level Sentiment Analysis (DSA) is more challenging due to vague
semantic links and complicate sentiment information. Recent works have been
devoted to leveraging text summarization and have achieved promising results.
However, these summarization-based methods did not take full advantage of the
summary including ignoring the inherent interactions between the summary and
document. As a result, they limited the representation to express major points
in the document, which is highly indicative of the key sentiment. In this
paper, we study how to effectively generate a discriminative representation
with explicit subject patterns and sentiment contexts for DSA. A Hierarchical
Interaction Networks (HIN) is proposed to explore bidirectional interactions
between the summary and document at multiple granularities and learn
subject-oriented document representations for sentiment classification.
Furthermore, we design a Sentiment-based Rethinking mechanism (SR) by refining
the HIN with sentiment label information to learn a more sentiment-aware
document representation. We extensively evaluate our proposed models on three
public datasets. The experimental results consistently demonstrate the
effectiveness of our proposed models and show that HIN-SR outperforms various
state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Intent Detection and Slot Filling for Vietnamese. (arXiv:2104.02021v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1">Mai Hoang Dao</a>, <a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1">Thinh Hung Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Dat Quoc Nguyen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02021">
                                    <div class="article-summary-box-inner">
                                        <span>Intent detection and slot filling are important tasks in spoken and natural
language understanding. However, Vietnamese is a low-resource language in these
research topics. In this paper, we present the first public intent detection
and slot filling dataset for Vietnamese. In addition, we also propose a joint
model for intent detection and slot filling, that extends the recent
state-of-the-art JointBERT+CRF model with an intent-slot attention layer to
explicitly incorporate intent context information into slot filling via &quot;soft&quot;
intent label embedding. Experimental results on our Vietnamese dataset show
that our proposed model significantly outperforms JointBERT+CRF. We publicly
release our dataset and the implementation of our model at:
https://github.com/VinAIResearch/JointIDSF</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1">Danilo Dessi</a>, <a href="http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1">Rim Helaoui</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Vivek Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1">Diego Reforgiato Recupero</a>, <a href="http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1">Daniele Riboni</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09632">
                                    <div class="article-summary-box-inner">
                                        <span>Today, we are seeing an ever-increasing number of clinical notes that contain
clinical results, images, and textual descriptions of patient&#x27;s health state.
All these data can be analyzed and employed to cater novel services that can
help people and domain experts with their common healthcare tasks. However,
many technologies such as Deep Learning and tools like Word Embeddings have
started to be investigated only recently, and many challenges remain open when
it comes to healthcare domain applications. To address these challenges, we
propose the use of Deep Learning and Word Embeddings for identifying sixteen
morbidity types within textual descriptions of clinical records. For this
purpose, we have used a Deep Learning model based on Bidirectional Long-Short
Term Memory (LSTM) layers which can exploit state-of-the-art vector
representations of data such as Word Embeddings. We have employed pre-trained
Word Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained
on the target domain. Furthermore, we have compared the performances of the
deep learning approaches against the traditional tf-idf using Support Vector
Machine and Multilayer perceptron (our baselines). From the obtained results it
seems that the latter outperforms the combination of Deep Learning approaches
using any word embeddings. Our preliminary results indicate that there are
specific features that make the dataset biased in favour of traditional machine
learning approaches.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DefSent: Sentence Embeddings using Definition Sentences. (arXiv:2105.04339v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tsukagoshi_H/0/1/0/all/0/1">Hayato Tsukagoshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sasano_R/0/1/0/all/0/1">Ryohei Sasano</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1">Koichi Takeda</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04339">
                                    <div class="article-summary-box-inner">
                                        <span>Sentence embedding methods using natural language inference (NLI) datasets
have been successfully applied to various tasks. However, these methods are
only available for limited languages due to relying heavily on the large NLI
datasets. In this paper, we propose DefSent, a sentence embedding method that
uses definition sentences from a word dictionary, which performs comparably on
unsupervised semantics textual similarity (STS) tasks and slightly better on
SentEval tasks than conventional methods. Since dictionaries are available for
many languages, DefSent is more broadly applicable than methods using NLI
datasets without constructing additional datasets. We demonstrate that DefSent
performs comparably on unsupervised semantics textual similarity (STS) tasks
and slightly better on SentEval tasks to the methods using large NLI datasets.
Our code is publicly available at https://github.com/hpprc/defsent .</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Vocabulary Learning via Optimal Transport for Machine Translation. (arXiv:2012.15671v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jingjing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1">Chun Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zaixiang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15671">
                                    <div class="article-summary-box-inner">
                                        <span>The choice of token vocabulary affects the performance of machine
translation. This paper aims to figure out what is a good vocabulary and
whether one can find the optimal vocabulary without trial training. To answer
these questions, we first provide an alternative understanding of the role of
vocabulary from the perspective of information theory. Motivated by this, we
formulate the quest of vocabularization -- finding the best token dictionary
with a proper size -- as an optimal transport (OT) problem.We We propose VOLT,
a simple and efficient solution without trial training. Empirical results show
that VOLT outperforms widely-used vocabularies in diverse scenarios, including
WMT-14 English-German and TED&#x27;s 52 translation directions. For example, VOLT
achieves 70% vocabulary size reduction and 0.5 BLEU gain on English-German
translation. Also, compared to BPE-search, VOLT reduces the search time from
384 GPU hours to 30 GPU hours on English-German translation. Codes are
available at https://github.com/Jingjing-NLP/VOLT .</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Case Studies on using Natural Language Processing Techniques in Customer Relationship Management Software. (arXiv:2106.05160v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1">&#x15e;&#xfc;kr&#xfc; Ozan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05160">
                                    <div class="article-summary-box-inner">
                                        <span>How can a text corpus stored in a customer relationship management (CRM)
database be used for data mining and segmentation? In order to answer this
question we inherited the state of the art methods commonly used in natural
language processing (NLP) literature, such as word embeddings, and deep
learning literature, such as recurrent neural networks (RNN). We used the text
notes from a CRM system which are taken by customer representatives of an
internet ads consultancy agency between years 2009 and 2020. We trained word
embeddings by using the corresponding text corpus and showed that these word
embeddings can not only be used directly for data mining but also be used in
RNN architectures, which are deep learning frameworks built with long short
term memory (LSTM) units, for more comprehensive segmentation objectives. The
results prove that structured text data in a CRM can be used to mine out very
valuable information and any CRM can be equipped with useful NLP features once
the problem definitions are properly built and the solution methods are
conveniently implemented.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks. (arXiv:2011.02511v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1">Julia Kreutzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1">Stefan Riezler</a>, <a href="http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1">Carolin Lawrence</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.02511">
                                    <div class="article-summary-box-inner">
                                        <span>Large volumes of interaction logs can be collected from NLP systems that are
deployed in the real world. How can this wealth of information be leveraged?
Using such interaction logs in an offline reinforcement learning (RL) setting
is a promising approach. However, due to the nature of NLP tasks and the
constraints of production systems, a series of challenges arise. We present a
concise overview of these challenges and discuss possible solutions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-stage Sequence-to-Sequence Training. (arXiv:2103.16809v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1">Berrak Sisman</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haizhou Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16809">
                                    <div class="article-summary-box-inner">
                                        <span>Emotional voice conversion (EVC) aims to change the emotional state of an
utterance while preserving the linguistic content and speaker identity. In this
paper, we propose a novel 2-stage training strategy for sequence-to-sequence
emotional voice conversion with a limited amount of emotional speech data. We
note that the proposed EVC framework leverages text-to-speech (TTS) as they
share a common goal that is to generate high-quality expressive voice. In stage
1, we perform style initialization with a multi-speaker TTS corpus, to
disentangle speaking style and linguistic content. In stage 2, we perform
emotion training with a limited amount of emotional speech data, to learn how
to disentangle emotional style and linguistic information from the speech. The
proposed framework can perform both spectrum and prosody conversion and
achieves significant improvement over the state-of-the-art baselines in both
objective and subjective evaluation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Transient Chaos in BERT. (arXiv:2106.03181v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1">Katsuma Inoue</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohara_S/0/1/0/all/0/1">Soh Ohara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1">Yasuo Kuniyoshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakajima_K/0/1/0/all/0/1">Kohei Nakajima</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03181">
                                    <div class="article-summary-box-inner">
                                        <span>Language is an outcome of our complex and dynamic human-interactions and the
technique of natural language processing (NLP) is hence built on human
linguistic activities. Bidirectional Encoder Representations from Transformers
(BERT) has recently gained its popularity by establishing the state-of-the-art
scores in several NLP benchmarks. A Lite BERT (ALBERT) is literally
characterized as a lightweight version of BERT, in which the number of BERT
parameters is reduced by repeatedly applying the same neural network called
Transformer&#x27;s encoder layer. By pre-training the parameters with a massive
amount of natural language data, ALBERT can convert input sentences into
versatile high-dimensional vectors potentially capable of solving multiple NLP
tasks. In that sense, ALBERT can be regarded as a well-designed
high-dimensional dynamical system whose operator is the Transformer&#x27;s encoder,
and essential structures of human language are thus expected to be encapsulated
in its dynamics. In this study, we investigated the embedded properties of
ALBERT to reveal how NLP tasks are effectively solved by exploiting its
dynamics. We thereby aimed to explore the nature of human language from the
dynamical expressions of the NLP model. Our short-term analysis clarified that
the pre-trained model stably yields trajectories with higher dimensionality,
which would enhance the expressive capacity required for NLP tasks. Also, our
long-term analysis revealed that ALBERT intrinsically shows transient chaos, a
typical nonlinear phenomenon showing chaotic dynamics only in its transient,
and the pre-trained ALBERT model tends to produce the chaotic trajectory for a
significantly longer time period compared to a randomly-initialized one. Our
results imply that local chaoticity would contribute to improving NLP
performance, uncovering a novel aspect in the role of chaotic dynamics in human
language behaviors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">High-Quality Diversification for Task-Oriented Dialogue Systems. (arXiv:2106.00891v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhiwen Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_H/0/1/0/all/0/1">Hrishikesh Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Grace Hui Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00891">
                                    <div class="article-summary-box-inner">
                                        <span>Many task-oriented dialogue systems use deep reinforcement learning (DRL) to
learn policies that respond to the user appropriately and complete the tasks
successfully. Training DRL agents with diverse dialogue trajectories prepare
them well for rare user requests and unseen situations. One effective
diversification method is to let the agent interact with a diverse set of
learned user models. However, trajectories created by these artificial user
models may contain generation errors, which can quickly propagate into the
agent&#x27;s policy. It is thus important to control the quality of the
diversification and resist the noise. In this paper, we propose a novel
dialogue diversification method for task-oriented dialogue systems trained in
simulators. Our method, Intermittent Short Extension Ensemble (I-SEE),
constrains the intensity to interact with an ensemble of diverse user models
and effectively controls the quality of the diversification. Evaluations on the
Multiwoz dataset show that I-SEE successfully boosts the performance of several
state-of-the-art DRL dialogue agents.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1">Nihal Potdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1">Anderson R. Avila</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1">Chao Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yiran Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiao Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10042">
                                    <div class="article-summary-box-inner">
                                        <span>End-to-end spoken language understanding (SLU) has recently attracted
increasing interest. Compared to the conventional tandem-based approach that
combines speech recognition and language understanding as separate modules, the
new approach extracts users&#x27; intentions directly from the speech signals,
resulting in joint optimization and low latency. Such an approach, however, is
typically designed to process one intention at a time, which leads users to
take multiple rounds to fulfill their requirements while interacting with a
dialogue system. In this paper, we propose a streaming end-to-end framework
that can process multiple intentions in an online and incremental way. The
backbone of our framework is a unidirectional RNN trained with the
connectionist temporal classification (CTC) criterion. By this design, an
intention can be identified when sufficient evidence has been accumulated, and
multiple intentions can be identified sequentially. We evaluate our solution on
the Fluent Speech Commands (FSC) dataset and the intent detection accuracy is
about 97 % on all multi-intent settings. This result is comparable to the
performance of the state-of-the-art non-streaming models, but is achieved in an
online and incremental way. We also employ our model to a keyword spotting task
using the Google Speech Commands dataset and the results are also highly
promising.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes. (arXiv:2012.14210v2 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1">Nils Reimers</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1">Iryna Gurevych</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14210">
                                    <div class="article-summary-box-inner">
                                        <span>Information Retrieval using dense low-dimensional representations recently
became popular and showed out-performance to traditional sparse-representations
like BM25. However, no previous work investigated how dense representations
perform with large index sizes. We show theoretically and empirically that the
performance for dense representations decreases quicker than sparse
representations for increasing index sizes. In extreme cases, this can even
lead to a tipping point where at a certain index size sparse representations
outperform dense representations. We show that this behavior is tightly
connected to the number of dimensions of the representations: The lower the
dimension, the higher the chance for false positives, i.e. returning irrelevant
documents.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Open Domain Question Answering over Tables via Dense Retrieval. (arXiv:2103.12011v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1">Jonathan Herzig</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1">Thomas M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Krichene_S/0/1/0/all/0/1">Syrine Krichene</a>, <a href="http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1">Julian Martin Eisenschlos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12011">
                                    <div class="article-summary-box-inner">
                                        <span>Recent advances in open-domain QA have led to strong models based on dense
retrieval, but only focused on retrieving textual passages. In this work, we
tackle open-domain QA over tables for the first time, and show that retrieval
can be improved by a retriever designed to handle tabular context. We present
an effective pre-training procedure for our retriever and improve retrieval
quality with mined hard negatives. As relevant datasets are missing, we extract
a subset of Natural Questions (Kwiatkowski et al., 2019) into a Table QA
dataset. We find that our retriever improves retrieval results from 72.0 to
81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match, over a
BERT based retriever.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Comparative Study on Neural Architectures and Training Methods for Japanese Speech Recognition. (arXiv:2106.05111v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Karita_S/0/1/0/all/0/1">Shigeki Karita</a>, <a href="http://arxiv.org/find/cs/1/au:+Kubo_Y/0/1/0/all/0/1">Yotaro Kubo</a>, <a href="http://arxiv.org/find/cs/1/au:+Bacchiani_M/0/1/0/all/0/1">Michiel Adriaan Unico Bacchiani</a>, <a href="http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1">Llion Jones</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05111">
                                    <div class="article-summary-box-inner">
                                        <span>End-to-end (E2E) modeling is advantageous for automatic speech recognition
(ASR) especially for Japanese since word-based tokenization of Japanese is not
trivial, and E2E modeling is able to model character sequences directly. This
paper focuses on the latest E2E modeling techniques, and investigates their
performances on character-based Japanese ASR by conducting comparative
experiments. The results are analyzed and discussed in order to understand the
relative advantages of long short-term memory (LSTM), and Conformer models in
combination with connectionist temporal classification, transducer, and
attention-based loss functions. Furthermore, the paper investigates on
effectivity of the recent training techniques such as data augmentation
(SpecAugment), variational noise injection, and exponential moving average. The
best configuration found in the paper achieved the state-of-the-art character
error rates of 4.1%, 3.2%, and 3.5% for Corpus of Spontaneous Japanese (CSJ)
eval1, eval2, and eval3 tasks, respectively. The system is also shown to be
computationally efficient thanks to the efficiency of Conformer transducers.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Convolutional Complex Knowledge Graph Embeddings. (arXiv:2008.03130v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1">Caglar Demir</a>, <a href="http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1">Axel-Cyrille Ngonga Ngomo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.03130">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we study the problem of learning continuous vector
representations of knowledge graphs for predicting missing links. We present a
new approach called ConEx, which infers missing links by leveraging the
composition of a 2D convolution with a Hermitian inner product of
complex-valued embedding vectors. We evaluate ConEx against state-of-the-art
approaches on the WN18RR, FB15K-237, KINSHIP and UMLS benchmark datasets. Our
experimental results show that ConEx achieves a performance superior to that of
state-of-the-art approaches such as RotatE, QuatE and TuckER on the link
prediction task on all datasets while requiring at least 8 times fewer
parameters. We ensure the reproducibility of our results by providing an
open-source implementation which includes the training, evaluation scripts
along with pre-trained models at https://github.com/conex-kge/ConEx.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Coreference Reasoning in Machine Reading Comprehension. (arXiv:2012.15573v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Mingzhu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1">Nafise Sadat Moosavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1">Dan Roth</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1">Iryna Gurevych</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15573">
                                    <div class="article-summary-box-inner">
                                        <span>Coreference resolution is essential for natural language understanding and
has been long studied in NLP. In recent years, as the format of Question
Answering (QA) became a standard for machine reading comprehension (MRC), there
have been data collection efforts, e.g., Dasigi et al. (2019), that attempt to
evaluate the ability of MRC models to reason about coreference. However, as we
show, coreference reasoning in MRC is a greater challenge than earlier thought;
MRC datasets do not reflect the natural distribution and, consequently, the
challenges of coreference reasoning. Specifically, success on these datasets
does not reflect a model&#x27;s proficiency in coreference reasoning. We propose a
methodology for creating MRC datasets that better reflect the challenges of
coreference reasoning and use it to create a sample evaluation set. The results
on our dataset show that state-of-the-art models still struggle with these
phenomena. Furthermore, we develop an effective way to use naturally occurring
coreference phenomena from existing coreference resolution datasets when
training MRC models. This allows us to show an improvement in the coreference
reasoning abilities of state-of-the-art models. The code and the resulting
dataset are available at https://github.com/UKPLab/coref-reasoning-in-qa.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. (arXiv:2106.04632v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1">Jie Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1">Zhe Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Licheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yen-Chun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pillai_R/0/1/0/all/0/1">Rohit Pillai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Luowei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Eric Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1">Tamara Lee Berg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingjing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zicheng Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04632">
                                    <div class="article-summary-box-inner">
                                        <span>Most existing video-and-language (VidL) research focuses on a single dataset,
or multiple datasets of a single task. In reality, a truly useful VidL system
is expected to be easily generalizable to diverse tasks, domains, and datasets.
To facilitate the evaluation of such systems, we introduce Video-And-Language
Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets
over 3 popular tasks: (i) text-to-video retrieval; (ii) video question
answering; and (iii) video captioning. VALUE benchmark aims to cover a broad
range of video genres, video lengths, data volumes, and task difficulty levels.
Rather than focusing on single-channel videos with visual information only,
VALUE promotes models that leverage information from both video frames and
their associated subtitles, as well as models that share knowledge across
multiple tasks. We evaluate various baseline methods with and without
large-scale VidL pre-training, and systematically investigate the impact of
video input channels, fusion methods, and different video representations. We
also study the transferability between tasks, and conduct multi-task learning
under different settings. The significant gap between our best model and human
performance calls for future study for advanced VidL models. VALUE is available
at https://value-leaderboard.github.io/.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bayesian Attention Belief Networks. (arXiv:2106.05251v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shujian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xinjie Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingyuan Zhou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05251">
                                    <div class="article-summary-box-inner">
                                        <span>Attention-based neural networks have achieved state-of-the-art results on a
wide range of tasks. Most such models use deterministic attention while
stochastic attention is less explored due to the optimization difficulties or
complicated model design. This paper introduces Bayesian attention belief
networks, which construct a decoder network by modeling unnormalized attention
weights with a hierarchy of gamma distributions, and an encoder network by
stacking Weibull distributions with a deterministic-upward-stochastic-downward
structure to approximate the posterior. The resulting auto-encoding networks
can be optimized in a differentiable way with a variational lower bound. It is
simple to convert any models with deterministic attention, including pretrained
ones, to the proposed Bayesian attention belief networks. On a variety of
language understanding tasks, we show that our method outperforms deterministic
attention and state-of-the-art stochastic attention in accuracy, uncertainty
estimation, generalization across domains, and robustness to adversarial
attacks. We further demonstrate the general applicability of our method on
neural machine translation and visual question answering, showing great
potential of incorporating our method into various attention-related tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">What Would a Teacher Do? Predicting Future Talk Moves. (arXiv:2106.05249v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ganesh_A/0/1/0/all/0/1">Ananya Ganesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1">Martha Palmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1">Katharina Kann</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05249">
                                    <div class="article-summary-box-inner">
                                        <span>Recent advances in natural language processing (NLP) have the ability to
transform how classroom learning takes place. Combined with the increasing
integration of technology in today&#x27;s classrooms, NLP systems leveraging
question answering and dialog processing techniques can serve as private tutors
or participants in classroom discussions to increase student engagement and
learning. To progress towards this goal, we use the classroom discourse
framework of academically productive talk (APT) to learn strategies that make
for the best learning experience. In this paper, we introduce a new task,
called future talk move prediction (FTMP): it consists of predicting the next
talk move -- an utterance strategy from APT -- given a conversation history
with its corresponding talk moves. We further introduce a neural network model
for this task, which outperforms multiple baselines by a large margin. Finally,
we compare our model&#x27;s performance on FTMP to human performance and show
several similarities between the two.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Class-Transductive Intent Representations for Zero-shot Intent Detection. (arXiv:2012.01721v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1">Qingyi Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuanxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1">Peng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangnan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiping Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01721">
                                    <div class="article-summary-box-inner">
                                        <span>Zero-shot intent detection (ZSID) aims to deal with the continuously emerging
intents without annotated training data. However, existing ZSID systems suffer
from two limitations: 1) They are not good at modeling the relationship between
seen and unseen intents. 2) They cannot effectively recognize unseen intents
under the generalized intent detection (GZSID) setting. A critical problem
behind these limitations is that the representations of unseen intents cannot
be learned in the training stage. To address this problem, we propose a novel
framework that utilizes unseen class labels to learn Class-Transductive Intent
Representations (CTIR). Specifically, we allow the model to predict unseen
intents during training, with the corresponding label names serving as input
utterances. On this basis, we introduce a multi-task learning objective, which
encourages the model to learn the distinctions among intents, and a similarity
scorer, which estimates the connections among intents more accurately. CTIR is
easy to implement and can be integrated with existing methods. Experiments on
two real-world datasets show that CTIR brings considerable improvement to the
baseline systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Phraseformer: Multimodal Key-phrase Extraction using Transformer and Graph Embedding. (arXiv:2106.04939v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nikzad_Khasmakhi_N/0/1/0/all/0/1">Narjes Nikzad-Khasmakhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1">Mohammad-Reza Feizi-Derakhshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Asgari_Chenaghlu_M/0/1/0/all/0/1">Meysam Asgari-Chenaghlu</a>, <a href="http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1">Mohammad-Ali Balafar</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_A/0/1/0/all/0/1">Ali-Reza Feizi-Derakhshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahkar_Farshi_T/0/1/0/all/0/1">Taymaz Rahkar-Farshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1">Majid Ramezani</a>, <a href="http://arxiv.org/find/cs/1/au:+Jahanbakhsh_Nagadeh_Z/0/1/0/all/0/1">Zoleikha Jahanbakhsh-Nagadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zafarani_Moattar_E/0/1/0/all/0/1">Elnaz Zafarani-Moattar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranjbar_Khadivi_M/0/1/0/all/0/1">Mehrdad Ranjbar-Khadivi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04939">
                                    <div class="article-summary-box-inner">
                                        <span>Background: Keyword extraction is a popular research topic in the field of
natural language processing. Keywords are terms that describe the most relevant
information in a document. The main problem that researchers are facing is how
to efficiently and accurately extract the core keywords from a document.
However, previous keyword extraction approaches have utilized the text and
graph features, there is the lack of models that can properly learn and combine
these features in a best way.

Methods: In this paper, we develop a multimodal Key-phrase extraction
approach, namely Phraseformer, using transformer and graph embedding
techniques. In Phraseformer, each keyword candidate is presented by a vector
which is the concatenation of the text and structure learning representations.
Phraseformer takes the advantages of recent researches such as BERT and ExEm to
preserve both representations. Also, the Phraseformer treats the key-phrase
extraction task as a sequence labeling problem solved using classification
task.

Results: We analyze the performance of Phraseformer on three datasets
including Inspec, SemEval2010 and SemEval 2017 by F1-score. Also, we
investigate the performance of different classifiers on Phraseformer method
over Inspec dataset. Experimental results demonstrate the effectiveness of
Phraseformer method over the three datasets used. Additionally, the Random
Forest classifier gain the highest F1-score among all classifiers.

Conclusions: Due to the fact that the combination of BERT and ExEm is more
meaningful and can better represent the semantic of words. Hence, Phraseformer
significantly outperforms single-modality methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Which transformer architecture fits my data? A vocabulary bottleneck in self-attention. (arXiv:2105.03928v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1">Noam Wies</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1">Yoav Levine</a>, <a href="http://arxiv.org/find/cs/1/au:+Jannai_D/0/1/0/all/0/1">Daniel Jannai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1">Amnon Shashua</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03928">
                                    <div class="article-summary-box-inner">
                                        <span>After their successful debut in natural language processing, Transformer
architectures are now becoming the de-facto standard in many domains. An
obstacle for their deployment over new modalities is the architectural
configuration: the optimal depth-to-width ratio has been shown to dramatically
vary across data types (e.g., $10$x larger over images than over language). We
theoretically predict the existence of an embedding rank bottleneck that limits
the contribution of self-attention width to the Transformer expressivity. We
thus directly tie the input vocabulary size and rank to the optimal
depth-to-width ratio, since a small vocabulary size or rank dictates an added
advantage of depth over width. We empirically demonstrate the existence of this
bottleneck and its implications on the depth-to-width interplay of Transformer
architectures, linking the architecture variability across domains to the often
glossed-over usage of different vocabulary sizes or embedding ranks in
different domains. As an additional benefit, our rank bottlenecking framework
allows us to identify size redundancies of $25\%-50\%$ in leading NLP models
such as ALBERT and T5.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Zero-shot Sequence Labeling for Transformer-based Sentence Classifiers. (arXiv:2103.14465v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bujel_K/0/1/0/all/0/1">Kamil Bujel</a>, <a href="http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1">Helen Yannakoudakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1">Marek Rei</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14465">
                                    <div class="article-summary-box-inner">
                                        <span>We investigate how sentence-level transformers can be modified into effective
sequence labelers at the token level without any direct supervision. Existing
approaches to zero-shot sequence labeling do not perform well when applied on
transformer-based architectures. As transformers contain multiple layers of
multi-head self-attention, information in the sentence gets distributed between
many tokens, negatively affecting zero-shot token-level performance. We find
that a soft attention module which explicitly encourages sharpness of attention
weights can significantly outperform existing methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Parameter-Efficient Transfer Learning with Diff Pruning. (arXiv:2012.07463v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Demi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1">Alexander M. Rush</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yoon Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07463">
                                    <div class="article-summary-box-inner">
                                        <span>While task-specific finetuning of pretrained networks has led to significant
empirical advances in NLP, the large size of networks makes finetuning
difficult to deploy in multi-task, memory-constrained settings. We propose diff
pruning as a simple approach to enable parameter-efficient transfer learning
within the pretrain-finetune framework. This approach views finetuning as
learning a task-specific diff vector that is applied on top of the pretrained
parameter vector, which remains fixed and is shared across different tasks. The
diff vector is adaptively pruned during training with a differentiable
approximation to the L0-norm penalty to encourage sparsity. Diff pruning
becomes parameter-efficient as the number of tasks increases, as it requires
storing only the nonzero positions and weights of the diff vector for each
task, while the cost of storing the shared pretrained model remains constant.
It further does not require access to all tasks during training, which makes it
attractive in settings where tasks arrive in stream or the set of tasks is
unknown. We find that models finetuned with diff pruning can match the
performance of fully finetuned baselines on the GLUE benchmark while only
modifying 0.5% of the pretrained model&#x27;s parameters per task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Multilingual Representation for Natural Language Understanding with Enhanced Cross-Lingual Supervision. (arXiv:2106.05166v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yinpeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liangyou Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05166">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, pre-training multilingual language models has shown great potential
in learning multilingual representation, a crucial topic of natural language
processing. Prior works generally use a single mixed attention (MA) module,
following TLM (Conneau and Lample, 2019), for attending to intra-lingual and
cross-lingual contexts equivalently and simultaneously. In this paper, we
propose a network named decomposed attention (DA) as a replacement of MA. The
DA consists of an intra-lingual attention (IA) and a cross-lingual attention
(CA), which model intralingual and cross-lingual supervisions respectively. In
addition, we introduce a language-adaptive re-weighting strategy during
training to further boost the model&#x27;s performance. Experiments on various
cross-lingual natural language understanding (NLU) tasks show that the proposed
architecture and learning strategy significantly improve the model&#x27;s
cross-lingual transferability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation. (arXiv:2004.08694v4 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1">Kaustubh D. Dhole</a>, <a href="http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1">Christopher D. Manning</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.08694">
                                    <div class="article-summary-box-inner">
                                        <span>Question Generation (QG) is fundamentally a simple syntactic transformation;
however, many aspects of semantics influence what questions are good to form.
We implement this observation by developing Syn-QG, a set of transparent
syntactic rules leveraging universal dependencies, shallow semantic parsing,
lexical resources, and custom rules which transform declarative sentences into
question-answer pairs. We utilize PropBank argument descriptions and VerbNet
state predicates to incorporate shallow semantic content, which helps generate
questions of a descriptive nature and produce inferential and semantically
richer questions than existing systems. In order to improve syntactic fluency
and eliminate grammatically incorrect questions, we employ back-translation
over the output of these syntactic rules. A set of crowd-sourced evaluations
shows that our system can generate a larger number of highly grammatical and
relevant questions than previous QG systems and that back-translation
drastically improves grammaticality at a slight cost of generating irrelevant
questions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Supervised Domain Adaptation by Augmenting Pre-trained Models with Random Units. (arXiv:2106.04935v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Meftah_S/0/1/0/all/0/1">Sara Meftah</a>, <a href="http://arxiv.org/find/cs/1/au:+Semmar_N/0/1/0/all/0/1">Nasredine Semmar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamaazousti_Y/0/1/0/all/0/1">Youssef Tamaazousti</a>, <a href="http://arxiv.org/find/cs/1/au:+Essafi_H/0/1/0/all/0/1">Hassane Essafi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadat_F/0/1/0/all/0/1">Fatiha Sadat</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04935">
                                    <div class="article-summary-box-inner">
                                        <span>Neural Transfer Learning (TL) is becoming ubiquitous in Natural Language
Processing (NLP), thanks to its high performance on many tasks, especially in
low-resourced scenarios. Notably, TL is widely used for neural domain
adaptation to transfer valuable knowledge from high-resource to low-resource
domains. In the standard fine-tuning scheme of TL, a model is initially
pre-trained on a source domain and subsequently fine-tuned on a target domain
and, therefore, source and target domains are trained using the same
architecture. In this paper, we show through interpretation methods that such
scheme, despite its efficiency, is suffering from a main limitation. Indeed,
although capable of adapting to new domains, pre-trained neurons struggle with
learning certain patterns that are specific to the target domain. Moreover, we
shed light on the hidden negative transfer occurring despite the high
relatedness between source and target domains, which may mitigate the final
gain brought by transfer learning. To address these problems, we propose to
augment the pre-trained model with normalised, weighted and randomly
initialised units that foster a better adaptation while maintaining the
valuable source knowledge. We show that our approach exhibits significant
improvements to the standard fine-tuning scheme for neural domain adaptation
from the news domain to the social media domain on four NLP tasks:
part-of-speech tagging, chunking, named entity recognition and morphosyntactic
tagging.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Investigating Memorization of Conspiracy Theories in Text Generation. (arXiv:2101.00379v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1">Sharon Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1">Michael Saxon</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00379">
                                    <div class="article-summary-box-inner">
                                        <span>The adoption of natural language generation (NLG) models can leave
individuals vulnerable to the generation of harmful information memorized by
the models, such as conspiracy theories. While previous studies examine
conspiracy theories in the context of social media, they have not evaluated
their presence in the new space of generative language models. In this work, we
investigate the capability of language models to generate conspiracy theory
text. Specifically, we aim to answer: can we test pretrained generative
language models for the memorization and elicitation of conspiracy theories
without access to the model&#x27;s training data? We highlight the difficulties of
this task and discuss it in the context of memorization, generalization, and
hallucination. Utilizing a new dataset consisting of conspiracy theory topics
and machine-generated conspiracy theories helps us discover that many
conspiracy theories are deeply rooted in the pretrained language models. Our
experiments demonstrate a relationship between model parameters such as size
and temperature and their propensity to generate conspiracy theory text. These
results indicate the need for a more thorough review of NLG applications before
release and an in-depth discussion of the drawbacks of memorization in
generative language models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xiao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mingxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Liwei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09501">
                                    <div class="article-summary-box-inner">
                                        <span>Existing multilingual machine translation approaches mainly focus on
English-centric directions, while the non-English directions still lag behind.
In this work, we aim to build a many-to-many translation system with an
emphasis on the quality of non-English language directions. Our intuition is
based on the hypothesis that a universal cross-language representation leads to
better multilingual translation performance. To this end, we propose mRASP2, a
training method to obtain a single unified multilingual translation model.
mRASP2 is empowered by two techniques: a) a contrastive learning scheme to
close the gap among representations of different languages, and b) data
augmentation on both multiple parallel and monolingual data to further align
token representations. For English-centric directions, mRASP2 outperforms
existing best unified model and achieves competitive or even better performance
than the pre-trained and fine-tuned model mBART on tens of WMT&#x27;s translation
directions. For non-English directions, mRASP2 achieves an improvement of
average 10+ BLEU compared with the multilingual Transformer baseline. Code,
data and trained models are available at https://github.com/PANXiao1994/mRASP2.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On the Lack of Robust Interpretability of Neural Text Classifiers. (arXiv:2106.04631v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1">Muhammad Bilal Zafar</a>, <a href="http://arxiv.org/find/cs/1/au:+Donini_M/0/1/0/all/0/1">Michele Donini</a>, <a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1">Dylan Slack</a>, <a href="http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1">C&#xe9;dric Archambeau</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Sanjiv Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1">Krishnaram Kenthapadi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04631">
                                    <div class="article-summary-box-inner">
                                        <span>With the ever-increasing complexity of neural language models, practitioners
have turned to methods for understanding the predictions of these models. One
of the most well-adopted approaches for model interpretability is feature-based
interpretability, i.e., ranking the features in terms of their impact on model
predictions. Several prior studies have focused on assessing the fidelity of
feature-based interpretability methods, i.e., measuring the impact of dropping
the top-ranked features on the model output. However, relatively little work
has been conducted on quantifying the robustness of interpretations. In this
work, we assess the robustness of interpretations of neural text classifiers,
specifically, those based on pretrained Transformer encoders, using two
randomization tests. The first compares the interpretations of two models that
are identical except for their initializations. The second measures whether the
interpretations differ between a model with trained parameters and a model with
random parameters. Both tests show surprising deviations from expected
behavior, raising questions about the extent of insights that practitioners may
draw from interpretations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Extractive Search. (arXiv:2106.04612v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1">Shauli Ravfogel</a>, <a href="http://arxiv.org/find/cs/1/au:+Taub_Tabib_H/0/1/0/all/0/1">Hillel Taub-Tabib</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1">Yoav Goldberg</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04612">
                                    <div class="article-summary-box-inner">
                                        <span>Domain experts often need to extract structured information from large
corpora. We advocate for a search paradigm called &#x60;&#x60;extractive search&#x27;&#x27;, in
which a search query is enriched with capture-slots, to allow for such rapid
extraction. Such an extractive search system can be built around syntactic
structures, resulting in high-precision, low-recall results. We show how the
recall can be improved using neural retrieval and alignment. The goals of this
paper are to concisely introduce the extractive-search paradigm; and to
demonstrate a prototype neural retrieval system for extractive search and its
benefits and potential. Our prototype is available at
\url{https://spike.neural-sim.apps.allenai.org/} and a video demonstration is
available at \url{https://vimeo.com/559586687}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech to Standard German Text Corpus. (arXiv:2010.02810v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pluss_M/0/1/0/all/0/1">Michel Pl&#xfc;ss</a>, <a href="http://arxiv.org/find/cs/1/au:+Neukom_L/0/1/0/all/0/1">Lukas Neukom</a>, <a href="http://arxiv.org/find/cs/1/au:+Scheller_C/0/1/0/all/0/1">Christian Scheller</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogel_M/0/1/0/all/0/1">Manfred Vogel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.02810">
                                    <div class="article-summary-box-inner">
                                        <span>We present the Swiss Parliaments Corpus (SPC), an automatically aligned Swiss
German speech to Standard German text corpus. This first version of the corpus
is based on publicly available data of the Bernese cantonal parliament and
consists of 293 hours of data. It was created using a novel forced sentence
alignment procedure and an alignment quality estimator, which can be used to
trade off corpus size and quality. We trained Automatic Speech Recognition
(ASR) models as baselines on different subsets of the data and achieved a Word
Error Rate (WER) of 0.278 and a BLEU score of 0.586 on the SPC test set. The
corpus is freely available for download.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding. (arXiv:2106.04970v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1">Tao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Houfeng Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04970">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the
online inference efficiency of the Transformer for instantaneous Grammatical
Error Correction (GEC). SAD optimizes the online inference efficiency for GEC
by two innovations: 1) it aggressively decodes as many tokens as possible in
parallel instead of always decoding only one token in each step to improve
computational parallelism; 2) it uses a shallow decoder instead of the
conventional Transformer architecture with balanced encoder-decoder depth to
reduce the computational cost during inference. Experiments in both English and
Chinese GEC benchmarks show that aggressive decoding could yield the same
predictions as greedy decoding but with a significant speedup for online
inference. Its combination with the shallow decoder could offer an even higher
online inference speedup over the powerful Transformer baseline without quality
loss. Not only does our approach allow a single model to achieve the
state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14
and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference
speedup over the Transformer-big model, but also it is easily adapted to other
languages. Our code is available at
https://github.com/AutoTemp/Shallow-Aggressive-Decoding.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation. (arXiv:2106.05093v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1">Cunxiao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhaopeng Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jing Jiang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05093">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a new training objective named order-agnostic cross entropy (OaXE)
for fully non-autoregressive translation (NAT) models. OaXE improves the
standard cross-entropy loss to ameliorate the effect of word reordering, which
is a common source of the critical multimodality problem in NAT. Concretely,
OaXE removes the penalty for word order errors, and computes the cross entropy
loss based on the best possible alignment between model predictions and target
tokens. Since the log loss is very sensitive to invalid references, we leverage
cross entropy initialization and loss truncation to ensure the model focuses on
a good part of the search space. Extensive experiments on major WMT benchmarks
show that OaXE substantially improves translation performance, setting new
state of the art for fully NAT models. Further analyses show that OaXE
alleviates the multimodality problem by reducing token repetitions and
increasing prediction confidence. Our code, data, and trained models are
available at https://github.com/tencent-ailab/ICML21_OAXE.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Joint System-Wise Optimization for Pipeline Goal-Oriented Dialog System. (arXiv:2106.04835v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zichuan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bowen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaodong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tengyu Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04835">
                                    <div class="article-summary-box-inner">
                                        <span>Recent work (Takanobu et al., 2020) proposed the system-wise evaluation on
dialog systems and found that improvement on individual components (e.g., NLU,
policy) in prior work may not necessarily bring benefit to pipeline systems in
system-wise evaluation. To improve the system-wise performance, in this paper,
we propose new joint system-wise optimization techniques for the pipeline
dialog system. First, we propose a new data augmentation approach which
automates the labeling process for NLU training. Second, we propose a novel
stochastic policy parameterization with Poisson distribution that enables
better exploration and offers a principled way to compute policy gradient.
Third, we propose a reward bonus to help policy explore successful dialogs. Our
approaches outperform the competitive pipeline systems from Takanobu et al.
(2020) by big margins of 12% success rate in automatic system-wise evaluation
and of 16% success rate in human evaluation on the standard multi-domain
benchmark dataset MultiWOZ 2.1, and also outperform the recent state-of-the-art
end-to-end trained model from DSTC9.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Auto-tagging of Short Conversational Sentences using Natural Language Processing Methods. (arXiv:2106.04959v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1">&#x15e;&#xfc;kr&#xfc; Ozan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1">D. Emre Ta&#x15f;ar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04959">
                                    <div class="article-summary-box-inner">
                                        <span>In this study, we aim to find a method to auto-tag sentences specific to a
domain. Our training data comprises short conversational sentences extracted
from chat conversations between company&#x27;s customer representatives and web site
visitors. We manually tagged approximately 14 thousand visitor inputs into ten
basic categories, which will later be used in a transformer-based language
model with attention mechanisms for the ultimate goal of developing a chatbot
application that can produce meaningful dialogue. We considered three different
state-of-the-art models and reported their auto-tagging capabilities. We
achieved the best performance with the bidirectional encoder representation
from transformers (BERT) model. Implementation of the models used in these
experiments can be cloned from our GitHub repository and tested for similar
auto-tagging problems without much effort.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Making Better Use of Bilingual Information for Cross-Lingual AMR Parsing. (arXiv:2106.04814v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yitao Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhe Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1">Xiaojun Wan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04814">
                                    <div class="article-summary-box-inner">
                                        <span>Abstract Meaning Representation (AMR) is a rooted, labeled, acyclic graph
representing the semantics of natural language. As previous works show,
although AMR is designed for English at first, it can also represent semantics
in other languages. However, they find that concepts in their predicted AMR
graphs are less specific. We argue that the misprediction of concepts is due to
the high relevance between English tokens and AMR concepts. In this work, we
introduce bilingual input, namely the translated texts as well as non-English
texts, in order to enable the model to predict more accurate concepts. Besides,
we also introduce an auxiliary task, requiring the decoder to predict the
English sequences at the same time. The auxiliary task can help the decoder
understand what exactly the corresponding English tokens are. Our proposed
cross-lingual AMR parser surpasses previous state-of-the-art parser by 10.6
points on Smatch F1 score. The ablation study also demonstrates the efficacy of
our proposed modules.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Probing Multilingual Language Models for Discourse. (arXiv:2106.04832v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kurfali_M/0/1/0/all/0/1">Murathan Kurfal&#x131;</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostling_R/0/1/0/all/0/1">Robert &#xd6;stling</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04832">
                                    <div class="article-summary-box-inner">
                                        <span>Pre-trained multilingual language models have become an important building
block in multilingual natural language processing. In the present paper, we
investigate a range of such models to find out how well they transfer
discourse-level knowledge across languages. This is done with a systematic
evaluation on a broader set of discourse-level tasks than has been previously
been assembled. We find that the XLM-RoBERTa family of models consistently show
the best performance, by simultaneously being good monolingual models and
degrading relatively little in a zero-shot setting. Our results also indicate
that model distillation may hurt the ability of cross-lingual transfer of
sentence representations, while language dissimilarity at most has a modest
effect. We hope that our test suite, covering 5 tasks with a total of 22
languages in 10 distinct families, will serve as a useful evaluation platform
for multilingual performance at and beyond the sentence level.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Psycholinguistic Tripartite Graph Network for Personality Detection. (arXiv:2106.04963v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Feifan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1">Haolan Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1">Xiaojun Quan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04963">
                                    <div class="article-summary-box-inner">
                                        <span>Most of the recent work on personality detection from online posts adopts
multifarious deep neural networks to represent the posts and builds predictive
models in a data-driven manner, without the exploitation of psycholinguistic
knowledge that may unveil the connections between one&#x27;s language usage and his
psychological traits. In this paper, we propose a psycholinguistic
knowledge-based tripartite graph network, TrigNet, which consists of a
tripartite graph network and a BERT-based graph initializer. The graph network
injects structural psycholinguistic knowledge from LIWC, a computerized
instrument for psycholinguistic analysis, by constructing a heterogeneous
tripartite graph. The graph initializer is employed to provide initial
embeddings for the graph nodes. To reduce the computational cost in graph
learning, we further propose a novel flow graph attention network (GAT) that
only transmits messages between neighboring parties in the tripartite graph.
Benefiting from the tripartite graph, TrigNet can aggregate post information
from a psychological perspective, which is a novel way of exploiting domain
knowledge. Extensive experiments on two datasets show that TrigNet outperforms
the existing state-of-art model by 3.47 and 2.10 points in average F1.
Moreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%,
respectively, in comparison to the original GAT in our setting.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DGA-Net Dynamic Gaussian Attention Network for Sentence Semantic Matching. (arXiv:2106.04905v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1">Guangyi Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1">Enhong Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04905">
                                    <div class="article-summary-box-inner">
                                        <span>Sentence semantic matching requires an agent to determine the semantic
relation between two sentences, where much recent progress has been made by the
advancement of representation learning techniques and inspiration of human
behaviors. Among all these methods, attention mechanism plays an essential role
by selecting important parts effectively. However, current attention methods
either focus on all the important parts in a static way or only select one
important part at one attention step dynamically, which leaves a large space
for further improvement. To this end, in this paper, we design a novel Dynamic
Gaussian Attention Network (DGA-Net) to combine the advantages of current
static and dynamic attention methods. More specifically, we first leverage
pre-trained language model to encode the input sentences and construct semantic
representations from a global perspective. Then, we develop a Dynamic Gaussian
Attention (DGA) to dynamically capture the important parts and corresponding
local contexts from a detailed perspective. Finally, we combine the global
information and detailed local information together to decide the semantic
relation of sentences comprehensively and precisely. Extensive experiments on
two popular sentence semantic matching tasks demonstrate that our proposed
DGA-Net is effective in improving the ability of attention mechanism.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unsupervised Automatic Speech Recognition: A Review. (arXiv:2106.04897v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Aldarmaki_H/0/1/0/all/0/1">Hanan Aldarmaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Ullah_A/0/1/0/all/0/1">Asad Ullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaki_N/0/1/0/all/0/1">Nazar Zaki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04897">
                                    <div class="article-summary-box-inner">
                                        <span>Automatic Speech Recognition (ASR) systems can be trained to achieve
remarkable performance given large amounts of manually transcribed speech, but
large labeled data sets can be difficult or expensive to acquire for all
languages of interest. In this paper, we review the research literature to
identify models and ideas that could lead to fully unsupervised ASR, including
unsupervised segmentation of the speech signal, unsupervised mapping from
speech segments to text, and semi-supervised models with nominal amounts of
labeled examples. The objective of the study is to identify the limitations of
what can be learned from speech data alone and to understand the minimum
requirements for speech recognition. Identifying these limitations would help
optimize the resources and efforts in ASR development for low-resource
languages.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sentence Embeddings using Supervised Contrastive Learning. (arXiv:2106.04791v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1">Danqi Liao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04791">
                                    <div class="article-summary-box-inner">
                                        <span>Sentence embeddings encode sentences in fixed dense vectors and have played
an important role in various NLP tasks and systems. Methods for building
sentence embeddings include unsupervised learning such as Quick-Thoughts and
supervised learning such as InferSent. With the success of pretrained NLP
models, recent research shows that fine-tuning pretrained BERT on SNLI and
Multi-NLI data creates state-of-the-art sentence embeddings, outperforming
previous sentence embeddings methods on various evaluation benchmarks. In this
paper, we propose a new method to build sentence embeddings by doing supervised
contrastive learning. Specifically our method fine-tunes pretrained BERT on
SNLI data, incorporating both supervised crossentropy loss and supervised
contrastive loss. Compared with baseline where fine-tuning is only done with
supervised cross-entropy loss similar to current state-of-the-art method SBERT,
our supervised contrastive method improves 2.8% in average on Semantic Textual
Similarity (STS) benchmarks and 1.05% in average on various sentence transfer
tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Automatic Sexism Detection with Multilingual Transformer Models. (arXiv:2106.04908v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mina_S/0/1/0/all/0/1">Sch&#xfc;tz Mina</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaqueline_B/0/1/0/all/0/1">Boeck Jaqueline</a>, <a href="http://arxiv.org/find/cs/1/au:+Daria_L/0/1/0/all/0/1">Liakhovets Daria</a>, <a href="http://arxiv.org/find/cs/1/au:+Djordje_S/0/1/0/all/0/1">Slijep&#x10d;evi&#x107; Djordje</a>, <a href="http://arxiv.org/find/cs/1/au:+Armin_K/0/1/0/all/0/1">Kirchknopf Armin</a>, <a href="http://arxiv.org/find/cs/1/au:+Manuel_H/0/1/0/all/0/1">Hecht Manuel</a>, <a href="http://arxiv.org/find/cs/1/au:+Johannes_B/0/1/0/all/0/1">Bogensperger Johannes</a>, <a href="http://arxiv.org/find/cs/1/au:+Sven_S/0/1/0/all/0/1">Schlarb Sven</a>, <a href="http://arxiv.org/find/cs/1/au:+Alexander_S/0/1/0/all/0/1">Schindler Alexander</a>, <a href="http://arxiv.org/find/cs/1/au:+Matthias_Z/0/1/0/all/0/1">Zeppelzauer Matthias</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04908">
                                    <div class="article-summary-box-inner">
                                        <span>Sexism has become an increasingly major problem on social networks during the
last years. The first shared task on sEXism Identification in Social neTworks
(EXIST) at IberLEF 2021 is an international competition in the field of Natural
Language Processing (NLP) with the aim to automatically identify sexism in
social media content by applying machine learning methods. Thereby sexism
detection is formulated as a coarse (binary) classification problem and a
fine-grained classification task that distinguishes multiple types of sexist
content (e.g., dominance, stereotyping, and objectification). This paper
presents the contribution of the AIT_FHSTP team at the EXIST2021 benchmark for
both tasks. To solve the tasks we applied two multilingual transformer models,
one based on multilingual BERT and one based on XLM-R. Our approach uses two
different strategies to adapt the transformers to the detection of sexist
content: first, unsupervised pre-training with additional data and second,
supervised fine-tuning with additional and augmented data. For both tasks our
best model is XLM-R with unsupervised pre-training on the EXIST data and
additional datasets and fine-tuning on the provided dataset. The best run for
the binary classification (task 1) achieves a macro F1-score of 0.7752 and
scores 5th rank in the benchmark; for the multiclass classification (task 2)
our best submission scores 6th rank with a macro F1-score of 0.5589.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Energy-Based Models for Code Generation under Compilability Constraints. (arXiv:2106.04985v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1">Tomasz Korbak</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1">Hady Elsahar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dymetman_M/0/1/0/all/0/1">Marc Dymetman</a>, <a href="http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1">Germ&#xe1;n Kruszewski</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04985">
                                    <div class="article-summary-box-inner">
                                        <span>Neural language models can be successfully trained on source code, leading to
applications such as code completion. However, their versatile autoregressive
self-supervision objective overlooks important global sequence-level features
that are present in the data such as syntactic correctness or compilability. In
this work, we pose the problem of learning to generate compilable code as
constraint satisfaction. We define an Energy-Based Model (EBM) representing a
pre-trained generative model with an imposed constraint of generating only
compilable sequences. We then use the KL-Adaptive Distributional Policy
Gradient algorithm (Khalifa et al., 2021) to train a generative model
approximating the EBM. We conduct experiments showing that our proposed
approach is able to improve compilability rates without sacrificing diversity
and complexity of the generated samples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">UniKeyphrase: A Unified Extraction and Generation Framework for Keyphrase Prediction. (arXiv:2106.04847v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Huanqin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_D/0/1/0/all/0/1">Dan Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Feng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Di Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04847">
                                    <div class="article-summary-box-inner">
                                        <span>Keyphrase Prediction (KP) task aims at predicting several keyphrases that can
summarize the main idea of the given document. Mainstream KP methods can be
categorized into purely generative approaches and integrated models with
extraction and generation. However, these methods either ignore the diversity
among keyphrases or only weakly capture the relation across tasks implicitly.
In this paper, we propose UniKeyphrase, a novel end-to-end learning framework
that jointly learns to extract and generate keyphrases. In UniKeyphrase,
stacked relation layer and bag-of-words constraint are proposed to fully
exploit the latent semantic relation between extraction and generation in the
view of model structure and training process, respectively. Experiments on KP
benchmarks demonstrate that our joint approach outperforms mainstream methods
by a large margin.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Catchphrase: Automatic Detection of Cultural References. (arXiv:2106.04830v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sweed_N/0/1/0/all/0/1">Nir Sweed</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1">Dafna Shahaf</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04830">
                                    <div class="article-summary-box-inner">
                                        <span>A snowclone is a customizable phrasal template that can be realized in
multiple, instantly recognized variants. For example, &#x60;&#x60;* is the new *&quot; (Orange
is the new black, 40 is the new 30). Snowclones are extensively used in social
media. In this paper, we study snowclones originating from pop-culture quotes;
our goal is to automatically detect cultural references in text. We introduce a
new, publicly available data set of pop-culture quotes and their corresponding
snowclone usages and train models on them. We publish code for Catchphrase, an
internet browser plugin to automatically detect and mark references in
real-time, and examine its performance via a user study. Aside from assisting
people to better comprehend cultural references, we hope that detecting
snowclones can complement work on paraphrasing and help to tackle long-standing
questions in social science about the dynamics of information propagation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fragmented and Valuable: Following Sentiment Changes in Food Tweets. (arXiv:2106.04903v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1">Maija K&#x101;le</a>, <a href="http://arxiv.org/find/cs/1/au:+Rikters_M/0/1/0/all/0/1">Mat&#x12b;ss Rikters</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04903">
                                    <div class="article-summary-box-inner">
                                        <span>We analysed sentiment and frequencies related to smell, taste and temperature
expressed by food tweets in the Latvian language. To get a better understanding
of the role of smell, taste and temperature in the mental map of food
associations, we looked at such categories as &#x27;tasty&#x27; and &#x27;healthy&#x27;, which
turned out to be mutually exclusive. By analysing the occurrence frequency of
words associated with these categories, we discovered that food discourse
overall was permeated by &#x60;tasty&#x27; while the category of &#x27;healthy&#x27; was relatively
small. Finally, we used the analysis of temporal dynamics to see if we can
trace seasonality or other temporal aspects in smell, taste and temperature as
reflected in food tweets. Understanding the composition of social media content
with relation to smell, taste and temperature in food tweets allows us to
develop our work further - on food culture/seasonality and its relation to
temperature, on our limited capacity to express smell-related sentiments, and
the lack of the paradigm of taste in discussing food healthiness.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Review of Human Evaluation for Style Transfer. (arXiv:2106.04747v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Briakou_E/0/1/0/all/0/1">Eleftheria Briakou</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1">Sweta Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Ke Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1">Joel Tetreault</a>, <a href="http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1">Marine Carpuat</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04747">
                                    <div class="article-summary-box-inner">
                                        <span>This paper reviews and summarizes human evaluation practices described in 97
style transfer papers with respect to three main evaluation aspects: style
transfer, meaning preservation, and fluency. In principle, evaluations by human
raters should be the most reliable. However, in style transfer papers, we find
that protocols for human evaluations are often underspecified and not
standardized, which hampers the reproducibility of research in this field and
progress toward better human and automatic evaluation methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On Sample Based Explanation Methods for NLP:Efficiency, Faithfulness, and Semantic Evaluation. (arXiv:2106.04753v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Ziming Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yada Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1">Guangnan Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1">Xiaodong Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fan Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04753">
                                    <div class="article-summary-box-inner">
                                        <span>In the recent advances of natural language processing, the scale of the
state-of-the-art models and datasets is usually extensive, which challenges the
application of sample-based explanation methods in many aspects, such as
explanation interpretability, efficiency, and faithfulness. In this work, for
the first time, we can improve the interpretability of explanations by allowing
arbitrary text sequences as the explanation unit. On top of this, we implement
a hessian-free method with a model faithfulness guarantee. Finally, to compare
our method with the others, we propose a semantic-based evaluation metric that
can better align with humans&#x27; judgment of explanations than the widely adopted
diagnostic or re-training measures. The empirical results on multiple real data
sets demonstrate the proposed method&#x27;s superior performance to popular
explanation techniques such as Influence Function or TracIn on semantic
evaluation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data Expansion using Back Translation and Paraphrasing for Hate Speech Detection. (arXiv:2106.04681v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Beddiar_D/0/1/0/all/0/1">Djamila Romaissa Beddiar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jahan_M/0/1/0/all/0/1">Md Saroar Jahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Oussalah_M/0/1/0/all/0/1">Mourad Oussalah</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04681">
                                    <div class="article-summary-box-inner">
                                        <span>With proliferation of user generated contents in social media platforms,
establishing mechanisms to automatically identify toxic and abusive content
becomes a prime concern for regulators, researchers, and society. Keeping the
balance between freedom of speech and respecting each other dignity is a major
concern of social media platform regulators. Although, automatic detection of
offensive content using deep learning approaches seems to provide encouraging
results, training deep learning-based models requires large amounts of
high-quality labeled data, which is often missing. In this regard, we present
in this paper a new deep learning-based method that fuses a Back Translation
method, and a Paraphrasing technique for data augmentation. Our pipeline
investigates different word-embedding-based architectures for classification of
hate speech. The back translation technique relies on an encoder-decoder
architecture pre-trained on a large corpus and mostly used for machine
translation. In addition, paraphrasing exploits the transformer model and the
mixture of experts to generate diverse paraphrases. Finally, LSTM, and CNN are
compared to seek enhanced classification results. We evaluate our proposal on
five publicly available datasets; namely, AskFm corpus, Formspring dataset,
Warner and Waseem dataset, Olid, and Wikipedia toxic comments dataset. The
performance of the proposal together with comparison to some related
state-of-art results demonstrate the effectiveness and soundness of our
proposal.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DravidianMultiModality: A Dataset for Multi-modal Sentiment Analysis in Tamil and Malayalam. (arXiv:2106.04853v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1">Bharathi Raja Chakravarthi</a>, <a href="http://arxiv.org/find/cs/1/au:+K_J/0/1/0/all/0/1">Jishnu Parameswaran P.K</a>, <a href="http://arxiv.org/find/cs/1/au:+B_P/0/1/0/all/0/1">Premjith B</a>, <a href="http://arxiv.org/find/cs/1/au:+Soman_K/0/1/0/all/0/1">K.P Soman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponnusamy_R/0/1/0/all/0/1">Rahul Ponnusamy</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumaresan_P/0/1/0/all/0/1">Prasanna Kumar Kumaresan</a>, <a href="http://arxiv.org/find/cs/1/au:+Thamburaj_K/0/1/0/all/0/1">Kingston Pal Thamburaj</a>, <a href="http://arxiv.org/find/cs/1/au:+McCrae_J/0/1/0/all/0/1">John P. McCrae</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04853">
                                    <div class="article-summary-box-inner">
                                        <span>Human communication is inherently multimodal and asynchronous. Analyzing
human emotions and sentiment is an emerging field of artificial intelligence.
We are witnessing an increasing amount of multimodal content in local languages
on social media about products and other topics. However, there are not many
multimodal resources available for under-resourced Dravidian languages. Our
study aims to create a multimodal sentiment analysis dataset for the
under-resourced Tamil and Malayalam languages. First, we downloaded product or
movies review videos from YouTube for Tamil and Malayalam. Next, we created
captions for the videos with the help of annotators. Then we labelled the
videos for sentiment, and verified the inter-annotator agreement using Fleiss&#x27;s
Kappa. This is the first multimodal sentiment analysis dataset for Tamil and
Malayalam by volunteer annotators.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FastSeq: Make Sequence Generation Faster. (arXiv:2106.04718v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1">Fei Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiusheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1">Nikhil Bhendawade</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1">Ting Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yeyun Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1">Nan Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_D/0/1/0/all/0/1">Desheng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1">Bingyu Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruifei Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04718">
                                    <div class="article-summary-box-inner">
                                        <span>Transformer-based models have made tremendous impacts in natural language
generation. However the inference speed is a bottleneck due to large model size
and intensive computing involved in auto-regressive decoding process. We
develop FastSeq framework to accelerate sequence generation without accuracy
loss. The proposed optimization techniques include an attention cache
optimization, an efficient algorithm for detecting repeated n-grams, and an
asynchronous generation pipeline with parallel I/O. These optimizations are
general enough to be applicable to Transformer-based models (e.g., T5, GPT2,
and UniLM). Our benchmark results on a set of widely used and diverse models
demonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use
with a simple one-line code change. The source code is available at
https://github.com/microsoft/fastseq.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study of the 2019 Indian Election on WhatsApp. (arXiv:2106.04726v1 [cs.SI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1">Ashkan Kazemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1">Kiran Garimella</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1">Gautam Kishore Shahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaffney_D/0/1/0/all/0/1">Devin Gaffney</a>, <a href="http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1">Scott A. Hale</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04726">
                                    <div class="article-summary-box-inner">
                                        <span>WhatsApp is a popular chat application used by over 2 billion users
worldwide. However, due to end-to-end encryption, there is currently no easy
way to fact-check content on WhatsApp at scale. In this paper, we analyze the
usefulness of a crowd-sourced system on WhatsApp through which users can submit
&quot;tips&quot; containing messages they want fact-checked. We compare the tips sent to
a WhatsApp tipline run during the 2019 Indian national elections with the
messages circulating in large, public groups on WhatsApp and other social media
platforms during the same period. We find that tiplines are a very useful lens
into WhatsApp conversations: a significant fraction of messages and images sent
to the tipline match with the content being shared on public WhatsApp groups
and other social media. Our analysis also shows that tiplines cover the most
popular content well, and a majority of such content is often shared to the
tipline before appearing in large, public WhatsApp groups. Overall, the
analysis suggests tiplines can be an effective source for discovering content
to fact-check.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PAM: Understanding Product Images in Cross Product Category Attribute Extraction. (arXiv:2106.04630v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1">Rongmei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jie Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1">Nasser Zalmout</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Li Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xin Luna Dong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04630">
                                    <div class="article-summary-box-inner">
                                        <span>Understanding product attributes plays an important role in improving online
shopping experience for customers and serves as an integral part for
constructing a product knowledge graph. Most existing methods focus on
attribute extraction from text description or utilize visual information from
product images such as shape and color. Compared to the inputs considered in
prior works, a product image in fact contains more information, represented by
a rich mixture of words and visual clues with a layout carefully designed to
impress customers. This work proposes a more inclusive framework that fully
utilizes these different modalities for attribute extraction. Inspired by
recent works in visual question answering, we use a transformer based sequence
to sequence model to fuse representations of product text, Optical Character
Recognition (OCR) tokens and visual objects detected in the product image. The
framework is further extended with the capability to extract attribute value
across multiple product categories with a single model, by training the decoder
to predict both product category and attribute value and conditioning its
output on product category. The model provides a unified attribute extraction
solution desirable at an e-commerce platform that offers numerous product
categories with a diverse body of product attributes. We evaluated the model on
two product attributes, one with many possible values and one with a small set
of possible values, over 14 product categories and found the model could
achieve 15% gain on the Recall and 10% gain on the F1 score compared to
existing methods using text-only features.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1">Nicolai Pogrebnyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1">Shohreh Shaghaghian</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04641">
                                    <div class="article-summary-box-inner">
                                        <span>Transfer learning methods, and in particular domain adaptation, help exploit
labeled data in one domain to improve the performance of a certain task in
another domain. However, it is still not clear what factors affect the success
of domain adaptation. This paper models adaptation success and selection of the
most suitable source domains among several candidates in text similarity. We
use descriptive domain information and cross-domain similarity metrics as
predictive features. While mostly positive, the results also point to some
domains where adaptation success was difficult to predict.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Compacter: Efficient Low-Rank Hypercomplex Adapter Layers. (arXiv:2106.04647v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mahabadi_R/0/1/0/all/0/1">Rabeeh Karimi Mahabadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1">James Henderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1">Sebastian Ruder</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04647">
                                    <div class="article-summary-box-inner">
                                        <span>Adapting large-scale pretrained language models to downstream tasks via
fine-tuning is the standard method for achieving state-of-the-art performance
on NLP benchmarks. However, fine-tuning all weights of models with millions or
billions of parameters is sample-inefficient, unstable in low-resource
settings, and wasteful as it requires storing a separate copy of the model for
each task. Recent work has developed parameter-efficient fine-tuning methods,
but these approaches either still require a relatively large number of
parameters or underperform standard fine-tuning. In this work, we propose
Compacter, a method for fine-tuning large-scale language models with a better
trade-off between task performance and the number of trainable parameters than
prior work. Compacter accomplishes this by building on top of ideas from
adapters, low-rank optimization, and parameterized hypercomplex multiplication
layers.

Specifically, Compacter inserts task-specific weight matrices into a
pretrained model&#x27;s weights, which are computed efficiently as a sum of
Kronecker products between shared &#x60;&#x60;slow&#x27;&#x27; weights and &#x60;&#x60;fast&#x27;&#x27; rank-one
matrices defined per Compacter layer. By only training 0.047% of a pretrained
model&#x27;s parameters, Compacter performs on par with standard fine-tuning on GLUE
and outperforms fine-tuning in low-resource settings. Our code is publicly
available in https://github.com/rabeehk/compacter/</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sequential End-to-End Intent and Slot Label Classification and Localization. (arXiv:2106.04660v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yiran Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1">Nihal Potdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1">Anderson R. Avila</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04660">
                                    <div class="article-summary-box-inner">
                                        <span>Human-computer interaction (HCI) is significantly impacted by delayed
responses from a spoken dialogue system. Hence, end-to-end (e2e) spoken
language understanding (SLU) solutions have recently been proposed to decrease
latency. Such approaches allow for the extraction of semantic information
directly from the speech signal, thus bypassing the need for a transcript from
an automatic speech recognition (ASR) system. In this paper, we propose a
compact e2e SLU architecture for streaming scenarios, where chunks of the
speech signal are processed continuously to predict intent and slot values. Our
model is based on a 3D convolutional neural network (3D-CNN) and a
unidirectional long short-term memory (LSTM). We compare the performance of two
alignment-free losses: the connectionist temporal classification (CTC) method
and its adapted version, namely connectionist temporal localization (CTL). The
latter performs not only the classification but also localization of sequential
audio events. The proposed solution is evaluated on the Fluent Speech Command
dataset and results show our model ability to process incoming speech signal,
reaching accuracy as high as 98.97 % for CTC and 98.78 % for CTL on
single-label classification, and as high as 95.69 % for CTC and 95.28 % for CTL
on two-label prediction.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Comprehension Based Question Answering using Bloom&#x27;s Taxonomy. (arXiv:2106.04653v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sahu_P/0/1/0/all/0/1">Pritish Sahu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1">Michael Cogswell</a>, <a href="http://arxiv.org/find/cs/1/au:+Rutherford_Quach_S/0/1/0/all/0/1">Sara Rutherford-Quach</a>, <a href="http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1">Ajay Divakaran</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04653">
                                    <div class="article-summary-box-inner">
                                        <span>Current pre-trained language models have lots of knowledge, but a more
limited ability to use that knowledge. Bloom&#x27;s Taxonomy helps educators teach
children how to use knowledge by categorizing comprehension skills, so we use
it to analyze and improve the comprehension skills of large pre-trained
language models. Our experiments focus on zero-shot question answering, using
the taxonomy to provide proximal context that helps the model answer questions
by being relevant to those questions. We show targeting context in this manner
improves performance across 4 popular common sense question answer datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.CV"">cs.CV updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose. (arXiv:2106.01981v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Oreshkin_B/0/1/0/all/0/1">Boris N. Oreshkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bocquelet_F/0/1/0/all/0/1">Florent Bocquelet</a>, <a href="http://arxiv.org/find/cs/1/au:+Harvey_F/0/1/0/all/0/1">F&#xe9;lix G. Harvey</a>, <a href="http://arxiv.org/find/cs/1/au:+Raitt_B/0/1/0/all/0/1">Bay Raitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Laflamme_D/0/1/0/all/0/1">Dominic Laflamme</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01981">
                                    <div class="article-summary-box-inner">
                                        <span>Our work focuses on the development of a learnable neural representation of
human pose for advanced AI assisted animation tooling. Specifically, we tackle
the problem of constructing a full static human pose based on sparse and
variable user inputs (e.g. locations and/or orientations of a subset of body
joints). To solve this problem, we propose a novel neural architecture that
combines residual connections with prototype encoding of a partially specified
pose to create a new complete pose from the learned latent space. We show that
our architecture outperforms a baseline based on Transformer, both in terms of
accuracy and computational efficiency. Additionally, we develop a user
interface to integrate our neural model in Unity, a real-time 3D development
platform. Furthermore, we introduce two new datasets representing the static
human pose modeling problem, based on high-quality human motion capture data,
which will be released publicly along with model code.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Comparison for Anti-noise Robustness of Deep Learning Classification Methods on a Tiny Object Image Dataset: from Convolutional Neural Network to Visual Transformer and Performer. (arXiv:2106.01927v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Ao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Haoyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hechen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1">Peng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Weiming Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wanli Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1">Shuojia Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1">Marcin Grzegorzek</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01927">
                                    <div class="article-summary-box-inner">
                                        <span>Image classification has achieved unprecedented advance with the the rapid
development of deep learning. However, the classification of tiny object images
is still not well investigated. In this paper, we first briefly review the
development of Convolutional Neural Network and Visual Transformer in deep
learning, and introduce the sources and development of conventional noises and
adversarial attacks. Then we use various models of Convolutional Neural Network
and Visual Transformer to conduct a series of experiments on the image dataset
of tiny objects (sperms and impurities), and compare various evaluation metrics
in the experimental results to obtain a model with stable performance. Finally,
we discuss the problems in the classification of tiny objects and make a
prospect for the classification of tiny objects in the future.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style. (arXiv:2106.04619v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1">Julius von K&#xfc;gelgen</a>, <a href="http://arxiv.org/find/stat/1/au:+Sharma_Y/0/1/0/all/0/1">Yash Sharma</a>, <a href="http://arxiv.org/find/stat/1/au:+Gresele_L/0/1/0/all/0/1">Luigi Gresele</a>, <a href="http://arxiv.org/find/stat/1/au:+Brendel_W/0/1/0/all/0/1">Wieland Brendel</a>, <a href="http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/stat/1/au:+Besserve_M/0/1/0/all/0/1">Michel Besserve</a>, <a href="http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1">Francesco Locatello</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04619">
                                    <div class="article-summary-box-inner">
                                        <span>Self-supervised representation learning has shown remarkable success in a
number of domains. A common practice is to perform data augmentation via
hand-crafted transformations intended to leave the semantics of the data
invariant. We seek to understand the empirical success of this approach from a
theoretical perspective. We formulate the augmentation process as a latent
variable model by postulating a partition of the latent representation into a
content component, which is assumed invariant to augmentation, and a style
component, which is allowed to change. Unlike prior work on disentanglement and
independent component analysis, we allow for both nontrivial statistical and
causal dependencies in the latent space. We study the identifiability of the
latent representation based on pairs of views of the observations and prove
sufficient conditions that allow us to identify the invariant content partition
up to an invertible mapping in both generative and discriminative settings. We
find numerical simulations with dependent latent variables are consistent with
our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional,
visually complex images with rich causal dependencies, which we use to study
the effect of data augmentations performed in practice.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PAM: Understanding Product Images in Cross Product Category Attribute Extraction. (arXiv:2106.04630v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1">Rongmei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jie Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1">Nasser Zalmout</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Li Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xin Luna Dong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04630">
                                    <div class="article-summary-box-inner">
                                        <span>Understanding product attributes plays an important role in improving online
shopping experience for customers and serves as an integral part for
constructing a product knowledge graph. Most existing methods focus on
attribute extraction from text description or utilize visual information from
product images such as shape and color. Compared to the inputs considered in
prior works, a product image in fact contains more information, represented by
a rich mixture of words and visual clues with a layout carefully designed to
impress customers. This work proposes a more inclusive framework that fully
utilizes these different modalities for attribute extraction. Inspired by
recent works in visual question answering, we use a transformer based sequence
to sequence model to fuse representations of product text, Optical Character
Recognition (OCR) tokens and visual objects detected in the product image. The
framework is further extended with the capability to extract attribute value
across multiple product categories with a single model, by training the decoder
to predict both product category and attribute value and conditioning its
output on product category. The model provides a unified attribute extraction
solution desirable at an e-commerce platform that offers numerous product
categories with a diverse body of product attributes. We evaluated the model on
two product attributes, one with many possible values and one with a small set
of possible values, over 14 product categories and found the model could
achieve 15% gain on the Recall and 10% gain on the F1 score compared to
existing methods using text-only features.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. (arXiv:2106.04632v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1">Jie Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1">Zhe Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Licheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yen-Chun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pillai_R/0/1/0/all/0/1">Rohit Pillai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1">Yu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1">Luowei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Eric Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">William Yang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1">Tamara Lee Berg</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingjing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zicheng Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04632">
                                    <div class="article-summary-box-inner">
                                        <span>Most existing video-and-language (VidL) research focuses on a single dataset,
or multiple datasets of a single task. In reality, a truly useful VidL system
is expected to be easily generalizable to diverse tasks, domains, and datasets.
To facilitate the evaluation of such systems, we introduce Video-And-Language
Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets
over 3 popular tasks: (i) text-to-video retrieval; (ii) video question
answering; and (iii) video captioning. VALUE benchmark aims to cover a broad
range of video genres, video lengths, data volumes, and task difficulty levels.
Rather than focusing on single-channel videos with visual information only,
VALUE promotes models that leverage information from both video frames and
their associated subtitles, as well as models that share knowledge across
multiple tasks. We evaluate various baseline methods with and without
large-scale VidL pre-training, and systematically investigate the impact of
video input channels, fusion methods, and different video representations. We
also study the transferability between tasks, and conduct multi-task learning
under different settings. The significant gap between our best model and human
performance calls for future study for advanced VidL models. VALUE is available
at https://value-leaderboard.github.io/.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Is Space-Time Attention All You Need for Video Understanding?. (arXiv:2102.05095v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1">Gedas Bertasius</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Heng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1">Lorenzo Torresani</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05095">
                                    <div class="article-summary-box-inner">
                                        <span>We present a convolution-free approach to video classification built
exclusively on self-attention over space and time. Our method, named
&quot;TimeSformer,&quot; adapts the standard Transformer architecture to video by
enabling spatiotemporal feature learning directly from a sequence of
frame-level patches. Our experimental study compares different self-attention
schemes and suggests that &quot;divided attention,&quot; where temporal attention and
spatial attention are separately applied within each block, leads to the best
video classification accuracy among the design choices considered. Despite the
radically new design, TimeSformer achieves state-of-the-art results on several
action recognition benchmarks, including the best reported accuracy on
Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks,
our model is faster to train, it can achieve dramatically higher test
efficiency (at a small drop in accuracy), and it can also be applied to much
longer video clips (over one minute long). Code and models are available at:
https://github.com/facebookresearch/TimeSformer.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Neural Network Subspaces. (arXiv:2102.10472v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1">Mitchell Wortsman</a>, <a href="http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1">Maxwell Horton</a>, <a href="http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1">Carlos Guestrin</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1">Ali Farhadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1">Mohammad Rastegari</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10472">
                                    <div class="article-summary-box-inner">
                                        <span>Recent observations have advanced our understanding of the neural network
optimization landscape, revealing the existence of (1) paths of high accuracy
containing diverse solutions and (2) wider minima offering improved
performance. Previous methods observing diverse paths require multiple training
runs. In contrast we aim to leverage both property (1) and (2) with a single
method and in a single training run. With a similar computational cost as
training one model, we learn lines, curves, and simplexes of high-accuracy
neural networks. These neural network subspaces contain diverse solutions that
can be ensembled, approaching the ensemble performance of independently trained
networks without the training cost. Moreover, using the subspace midpoint
boosts accuracy, calibration, and robustness to label noise, outperforming
Stochastic Weight Averaging.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Enhance Convolutional Neural Networks with Noise Incentive Block. (arXiv:2012.12109v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Menghan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1">Chu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1">Tien-Tsin Wong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.12109">
                                    <div class="article-summary-box-inner">
                                        <span>As a generic modeling tool, Convolutional Neural Networks (CNNs) have been
widely employed in image generation and translation tasks. However, when fed
with a flat input, current CNN models may fail to generate vivid results due to
the spatially shared convolution kernels. We call it the flatness degradation
of CNNs. Unfortunately, such degradation is the greatest obstacles to generate
a spatially-variant output from a flat input, which has been barely discussed
in the previous literature. To tackle this problem, we propose a model agnostic
solution, i.e. Noise Incentive Block (NIB), which serves as a generic plug-in
for any CNN generation model. The key idea is to break the flat input condition
while keeping the intactness of the original information. Specifically, the NIB
perturbs the input data symmetrically with a noise map and reassembles them in
the feature domain as driven by the objective function. Extensive experiments
show that existing CNN models equipped with NIB survive from the flatness
degradation and are able to generate visually better results with richer
details in some specific image generation tasks given flat inputs, e.g.
semantic image synthesis, data-hidden image generation, and deep neural
dithering.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-supervision of Feature Transformation for Further Improving Supervised Learning. (arXiv:2106.04922v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zilin Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuhang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaomin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Ming Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04922">
                                    <div class="article-summary-box-inner">
                                        <span>Self-supervised learning, which benefits from automatically constructing
labels through pre-designed pretext task, has recently been applied for
strengthen supervised learning. Since previous self-supervised pretext tasks
are based on input, they may incur huge additional training overhead. In this
paper we find that features in CNNs can be also used for self-supervision. Thus
we creatively design the \emph{feature-based pretext task} which requires only
a small amount of additional training overhead. In our task we discard
different particular regions of features, and then train the model to
distinguish these different features. In order to fully apply our feature-based
pretext task in supervised learning, we also propose a novel learning framework
containing multi-classifiers for further improvement. Original labels will be
expanded to joint labels via self-supervision of feature transformations. With
more semantic information provided by our self-supervised tasks, this approach
can train CNNs more effectively. Extensive experiments on various supervised
learning tasks demonstrate the accuracy improvement and wide applicability of
our method.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">I Don&#x27;t Need $\mathbf{u}$: Identifiable Non-Linear ICA Without Side Information. (arXiv:2106.05238v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Willetts_M/0/1/0/all/0/1">Matthew Willetts</a>, <a href="http://arxiv.org/find/cs/1/au:+Paige_B/0/1/0/all/0/1">Brooks Paige</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05238">
                                    <div class="article-summary-box-inner">
                                        <span>In this work we introduce a new approach for identifiable non-linear ICA
models. Recently there has been a renaissance in identifiability results in
deep generative models, not least for non-linear ICA. These prior works,
however, have assumed access to a sufficiently-informative auxiliary set of
observations, denoted $\mathbf{u}$. We show here how identifiability can be
obtained in the absence of this side-information, rendering possible
fully-unsupervised identifiable non-linear ICA. While previous theoretical
results have established the impossibility of identifiable non-linear ICA in
the presence of infinitely-flexible universal function approximators, here we
rely on the intrinsically-finite modelling capacity of any particular chosen
parameterisation of a deep generative model. In particular, we focus on
generative models which perform clustering in their latent space -- a model
structure which matches previous identifiable models, but with the learnt
clustering providing a synthetic form of auxiliary information. We evaluate our
proposals using VAEs, on synthetic and image datasets, and find that the
learned clusterings function effectively: deep generative models with latent
clusterings are empirically identifiable, to the same degree as models which
rely on side information.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GaitGraph: Graph Convolutional Network for Skeleton-Based Gait Recognition. (arXiv:2101.11228v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Teepe_T/0/1/0/all/0/1">Torben Teepe</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1">Ali Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilg_J/0/1/0/all/0/1">Johannes Gilg</a>, <a href="http://arxiv.org/find/cs/1/au:+Herzog_F/0/1/0/all/0/1">Fabian Herzog</a>, <a href="http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1">Stefan H&#xf6;rmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1">Gerhard Rigoll</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11228">
                                    <div class="article-summary-box-inner">
                                        <span>Gait recognition is a promising video-based biometric for identifying
individual walking patterns from a long distance. At present, most gait
recognition methods use silhouette images to represent a person in each frame.
However, silhouette images can lose fine-grained spatial information, and most
papers do not regard how to obtain these silhouettes in complex scenes.
Furthermore, silhouette images contain not only gait features but also other
visual clues that can be recognized. Hence these approaches can not be
considered as strict gait recognition.

We leverage recent advances in human pose estimation to estimate robust
skeleton poses directly from RGB images to bring back model-based gait
recognition with a cleaner representation of gait. Thus, we propose GaitGraph
that combines skeleton poses with Graph Convolutional Network (GCN) to obtain a
modern model-based approach for gait recognition. The main advantages are a
cleaner, more elegant extraction of the gait features and the ability to
incorporate powerful spatio-temporal modeling using GCN. Experiments on the
popular CASIA-B gait dataset show that our method archives state-of-the-art
performance in model-based gait recognition.

The code and models are publicly available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Implicit field learning for unsupervised anomaly detection in medical images. (arXiv:2106.05214v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Marimont_S/0/1/0/all/0/1">Sergio Naval Marimont</a>, <a href="http://arxiv.org/find/eess/1/au:+Tarroni_G/0/1/0/all/0/1">Giacomo Tarroni</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05214">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a novel unsupervised out-of-distribution detection method for
medical images based on implicit fields image representations. In our approach,
an auto-decoder feed-forward neural network learns the distribution of healthy
images in the form of a mapping between spatial coordinates and probabilities
over a proxy for tissue types. At inference time, the learnt distribution is
used to retrieve, from a given test image, a restoration, i.e. an image
maximally consistent with the input one but belonging to the healthy
distribution. Anomalies are localized using the voxel-wise probability
predicted by our model for the restored image. We tested our approach in the
task of unsupervised localization of gliomas on brain MR images and compared it
to several other VAE-based anomaly detection methods. Results show that the
proposed technique substantially outperforms them (average DICE 0.640 vs 0.518
for the best performing VAE-based alternative) while also requiring
considerably less computing time.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1">Wang Yifan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1">Lukas Rahmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1">Olga Sorkine-Hornung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05187">
                                    <div class="article-summary-box-inner">
                                        <span>We present implicit displacement fields, a novel representation for detailed
3D geometry. Inspired by a classic surface deformation technique, displacement
mapping, our method represents a complex surface as a smooth base surface plus
a displacement along the base&#x27;s normal directions, resulting in a
frequency-based shape decomposition, where the high frequency signal is
constrained geometrically by the low frequency signal. Importantly, this
disentanglement is unsupervised thanks to a tailored architectural design that
has an innate frequency hierarchy by construction. We explore implicit
displacement field surface reconstruction and detail transfer and demonstrate
superior representational power, training stability and generalizability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PCNet: A Structure Similarity Enhancement Method for Multispectral and Multimodal Image Registration. (arXiv:2106.05124v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1">Si-Yuan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Hui-Liang Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1">Lun Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shu-Jie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chunguang Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05124">
                                    <div class="article-summary-box-inner">
                                        <span>Multispectral and multimodal image processing is important in the community
of computer vision and computational photography. As the acquired multispectral
and multimodal data are generally misaligned due to the alternation or movement
of the image device, the image registration procedure is necessary. The
registration of multispectral or multimodal image is challenging due to the
non-linear intensity and gradient variation. To cope with this challenge, we
propose the phase congruency network (PCNet), which is able to enhance the
structure similarity and alleviate the non-linear intensity and gradient
variation. The images can then be aligned using the similarity enhanced
features produced by the network. PCNet is constructed under the guidance of
the phase congruency prior. The network contains three trainable layers
accompany with the modified learnable Gabor kernels according to the phase
congruency theory. Thanks to the prior knowledge, PCNet is extremely
light-weight and can be trained on quite a small amount of multispectral data.
PCNet can be viewed to be fully convolutional and hence can take input of
arbitrary sizes. Once trained, PCNet is applicable on a variety of
multispectral and multimodal data such as RGB/NIR and flash/no-flash images
without additional further tuning. Experimental results validate that PCNet
outperforms current state-of-the-art registration algorithms, including the
deep-learning based ones that have the number of parameters hundreds times
compared to PCNet. Thanks to the similarity enhancement training, PCNet
outperforms the original phase congruency algorithm with two-thirds less
feature channels.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cervical Cytology Classification Using PCA &amp; GWO Enhanced Deep Features Selection. (arXiv:2106.04919v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Basak_H/0/1/0/all/0/1">Hritam Basak</a>, <a href="http://arxiv.org/find/cs/1/au:+Kundu_R/0/1/0/all/0/1">Rohit Kundu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1">Sukanta Chakraborty</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1">Nibaran Das</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04919">
                                    <div class="article-summary-box-inner">
                                        <span>Cervical cancer is one of the most deadly and common diseases among women
worldwide. It is completely curable if diagnosed in an early stage, but the
tedious and costly detection procedure makes it unviable to conduct
population-wise screening. Thus, to augment the effort of the clinicians, in
this paper, we propose a fully automated framework that utilizes Deep Learning
and feature selection using evolutionary optimization for cytology image
classification. The proposed framework extracts Deep feature from several
Convolution Neural Network models and uses a two-step feature reduction
approach to ensure reduction in computation cost and faster convergence. The
features extracted from the CNN models form a large feature space whose
dimensionality is reduced using Principal Component Analysis while preserving
99% of the variance. A non-redundant, optimal feature subset is selected from
this feature space using an evolutionary optimization algorithm, the Grey Wolf
Optimizer, thus improving the classification performance. Finally, the selected
feature subset is used to train an SVM classifier for generating the final
predictions. The proposed framework is evaluated on three publicly available
benchmark datasets: Mendeley Liquid Based Cytology (4-class) dataset, Herlev
Pap Smear (7-class) dataset, and the SIPaKMeD Pap Smear (5-class) dataset
achieving classification accuracies of 99.47%, 98.32% and 97.87% respectively,
thus justifying the reliability of the approach. The relevant codes for the
proposed approach can be found in:
https://github.com/DVLP-CMATERJU/Two-Step-Feature-Enhancement</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Explainable Abnormal Infant Movements Identification: A Body-part Based Prediction and Visualisation Framework. (arXiv:2106.04966v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+McCay_K/0/1/0/all/0/1">Kevin D. McCay</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1">Edmond S. L. Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakkos_D/0/1/0/all/0/1">Dimitrios Sakkos</a>, <a href="http://arxiv.org/find/cs/1/au:+Woo_W/0/1/0/all/0/1">Wai Lok Woo</a>, <a href="http://arxiv.org/find/cs/1/au:+Marcroft_C/0/1/0/all/0/1">Claire Marcroft</a>, <a href="http://arxiv.org/find/cs/1/au:+Dulson_P/0/1/0/all/0/1">Patricia Dulson</a>, <a href="http://arxiv.org/find/cs/1/au:+Embleton_N/0/1/0/all/0/1">Nicholas D. Embleton</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04966">
                                    <div class="article-summary-box-inner">
                                        <span>Providing early diagnosis of cerebral palsy (CP) is key to enhancing the
developmental outcomes for those affected. Diagnostic tools such as the General
Movements Assessment (GMA), have produced promising results in early diagnosis,
however these manual methods can be laborious.

In this paper, we propose a new framework for the automated classification of
infant body movements, based upon the GMA, which unlike previous methods, also
incorporates a visualization framework to aid with interpretability. Our
proposed framework segments extracted features to detect the presence of
Fidgety Movements (FMs) associated with the GMA spatiotemporally. These
features are then used to identify the body-parts with the greatest
contribution towards a classification decision and highlight the related
body-part segment providing visual feedback to the user.

We quantitatively compare the proposed framework&#x27;s classification performance
with several other methods from the literature and qualitatively evaluate the
visualization&#x27;s veracity. Our experimental results show that the proposed
method performs more robustly than comparable techniques in this setting whilst
simultaneously providing relevant visual interpretability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CLCC: Contrastive Learning for Color Constancy. (arXiv:2106.04989v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lo_Y/0/1/0/all/0/1">Yi-Chen Lo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Chia-Che Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1">Hsuan-Chao Chiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yu-Hao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chia-Ping Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1">Yu-Lin Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jou_K/0/1/0/all/0/1">Kevin Jou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04989">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we present CLCC, a novel contrastive learning framework for
color constancy. Contrastive learning has been applied for learning
high-quality visual representations for image classification. One key aspect to
yield useful representations for image classification is to design illuminant
invariant augmentations. However, the illuminant invariant assumption conflicts
with the nature of the color constancy task, which aims to estimate the
illuminant given a raw image. Therefore, we construct effective contrastive
pairs for learning better illuminant-dependent features via a novel raw-domain
color augmentation. On the NUS-8 dataset, our method provides $17.5\%$ relative
improvements over a strong baseline, reaching state-of-the-art performance
without increasing model complexity. Furthermore, our method achieves
competitive performance on the Gehler dataset with $3\times$ fewer parameters
compared to top-ranking deep learning methods. More importantly, we show that
our model is more robust to different scenes under close proximity of
illuminants, significantly reducing $28.7\%$ worst-case error in data-sparse
regions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoqing Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1">Yan Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1">Jiansheng Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1">Yanwu Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1">Risa Higashita</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1">Jiang Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04830">
                                    <div class="article-summary-box-inner">
                                        <span>Cataract is one of the leading causes of reversible visual impairment and
blindness globally. Over the years, researchers have achieved significant
progress in developing state-of-the-art artificial intelligence techniques for
automatic cataract classification and grading, helping clinicians prevent and
treat cataract in time. This paper provides a comprehensive survey of recent
advances in machine learning for cataract classification and grading based on
ophthalmic images. We summarize existing literature from two research
directions: conventional machine learning techniques and deep learning
techniques. This paper also provides insights into existing works of both
merits and limitations. In addition, we discuss several challenges of automatic
cataract classification and grading based on machine learning techniques and
present possible solutions to these challenges for future research.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Gaussian Mixture Estimation from Weighted Samples. (arXiv:2106.05109v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Frisch_D/0/1/0/all/0/1">Daniel Frisch</a>, <a href="http://arxiv.org/find/stat/1/au:+Hanebeck_U/0/1/0/all/0/1">Uwe D. Hanebeck</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05109">
                                    <div class="article-summary-box-inner">
                                        <span>We consider estimating the parameters of a Gaussian mixture density with a
given number of components best representing a given set of weighted samples.
We adopt a density interpretation of the samples by viewing them as a discrete
Dirac mixture density over a continuous domain with weighted components. Hence,
Gaussian mixture fitting is viewed as density re-approximation. In order to
speed up computation, an expectation-maximization method is proposed that
properly considers not only the sample locations, but also the corresponding
weights. It is shown that methods from literature do not treat the weights
correctly, resulting in wrong estimates. This is demonstrated with simple
counterexamples. The proposed method works in any number of dimensions with the
same computational load as standard Gaussian mixture estimators for unweighted
samples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">NeRF in detail: Learning to sample for view synthesis. (arXiv:2106.05264v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_R/0/1/0/all/0/1">Relja Arandjelovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1">Andrew Zisserman</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05264">
                                    <div class="article-summary-box-inner">
                                        <span>Neural radiance fields (NeRF) methods have demonstrated impressive novel view
synthesis performance. The core approach is to render individual rays by
querying a neural network at points sampled along the ray to obtain the density
and colour of the sampled points, and integrating this information using the
rendering equation. Since dense sampling is computationally prohibitive, a
common solution is to perform coarse-to-fine sampling.

In this work we address a clear limitation of the vanilla coarse-to-fine
approach -- that it is based on a heuristic and not trained end-to-end for the
task at hand. We introduce a differentiable module that learns to propose
samples and their importance for the fine network, and consider and compare
multiple alternatives for its neural architecture. Training the proposal module
from scratch can be unstable due to lack of supervision, so an effective
pre-training strategy is also put forward. The approach, named &#x60;NeRF in detail&#x27;
(NeRF-ID), achieves superior view synthesis quality over NeRF and the
state-of-the-art on the synthetic Blender benchmark and on par or better
performance on the real LLFF-NeRF scenes. Furthermore, by leveraging the
predicted sample importance, a 25% saving in computation can be achieved
without significantly sacrificing the rendering quality.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Densely connected multidilated convolutional networks for dense prediction tasks. (arXiv:2011.11844v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Takahashi_N/0/1/0/all/0/1">Naoya Takahashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1">Yuki Mitsufuji</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11844">
                                    <div class="article-summary-box-inner">
                                        <span>Tasks that involve high-resolution dense prediction require a modeling of
both local and global patterns in a large input field. Although the local and
global structures often depend on each other and their simultaneous modeling is
important, many convolutional neural network (CNN)-based approaches interchange
representations in different resolutions only a few times. In this paper, we
claim the importance of a dense simultaneous modeling of multiresolution
representation and propose a novel CNN architecture called densely connected
multidilated DenseNet (D3Net). D3Net involves a novel multidilated convolution
that has different dilation factors in a single layer to model different
resolutions simultaneously. By combining the multidilated convolution with the
DenseNet architecture, D3Net incorporates multiresolution learning with an
exponentially growing receptive field in almost all layers, while avoiding the
aliasing problem that occurs when we naively incorporate the dilated
convolution in DenseNet. Experiments on the image semantic segmentation task
using Cityscapes and the audio source separation task using MUSDB18 show that
the proposed method has superior performance over state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Rethinking Class Relations: Absolute-relative Supervised and Unsupervised Few-shot Learning. (arXiv:2001.03919v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongguang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1">Piotr Koniusz</a>, <a href="http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1">Songlei Jian</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongdong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip H. S. Torr</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.03919">
                                    <div class="article-summary-box-inner">
                                        <span>The majority of existing few-shot learning methods describe image relations
with binary labels. However, such binary relations are insufficient to teach
the network complicated real-world relations, due to the lack of decision
smoothness. Furthermore, current few-shot learning models capture only the
similarity via relation labels, but they are not exposed to class concepts
associated with objects, which is likely detrimental to the classification
performance due to underutilization of the available class labels. To
paraphrase, children learn the concept of tiger from a few of actual examples
as well as from comparisons of tiger to other animals. Thus, we hypothesize
that in fact both similarity and class concept learning must be occurring
simultaneously. With these observations at hand, we study the fundamental
problem of simplistic class modeling in current few-shot learning methods. We
rethink the relations between class concepts, and propose a novel
Absolute-relative Learning paradigm to fully take advantage of label
information to refine the image representations and correct the relation
understanding in both supervised and unsupervised scenarios. Our proposed
paradigm improves the performance of several the state-of-the-art models on
publicly available datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition. (arXiv:2007.01755v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1">Bin-Bin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong-Yu Zhou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01755">
                                    <div class="article-summary-box-inner">
                                        <span>Multi-label image recognition is a practical and challenging task compared to
single-label image classification. However, previous works may be suboptimal
because of a great number of object proposals or complex attentional region
generation modules. In this paper, we propose a simple but efficient two-stream
framework to recognize multi-category objects from global image to local
regions, similar to how human beings perceive objects. To bridge the gap
between global and local streams, we propose a multi-class attentional region
module which aims to make the number of attentional regions as small as
possible and keep the diversity of these regions as high as possible. Our
method can efficiently and effectively recognize multi-class objects with an
affordable computation cost and a parameter-free region localization module.
Over three benchmarks on multi-label image classification, we create new
state-of-the-art results with a single model only using image semantics without
label dependency. In addition, the effectiveness of the proposed method is
extensively demonstrated under different factors such as global pooling
strategy, input size and network architecture. Code has been made available
at~\url{https://github.com/gaobb/MCAR}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Training Stronger Video Vision Transformers for EPIC-KITCHENS-100 Action Recognition. (arXiv:2106.05058v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Ziyuan Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1">Zhiwu Qing</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yutong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shiwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jianwen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1">Zhurong Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1">Mingqian Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1">Nong Sang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1">Marcelo H. Ang Jr</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05058">
                                    <div class="article-summary-box-inner">
                                        <span>With the recent surge in the research of vision transformers, they have
demonstrated remarkable potential for various challenging computer vision
applications, such as image recognition, point cloud classification as well as
video understanding. In this paper, we present empirical results for training a
stronger video vision transformer on the EPIC-KITCHENS-100 Action Recognition
dataset. Specifically, we explore training techniques for video vision
transformers, such as augmentations, resolutions as well as initialization,
etc. With our training recipe, a single ViViT model achieves the performance of
47.4\% on the validation set of EPIC-KITCHENS-100 dataset, outperforming what
is reported in the original paper by 3.4%. We found that video transformers are
especially good at predicting the noun in the verb-noun action prediction task.
This makes the overall action prediction accuracy of video transformers notably
higher than convolutional ones. Surprisingly, even the best video transformers
underperform the convolutional networks on the verb prediction. Therefore, we
combine the video vision transformers and some of the convolutional video
networks and present our solution to the EPIC-KITCHENS-100 Action Recognition
competition.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Affordance Transfer Learning for Human-Object Interaction Detection. (arXiv:2104.02867v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1">Zhi Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Baosheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1">Xiaojiang Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02867">
                                    <div class="article-summary-box-inner">
                                        <span>Reasoning the human-object interactions (HOI) is essential for deeper scene
understanding, while object affordances (or functionalities) are of great
importance for human to discover unseen HOIs with novel objects. Inspired by
this, we introduce an affordance transfer learning approach to jointly detect
HOIs with novel objects and recognize affordances. Specifically, HOI
representations can be decoupled into a combination of affordance and object
representations, making it possible to compose novel interactions by combining
affordance representations and novel object representations from additional
images, i.e. transferring the affordance to novel objects. With the proposed
affordance transfer learning, the model is also capable of inferring the
affordances of novel objects from known affordance representations. The
proposed method can thus be used to 1) improve the performance of HOI
detection, especially for the HOIs with unseen objects; and 2) infer the
affordances of novel objects. Experimental results on two datasets, HICO-DET
and HOI-COCO (from V-COCO), demonstrate significant improvements over recent
state-of-the-art methods for HOI detection and object affordance detection.
Code is available at https://github.com/zhihou7/HOI-CL</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Benchmarking Representation Learning for Natural World Image Collections. (arXiv:2103.16483v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Horn_G/0/1/0/all/0/1">Grant Van Horn</a>, <a href="http://arxiv.org/find/cs/1/au:+Cole_E/0/1/0/all/0/1">Elijah Cole</a>, <a href="http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1">Sara Beery</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1">Kimberly Wilber</a>, <a href="http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1">Serge Belongie</a>, <a href="http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1">Oisin Mac Aodha</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16483">
                                    <div class="article-summary-box-inner">
                                        <span>Recent progress in self-supervised learning has resulted in models that are
capable of extracting rich representations from image collections without
requiring any explicit label supervision. However, to date the vast majority of
these approaches have restricted themselves to training on standard benchmark
datasets such as ImageNet. We argue that fine-grained visual categorization
problems, such as plant and animal species classification, provide an
informative testbed for self-supervised learning. In order to facilitate
progress in this area we present two new natural world visual classification
datasets, iNat2021 and NeWT. The former consists of 2.7M images from 10k
different species uploaded by users of the citizen science application
iNaturalist. We designed the latter, NeWT, in collaboration with domain experts
with the aim of benchmarking the performance of representation learning
algorithms on a suite of challenging natural world binary classification tasks
that go beyond standard species classification. These two new datasets allow us
to explore questions related to large-scale representation and transfer
learning in the context of fine-grained categories. We provide a comprehensive
analysis of feature extractors trained with and without supervision on ImageNet
and iNat2021, shedding light on the strengths and weaknesses of different
learned features across a diverse set of tasks. We find that features produced
by standard supervised methods still outperform those produced by
self-supervised approaches such as SimCLR. However, improved self-supervised
learning methods are constantly being released and the iNat2021 and NeWT
datasets are a valuable resource for tracking their progress.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Continuous Learning and Adaptation with Membrane Potential and Activation Threshold Homeostasis. (arXiv:2104.10851v3 [cs.NE] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hadjiivanov_A/0/1/0/all/0/1">Alexander Hadjiivanov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10851">
                                    <div class="article-summary-box-inner">
                                        <span>Most classical (non-spiking) neural network models disregard internal neuron
dynamics and treat neurons as simple input integrators. However, biological
neurons have an internal state governed by complex dynamics that plays a
crucial role in learning, adaptation and the overall network activity and
behaviour. This paper presents the Membrane Potential and Activation Threshold
Homeostasis (MPATH) neuron model, which combines several biologically inspired
mechanisms to efficiently simulate internal neuron dynamics with a single
parameter analogous to the membrane time constant in biological neurons. The
model allows neurons to maintain a form of dynamic equilibrium by automatically
regulating their activity when presented with fluctuating input. One
consequence of the MPATH model is that it imbues neurons with a sense of time
without recurrent connections, paving the way for modelling processes that
depend on temporal aspects of neuron activity. Experiments demonstrate the
model&#x27;s ability to adapt to and continually learn from its input.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robustness in Compressed Neural Networks for Object Detection. (arXiv:2102.05509v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1">Sebastian Cygert</a>, <a href="http://arxiv.org/find/cs/1/au:+Czyzewski_A/0/1/0/all/0/1">Andrzej Czy&#x17c;ewski</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05509">
                                    <div class="article-summary-box-inner">
                                        <span>Model compression techniques allow to significantly reduce the computational
cost associated with data processing by deep neural networks with only a minor
decrease in average accuracy. Simultaneously, reducing the model size may have
a large effect on noisy cases or objects belonging to less frequent classes. It
is a crucial problem from the perspective of the models&#x27; safety, especially for
object detection in the autonomous driving setting, which is considered in this
work. It was shown in the paper that the sensitivity of compressed models to
different distortion types is nuanced, and some of the corruptions are heavily
impacted by the compression methods (i.e., additive noise), while others (blur
effect) are only slightly affected. A common way to improve the robustness of
models is to use data augmentation, which was confirmed to positively affect
models&#x27; robustness, also for highly compressed models. It was further shown
that while data imbalance methods brought only a slight increase in accuracy
for the baseline model (without compression), the impact was more striking at
higher compression rates for the structured pruning. Finally, methods for
handling data imbalance brought a significant improvement of the pruned models&#x27;
worst-detected class accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bayesian Triplet Loss: Uncertainty Quantification in Image Retrieval. (arXiv:2011.12663v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Warburg_F/0/1/0/all/0/1">Frederik Warburg</a>, <a href="http://arxiv.org/find/cs/1/au:+Jorgensen_M/0/1/0/all/0/1">Martin J&#xf8;rgensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1">Javier Civera</a>, <a href="http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1">S&#xf8;ren Hauberg</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12663">
                                    <div class="article-summary-box-inner">
                                        <span>Uncertainty quantification in image retrieval is crucial for downstream
decisions, yet it remains a challenging and largely unexplored problem. Current
methods for estimating uncertainties are poorly calibrated, computationally
expensive, or based on heuristics. We present a new method that views image
embeddings as stochastic features rather than deterministic features. Our two
main contributions are (1) a likelihood that matches the triplet constraint and
that evaluates the probability of an anchor being closer to a positive than a
negative; and (2) a prior over the feature space that justifies the
conventional l2 normalization. To ensure computational efficiency, we derive a
variational approximation of the posterior, called the Bayesian triplet loss,
that produces state-of-the-art uncertainty estimates and matches the predictive
performance of current state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Monocular 3D Object Detection with Decoupled Structured Polygon Estimation and Height-Guided Depth Estimation. (arXiv:2002.01619v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yingjie Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Buyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1">Zeyu Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xingyu Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaogang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.01619">
                                    <div class="article-summary-box-inner">
                                        <span>Monocular 3D object detection task aims to predict the 3D bounding boxes of
objects based on monocular RGB images. Since the location recovery in 3D space
is quite difficult on account of absence of depth information, this paper
proposes a novel unified framework which decomposes the detection problem into
a structured polygon prediction task and a depth recovery task. Different from
the widely studied 2D bounding boxes, the proposed novel structured polygon in
the 2D image consists of several projected surfaces of the target object.
Compared to the widely-used 3D bounding box proposals, it is shown to be a
better representation for 3D detection. In order to inversely project the
predicted 2D structured polygon to a cuboid in the 3D physical world, the
following depth recovery task uses the object height prior to complete the
inverse projection transformation with the given camera projection matrix.
Moreover, a fine-grained 3D box refinement scheme is proposed to further
rectify the 3D detection results. Experiments are conducted on the challenging
KITTI benchmark, in which our method achieves state-of-the-art detection
accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-Facet Clustering Variational Autoencoders. (arXiv:2106.05241v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Falck_F/0/1/0/all/0/1">Fabian Falck</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1">Haoting Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Willetts_M/0/1/0/all/0/1">Matthew Willetts</a>, <a href="http://arxiv.org/find/stat/1/au:+Nicholson_G/0/1/0/all/0/1">George Nicholson</a>, <a href="http://arxiv.org/find/stat/1/au:+Yau_C/0/1/0/all/0/1">Christopher Yau</a>, <a href="http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1">Christopher C Holmes</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05241">
                                    <div class="article-summary-box-inner">
                                        <span>Work in deep clustering focuses on finding a single partition of data.
However, high-dimensional data, such as images, typically feature multiple
interesting characteristics one could cluster over. For example, images of
objects against a background could be clustered over the shape of the object
and separately by the colour of the background. In this paper, we introduce
Multi-Facet Clustering Variational Autoencoders (MFCVAE), a novel class of
variational autoencoders with a hierarchy of latent variables, each with a
Mixture-of-Gaussians prior, that learns multiple clusterings simultaneously,
and is trained fully unsupervised and end-to-end. MFCVAE uses a
progressively-trained ladder architecture which leads to highly stable
performance. We provide novel theoretical results for optimising the ELBO
analytically with respect to the categorical variational posterior
distribution, and corrects earlier influential theoretical work. On image
benchmarks, we demonstrate that our approach separates out and clusters over
different aspects of the data in a disentangled manner. We also show other
advantages of our model: the compositionality of its latent space and that it
provides controlled generation of samples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning to Generate Noise for Multi-Attack Robustness. (arXiv:2006.12135v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1">Divyam Madaan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1">Jinwoo Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12135">
                                    <div class="article-summary-box-inner">
                                        <span>Adversarial learning has emerged as one of the successful techniques to
circumvent the susceptibility of existing methods against adversarial
perturbations. However, the majority of existing defense methods are tailored
to defend against a single category of adversarial perturbation (e.g.
$\ell_\infty$-attack). In safety-critical applications, this makes these
methods extraneous as the attacker can adopt diverse adversaries to deceive the
system. Moreover, training on multiple perturbations simultaneously
significantly increases the computational overhead during training. To address
these challenges, we propose a novel meta-learning framework that explicitly
learns to generate noise to improve the model&#x27;s robustness against multiple
types of attacks. Its key component is Meta Noise Generator (MNG) that outputs
optimal noise to stochastically perturb a given sample, such that it helps
lower the error on diverse adversarial perturbations. By utilizing samples
generated by MNG, we train a model by enforcing the label consistency across
multiple perturbations. We validate the robustness of models trained by our
scheme on various datasets and against a wide variety of perturbations,
demonstrating that it significantly outperforms the baselines across multiple
perturbations with a marginal computational cost.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">We Can Always Catch You: Detecting Adversarial Patched Objects WITH or WITHOUT Signature. (arXiv:2106.05261v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1">Bin Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiachun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jianjun Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05261">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, the object detection based on deep learning has proven to be
vulnerable to adversarial patch attacks. The attackers holding a specially
crafted patch can hide themselves from the state-of-the-art person detectors,
e.g., YOLO, even in the physical world. This kind of attack can bring serious
security threats, such as escaping from surveillance cameras. In this paper, we
deeply explore the detection problems about the adversarial patch attacks to
the object detection. First, we identify a leverageable signature of existing
adversarial patches from the point of the visualization explanation. A fast
signature-based defense method is proposed and demonstrated to be effective.
Second, we design an improved patch generation algorithm to reveal the risk
that the signature-based way may be bypassed by the techniques emerging in the
future. The newly generated adversarial patches can successfully evade the
proposed signature-based defense. Finally, we present a novel
signature-independent detection method based on the internal content semantics
consistency rather than any attack-specific prior knowledge. The fundamental
intuition is that the adversarial object can appear locally but disappear
globally in an input image. The experiments demonstrate that the
signature-independent method can effectively detect the existing and improved
attacks. It has also proven to be a general method by detecting unforeseen and
even other types of attacks without any attack-specific prior knowledge. The
two proposed detection methods can be adopted in different scenarios, and we
believe that combining them can offer a comprehensive protection.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CARPe Posterum: A Convolutional Approach for Real-time Pedestrian Path Prediction. (arXiv:2005.12469v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1">Mat&#xed;as Mendieta</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1">Hamed Tabkhi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.12469">
                                    <div class="article-summary-box-inner">
                                        <span>Pedestrian path prediction is an essential topic in computer vision and video
understanding. Having insight into the movement of pedestrians is crucial for
ensuring safe operation in a variety of applications including autonomous
vehicles, social robots, and environmental monitoring. Current works in this
area utilize complex generative or recurrent methods to capture many possible
futures. However, despite the inherent real-time nature of predicting future
paths, little work has been done to explore accurate and computationally
efficient approaches for this task. To this end, we propose a convolutional
approach for real-time pedestrian path prediction, CARPe. It utilizes a
variation of Graph Isomorphism Networks in combination with an agile
convolutional neural network design to form a fast and accurate path prediction
approach. Notable results in both inference speed and prediction accuracy are
achieved, improving FPS considerably in comparison to current state-of-the-art
methods while delivering competitive accuracy on well-known path prediction
datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Is it Enough to Optimize CNN Architectures on ImageNet?. (arXiv:2103.09108v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tuggener_L/0/1/0/all/0/1">Lukas Tuggener</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1">J&#xfc;rgen Schmidhuber</a>, <a href="http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1">Thilo Stadelmann</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09108">
                                    <div class="article-summary-box-inner">
                                        <span>An implicit but pervasive hypothesis of modern computer vision research is
that convolutional neural network (CNN) architectures that perform better on
ImageNet will also perform better on other vision datasets. We challenge this
hypothesis through an extensive empirical study for which we train 500 sampled
CNN architectures on ImageNet as well as 8 other image classification datasets
from a wide array of application domains. The relationship between architecture
and performance varies wildly, depending on the datasets. For some of them, the
performance correlation with ImageNet is even negative. Clearly, it is not
enough to optimize architectures solely for ImageNet when aiming for progress
that is relevant for all applications. Therefore, we identify two
dataset-specific performance indicators: the cumulative width across layers as
well as the total depth of the network. Lastly, we show that the range of
dataset variability covered by ImageNet can be significantly extended by adding
ImageNet subsets restricted to few classes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Boosting Adversarial Attacks on Neural Networks with Better Optimizer. (arXiv:2012.00567v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Heng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hengwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_R/0/1/0/all/0/1">Ruiyu Dou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.00567">
                                    <div class="article-summary-box-inner">
                                        <span>Convolutional neural networks have outperformed humans in image recognition
tasks, but they remain vulnerable to attacks from adversarial examples. Since
these data are crafted by adding imperceptible noise to normal images, their
existence poses potential security threats to deep learning systems.
Sophisticated adversarial examples with strong attack performance can also be
used as a tool to evaluate the robustness of a model. However, the success rate
of adversarial attacks can be further improved in black-box environments.
Therefore, this study combines a modified Adam gradient descent algorithm with
the iterative gradient-based attack method. The proposed Adam Iterative Fast
Gradient Method is then used to improve the transferability of adversarial
examples. Extensive experiments on ImageNet showed that the proposed method
offers a higher attack success rate than existing iterative methods. By
extending our method, we achieved a state-of-the-art attack success rate of
95.0% on defense models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in Time. (arXiv:2106.05266v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shaowei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1">Hanwen Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiarui Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sifei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05266">
                                    <div class="article-summary-box-inner">
                                        <span>Estimating 3D hand and object pose from a single image is an extremely
challenging problem: hands and objects are often self-occluded during
interactions, and the 3D annotations are scarce as even humans cannot directly
label the ground-truths from a single image perfectly. To tackle these
challenges, we propose a unified framework for estimating the 3D hand and
object poses with semi-supervised learning. We build a joint learning framework
where we perform explicit contextual reasoning between hand and object
representations by a Transformer. Going beyond limited 3D annotations in a
single image, we leverage the spatial-temporal consistency in large-scale
hand-object videos as a constraint for generating pseudo labels in
semi-supervised learning. Our method not only improves hand pose estimation in
challenging real-world dataset, but also substantially improve the object pose
which has fewer ground-truths per instance. By training with large-scale
diverse videos, our model also generalizes better across multiple out-of-domain
datasets. Project page and code: https://stevenlsw.github.io/Semi-Hand-Object</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Generative Models as a Data Source for Multiview Representation Learning. (arXiv:2106.05258v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jahanian_A/0/1/0/all/0/1">Ali Jahanian</a>, <a href="http://arxiv.org/find/cs/1/au:+Puig_X/0/1/0/all/0/1">Xavier Puig</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yonglong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1">Phillip Isola</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05258">
                                    <div class="article-summary-box-inner">
                                        <span>Generative models are now capable of producing highly realistic images that
look nearly indistinguishable from the data on which they are trained. This
raises the question: if we have good enough generative models, do we still need
datasets? We investigate this question in the setting of learning
general-purpose visual representations from a black-box generative model rather
than directly from data. Given an off-the-shelf image generator without any
access to its training data, we train representations from the samples output
by this generator. We compare several representation learning methods that can
be applied to this setting, using the latent space of the generator to generate
multiple &quot;views&quot; of the same semantic content. We show that for contrastive
methods, this multiview data can naturally be used to identify positive pairs
(nearby in latent space) and negative pairs (far apart in latent space). We
find that the resulting representations rival those learned directly from real
data, but that good performance requires care in the sampling strategy applied
and the training method. Generative models can be viewed as a compressed and
organized copy of a dataset, and we envision a future where more and more
&quot;model zoos&quot; proliferate while datasets become increasingly unwieldy, missing,
or private. This paper suggests several techniques for dealing with visual
representation learning in such a future. Code is released on our project page:
https://ali-design.github.io/GenRep/</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial Evaluation of Multimodal Models under Realistic Gray Box Assumption. (arXiv:2011.12902v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1">Ivan Evtimov</a>, <a href="http://arxiv.org/find/cs/1/au:+Howes_R/0/1/0/all/0/1">Russel Howes</a>, <a href="http://arxiv.org/find/cs/1/au:+Dolhansky_B/0/1/0/all/0/1">Brian Dolhansky</a>, <a href="http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1">Hamed Firooz</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1">Cristian Canton Ferrer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12902">
                                    <div class="article-summary-box-inner">
                                        <span>This work examines the vulnerability of multimodal (image + text) models to
adversarial threats similar to those discussed in previous literature on
unimodal (image- or text-only) models. We introduce realistic assumptions of
partial model knowledge and access, and discuss how these assumptions differ
from the standard &quot;black-box&quot;/&quot;white-box&quot; dichotomy common in current
literature on adversarial attacks. Working under various levels of these
&quot;gray-box&quot; assumptions, we develop new attack methodologies unique to
multimodal classification and evaluate them on the Hateful Memes Challenge
classification task. We find that attacking multiple modalities yields stronger
attacks than unimodal attacks alone (inducing errors in up to 73% of cases),
and that the unimodal image attacks on multimodal classifiers we explored were
stronger than character-based text augmentation attacks (inducing errors on
average in 45% and 30% of cases, respectively).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An Efficient Point of Gaze Estimator for Low-Resolution Imaging Systems Using Extracted Ocular Features Based Neural Architecture. (arXiv:2106.05106v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sahay_A/0/1/0/all/0/1">Atul Sahay</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_I/0/1/0/all/0/1">Imon Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Arya_K/0/1/0/all/0/1">Kavi Arya</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05106">
                                    <div class="article-summary-box-inner">
                                        <span>A user&#x27;s eyes provide means for Human Computer Interaction (HCI) research as
an important modal. The time to time scientific explorations of the eye has
already seen an upsurge of the benefits in HCI applications from gaze
estimation to the measure of attentiveness of a user looking at a screen for a
given time period. The eye tracking system as an assisting, interactive tool
can be incorporated by physically disabled individuals, fitted best for those
who have eyes as only a limited set of communication. The threefold objective
of this paper is - 1. To introduce a neural network based architecture to
predict users&#x27; gaze at 9 positions displayed in the 11.31{\deg} visual range on
the screen, through a low resolution based system such as a webcam in real time
by learning various aspects of eyes as an ocular feature set. 2.A collection of
coarsely supervised feature set obtained in real time which is also validated
through the user case study presented in the paper for 21 individuals ( 17 men
and 4 women ) from whom a 35k set of instances was derived with an accuracy
score of 82.36% and f1_score of 82.2% and 3.A detailed study over applicability
and underlying challenges of such systems. The experimental results verify the
feasibility and validity of the proposed eye gaze tracking model.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Understanding Neural Networks and Individual Neuron Importance via Information-Ordered Cumulative Ablation. (arXiv:1804.06679v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1">Rana Ali Amjad</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kairen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiger_B/0/1/0/all/0/1">Bernhard C. Geiger</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.06679">
                                    <div class="article-summary-box-inner">
                                        <span>In this work, we investigate the use of three information-theoretic
quantities -- entropy, mutual information with the class variable, and a class
selectivity measure based on Kullback-Leibler divergence -- to understand and
study the behavior of already trained fully-connected feed-forward neural
networks. We analyze the connection between these information-theoretic
quantities and classification performance on the test set by cumulatively
ablating neurons in networks trained on MNIST, FashionMNIST, and CIFAR-10. Our
results parallel those recently published by Morcos et al., indicating that
class selectivity is not a good indicator for classification performance.
However, looking at individual layers separately, both mutual information and
class selectivity are positively correlated with classification performance, at
least for networks with ReLU activation functions. We provide explanations for
this phenomenon and conclude that it is ill-advised to compare the proposed
information-theoretic quantities across layers. Furthermore, we show that
cumulative ablation of neurons with ascending or descending
information-theoretic quantities can be used to formulate hypotheses regarding
the joint behavior of multiple neurons, such as redundancy and synergy, with
comparably low computational cost. We also draw connections to the information
bottleneck theory for neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images. (arXiv:2106.00116v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1">Mehdi Cherti</a>, <a href="http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1">Jenia Jitsev</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00116">
                                    <div class="article-summary-box-inner">
                                        <span>Transfer learning aims to exploit pre-trained models for more efficient
follow-up training on wide range of downstream tasks and datasets, enabling
successful training also on small data. Recent line of work posits strong
benefits for model generalization and transfer when model size, data size, and
compute budget are increased for the pre-training. It remains however still
largely unclear whether the observed transfer improvement due to increase in
scale also holds when source and target data distributions are far apart from
each other. In this work we conduct large-scale pre-training on large source
datasets of either natural (ImageNet-21k/1k) or medical chest X-Ray images and
compare full and few-shot transfer using different target datasets from both
natural and medical imaging domains. Our observations provide evidence that
while pre-training and transfer on closely related datasets do show clear
benefit of increasing model and data size during pre-training, such benefits
are not clearly visible when source and target datasets are further apart.
These observations hold across both full and few-shot transfer and indicate
that scaling laws pointing to improvement of generalization and transfer with
increasing model and data size are incomplete and should be revised by taking
into account the type and proximity of the source and target data, to correctly
predict the effect of model and data scale during pre-training on transfer.
Remarkably, in full shot transfer to a large X-Ray chest imaging target
(PadChest), the largest model pre-trained on ImageNet-21k slightly outperforms
best models pre-trained on large X-Ray chest imaging data. This indicates
possibility to obtain high quality models for domain-specific transfer even
without access to large domain-specific data, by pre-training instead on
comparably very large, generic source data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for Investigating Learned Representations. (arXiv:1905.13308v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Livezey_J/0/1/0/all/0/1">Jesse A. Livezey</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_A/0/1/0/all/0/1">Ahyeon Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_J/0/1/0/all/0/1">Jacob Yeung</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1">Kristofer E. Bouchard</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.13308">
                                    <div class="article-summary-box-inner">
                                        <span>Hierarchy and compositionality are common latent properties in many natural
and scientific datasets. Determining when a deep network&#x27;s hidden activations
represent hierarchy and compositionality is important both for understanding
deep representation learning and for applying deep networks in domains where
interpretability is crucial. However, current benchmark machine learning
datasets either have little hierarchical or compositional structure, or the
structure is not known. This gap impedes precise analysis of a network&#x27;s
representations and thus hinders development of new methods that can learn such
properties. To address this gap, we developed a new benchmark dataset with
known hierarchical and compositional structure. The Hangul Fonts Dataset (HFD)
is comprised of 35 fonts from the Korean writing system (Hangul), each with
11,172 blocks (syllables) composed from the product of initial consonant,
medial vowel, and final consonant glyphs. All blocks can be grouped into a few
geometric types which induces a hierarchy across blocks. In addition, each
block is composed of individual glyphs with rotations, translations, scalings,
and naturalistic style variation across fonts. We find that both shallow and
deep unsupervised methods only show modest evidence of hierarchy and
compositionality in their representations of the HFD compared to supervised
deep networks. Supervised deep network representations contain structure
related to the geometrical hierarchy of the characters, but the compositional
structure of the data is not evident. Thus, HFD enables the identification of
shortcomings in existing methods, a critical first step toward developing new
machine learning algorithms to extract hierarchical and compositional structure
in the context of naturalistic variability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Rethinking Space-Time Networks with Improved Memory Coverage for Efficient Video Object Segmentation. (arXiv:2106.05210v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Ho Kei Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1">Yu-Wing Tai</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1">Chi-Keung Tang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05210">
                                    <div class="article-summary-box-inner">
                                        <span>This paper presents a simple yet effective approach to modeling space-time
correspondences in the context of video object segmentation. Unlike most
existing approaches, we establish correspondences directly between frames
without re-encoding the mask features for every object, leading to a highly
efficient and robust framework. With the correspondences, every node in the
current query frame is inferred by aggregating features from the past in an
associative fashion. We cast the aggregation process as a voting problem and
find that the existing inner-product affinity leads to poor use of memory with
a small (fixed) subset of memory nodes dominating the votes, regardless of the
query. In light of this phenomenon, we propose using the negative squared
Euclidean distance instead to compute the affinities. We validated that every
memory node now has a chance to contribute, and experimentally showed that such
diversified voting is beneficial to both memory efficiency and inference
accuracy. The synergy of correspondence networks and diversified voting works
exceedingly well, achieves new state-of-the-art results on both DAVIS and
YouTubeVOS datasets while running significantly faster at 20+ FPS for multiple
objects without bells and whistles.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">All Tokens Matter: Token Labeling for Training Better Vision Transformers. (arXiv:2104.10858v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zihang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1">Qibin Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Li Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Daquan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yujun Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaojie Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Anran Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiashi Feng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10858">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we present token labeling -- a new training objective for
training high-performance vision transformers (ViTs). Different from the
standard training objective of ViTs that computes the classification loss on an
additional trainable class token, our proposed one takes advantage of all the
image patch tokens to compute the training loss in a dense manner.
Specifically, token labeling reformulates the image classification problem into
multiple token-level recognition problems and assigns each patch token with an
individual location-specific supervision generated by a machine annotator.
Experiments show that token labeling can clearly and consistently improve the
performance of various ViT models across a wide spectrum. For a vision
transformer with 26M learnable parameters serving as an example, with token
labeling, the model can achieve 84.4% Top-1 accuracy on ImageNet. The result
can be further increased to 86.4% by slightly scaling the model size up to
150M, delivering the minimal-sized model among previous models (250M+) reaching
86%. We also show that token labeling can clearly improve the generalization
capability of the pre-trained models on downstream tasks with dense prediction,
such as semantic segmentation. Our code and all the training details will be
made publicly available at https://github.com/zihangJiang/TokenLabeling.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">More than meets the eye: Self-supervised depth reconstruction from brain activity. (arXiv:2106.05113v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gaziv_G/0/1/0/all/0/1">Guy Gaziv</a>, <a href="http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1">Michal Irani</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05113">
                                    <div class="article-summary-box-inner">
                                        <span>In the past few years, significant advancements were made in reconstruction
of observed natural images from fMRI brain recordings using deep-learning
tools. Here, for the first time, we show that dense 3D depth maps of observed
2D natural images can also be recovered directly from fMRI brain recordings. We
use an off-the-shelf method to estimate the unknown depth maps of natural
images. This is applied to both: (i) the small number of images presented to
subjects in an fMRI scanner (images for which we have fMRI recordings -
referred to as &quot;paired&quot; data), and (ii) a very large number of natural images
with no fMRI recordings (&quot;unpaired data&quot;). The estimated depth maps are then
used as an auxiliary reconstruction criterion to train for depth reconstruction
directly from fMRI. We propose two main approaches: Depth-only recovery and
joint image-depth RGBD recovery. Because the number of available &quot;paired&quot;
training data (images with fMRI) is small, we enrich the training data via
self-supervised cycle-consistent training on many &quot;unpaired&quot; data (natural
images &amp; depth maps without fMRI). This is achieved using our newly defined and
trained Depth-based Perceptual Similarity metric as a reconstruction criterion.
We show that predicting the depth map directly from fMRI outperforms its
indirect sequential recovery from the reconstructed images. We further show
that activations from early cortical visual areas dominate our depth
reconstruction results, and propose means to characterize fMRI voxels by their
degree of depth-information tuning. This work adds an important layer of
decoded information, extending the current envelope of visual brain decoding
capabilities.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Application of Deep Learning in Generating Desired Design Options: Experiments Using Synthetic Training Dataset. (arXiv:2001.05849v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Shaghaghian_Z/0/1/0/all/0/1">Zohreh Shaghaghian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1">Wei Yan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.05849">
                                    <div class="article-summary-box-inner">
                                        <span>Most design methods contain a forward framework, asking for primary
specifications of a building to generate an output or assess its performance.
However, architects urge for specific objectives though uncertain of the proper
design parameters. Deep Learning (DL) algorithms provide an intelligent
workflow in which the system can learn from sequential training experiments.
This study applies a method using DL algorithms towards generating demanded
design options. In this study, an object recognition problem is investigated to
initially predict the label of unseen sample images based on training dataset
consisting of different types of synthetic 2D shapes; later, a generative DL
algorithm is applied to be trained and generate new shapes for given labels. In
the next step, the algorithm is trained to generate a window/wall pattern for
desired light/shadow performance based on the spatial daylight autonomy (sDA)
metrics. The experiments show promising results both in predicting unseen
sample shapes and generating new design options.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-Modal Contrastive Learning for Text-to-Image Generation. (arXiv:2101.04702v4 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1">Jing Yu Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1">Jason Baldridge</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Honglak Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yinfei Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04702">
                                    <div class="article-summary-box-inner">
                                        <span>The output of text-to-image synthesis systems should be coherent, clear,
photo-realistic scenes with high semantic fidelity to their conditioned text
descriptions. Our Cross-Modal Contrastive Generative Adversarial Network
(XMC-GAN) addresses this challenge by maximizing the mutual information between
image and text. It does this via multiple contrastive losses which capture
inter-modality and intra-modality correspondences. XMC-GAN uses an attentional
self-modulation generator, which enforces strong text-image correspondence, and
a contrastive discriminator, which acts as a critic as well as a feature
encoder for contrastive learning. The quality of XMC-GAN&#x27;s output is a major
step up from previous models, as we show on three challenging datasets. On
MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33,
but--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1
for image-text alignment, compared to three other recent models. XMC-GAN also
generalizes to the challenging Localized Narratives dataset (which has longer,
more detailed descriptions), improving state-of-the-art FID from 48.70 to
14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images
data, establishing a strong benchmark FID score of 26.91.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Salient Object Ranking with Position-Preserved Attention. (arXiv:2106.05047v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1">Hao Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Daoxin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Minghao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiawei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yao Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1">Deng Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaofei He</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05047">
                                    <div class="article-summary-box-inner">
                                        <span>Instance segmentation can detect where the objects are in an image, but hard
to understand the relationship between them. We pay attention to a typical
relationship, relative saliency. A closely related task, salient object
detection, predicts a binary map highlighting a visually salient region while
hard to distinguish multiple objects. Directly combining two tasks by
post-processing also leads to poor performance. There is a lack of research on
relative saliency at present, limiting the practical applications such as
content-aware image cropping, video summary, and image labeling.

In this paper, we study the Salient Object Ranking (SOR) task, which manages
to assign a ranking order of each detected object according to its visual
saliency. We propose the first end-to-end framework of the SOR task and solve
it in a multi-task learning fashion. The framework handles instance
segmentation and salient object ranking simultaneously. In this framework, the
SOR branch is independent and flexible to cooperate with different detection
methods, so that easy to use as a plugin. We also introduce a
Position-Preserved Attention (PPA) module tailored for the SOR branch. It
consists of the position embedding stage and feature interaction stage.
Considering the importance of position in saliency comparison, we preserve
absolute coordinates of objects in ROI pooling operation and then fuse
positional information with semantic features in the first stage. In the
feature interaction stage, we apply the attention mechanism to obtain
proposals&#x27; contextualized representations to predict their relative ranking
orders. Extensive experiments have been conducted on the ASR dataset. Without
bells and whistles, our proposed method outperforms the former state-of-the-art
method significantly. The code will be released publicly available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Transformer in Convolutional Neural Networks. (arXiv:2106.03180v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1">Guolei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1">Yu Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Le Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1">Ajad Chhatkuli</a>, <a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1">Luc Van Gool</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03180">
                                    <div class="article-summary-box-inner">
                                        <span>We tackle the low-efficiency flaw of vision transformer caused by the high
computational/space complexity in Multi-Head Self-Attention (MHSA). To this
end, we propose the Hierarchical MHSA (H-MHSA), whose representation is
computed in a hierarchical manner. Specifically, our H-MHSA first learns
feature relationships within small grids by viewing image patches as tokens.
Then, small grids are merged into larger ones, within which feature
relationship is learned by viewing each small grid at the preceding step as a
token. This process is iterated to gradually reduce the number of tokens. The
H-MHSA module is readily pluggable into any CNN architectures and amenable to
training via backpropagation. We call this new backbone TransCNN, and it
essentially inherits the advantages of both transformer and CNN. Experiments
demonstrate that TransCNN achieves state-of-the-art accuracy for image
recognition. Code and pretrained models are available at
https://github.com/yun-liu/TransCNN. This technical report will keep updating
by adding more experiments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">XIRL: Cross-embodiment Inverse Reinforcement Learning. (arXiv:2106.03911v1 [cs.RO] CROSS LISTED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zakka_K/0/1/0/all/0/1">Kevin Zakka</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1">Andy Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1">Pete Florence</a>, <a href="http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1">Jonathan Tompson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1">Jeannette Bohg</a>, <a href="http://arxiv.org/find/cs/1/au:+Dwibedi_D/0/1/0/all/0/1">Debidatta Dwibedi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03911">
                                    <div class="article-summary-box-inner">
                                        <span>We investigate the visual cross-embodiment imitation setting, in which agents
learn policies from videos of other agents (such as humans) demonstrating the
same task, but with stark differences in their embodiments -- shape, actions,
end-effector dynamics, etc. In this work, we demonstrate that it is possible to
automatically discover and learn vision-based reward functions from
cross-embodiment demonstration videos that are robust to these differences.
Specifically, we present a self-supervised method for Cross-embodiment Inverse
Reinforcement Learning (XIRL) that leverages temporal cycle-consistency
constraints to learn deep visual embeddings that capture task progression from
offline videos of demonstrations across multiple expert agents, each performing
the same task differently due to embodiment differences. Prior to our work,
producing rewards from self-supervised embeddings has typically required
alignment with a reference trajectory, which may be difficult to acquire. We
show empirically that if the embeddings are aware of task-progress, simply
taking the negative distance between the current state and goal state in the
learned embedding space is useful as a reward for training policies with
reinforcement learning. We find our learned reward function not only works for
embodiments seen during training, but also generalizes to entirely new
embodiments. We also find that XIRL policies are more sample efficient than
baselines, and in some cases exceed the sample efficiency of the same agent
trained with ground truth sparse rewards.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Programmable 3D snapshot microscopy with Fourier convolutional networks. (arXiv:2104.10611v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Deb_D/0/1/0/all/0/1">Diptodip Deb</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiao_Z/0/1/0/all/0/1">Zhenfei Jiao</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1">Alex B. Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Ahrens_M/0/1/0/all/0/1">Misha B. Ahrens</a>, <a href="http://arxiv.org/find/eess/1/au:+Podgorski_K/0/1/0/all/0/1">Kaspar Podgorski</a>, <a href="http://arxiv.org/find/eess/1/au:+Turaga_S/0/1/0/all/0/1">Srinivas C. Turaga</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10611">
                                    <div class="article-summary-box-inner">
                                        <span>3D snapshot microscopy enables fast volumetric imaging by capturing a 3D
volume in a single 2D camera image, and has found a variety of biological
applications such as whole brain imaging of fast neural activity in larval
zebrafish. The optimal microscope design for this optical 3D-to-2D encoding is
both sample- and task-dependent, with no general solution known. Highly
programmable optical elements create new possibilities for sample-specific
computational optimization of microscope parameters, e.g. tuning the collection
of light for a given sample structure. We perform such optimization with deep
learning, using a differentiable wave-optics simulation of light propagation
through a programmable microscope and a neural network to reconstruct volumes
from the microscope image. We introduce a class of global kernel Fourier
convolutional neural networks which can efficiently decode information from
multiple depths in the volume, globally encoded across a 3D snapshot image. We
show that our proposed networks succeed in large field of view volume
reconstruction and microscope parameter optimization where traditional networks
fail. We also show that our networks outperform the state-of-the-art learned
reconstruction algorithms for lensless computational photography.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Agile wide-field imaging with selective high resolution. (arXiv:2106.05082v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1">Lintao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_L/0/1/0/all/0/1">Liheng Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tiexin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05082">
                                    <div class="article-summary-box-inner">
                                        <span>Wide-field and high-resolution (HR) imaging is essential for various
applications such as aviation reconnaissance, topographic mapping and safety
monitoring. The existing techniques require a large-scale detector array to
capture HR images of the whole field, resulting in high complexity and heavy
cost. In this work, we report an agile wide-field imaging framework with
selective high resolution that requires only two detectors. It builds on the
statistical sparsity prior of natural scenes that the important targets locate
only at small regions of interests (ROI), instead of the whole field. Under
this assumption, we use a short-focal camera to image wide field with a certain
low resolution, and use a long-focal camera to acquire the HR images of ROI. To
automatically locate ROI in the wide field in real time, we propose an
efficient deep-learning based multiscale registration method that is robust and
blind to the large setting differences (focal, white balance, etc) between the
two cameras. Using the registered location, the long-focal camera mounted on a
gimbal enables real-time tracking of the ROI for continuous HR imaging. We
demonstrated the novel imaging framework by building a proof-of-concept setup
with only 1181 gram weight, and assembled it on an unmanned aerial vehicle for
air-to-ground monitoring. Experiments show that the setup maintains
120$^{\circ}$ wide field-of-view (FOV) with selective 0.45$mrad$ instantaneous
FOV.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dual-Modality Vehicle Anomaly Detection via Bilateral Trajectory Tracing. (arXiv:2106.05003v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jingyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1">Guanchen Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuchen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Wenwei Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kangmin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1">Tianyi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhe Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1">Wanping Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Hao Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenzhong Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05003">
                                    <div class="article-summary-box-inner">
                                        <span>Traffic anomaly detection has played a crucial role in Intelligent
Transportation System (ITS). The main challenges of this task lie in the highly
diversified anomaly scenes and variational lighting conditions. Although much
work has managed to identify the anomaly in homogenous weather and scene, few
resolved to cope with complex ones. In this paper, we proposed a dual-modality
modularized methodology for the robust detection of abnormal vehicles. We
introduced an integrated anomaly detection framework comprising the following
modules: background modeling, vehicle tracking with detection, mask
construction, Region of Interest (ROI) backtracking, and dual-modality tracing.
Concretely, we employed background modeling to filter the motion information
and left the static information for later vehicle detection. For the vehicle
detection and tracking module, we adopted YOLOv5 and multi-scale tracking to
localize the anomalies. Besides, we utilized the frame difference and tracking
results to identify the road and obtain the mask. In addition, we introduced
multiple similarity estimation metrics to refine the anomaly period via
backtracking. Finally, we proposed a dual-modality bilateral tracing module to
refine the time further. The experiments conducted on the Track 4 testset of
the NVIDIA 2021 AI City Challenge yielded a result of 0.9302 F1-Score and
3.4039 root mean square error (RMSE), indicating the effectiveness of our
framework.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-supervised Feature Enhancement: Applying Internal Pretext Task to Supervised Learning. (arXiv:2106.04921v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuhang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1">Zilin Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaomin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Ming Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04921">
                                    <div class="article-summary-box-inner">
                                        <span>Traditional self-supervised learning requires CNNs using external pretext
tasks (i.e., image- or video-based tasks) to encode high-level semantic visual
representations. In this paper, we show that feature transformations within
CNNs can also be regarded as supervisory signals to construct the
self-supervised task, called \emph{internal pretext task}. And such a task can
be applied for the enhancement of supervised learning. Specifically, we first
transform the internal feature maps by discarding different channels, and then
define an additional internal pretext task to identify the discarded channels.
CNNs are trained to predict the joint labels generated by the combination of
self-supervised labels and original labels. By doing so, we let CNNs know which
channels are missing while classifying in the hope to mine richer feature
information. Extensive experiments show that our approach is effective on
various models and datasets. And it&#x27;s worth noting that we only incur
negligible computational overhead. Furthermore, our approach can also be
compatible with other methods to get better results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A machine learning pipeline for aiding school identification from child trafficking images. (arXiv:2106.05215v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1">Sumit Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Sederholm_T/0/1/0/all/0/1">Tina Sederholm</a>, <a href="http://arxiv.org/find/cs/1/au:+Roman_A/0/1/0/all/0/1">Anthony C. Roman</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankar_R/0/1/0/all/0/1">Ria Sankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Caltagirone_S/0/1/0/all/0/1">Sherrie Caltagirone</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1">Juan Lavista Ferres</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05215">
                                    <div class="article-summary-box-inner">
                                        <span>Child trafficking in a serious problem around the world. Every year there are
more than 4 million victims of child trafficking around the world, many of them
for the purposes of child sexual exploitation. In collaboration with UK Police
and a non-profit focused on child abuse prevention, Global Emancipation
Network, we developed a proof-of-concept machine learning pipeline to aid the
identification of children from intercepted images. In this work, we focus on
images that contain children wearing school uniforms to identify the school of
origin. In the absence of a machine learning pipeline, this hugely time
consuming and labor intensive task is manually conducted by law enforcement
personnel. Thus, by automating aspects of the school identification process, we
hope to significantly impact the speed of this portion of child identification.
Our proposed pipeline consists of two machine learning models: i) to identify
whether an image of a child contains a school uniform in it, and ii)
identification of attributes of different school uniform items (such as
color/texture of shirts, sweaters, blazers etc.). We describe the data
collection, labeling, model development and validation process, along with
strategies for efficient searching of schools using the model predictions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Real Time Egocentric Object Segmentation: THU-READ Labeling and Benchmarking Results. (arXiv:2106.04957v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Sosa_E/0/1/0/all/0/1">E. Gonzalez-Sosa</a>, <a href="http://arxiv.org/find/cs/1/au:+Robledo_G/0/1/0/all/0/1">G. Robledo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Morin_D/0/1/0/all/0/1">D. Gonzalez-Morin</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_Garcia_P/0/1/0/all/0/1">P. Perez-Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Villegas_A/0/1/0/all/0/1">A. Villegas</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04957">
                                    <div class="article-summary-box-inner">
                                        <span>Egocentric segmentation has attracted recent interest in the computer vision
community due to their potential in Mixed Reality (MR) applications. While most
previous works have been focused on segmenting egocentric human body parts
(mainly hands), little attention has been given to egocentric objects. Due to
the lack of datasets of pixel-wise annotations of egocentric objects, in this
paper we contribute with a semantic-wise labeling of a subset of 2124 images
from the RGB-D THU-READ Dataset. We also report benchmarking results using
Thundernet, a real-time semantic segmentation network, that could allow future
integration with end-to-end MR applications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A multi-stage GAN for multi-organ chest X-ray image generation and segmentation. (arXiv:2106.05132v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Ciano_G/0/1/0/all/0/1">Giorgio Ciano</a>, <a href="http://arxiv.org/find/eess/1/au:+Andreini_P/0/1/0/all/0/1">Paolo Andreini</a>, <a href="http://arxiv.org/find/eess/1/au:+Mazzierli_T/0/1/0/all/0/1">Tommaso Mazzierli</a>, <a href="http://arxiv.org/find/eess/1/au:+Bianchini_M/0/1/0/all/0/1">Monica Bianchini</a>, <a href="http://arxiv.org/find/eess/1/au:+Scarselli_F/0/1/0/all/0/1">Franco Scarselli</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05132">
                                    <div class="article-summary-box-inner">
                                        <span>Multi-organ segmentation of X-ray images is of fundamental importance for
computer aided diagnosis systems. However, the most advanced semantic
segmentation methods rely on deep learning and require a huge amount of labeled
images, which are rarely available due to both the high cost of human resources
and the time required for labeling. In this paper, we present a novel
multi-stage generation algorithm based on Generative Adversarial Networks
(GANs) that can produce synthetic images along with their semantic labels and
can be used for data augmentation. The main feature of the method is that,
unlike other approaches, generation occurs in several stages, which simplifies
the procedure and allows it to be used on very small datasets. The method has
been evaluated on the segmentation of chest radiographic images, showing
promising results. The multistage approach achieves state-of-the-art and, when
very few images are used to train the GANs, outperforms the corresponding
single-stage approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ST++: Make Self-training Work Better for Semi-supervised Semantic Segmentation. (arXiv:2106.05095v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lihe Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1">Wei Zhuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1">Lei Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yinghuan Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yang Gao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05095">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we investigate if we could make the self-training -- a simple
but popular framework -- work better for semi-supervised segmentation. Since
the core issue in semi-supervised setting lies in effective and efficient
utilization of unlabeled data, we notice that increasing the diversity and
hardness of unlabeled data is crucial to performance improvement. Being aware
of this fact, we propose to adopt the most plain self-training scheme coupled
with appropriate strong data augmentations on unlabeled data (namely ST) for
this task, which surprisingly outperforms previous methods under various
settings without any bells and whistles. Moreover, to alleviate the negative
impact of the wrongly pseudo labeled images, we further propose an advanced
self-training framework (namely ST++), that performs selective re-training via
selecting and prioritizing the more reliable unlabeled images. As a result, the
proposed ST++ boosts the performance of semi-supervised model significantly and
surpasses existing methods by a large margin on the Pascal VOC 2012 and
Cityscapes benchmark. Overall, we hope this straightforward and simple
framework will serve as a strong baseline or competitor for future works. Code
is available at https://github.com/LiheYoung/ST-PlusPlus.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Analysis of convolutional neural network image classifiers in a hierarchical max-pooling model with additional local pooling. (arXiv:2106.05233v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Walter_B/0/1/0/all/0/1">Benjamin Walter</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05233">
                                    <div class="article-summary-box-inner">
                                        <span>Image classification is considered, and a hierarchical max-pooling model with
additional local pooling is introduced. Here the additional local pooling
enables the hierachical model to combine parts of the image which have a
variable relative distance towards each other. Various convolutional neural
network image classifiers are introduced and compared in view of their rate of
convergence. The finite sample size performance of the estimates is analyzed by
applying them to simulated and real data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning to Rank Words: Optimizing Ranking Metrics for Word Spotting. (arXiv:2106.05144v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1">Pau Riba</a>, <a href="http://arxiv.org/find/cs/1/au:+Molina_A/0/1/0/all/0/1">Adri&#xe0; Molina</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1">Lluis Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramos_Terrades_O/0/1/0/all/0/1">Oriol Ramos-Terrades</a>, <a href="http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1">Josep Llad&#xf3;s</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05144">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we explore and evaluate the use of ranking-based objective
functions for learning simultaneously a word string and a word image encoder.
We consider retrieval frameworks in which the user expects a retrieval list
ranked according to a defined relevance score. In the context of a word
spotting problem, the relevance score has been set according to the string edit
distance from the query string. We experimentally demonstrate the competitive
performance of the proposed model on query-by-string word spotting for both,
handwritten and real scene word images. We also provide the results for
query-by-example word spotting, although it is not the main focus of this work.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Distilling Image Classifiers in Object Detectors. (arXiv:2106.05209v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shuxuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1">Jose M. Alvarez</a>, <a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1">Mathieu Salzmann</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05209">
                                    <div class="article-summary-box-inner">
                                        <span>Knowledge distillation constitutes a simple yet effective way to improve the
performance of a compact student network by exploiting the knowledge of a more
powerful teacher. Nevertheless, the knowledge distillation literature remains
limited to the scenario where the student and the teacher tackle the same task.
Here, we investigate the problem of transferring knowledge not only across
architectures but also across tasks. To this end, we study the case of object
detection and, instead of following the standard detector-to-detector
distillation approach, introduce a classifier-to-detector knowledge transfer
framework. In particular, we propose strategies to exploit the classification
teacher to improve both the detector&#x27;s recognition accuracy and localization
performance. Our experiments on several detectors with different backbones
demonstrate the effectiveness of our approach, allowing us to outperform the
state-of-the-art detector-to-detector distillation methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Defending against Adversarial Examples via Attack-Invariant Features. (arXiv:2106.05036v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Dawei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1">Nannan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1">Chunlei Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xinbo Gao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05036">
                                    <div class="article-summary-box-inner">
                                        <span>Deep neural networks (DNNs) are vulnerable to adversarial noise. Their
adversarial robustness can be improved by exploiting adversarial examples.
However, given the continuously evolving attacks, models trained on seen types
of adversarial examples generally cannot generalize well to unseen types of
adversarial examples. To solve this problem, in this paper, we propose to
remove adversarial noise by learning generalizable invariant features across
attacks which maintain semantic classification information. Specifically, we
introduce an adversarial feature learning mechanism to disentangle invariant
features from adversarial noise. A normalization term has been proposed in the
encoded space of the attack-invariant features to address the bias issue
between the seen and unseen types of attacks. Empirical evaluations demonstrate
that our method could provide better protection in comparison to previous
state-of-the-art approaches, especially against unseen types of attacks and
adaptive attacks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Semi-supervised lane detection with Deep Hough Transform. (arXiv:2106.05094v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yancong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1">Silvia-Laura Pintea</a>, <a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1">Jan van Gemert</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05094">
                                    <div class="article-summary-box-inner">
                                        <span>Current work on lane detection relies on large manually annotated datasets.
We reduce the dependency on annotations by leveraging massive cheaply available
unlabelled data. We propose a novel loss function exploiting geometric
knowledge of lanes in Hough space, where a lane can be identified as a local
maximum. By splitting lanes into separate channels, we can localize each lane
via simple global max-pooling. The location of the maximum encodes the layout
of a lane, while the intensity indicates the the probability of a lane being
present. Maximizing the log-probability of the maximal bins helps neural
networks find lanes without labels. On the CULane and TuSimple datasets, we
show that the proposed Hough Transform loss improves performance significantly
by learning from large amounts of unlabelled images.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Automatic CT Segmentation from Bounding Box Annotations using Convolutional Neural Networks. (arXiv:2105.14314v3 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuanpeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_Q/0/1/0/all/0/1">Qinglei Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhiyi Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1">Shaolin Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1">Dexing Kong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14314">
                                    <div class="article-summary-box-inner">
                                        <span>Accurate segmentation for medical images is important for clinical diagnosis.
Existing automatic segmentation methods are mainly based on fully supervised
learning and have an extremely high demand for precise annotations, which are
very costly and time-consuming to obtain. To address this problem, we proposed
an automatic CT segmentation method based on weakly supervised learning, by
which one could train an accurate segmentation model only with weak annotations
in the form of bounding boxes. The proposed method is composed of two steps: 1)
generating pseudo masks with bounding box annotations by k-means clustering,
and 2) iteratively training a 3D U-Net convolutional neural network as a
segmentation model. Some data pre-processing methods are used to improve
performance. The method was validated on four datasets containing three types
of organs with a total of 627 CT volumes. For liver, spleen and kidney
segmentation, it achieved an accuracy of 95.19%, 92.11%, and 91.45%,
respectively. Experimental results demonstrate that our method is accurate,
efficient, and suitable for clinical use.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Rethink Transfer Learning in Medical Image Classification. (arXiv:2106.05152v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1">Le Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1">Hengyue Liang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1">Taihui Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1">Ju Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05152">
                                    <div class="article-summary-box-inner">
                                        <span>Transfer learning (TL) with deep convolutional neural networks (DCNNs) has
proved successful in medical image classification (MIC). However, the current
practice is puzzling, as MIC typically relies only on low- and/or mid-level
features that are learned in the bottom layers of DCNNs. Following this
intuition, we question the current strategies of TL in MIC. In this paper, we
perform careful experimental comparisons between shallow and deep networks for
classification on two chest x-ray datasets, using different TL strategies. We
find that deep models are not always favorable, and finetuning truncated deep
models almost always yields the best performance, especially in data-poor
regimes.

Project webpage:
https://github.com/sun-umn/Transfer-Learning-in-Medical-Imaging

Keywords: Transfer learning, Medical image classification, Feature hierarchy,
Medical imaging, Evaluation metrics, Imbalanced data</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An ordinal CNN approach for the assessment of neurological damage in Parkinson&#x27;s disease patients. (arXiv:2106.05230v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Barbero_Gomez_J/0/1/0/all/0/1">Javier Barbero-G&#xf3;mez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_P/0/1/0/all/0/1">Pedro-Antonio Guti&#xe9;rrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_V/0/1/0/all/0/1">V&#xed;ctor-Manuel Vargas</a>, <a href="http://arxiv.org/find/cs/1/au:+Vallejo_Casas_J/0/1/0/all/0/1">Juan-Antonio Vallejo-Casas</a>, <a href="http://arxiv.org/find/cs/1/au:+Hervas_Martinez_C/0/1/0/all/0/1">C&#xe9;sar Herv&#xe1;s-Mart&#xed;nez</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05230">
                                    <div class="article-summary-box-inner">
                                        <span>3D image scans are an assessment tool for neurological damage in Parkinson&#x27;s
disease (PD) patients. This diagnosis process can be automatized to help
medical staff through Decision Support Systems (DSSs), and Convolutional Neural
Networks (CNNs) are good candidates, because they are effective when applied to
spatial data. This paper proposes a 3D CNN ordinal model for assessing the
level or neurological damage in PD patients. Given that CNNs need large
datasets to achieve acceptable performance, a data augmentation method is
adapted to work with spatial data. We consider the Ordinal Graph-based
Oversampling via Shortest Paths (OGO-SP) method, which applies a gamma
probability distribution for inter-class data generation. A modification of
OGO-SP is proposed, the OGO-SP-$\beta$ algorithm, which applies the beta
distribution for generating synthetic samples in the inter-class region, a
better suited distribution when compared to gamma. The evaluation of the
different methods is based on a novel 3D image dataset provided by the Hospital
Universitario &#x27;Reina Sof\&#x27;ia&#x27; (C\&#x27;ordoba, Spain). We show how the ordinal
methodology improves the performance with respect to the nominal one, and how
OGO-SP-$\beta$ yields better performance than OGO-SP.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Knowledge distillation: A good teacher is patient and consistent. (arXiv:2106.05237v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1">Lucas Beyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaohua Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Royer_A/0/1/0/all/0/1">Am&#xe9;lie Royer</a>, <a href="http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1">Larisa Markeeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1">Rohan Anil</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1">Alexander Kolesnikov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05237">
                                    <div class="article-summary-box-inner">
                                        <span>There is a growing discrepancy in computer vision between large-scale models
that achieve state-of-the-art performance and models that are affordable in
practical applications. In this paper we address this issue and significantly
bridge the gap between these two types of models. Throughout our empirical
investigation we do not aim to necessarily propose a new method, but strive to
identify a robust and effective recipe for making state-of-the-art large scale
models affordable in practice. We demonstrate that, when performed correctly,
knowledge distillation can be a powerful tool for reducing the size of large
models without compromising their performance. In particular, we uncover that
there are certain implicit design choices, which may drastically affect the
effectiveness of distillation. Our key contribution is the explicit
identification of these design choices, which were not previously articulated
in the literature. We back up our findings by a comprehensive empirical study,
demonstrate compelling results on a wide range of vision datasets and, in
particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which
achieves 82.8\% top-1 accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Salient Positions based Attention Network for Image Classification. (arXiv:2106.04996v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1">Sheng Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kaiyu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhe Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04996">
                                    <div class="article-summary-box-inner">
                                        <span>The self-attention mechanism has attracted wide publicity for its most
important advantage of modeling long dependency, and its variations in computer
vision tasks, the non-local block tries to model the global dependency of the
input feature maps. Gathering global contextual information will inevitably
need a tremendous amount of memory and computing resources, which has been
extensively studied in the past several years. However, there is a further
problem with the self-attention scheme: is all information gathered from the
global scope helpful for the contextual modelling? To our knowledge, few
studies have focused on the problem. Aimed at both questions this paper
proposes the salient positions-based attention scheme SPANet, which is inspired
by some interesting observations on the attention maps and affinity matrices
generated in self-attention scheme. We believe these observations are
beneficial for better understanding of the self-attention. SPANet uses the
salient positions selection algorithm to select only a limited amount of
salient points to attend in the attention map computing. This approach will not
only spare a lot of memory and computing resources, but also try to distill the
positive information from the transformation of the input feature maps. In the
implementation, considering the feature maps with channel high dimensions,
which are completely different from the general visual image, we take the
squared power of the feature maps along the channel dimension as the saliency
metric of the positions. In general, different from the non-local block method,
SPANet models the contextual information using only the selected positions
instead of all, along the channel dimension instead of space dimension. Our
source code is available at https://github.com/likyoo/SPANet.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Grounding inductive biases in natural images:invariance stems from variations in data. (arXiv:2106.05121v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bouchacourt_D/0/1/0/all/0/1">Diane Bouchacourt</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1">Mark Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1">Ari S. Morcos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05121">
                                    <div class="article-summary-box-inner">
                                        <span>To perform well on unseen and potentially out-of-distribution samples, it is
desirable for machine learning models to have a predictable response with
respect to transformations affecting the factors of variation of the input.
Invariance is commonly achieved through hand-engineered data augmentation, but
do standard data augmentations address transformations that explain variations
in real data? While prior work has focused on synthetic data, we attempt here
to characterize the factors of variation in a real dataset, ImageNet, and study
the invariance of both standard residual networks and the recently proposed
vision transformer with respect to changes in these factors. We show standard
augmentation relies on a precise combination of translation and scale, with
translation recapturing most of the performance improvement -- despite the
(approximate) translation invariance built in to convolutional architectures,
such as residual networks. In fact, we found that scale and translation
invariance was similar across residual networks and vision transformer models
despite their markedly different inductive biases. We show the training data
itself is the main source of invariance, and that data augmentation only
further increases the learned invariances. Interestingly, the invariances
brought from the training process align with the ImageNet factors of variation
we found. Finally, we find that the main factors of variation in ImageNet
mostly relate to appearance and are specific to each class.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data. (arXiv:2106.05001v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1">Mi Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Fei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1">Dapeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiashi Feng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05001">
                                    <div class="article-summary-box-inner">
                                        <span>A central challenge in training classification models in the real-world
federated system is learning with non-IID data. To cope with this, most of the
existing works involve enforcing regularization in local optimization or
improving the model aggregation scheme at the server. Other works also share
public datasets or synthesized samples to supplement the training of
under-represented classes or introduce a certain level of personalization.
Though effective, they lack a deep understanding of how the data heterogeneity
affects each layer of a deep classification model. In this paper, we bridge
this gap by performing an experimental analysis of the representations learned
by different layers. Our observations are surprising: (1) there exists a
greater bias in the classifier than other layers, and (2) the classification
performance can be significantly improved by post-calibrating the classifier
after federated training. Motivated by the above findings, we propose a novel
and simple algorithm called Classifier Calibration with Virtual Representations
(CCVR), which adjusts the classifier using virtual representations sampled from
an approximated gaussian mixture model. Experimental results demonstrate that
CCVR achieves state-of-the-art performance on popular federated learning
benchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple
yet effective method can shed some light on the future research of federated
learning with non-IID data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation. (arXiv:2106.04732v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Berthelot_D/0/1/0/all/0/1">David Berthelot</a>, <a href="http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1">Rebecca Roelofs</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kihyuk Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1">Nicholas Carlini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1">Alex Kurakin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04732">
                                    <div class="article-summary-box-inner">
                                        <span>We extend semi-supervised learning to the problem of domain adaptation to
learn significantly higher-accuracy models that train on one data distribution
and test on a different one. With the goal of generality, we introduce
AdaMatch, a method that unifies the tasks of unsupervised domain adaptation
(UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation
(SSDA). In an extensive experimental study, we compare its behavior with
respective state-of-the-art techniques from SSL, SSDA, and UDA on vision
classification tasks. We find AdaMatch either matches or significantly exceeds
the state-of-the-art in each case using the same hyper-parameters regardless of
the dataset or task. For example, AdaMatch nearly doubles the accuracy compared
to that of the prior state-of-the-art on the UDA task for DomainNet and even
exceeds the accuracy of the prior state-of-the-art obtained with pre-training
by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by
providing AdaMatch with just one labeled example per class from the target
domain (i.e., the SSDA setting), we increase the target accuracy by an
additional 6.1%, and with 5 labeled examples, by 13.6%.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Tracking by Joint Local and Global Search: A Target-aware Attention based Approach. (arXiv:2106.04840v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Bin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yonghong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Feng Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04840">
                                    <div class="article-summary-box-inner">
                                        <span>Tracking-by-detection is a very popular framework for single object tracking
which attempts to search the target object within a local search window for
each frame. Although such local search mechanism works well on simple videos,
however, it makes the trackers sensitive to extremely challenging scenarios,
such as heavy occlusion and fast motion. In this paper, we propose a novel and
general target-aware attention mechanism (termed TANet) and integrate it with
tracking-by-detection framework to conduct joint local and global search for
robust tracking. Specifically, we extract the features of target object patch
and continuous video frames, then we concatenate and feed them into a decoder
network to generate target-aware global attention maps. More importantly, we
resort to adversarial training for better attention prediction. The appearance
and motion discriminator networks are designed to ensure its consistency in
spatial and temporal views. In the tracking procedure, we integrate the
target-aware attention with multiple trackers by exploring candidate search
regions for robust tracking. Extensive experiments on both short-term and
long-term tracking benchmark datasets all validated the effectiveness of our
algorithm. The project page of this paper can be found at
\url{https://sites.google.com/view/globalattentiontracking/home/extend}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Continuous-discrete multiple target tracking with out-of-sequence measurements. (arXiv:2106.04898v1 [eess.SY])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Garcia_Fernandez_A/0/1/0/all/0/1">&#xc1;ngel F. Garc&#xed;a-Fern&#xe1;ndez</a>, <a href="http://arxiv.org/find/eess/1/au:+Yi_W/0/1/0/all/0/1">Wei Yi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04898">
                                    <div class="article-summary-box-inner">
                                        <span>This paper derives the optimal Bayesian processing of an out-of-sequence
(OOS) set of measurements in continuous-time for multiple target tracking. We
consider a multi-target system modelled in continuous time that is discretised
at the time steps when we receive the measurements, which are distributed
according to the standard point target model. All information about this system
at the sampled time steps is provided by the posterior density on the set of
all trajectories. This density can be computed via the continuous-discrete
trajectory Poisson multi-Bernoulli mixture (TPMBM) filter. When we receive an
OOS measurement, the optimal Bayesian processing performs a retrodiction step
that adds trajectory information at the OOS measurement time stamp followed by
an update step. After the OOS measurement update, the posterior remains in
TPMBM form. We also provide a computationally lighter alternative based on a
trajectory Poisson multi-Bernoulli filter. The effectiveness of the two
approaches to handle OOS measurements is evaluated via simulations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">It Takes Two to Tango: Mixup for Deep Metric Learning. (arXiv:2106.04990v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Venkataramanan_S/0/1/0/all/0/1">Shashanka Venkataramanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Psomas_B/0/1/0/all/0/1">Bill Psomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1">Yannis Avrithis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1">Ewa Kijak</a>, <a href="http://arxiv.org/find/cs/1/au:+Amsaleg_L/0/1/0/all/0/1">Laurent Amsaleg</a>, <a href="http://arxiv.org/find/cs/1/au:+Karantzalos_K/0/1/0/all/0/1">Konstantinos Karantzalos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04990">
                                    <div class="article-summary-box-inner">
                                        <span>Metric learning involves learning a discriminative representation such that
embeddings of similar classes are encouraged to be close, while embeddings of
dissimilar classes are pushed far apart. State-of-the-art methods focus mostly
on sophisticated loss functions or mining strategies. On the one hand, metric
learning losses consider two or more examples at a time. On the other hand,
modern data augmentation methods for classification consider two or more
examples at a time. The combination of the two ideas is under-studied.

In this work, we aim to bridge this gap and improve representations using
mixup, which is a powerful data augmentation approach interpolating two or more
examples and corresponding target labels at a time. This task is challenging
because, unlike classification, the loss functions used in metric learning are
not additive over examples, so the idea of interpolating target labels is not
straightforward. To the best of our knowledge, we are the first to investigate
mixing examples and target labels for deep metric learning. We develop a
generalized formulation that encompasses existing metric learning loss
functions and modify it to accommodate for mixup, introducing Metric Mix, or
Metrix. We show that mixing inputs, intermediate representations or embeddings
along with target labels significantly improves representations and outperforms
state-of-the-art metric learning methods on four benchmark datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Exploiting Learned Symmetries in Group Equivariant Convolutions. (arXiv:2106.04914v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lengyel_A/0/1/0/all/0/1">Attila Lengyel</a>, <a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1">Jan C. van Gemert</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04914">
                                    <div class="article-summary-box-inner">
                                        <span>Group Equivariant Convolutions (GConvs) enable convolutional neural networks
to be equivariant to various transformation groups, but at an additional
parameter and compute cost. We investigate the filter parameters learned by
GConvs and find certain conditions under which they become highly redundant. We
show that GConvs can be efficiently decomposed into depthwise separable
convolutions while preserving equivariance properties and demonstrate improved
performance and data efficiency on two datasets. All code is publicly available
at github.com/Attila94/SepGrouPy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SHARP: Shape-Aware Reconstruction of People In Loose Clothing. (arXiv:2106.04778v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jinka_S/0/1/0/all/0/1">Sai Sagar Jinka</a>, <a href="http://arxiv.org/find/cs/1/au:+Chacko_R/0/1/0/all/0/1">Rohan Chacko</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1">Astitva Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Avinash Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1">P.J. Narayanan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04778">
                                    <div class="article-summary-box-inner">
                                        <span>3D human body reconstruction from monocular images is an interesting and
ill-posed problem in computer vision with wider applications in multiple
domains. In this paper, we propose SHARP, a novel end-to-end trainable network
that accurately recovers the detailed geometry and appearance of 3D people in
loose clothing from a monocular image. We propose a sparse and efficient fusion
of a parametric body prior with a non-parametric peeled depth map
representation of clothed models. The parametric body prior constraints our
model in two ways: first, the network retains geometrically consistent body
parts that are not occluded by clothing, and second, it provides a body shape
context that improves prediction of the peeled depth maps. This enables SHARP
to recover fine-grained 3D geometrical details with just L1 losses on the 2D
maps, given an input image. We evaluate SHARP on publicly available Cloth3D and
THuman datasets and report superior performance to state-of-the-art approaches.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Spatio-Temporal Dual-Stream Neural Network for Sequential Whole-Body PET Segmentation. (arXiv:2106.04961v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Liang_K/0/1/0/all/0/1">Kai-Chieh Liang</a>, <a href="http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1">Lei Bi</a>, <a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1">Ashnil Kumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Fulham_M/0/1/0/all/0/1">Michael Fulham</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1">Jinman Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04961">
                                    <div class="article-summary-box-inner">
                                        <span>Sequential whole-body 18F-Fluorodeoxyglucose (FDG) positron emission
tomography (PET) scans are regarded as the imaging modality of choice for the
assessment of treatment response in the lymphomas because they detect treatment
response when there may not be changes on anatomical imaging. Any computerized
analysis of lymphomas in whole-body PET requires automatic segmentation of the
studies so that sites of disease can be quantitatively monitored over time.
State-of-the-art PET image segmentation methods are based on convolutional
neural networks (CNNs) given their ability to leverage annotated datasets to
derive high-level features about the disease process. Such methods, however,
focus on PET images from a single time-point and discard information from other
scans or are targeted towards specific organs and cannot cater for the multiple
structures in whole-body PET images. In this study, we propose a
spatio-temporal &#x27;dual-stream&#x27; neural network (ST-DSNN) to segment sequential
whole-body PET scans. Our ST-DSNN learns and accumulates image features from
the PET images done over time. The accumulated image features are used to
enhance the organs / structures that are consistent over time to allow easier
identification of sites of active lymphoma. Our results show that our method
outperforms the state-of-the-art PET image segmentation methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CoAtNet: Marrying Convolution and Attention for All Data Sizes. (arXiv:2106.04803v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1">Zihang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hanxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1">Quoc V. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1">Mingxing Tan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04803">
                                    <div class="article-summary-box-inner">
                                        <span>Transformers have attracted increasing interests in computer vision, but they
still fall behind state-of-the-art convolutional networks. In this work, we
show that while Transformers tend to have larger model capacity, their
generalization can be worse than convolutional networks due to the lack of the
right inductive bias. To effectively combine the strengths from both
architectures, we present CoAtNets(pronounced &quot;coat&quot; nets), a family of hybrid
models built from two key insights:(1) depthwise Convolution and self-Attention
can be naturally unified via simple relative attention; (2) vertically stacking
convolution layers and attention layers in a principled way is surprisingly
effective in improving generalization, capacity and efficiency. Experiments
show that our CoAtNets achieve state-of-the-art performance under different
resource constraints across various datasets. For example, CoAtNet achieves
86.0% ImageNet top-1 accuracy without extra data, and 89.77% with extra JFT
data, outperforming prior arts of both convolutional networks and Transformers.
Notably, when pre-trained with 13M images fromImageNet-21K, our CoAtNet
achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images
from JFT while using 23x less data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study of the 2019 Indian Election on WhatsApp. (arXiv:2106.04726v1 [cs.SI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1">Ashkan Kazemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1">Kiran Garimella</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1">Gautam Kishore Shahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gaffney_D/0/1/0/all/0/1">Devin Gaffney</a>, <a href="http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1">Scott A. Hale</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04726">
                                    <div class="article-summary-box-inner">
                                        <span>WhatsApp is a popular chat application used by over 2 billion users
worldwide. However, due to end-to-end encryption, there is currently no easy
way to fact-check content on WhatsApp at scale. In this paper, we analyze the
usefulness of a crowd-sourced system on WhatsApp through which users can submit
&quot;tips&quot; containing messages they want fact-checked. We compare the tips sent to
a WhatsApp tipline run during the 2019 Indian national elections with the
messages circulating in large, public groups on WhatsApp and other social media
platforms during the same period. We find that tiplines are a very useful lens
into WhatsApp conversations: a significant fraction of messages and images sent
to the tipline match with the content being shared on public WhatsApp groups
and other social media. Our analysis also shows that tiplines cover the most
popular content well, and a majority of such content is often shared to the
tipline before appearing in large, public WhatsApp groups. Overall, the
analysis suggests tiplines can be an effective source for discovering content
to fact-check.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Densely connected normalizing flows. (arXiv:2106.04627v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Grcic_M/0/1/0/all/0/1">Matej Grci&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1">Ivan Grubi&#x161;i&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1">Sini&#x161;a &#x160;egvi&#x107;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04627">
                                    <div class="article-summary-box-inner">
                                        <span>Normalizing flows are bijective mappings between inputs and latent
representations with a fully factorized distribution. They are very attractive
due to exact likelihood evaluation and efficient sampling. However, their
effective capacity is often insufficient since the bijectivity constraint
limits the model width. We address this issue by incrementally padding
intermediate representations with noise. We precondition the noise in
accordance with previous invertible units, which we describe as cross-unit
coupling. Our invertible glow-like modules express intra-unit affine coupling
as a fusion of a densely connected block and Nystr\&quot;om self-attention. We refer
to our architecture as DenseFlow since both cross-unit and intra-unit couplings
rely on dense connectivity. Experiments show significant improvements due to
the proposed contributions, and reveal state-of-the-art density estimation
among all generative models under moderate computing budgets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Tiny Network for Recognition-Oriented Face Image Quality Assessment. (arXiv:2106.04852v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Baoyun Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Min Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Heng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhaoning Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04852">
                                    <div class="article-summary-box-inner">
                                        <span>Face recognition has made significant progress in recent years due to deep
convolutional neural networks (CNN). In many face recognition (FR) scenarios,
face images are acquired from a sequence with huge intra-variations. These
intra-variations, which are mainly affected by the low-quality face images,
cause instability of recognition performance. Previous works have focused on
ad-hoc methods to select frames from a video or use face image quality
assessment (FIQA) methods, which consider only a particular or combination of
several distortions.

In this work, we present an efficient non-reference image quality assessment
for FR that directly links image quality assessment (IQA) and FR. More
specifically, we propose a new measurement to evaluate image quality without
any reference. Based on the proposed quality measurement, we propose a deep
Tiny Face Quality network (tinyFQnet) to learn a quality prediction function
from data.

We evaluate the proposed method for different powerful FR models on two
classical video-based (or template-based) benchmark: IJB-B and YTF. Extensive
experiments show that, although the tinyFQnet is much smaller than the others,
the proposed method outperforms state-of-the-art quality assessment methods in
terms of effectiveness and efficiency.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fast Computational Ghost Imaging using Unpaired Deep Learning and a Constrained Generative Adversarial Network. (arXiv:2106.04822v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Alishahi_F/0/1/0/all/0/1">Fatemeh Alishahi</a>, <a href="http://arxiv.org/find/eess/1/au:+Mohajerin_Ariaei_A/0/1/0/all/0/1">Amirhossein Mohajerin-Ariaei</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04822">
                                    <div class="article-summary-box-inner">
                                        <span>The unpaired training can be the only option available for fast deep
learning-based ghost imaging, where obtaining a high signal-to-noise ratio
(SNR) image copy of each low SNR ghost image could be practically
time-consuming and challenging. This paper explores the capabilities of deep
learning to leverage computational ghost imaging when there is a lack of paired
training images. The deep learning approach proposed here enables fast ghost
imaging through reconstruction of high SNR images from faint and hastily shot
ghost images using a constrained Wasserstein generative adversarial network. In
the proposed approach, the objective function is regularized to enforce the
generation of faithful and relevant high SNR images to the ghost copies. This
regularization measures the distance between reconstructed images and the faint
ghost images in a low-noise manifold generated by a shadow network. The
performance of the constrained network is shown to be particularly important
for ghost images with low SNR. The proposed pipeline is able to reconstruct
high-quality images from the ghost images with SNR values not necessarily equal
to the SNR of the training set.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Check It Again: Progressive Visual Question Answering via Visual Entailment. (arXiv:2106.04605v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1">Qingyi Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1">Mingyu Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1">Peng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiping Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04605">
                                    <div class="article-summary-box-inner">
                                        <span>While sophisticated Visual Question Answering models have achieved remarkable
success, they tend to answer questions only according to superficial
correlations between question and answer. Several recent approaches have been
developed to address this language priors problem. However, most of them
predict the correct answer according to one best output without checking the
authenticity of answers. Besides, they only explore the interaction between
image and question, ignoring the semantics of candidate answers. In this paper,
we propose a select-and-rerank (SAR) progressive framework based on Visual
Entailment. Specifically, we first select the candidate answers relevant to the
question or the image, then we rerank the candidate answers by a visual
entailment task, which verifies whether the image semantically entails the
synthetic statement of the question and each candidate answer. Experimental
results show the effectiveness of our proposed framework, which establishes a
new state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Ex uno plures: Splitting One Model into an Ensemble of Subnetworks. (arXiv:2106.04767v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhilu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1">Vianne R. Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabuncu_M/0/1/0/all/0/1">Mert R. Sabuncu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04767">
                                    <div class="article-summary-box-inner">
                                        <span>Monte Carlo (MC) dropout is a simple and efficient ensembling method that can
improve the accuracy and confidence calibration of high-capacity deep neural
network models. However, MC dropout is not as effective as more
compute-intensive methods such as deep ensembles. This performance gap can be
attributed to the relatively poor quality of individual models in the MC
dropout ensemble and their lack of diversity. These issues can in turn be
traced back to the coupled training and substantial parameter sharing of the
dropout models. Motivated by this perspective, we propose a strategy to compute
an ensemble of subnetworks, each corresponding to a non-overlapping dropout
mask computed via a pruning strategy and trained independently. We show that
the proposed subnetwork ensembling method can perform as well as standard deep
ensembles in both accuracy and uncertainty estimates, yet with a computational
efficiency similar to MC dropout. Lastly, using several computer vision
datasets like CIFAR10/100, CUB200, and Tiny-Imagenet, we experimentally
demonstrate that subnetwork ensembling also consistently outperforms recently
proposed approaches that efficiently ensemble neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Point Cloud Upsampling via Disentangled Refinement. (arXiv:2106.04779v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruihui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xianzhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1">Pheng-Ann Heng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Chi-Wing Fu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04779">
                                    <div class="article-summary-box-inner">
                                        <span>Point clouds produced by 3D scanning are often sparse, non-uniform, and
noisy. Recent upsampling approaches aim to generate a dense point set, while
achieving both distribution uniformity and proximity-to-surface, and possibly
amending small holes, all in a single network. After revisiting the task, we
propose to disentangle the task based on its multi-objective nature and
formulate two cascaded sub-networks, a dense generator and a spatial refiner.
The dense generator infers a coarse but dense output that roughly describes the
underlying surface, while the spatial refiner further fine-tunes the coarse
output by adjusting the location of each point. Specifically, we design a pair
of local and global refinement units in the spatial refiner to evolve a coarse
feature map. Also, in the spatial refiner, we regress a per-point offset vector
to further adjust the coarse outputs in fine-scale. Extensive qualitative and
quantitative results on both synthetic and real-scanned datasets demonstrate
the superiority of our method over the state-of-the-arts.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Uncovering Closed-form Governing Equations of Nonlinear Dynamics from Videos. (arXiv:2106.04776v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Luan_L/0/1/0/all/0/1">Lele Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04776">
                                    <div class="article-summary-box-inner">
                                        <span>Distilling analytical models from data has the potential to advance our
understanding and prediction of nonlinear dynamics. Although discovery of
governing equations based on observed system states (e.g., trajectory time
series) has revealed success in a wide range of nonlinear dynamics, uncovering
the closed-form equations directly from raw videos still remains an open
challenge. To this end, we introduce a novel end-to-end unsupervised deep
learning framework to uncover the mathematical structure of equations that
governs the dynamics of moving objects in videos. Such an architecture consists
of (1) an encoder-decoder network that learns low-dimensional spatial/pixel
coordinates of the moving object, (2) a learnable Spatial-Physical
Transformation component that creates mapping between the extracted
spatial/pixel coordinates and the latent physical states of dynamics, and (3) a
numerical integrator-based sparse regression module that uncovers the
parsimonious closed-form governing equations of learned physical states and,
meanwhile, serves as a constraint to the autoencoder. The efficacy of the
proposed method is demonstrated by uncovering the governing equations of a
variety of nonlinear dynamical systems depicted by moving objects in videos.
The resulting computational framework enables discovery of parsimonious
interpretable model in a flexible and accessible sensing environment where only
videos are available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Accelerating Neural Architecture Search via Proxy Data. (arXiv:2106.04784v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1">Byunggook Na</a>, <a href="http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1">Jisoo Mok</a>, <a href="http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1">Hyeokjun Choe</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sungroh Yoon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04784">
                                    <div class="article-summary-box-inner">
                                        <span>Despite the increasing interest in neural architecture search (NAS), the
significant computational cost of NAS is a hindrance to researchers. Hence, we
propose to reduce the cost of NAS using proxy data, i.e., a representative
subset of the target data, without sacrificing search performance. Even though
data selection has been used across various fields, our evaluation of existing
selection methods for NAS algorithms offered by NAS-Bench-1shot1 reveals that
they are not always appropriate for NAS and a new selection method is
necessary. By analyzing proxy data constructed using various selection methods
through data entropy, we propose a novel proxy data selection method tailored
for NAS. To empirically demonstrate the effectiveness, we conduct thorough
experiments across diverse datasets, search spaces, and NAS algorithms.
Consequently, NAS algorithms with the proposed selection discover architectures
that are competitive with those obtained using the entire dataset. It
significantly reduces the search cost: executing DARTS with the proposed
selection requires only 40 minutes on CIFAR-10 and 7.5 hours on ImageNet with a
single GPU. Additionally, when the architecture searched on ImageNet using the
proposed selection is inversely transferred to CIFAR-10, a state-of-the-art
test error of 2.4\% is yielded. Our code is available at
https://github.com/nabk89/NAS-with-Proxy-data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">OODIn: An Optimised On-Device Inference Framework for Heterogeneous Mobile Devices. (arXiv:2106.04723v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1">Stylianos I. Venieris</a>, <a href="http://arxiv.org/find/cs/1/au:+Panopoulos_I/0/1/0/all/0/1">Ioannis Panopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Venieris_I/0/1/0/all/0/1">Iakovos S. Venieris</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04723">
                                    <div class="article-summary-box-inner">
                                        <span>Radical progress in the field of deep learning (DL) has led to unprecedented
accuracy in diverse inference tasks. As such, deploying DL models across mobile
platforms is vital to enable the development and broad availability of the
next-generation intelligent apps. Nevertheless, the wide and optimised
deployment of DL models is currently hindered by the vast system heterogeneity
of mobile devices, the varying computational cost of different DL models and
the variability of performance needs across DL applications. This paper
proposes OODIn, a framework for the optimised deployment of DL apps across
heterogeneous mobile devices. OODIn comprises a novel DL-specific software
architecture together with an analytical framework for modelling DL
applications that: (1) counteract the variability in device resources and DL
models by means of a highly parametrised multi-layer design; and (2) perform a
principled optimisation of both model- and system-level parameters through a
multi-objective formulation, designed for DL inference apps, in order to adapt
the deployment to the user-specified performance requirements and device
capabilities. Quantitative evaluation shows that the proposed framework
consistently outperforms status-quo designs across heterogeneous devices and
delivers up to 4.3x and 3.5x performance gain over highly optimised platform-
and model-aware designs respectively, while effectively adapting execution to
dynamic changes in resource availability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">TED-net: Convolution-free T2T Vision Transformer-based Encoder-decoder Dilation network for Low-dose CT Denoising. (arXiv:2106.04650v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1">Dayang Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1">Zhan Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1">Hengyong Yu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04650">
                                    <div class="article-summary-box-inner">
                                        <span>Low dose computed tomography is a mainstream for clinical applications.
How-ever, compared to normal dose CT, in the low dose CT (LDCT) images, there
are stronger noise and more artifacts which are obstacles for practical
applications. In the last few years, convolution-based end-to-end deep learning
methods have been widely used for LDCT image denoising. Recently, transformer
has shown superior performance over convolution with more feature interactions.
Yet its ap-plications in LDCT denoising have not been fully cultivated. Here,
we propose a convolution-free T2T vision transformer-based Encoder-decoder
Dilation net-work (TED-net) to enrich the family of LDCT denoising algorithms.
The model is free of convolution blocks and consists of a symmetric
encoder-decoder block with sole transformer. Our model is evaluated on the
AAPM-Mayo clinic LDCT Grand Challenge dataset, and results show outperformance
over the state-of-the-art denoising methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.IR"">cs.IR updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes. (arXiv:2012.14210v2 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1">Nils Reimers</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1">Iryna Gurevych</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14210">
                                    <div class="article-summary-box-inner">
                                        <span>Information Retrieval using dense low-dimensional representations recently
became popular and showed out-performance to traditional sparse-representations
like BM25. However, no previous work investigated how dense representations
perform with large index sizes. We show theoretically and empirically that the
performance for dense representations decreases quicker than sparse
representations for increasing index sizes. In extreme cases, this can even
lead to a tipping point where at a certain index size sparse representations
outperform dense representations. We show that this behavior is tightly
connected to the number of dimensions of the representations: The lower the
dimension, the higher the chance for false positives, i.e. returning irrelevant
documents.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Open-World Recommendation: An Inductive Model-based Collaborative Filtering Approach. (arXiv:2007.04833v2 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qitian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hengrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xiaofeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1">Hongyuan Zha</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.04833">
                                    <div class="article-summary-box-inner">
                                        <span>Recommendation models can effectively estimate underlying user interests and
predict one&#x27;s future behaviors by factorizing an observed user-item rating
matrix into products of two sets of latent factors. However, the user-specific
embedding factors can only be learned in a transductive way, making it
difficult to handle new users on-the-fly. In this paper, we propose an
inductive collaborative filtering framework that contains two representation
models. The first model follows conventional matrix factorization which
factorizes a group of key users&#x27; rating matrix to obtain meta latents. The
second model resorts to attention-based structure learning that estimates
hidden relations from query to key users and learns to leverage meta latents to
inductively compute embeddings for query users via neural message passing. Our
model enables inductive representation learning for users and meanwhile
guarantees equivalent representation capacity as matrix factorization.
Experiments demonstrate that our model achieves promising results for
recommendation on few-shot users with limited training ratings and new unseen
users which are commonly encountered in open-world recommender systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Balancing Reinforcement Learning Training Experiences in Interactive Information Retrieval. (arXiv:2006.03185v2 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Limin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhiwen Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Grace Hui Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.03185">
                                    <div class="article-summary-box-inner">
                                        <span>Interactive Information Retrieval (IIR) and Reinforcement Learning (RL) share
many commonalities, including an agent who learns while interacts, a long-term
and complex goal, and an algorithm that explores and adapts. To successfully
apply RL methods to IIR, one challenge is to obtain sufficient relevance labels
to train the RL agents, which are infamously known as sample inefficient.
However, in a text corpus annotated for a given query, it is not the relevant
documents but the irrelevant documents that predominate. This would cause very
unbalanced training experiences for the agent and prevent it from learning any
policy that is effective. Our paper addresses this issue by using domain
randomization to synthesize more relevant documents for the training. Our
experimental results on the Text REtrieval Conference (TREC) Dynamic Domain
(DD) 2017 Track show that the proposed method is able to boost an RL agent&#x27;s
learning effectiveness by 22\% in dealing with unseen situations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DeepTileBars: Visualizing Term Distribution for Neural Information Retrieval. (arXiv:1811.00606v3 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhiwen Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Grace Hui Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1811.00606">
                                    <div class="article-summary-box-inner">
                                        <span>Most neural Information Retrieval (Neu-IR) models derive query-to-document
ranking scores based on term-level matching. Inspired by TileBars, a classical
term distribution visualization method, in this paper, we propose a novel
Neu-IR model that handles query-to-document matching at the subtopic and higher
levels. Our system first splits the documents into topical segments,
&quot;visualizes&quot; the matchings between the query and the segments, and then feeds
an interaction matrix into a Neu-IR model, DeepTileBars, to obtain the final
ranking scores. DeepTileBars models the relevance signals occurring at
different granularities in a document&#x27;s topic hierarchy. It better captures the
discourse structure of a document and thus the matching patterns. Although its
design and implementation are light-weight, DeepTileBars outperforms other
state-of-the-art Neu-IR models on benchmark datasets including the Text
REtrieval Conference (TREC) 2010-2012 Web Tracks and LETOR 4.0.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Global Context Enhanced Graph Neural Networks for Session-based Recommendation. (arXiv:2106.05081v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1">Wei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1">Gao Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiao-Li Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1">Xian-Ling Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1">Minghui Qiu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05081">
                                    <div class="article-summary-box-inner">
                                        <span>Session-based recommendation (SBR) is a challenging task, which aims at
recommending items based on anonymous behavior sequences. Almost all the
existing solutions for SBR model user preference only based on the current
session without exploiting the other sessions, which may contain both relevant
and irrelevant item-transitions to the current session. This paper proposes a
novel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN)
to exploit item transitions over all sessions in a more subtle manner for
better inferring the user preference of the current session. Specifically,
GCE-GNN learns two levels of item embeddings from session graph and global
graph, respectively: (i) Session graph, which is to learn the session-level
item embedding by modeling pairwise item-transitions within the current
session; and (ii) Global graph, which is to learn the global-level item
embedding by modeling pairwise item-transitions over all sessions. In GCE-GNN,
we propose a novel global-level item representation learning layer, which
employs a session-aware attention mechanism to recursively incorporate the
neighbors&#x27; embeddings of each node on the global graph. We also design a
session-level item representation learning layer, which employs a GNN on the
session graph to learn session-level item embeddings within the current
session. Moreover, GCE-GNN aggregates the learnt item representations in the
two levels with a soft attention mechanism. Experiments on three benchmark
datasets demonstrate that GCE-GNN outperforms the state-of-the-art methods
consistently.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AutoFT: Automatic Fine-Tune for Parameters Transfer Learning in Click-Through Rate Prediction. (arXiv:2106.04873v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiangli Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1">Rong Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1">Ruiming Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhirong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiuqiang He</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04873">
                                    <div class="article-summary-box-inner">
                                        <span>Recommender systems are often asked to serve multiple recommendation
scenarios or domains. Fine-tuning a pre-trained CTR model from source domains
and adapting it to a target domain allows knowledge transferring. However,
optimizing all the parameters of the pre-trained network may result in
over-fitting if the target dataset is small and the number of parameters is
large. This leads us to think of directly reusing parameters in the pre-trained
model which represent more general features learned from multiple domains.
However, the design of freezing or fine-tuning layers of parameters requires
much manual effort since the decision highly depends on the pre-trained model
and target instances. In this work, we propose an end-to-end transfer learning
framework, called Automatic Fine-Tuning (AutoFT), for CTR prediction. AutoFT
consists of a field-wise transfer policy and a layer-wise transfer policy. The
field-wise transfer policy decides how the pre-trained embedding
representations are frozen or fine-tuned based on the given instance from the
target domain. The layer-wise transfer policy decides how the high?order
feature representations are transferred layer by layer. Extensive experiments
on two public benchmark datasets and one private industrial dataset demonstrate
that AutoFT can significantly improve the performance of CTR prediction
compared with state-of-the-art transferring approaches.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Corpus-Level End-to-End Exploration for Interactive Systems. (arXiv:1912.00753v2 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1">Zhiwen Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Grace Hui Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.00753">
                                    <div class="article-summary-box-inner">
                                        <span>A core interest in building Artificial Intelligence (AI) agents is to let
them interact with and assist humans. One example is Dynamic Search (DS), which
models the process that a human works with a search engine agent to accomplish
a complex and goal-oriented task. Early DS agents using Reinforcement Learning
(RL) have only achieved limited success for (1) their lack of direct control
over which documents to return and (2) the difficulty to recover from wrong
search trajectories. In this paper, we present a novel corpus-level end-to-end
exploration (CE3) method to address these issues. In our method, an entire text
corpus is compressed into a global low-dimensional representation, which
enables the agent to gain access to the full state and action spaces, including
the under-explored areas. We also propose a new form of retrieval function,
whose linear approximation allows end-to-end manipulation of documents.
Experiments on the Text REtrieval Conference (TREC) Dynamic Domain (DD) Track
show that CE3 outperforms the state-of-the-art DS systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Single-Server Private Linear Transformation: The Individual Privacy Case. (arXiv:2106.05222v1 [cs.IT])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Heidarzadeh_A/0/1/0/all/0/1">Anoosheh Heidarzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Esmati_N/0/1/0/all/0/1">Nahid Esmati</a>, <a href="http://arxiv.org/find/cs/1/au:+Sprintson_A/0/1/0/all/0/1">Alex Sprintson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05222">
                                    <div class="article-summary-box-inner">
                                        <span>This paper considers the single-server Private Linear Transformation (PLT)
problem with individual privacy guarantees. In this problem, there is a user
that wishes to obtain $L$ independent linear combinations of a $D$-subset of
messages belonging to a dataset of $K$ messages stored on a single server. The
goal is to minimize the download cost while keeping the identity of each
message required for the computation individually private. The individual
privacy requirement ensures that the identity of each individual message
required for the computation is kept private. This is in contrast to the
stricter notion of joint privacy that protects the entire set of identities of
all messages used for the computation, including the correlations between these
identities. The notion of individual privacy captures a broad set of practical
applications. For example, such notion is relevant when the dataset contains
information about individuals, each of them requires privacy guarantees for
their data access patterns. We focus on the setting in which the required
linear transformation is associated with a maximum distance separable (MDS)
matrix. In particular, we require that the matrix of coefficients pertaining to
the required linear combinations is the generator matrix of an MDS code. We
establish lower and upper bounds on the capacity of PLT with individual
privacy, where the capacity is defined as the supremum of all achievable
download rates. We show that our bounds are tight under certain conditions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DIGRAC: Digraph Clustering with Flow Imbalance. (arXiv:2106.05194v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+He_Y/0/1/0/all/0/1">Yixuan He</a>, <a href="http://arxiv.org/find/stat/1/au:+Reinert_G/0/1/0/all/0/1">Gesine Reinert</a>, <a href="http://arxiv.org/find/stat/1/au:+Cucuringu_M/0/1/0/all/0/1">Mihai Cucuringu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05194">
                                    <div class="article-summary-box-inner">
                                        <span>Node clustering is a powerful tool in the analysis of networks. Here, we
introduce a graph neural network framework with a novel scalable Directed Mixed
Path Aggregation(DIMPA) scheme to obtain node embeddings for directed networks
in a self-supervised manner, including a novel probabilistic imbalance loss.
The method is end-to-end in combining embedding generation and clustering
without an intermediate step. In contrast to standard approaches in the
literature, in this paper, directionality is not treated as a nuisance, but
rather contains the main signal. In particular, we leverage the recently
introduced cut flow imbalance measure, which is tightly related to
directionality; cut flow imbalance is optimized without resorting to spectral
methods or cluster labels. Experimental results on synthetic data, in the form
of directed stochastic block models and real-world data at different scales,
demonstrate that our method attains state-of-the-art results on directed
clustering, for a wide range of noise and sparsity levels, as well as graph
structures.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Helping results assessment by adding explainable elements to the deep relevance matching model. (arXiv:2106.05147v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chios_I/0/1/0/all/0/1">Ioannis Chios</a>, <a href="http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1">Suzan Verberne</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05147">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper we address the explainability of web search engines. We propose
two explainable elements on the search engine result page: a visualization of
query term weights and a visualization of passage relevance. The idea is that
search engines that indicate to the user why results are retrieved are valued
higher by users and gain user trust. We deduce the query term weights from the
term gating network in the Deep Relevance Matching Model (DRMM) and visualize
them as a doughnut chart. In addition, we train a passage-level ranker with
DRMM that selects the most relevant passage from each document and shows it as
snippet on the result page. Next to the snippet we show a document thumbnail
with this passage highlighted. We evaluate the proposed interface in an online
user study, asking users to judge the explainability and assessability of the
interface. We found that users judge our proposed interface significantly more
explainable and easier to assess than a regular search engine result page.
However, they are not significantly better in selecting the relevant documents
from the top-5. This indicates that the explainability of the search engine
result page leads to a better user experience. Thus, we conclude that the
proposed explainable elements are promising as visualization for search engine
users.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sirius: A Mutual Information Tool for Exploratory Visualization of Mixed Data. (arXiv:2106.05260v1 [stat.AP])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Adams_J/0/1/0/all/0/1">Jane L. Adams</a>, <a href="http://arxiv.org/find/stat/1/au:+Deluca_T/0/1/0/all/0/1">Todd F. Deluca</a>, <a href="http://arxiv.org/find/stat/1/au:+Danforth_C/0/1/0/all/0/1">Christopher M. Danforth</a>, <a href="http://arxiv.org/find/stat/1/au:+Dodds_P/0/1/0/all/0/1">Peter S. Dodds</a>, <a href="http://arxiv.org/find/stat/1/au:+Zheng_Y/0/1/0/all/0/1">Yuhang Zheng</a>, <a href="http://arxiv.org/find/stat/1/au:+Anastasakis_K/0/1/0/all/0/1">Konstantinos Anastasakis</a>, <a href="http://arxiv.org/find/stat/1/au:+Choi_B/0/1/0/all/0/1">Boyoon Choi</a>, <a href="http://arxiv.org/find/stat/1/au:+Min_A/0/1/0/all/0/1">Allison Min</a>, <a href="http://arxiv.org/find/stat/1/au:+Bessey_M/0/1/0/all/0/1">Michael M. Bessey</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05260">
                                    <div class="article-summary-box-inner">
                                        <span>Data scientists across disciplines are increasingly in need of exploratory
analysis tools for data sets with a high volume of features. We expand upon
graph mining approaches for exploratory analysis of high-dimensional data to
introduce Sirius, a visualization package for researchers to explore feature
relationships among mixed data types using mutual information and network
backbone sparsification. Visualizations of feature relationships aid data
scientists in finding meaningful dependence among features, which can engender
further analysis for feature selection, feature extraction, projection,
identification of proxy variables, or insight into temporal variation at the
macro scale. Graph mining approaches for feature analysis exist, such as
association networks of binary features, or correlation networks of
quantitative features, but mixed data types present a unique challenge for
developing comprehensive feature networks for exploratory analysis. Using an
information theoretic approach, Sirius supports heterogeneous data sets
consisting of binary, continuous quantitative, and discrete categorical data
types, and provides a user interface exploring feature pairs with high mutual
information scores. We leverage a backbone sparsification approach from network
theory as a dimensionality reduction technique, which probabilistically trims
edges according to the local network context. Sirius is an open source Python
package and Django web application for exploratory visualization, which can be
deployed in data analysis pipelines. The Sirius codebase and exemplary data
sets can be found at: https://github.com/compstorylab/sirius</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning to Rank Words: Optimizing Ranking Metrics for Word Spotting. (arXiv:2106.05144v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1">Pau Riba</a>, <a href="http://arxiv.org/find/cs/1/au:+Molina_A/0/1/0/all/0/1">Adri&#xe0; Molina</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1">Lluis Gomez</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramos_Terrades_O/0/1/0/all/0/1">Oriol Ramos-Terrades</a>, <a href="http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1">Josep Llad&#xf3;s</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05144">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we explore and evaluate the use of ranking-based objective
functions for learning simultaneously a word string and a word image encoder.
We consider retrieval frameworks in which the user expects a retrieval list
ranked according to a defined relevance score. In the context of a word
spotting problem, the relevance score has been set according to the string edit
distance from the query string. We experimentally demonstrate the competitive
performance of the proposed model on query-by-string word spotting for both,
handwritten and real scene word images. We also provide the results for
query-by-example word spotting, although it is not the main focus of this work.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Single-Server Private Linear Transformation: The Joint Privacy Case. (arXiv:2106.05220v1 [cs.IT])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Heidarzadeh_A/0/1/0/all/0/1">Anoosheh Heidarzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Esmati_N/0/1/0/all/0/1">Nahid Esmati</a>, <a href="http://arxiv.org/find/cs/1/au:+Sprintson_A/0/1/0/all/0/1">Alex Sprintson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05220">
                                    <div class="article-summary-box-inner">
                                        <span>This paper introduces the problem of Private Linear Transformation (PLT)
which generalizes the problems of private information retrieval and private
linear computation. The PLT problem includes one or more remote server(s)
storing (identical copies of) $K$ messages and a user who wants to compute $L$
independent linear combinations of a $D$-subset of messages. The objective of
the user is to perform the computation by downloading minimum possible amount
of information from the server(s), while protecting the identities of the $D$
messages required for the computation. In this work, we focus on the
single-server setting of the PLT problem when the identities of the $D$
messages required for the computation must be protected jointly. We consider
two different models, depending on whether the coefficient matrix of the
required $L$ linear combinations generates a Maximum Distance Separable (MDS)
code. We prove that the capacity for both models is given by $L/(K-D+L)$, where
the capacity is defined as the supremum of all achievable download rates. Our
converse proofs are based on linear-algebraic and information-theoretic
arguments that establish connections between PLT schemes and linear codes. We
also present an achievability scheme for each of the models being considered.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Extractive Search. (arXiv:2106.04612v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1">Shauli Ravfogel</a>, <a href="http://arxiv.org/find/cs/1/au:+Taub_Tabib_H/0/1/0/all/0/1">Hillel Taub-Tabib</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1">Yoav Goldberg</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04612">
                                    <div class="article-summary-box-inner">
                                        <span>Domain experts often need to extract structured information from large
corpora. We advocate for a search paradigm called &#x60;&#x60;extractive search&#x27;&#x27;, in
which a search query is enriched with capture-slots, to allow for such rapid
extraction. Such an extractive search system can be built around syntactic
structures, resulting in high-precision, low-recall results. We show how the
recall can be improved using neural retrieval and alignment. The goals of this
paper are to concisely introduce the extractive-search paradigm; and to
demonstrate a prototype neural retrieval system for extractive search and its
benefits and potential. Our prototype is available at
\url{https://spike.neural-sim.apps.allenai.org/} and a video demonstration is
available at \url{https://vimeo.com/559586687}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Initialization Matters: Regularizing Manifold-informed Initialization for Neural Recommendation Systems. (arXiv:2106.04993v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yinan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1">Chunyan Miao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04993">
                                    <div class="article-summary-box-inner">
                                        <span>Proper initialization is crucial to the optimization and the generalization
of neural networks. However, most existing neural recommendation systems
initialize the user and item embeddings randomly. In this work, we propose a
new initialization scheme for user and item embeddings called Laplacian
Eigenmaps with Popularity-based Regularization for Isolated Data (LEPORID).
LEPORID endows the embeddings with information regarding multi-scale
neighborhood structures on the data manifold and performs adaptive
regularization to compensate for high embedding variance on the tail of the
data distribution. Exploiting matrix sparsity, LEPORID embeddings can be
computed efficiently. We evaluate LEPORID in a wide range of neural
recommendation models. In contrast to the recent surprising finding that the
simple K-nearest-neighbor (KNN) method often outperforms neural recommendation
systems, we show that existing neural systems initialized with LEPORID often
perform on par or better than KNN. To maximize the effects of the
initialization, we propose the Dual-Loss Residual Recommendation (DLR2)
network, which, when initialized with LEPORID, substantially outperforms both
traditional and state-of-the-art neural recommender systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.LG"">cs.LG updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose. (arXiv:2106.01981v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Oreshkin_B/0/1/0/all/0/1">Boris N. Oreshkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bocquelet_F/0/1/0/all/0/1">Florent Bocquelet</a>, <a href="http://arxiv.org/find/cs/1/au:+Harvey_F/0/1/0/all/0/1">F&#xe9;lix G. Harvey</a>, <a href="http://arxiv.org/find/cs/1/au:+Raitt_B/0/1/0/all/0/1">Bay Raitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Laflamme_D/0/1/0/all/0/1">Dominic Laflamme</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01981">
                                    <div class="article-summary-box-inner">
                                        <span>Our work focuses on the development of a learnable neural representation of
human pose for advanced AI assisted animation tooling. Specifically, we tackle
the problem of constructing a full static human pose based on sparse and
variable user inputs (e.g. locations and/or orientations of a subset of body
joints). To solve this problem, we propose a novel neural architecture that
combines residual connections with prototype encoding of a partially specified
pose to create a new complete pose from the learned latent space. We show that
our architecture outperforms a baseline based on Transformer, both in terms of
accuracy and computational efficiency. Additionally, we develop a user
interface to integrate our neural model in Unity, a real-time 3D development
platform. Furthermore, we introduce two new datasets representing the static
human pose modeling problem, based on high-quality human motion capture data,
which will be released publicly along with model code.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Network insensitivity to parameter noise via adversarial regularization. (arXiv:2106.05009v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bucher_J/0/1/0/all/0/1">Julian B&#xfc;cher</a>, <a href="http://arxiv.org/find/cs/1/au:+Faber_F/0/1/0/all/0/1">Fynn Faber</a>, <a href="http://arxiv.org/find/cs/1/au:+Muir_D/0/1/0/all/0/1">Dylan R. Muir</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05009">
                                    <div class="article-summary-box-inner">
                                        <span>Neuromorphic neural network processors, in the form of compute-in-memory
crossbar arrays of memristors, or in the form of subthreshold analog and
mixed-signal ASICs, promise enormous advantages in compute density and energy
efficiency for NN-based ML tasks. However, these technologies are prone to
computational non-idealities, due to process variation and intrinsic device
physics. This degrades the task performance of networks deployed to the
processor, by introducing parameter noise into the deployed model. While it is
possible to calibrate each device, or train networks individually for each
processor, these approaches are expensive and impractical for commercial
deployment. Alternative methods are therefore needed to train networks that are
inherently robust against parameter variation, as a consequence of network
architecture and parameters. We present a new adversarial network optimisation
algorithm that attacks network parameters during training, and promotes robust
performance during inference in the face of parameter variation. Our approach
introduces a regularization term penalising the susceptibility of a network to
weight perturbation. We compare against previous approaches for producing
parameter insensitivity such as dropout, weight smoothing and introducing
parameter noise during training. We show that our approach produces models that
are more robust to targeted parameter variation, and equally robust to random
parameter variation. Our approach finds minima in flatter locations in the
weight-loss landscape compared with other approaches, highlighting that the
networks found by our technique are less sensitive to parameter perturbation.
Our work provides an approach to deploy neural network architectures to
inference devices that suffer from computational non-idealities, with minimal
loss of performance. ...</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Ensemble Search for Uncertainty Estimation and Dataset Shift. (arXiv:2006.08573v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1">Sheheryar Zaidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zela_A/0/1/0/all/0/1">Arber Zela</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsken_T/0/1/0/all/0/1">Thomas Elsken</a>, <a href="http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1">Chris Holmes</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1">Yee Whye Teh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.08573">
                                    <div class="article-summary-box-inner">
                                        <span>Ensembles of neural networks achieve superior performance compared to
stand-alone networks in terms of accuracy, uncertainty calibration and
robustness to dataset shift. \emph{Deep ensembles}, a state-of-the-art method
for uncertainty estimation, only ensemble random initializations of a
\emph{fixed} architecture. Instead, we propose two methods for automatically
constructing ensembles with \emph{varying} architectures, which implicitly
trade-off individual architectures&#x27; strengths against the ensemble&#x27;s diversity
and exploit architectural variation as a source of diversity. On a variety of
classification tasks and modern architecture search spaces, we show that the
resulting ensembles outperform deep ensembles not only in terms of accuracy but
also uncertainty calibration and robustness to dataset shift. Our further
analysis and ablation studies provide evidence of higher ensemble diversity due
to architectural variation, resulting in ensembles that can outperform deep
ensembles, even when having weaker average base learners.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training. (arXiv:2106.05091v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kimin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1">Laura Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1">Pieter Abbeel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05091">
                                    <div class="article-summary-box-inner">
                                        <span>Conveying complex objectives to reinforcement learning (RL) agents can often
be difficult, involving meticulous design of reward functions that are
sufficiently informative yet easy enough to provide. Human-in-the-loop RL
methods allow practitioners to instead interactively teach agents through
tailored feedback; however, such approaches have been challenging to scale
since human feedback is very expensive. In this work, we aim to make this
process more sample- and feedback-efficient. We present an off-policy,
interactive RL algorithm that capitalizes on the strengths of both feedback and
off-policy learning. Specifically, we learn a reward model by actively querying
a teacher&#x27;s preferences between two clips of behavior and use it to train an
agent. To enable off-policy learning, we relabel all the agent&#x27;s past
experience when its reward model changes. We additionally show that
pre-training our agents with unsupervised exploration substantially increases
the mileage of its queries. We demonstrate that our approach is capable of
learning tasks of higher complexity than previously considered by
human-in-the-loop methods, including a variety of locomotion and robotic
manipulation skills. We also show that our method is able to utilize real-time
human feedback to effectively prevent reward exploitation and learn new
behaviors that are difficult to specify with standard reward functions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Survival Machines: Fully Parametric Survival Regression and Representation Learning for Censored Data with Competing Risks. (arXiv:2003.01176v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nagpal_C/0/1/0/all/0/1">Chirag Nagpal</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xinyu Rachel Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1">Artur Dubrawski</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.01176">
                                    <div class="article-summary-box-inner">
                                        <span>We describe a new approach to estimating relative risks in time-to-event
prediction problems with censored data in a fully parametric manner. Our
approach does not require making strong assumptions of constant proportional
hazard of the underlying survival distribution, as required by the
Cox-proportional hazard model. By jointly learning deep nonlinear
representations of the input covariates, we demonstrate the benefits of our
approach when used to estimate survival risks through extensive experimentation
on multiple real world datasets with different levels of censoring. We further
demonstrate advantages of our model in the competing risks scenario. To the
best of our knowledge, this is the first work involving fully parametric
estimation of survival times with competing risks in the presence of censoring.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Polynomial magic! Hermite polynomials for private data generation. (arXiv:2106.05042v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1">Mijung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinaroz_M/0/1/0/all/0/1">Margarita Vinaroz</a>, <a href="http://arxiv.org/find/cs/1/au:+Charusaie_M/0/1/0/all/0/1">Mohammad-Amin Charusaie</a>, <a href="http://arxiv.org/find/cs/1/au:+Harder_F/0/1/0/all/0/1">Frederik Harder</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05042">
                                    <div class="article-summary-box-inner">
                                        <span>Kernel mean embedding is a useful tool to compare probability measures.
Despite its usefulness, kernel mean embedding considers infinite-dimensional
features, which are challenging to handle in the context of differentially
private data generation. A recent work proposes to approximate the kernel mean
embedding of data distribution using finite-dimensional random features, where
the sensitivity of the features becomes analytically tractable. More
importantly, this approach significantly reduces the privacy cost, compared to
other known privatization methods (e.g., DP-SGD), as the approximate kernel
mean embedding of the data distribution is privatized only once and can then be
repeatedly used during training of a generator without incurring any further
privacy cost. However, the required number of random features is excessively
high, often ten thousand to a hundred thousand, which worsens the sensitivity
of the approximate kernel mean embedding. To improve the sensitivity, we
propose to replace random features with Hermite polynomial features. Unlike the
random features, the Hermite polynomial features are ordered, where the
features at the low orders contain more information on the distribution than
those at the high orders. Hence, a relatively low order of Hermite polynomial
features can more accurately approximate the mean embedding of the data
distribution compared to a significantly higher number of random features. As a
result, using the Hermite polynomial features, we significantly improve the
privacy-accuracy trade-off, reflected in the high quality and diversity of the
generated data, when tested on several heterogeneous tabular datasets, as well
as several image benchmark datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding. (arXiv:2106.04970v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xin Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1">Tao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1">Furu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Houfeng Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04970">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the
online inference efficiency of the Transformer for instantaneous Grammatical
Error Correction (GEC). SAD optimizes the online inference efficiency for GEC
by two innovations: 1) it aggressively decodes as many tokens as possible in
parallel instead of always decoding only one token in each step to improve
computational parallelism; 2) it uses a shallow decoder instead of the
conventional Transformer architecture with balanced encoder-decoder depth to
reduce the computational cost during inference. Experiments in both English and
Chinese GEC benchmarks show that aggressive decoding could yield the same
predictions as greedy decoding but with a significant speedup for online
inference. Its combination with the shallow decoder could offer an even higher
online inference speedup over the powerful Transformer baseline without quality
loss. Not only does our approach allow a single model to achieve the
state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14
and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference
speedup over the Transformer-big model, but also it is easily adapted to other
languages. Our code is available at
https://github.com/AutoTemp/Shallow-Aggressive-Decoding.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Black-box density function estimation using recursive partitioning. (arXiv:2010.13632v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Bodin_E/0/1/0/all/0/1">Erik Bodin</a>, <a href="http://arxiv.org/find/stat/1/au:+Dai_Z/0/1/0/all/0/1">Zhenwen Dai</a>, <a href="http://arxiv.org/find/stat/1/au:+Campbell_N/0/1/0/all/0/1">Neill D. F. Campbell</a>, <a href="http://arxiv.org/find/stat/1/au:+Ek_C/0/1/0/all/0/1">Carl Henrik Ek</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13632">
                                    <div class="article-summary-box-inner">
                                        <span>We present a novel approach to Bayesian inference and general Bayesian
computation that is defined through a sequential decision loop. Our method
defines a recursive partitioning of the sample space. It neither relies on
gradients nor requires any problem-specific tuning, and is asymptotically exact
for any density function with a bounded domain. The output is an approximation
to the whole density function including the normalisation constant, via
partitions organised in efficient data structures. Such approximations may be
used for evidence estimation or fast posterior sampling, but also as building
blocks to treat a larger class of estimation problems. The algorithm shows
competitive performance to recent state-of-the-art methods on synthetic and
real-world problems including parameter inference for gravitational-wave
physics.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The dilemma of quantum neural networks. (arXiv:2106.04975v1 [quant-ph])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/quant-ph/1/au:+Qian_Y/0/1/0/all/0/1">Yang Qian</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wang_X/0/1/0/all/0/1">Xinbiao Wang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1">Yuxuan Du</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wu_X/0/1/0/all/0/1">Xingyao Wu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04975">
                                    <div class="article-summary-box-inner">
                                        <span>The core of quantum machine learning is to devise quantum models with good
trainability and low generalization error bound than their classical
counterparts to ensure better reliability and interpretability. Recent studies
confirmed that quantum neural networks (QNNs) have the ability to achieve this
goal on specific datasets. With this regard, it is of great importance to
understand whether these advantages are still preserved on real-world tasks.
Through systematic numerical experiments, we empirically observe that current
QNNs fail to provide any benefit over classical learning models. Concretely,
our results deliver two key messages. First, QNNs suffer from the severely
limited effective model capacity, which incurs poor generalization on
real-world datasets. Second, the trainability of QNNs is insensitive to
regularization techniques, which sharply contrasts with the classical scenario.
These empirical results force us to rethink the role of current QNNs and to
design novel protocols for solving real-world problems with quantum advantages.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Uncovering Closed-form Governing Equations of Nonlinear Dynamics from Videos. (arXiv:2106.04776v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Luan_L/0/1/0/all/0/1">Lele Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04776">
                                    <div class="article-summary-box-inner">
                                        <span>Distilling analytical models from data has the potential to advance our
understanding and prediction of nonlinear dynamics. Although discovery of
governing equations based on observed system states (e.g., trajectory time
series) has revealed success in a wide range of nonlinear dynamics, uncovering
the closed-form equations directly from raw videos still remains an open
challenge. To this end, we introduce a novel end-to-end unsupervised deep
learning framework to uncover the mathematical structure of equations that
governs the dynamics of moving objects in videos. Such an architecture consists
of (1) an encoder-decoder network that learns low-dimensional spatial/pixel
coordinates of the moving object, (2) a learnable Spatial-Physical
Transformation component that creates mapping between the extracted
spatial/pixel coordinates and the latent physical states of dynamics, and (3) a
numerical integrator-based sparse regression module that uncovers the
parsimonious closed-form governing equations of learned physical states and,
meanwhile, serves as a constraint to the autoencoder. The efficacy of the
proposed method is demonstrated by uncovering the governing equations of a
variety of nonlinear dynamical systems depicted by moving objects in videos.
The resulting computational framework enables discovery of parsimonious
interpretable model in a flexible and accessible sensing environment where only
videos are available.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multiple Kernel Representation Learning on Networks. (arXiv:2106.05057v1 [cs.SI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Celikkanat_A/0/1/0/all/0/1">Abdulkadir Celikkanat</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yanning Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1">Fragkiskos D. Malliaros</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05057">
                                    <div class="article-summary-box-inner">
                                        <span>Learning representations of nodes in a low dimensional space is a crucial
task with numerous interesting applications in network analysis, including link
prediction, node classification, and visualization. Two popular approaches for
this problem are matrix factorization and random walk-based models. In this
paper, we aim to bring together the best of both worlds, towards learning node
representations. In particular, we propose a weighted matrix factorization
model that encodes random walk-based information about nodes of the network.
The benefit of this novel formulation is that it enables us to utilize kernel
functions without realizing the exact proximity matrix so that it enhances the
expressiveness of existing matrix decomposition methods with kernels and
alleviates their computational complexities. We extend the approach with a
multiple kernel learning formulation that provides the flexibility of learning
the kernel as the linear combination of a dictionary of kernels in data-driven
fashion. We perform an empirical evaluation on real-world networks, showing
that the proposed model outperforms baseline node embedding algorithms in
downstream machine learning tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An Efficient Framework for Clustered Federated Learning. (arXiv:2006.04088v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1">Avishek Ghosh</a>, <a href="http://arxiv.org/find/stat/1/au:+Chung_J/0/1/0/all/0/1">Jichan Chung</a>, <a href="http://arxiv.org/find/stat/1/au:+Yin_D/0/1/0/all/0/1">Dong Yin</a>, <a href="http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1">Kannan Ramchandran</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04088">
                                    <div class="article-summary-box-inner">
                                        <span>We address the problem of federated learning (FL) where users are distributed
and partitioned into clusters. This setup captures settings where different
groups of users have their own objectives (learning tasks) but by aggregating
their data with others in the same cluster (same learning task), they can
leverage the strength in numbers in order to perform more efficient federated
learning. For this new framework of clustered federated learning, we propose
the Iterative Federated Clustering Algorithm (IFCA), which alternately
estimates the cluster identities of the users and optimizes model parameters
for the user clusters via gradient descent. We analyze the convergence rate of
this algorithm first in a linear model with squared loss and then for generic
strongly convex and smooth loss functions. We show that in both settings, with
good initialization, IFCA is guaranteed to converge, and discuss the optimality
of the statistical error rate. In particular, for the linear model with two
clusters, we can guarantee that our algorithm converges as long as the
initialization is slightly better than random. When the clustering structure is
ambiguous, we propose to train the models by combining IFCA with the weight
sharing technique in multi-task learning. In the experiments, we show that our
algorithm can succeed even if we relax the requirements on initialization with
random initialization and multiple restarts. We also present experimental
results showing that our algorithm is efficient in non-convex problems such as
neural networks. We demonstrate the benefits of IFCA over the baselines on
several clustered FL benchmarks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Interaction-Grounded Learning. (arXiv:2106.04887v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tengyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1">John Langford</a>, <a href="http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1">Paul Mineiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1">Ida Momennejad</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04887">
                                    <div class="article-summary-box-inner">
                                        <span>Consider a prosthetic arm, learning to adapt to its user&#x27;s control signals.
We propose Interaction-Grounded Learning for this novel setting, in which a
learner&#x27;s goal is to interact with the environment with no grounding or
explicit reward to optimize its policies. Such a problem evades common RL
solutions which require an explicit reward. The learning agent observes a
multidimensional context vector, takes an action, and then observes a
multidimensional feedback vector. This multidimensional feedback vector has no
explicit reward information. In order to succeed, the algorithm must learn how
to evaluate the feedback vector to discover a latent reward signal, with which
it can ground its policies without supervision. We show that in an
Interaction-Grounded Learning setting, with certain natural assumptions, a
learner can discover the latent reward and ground its policy for successful
interaction. We provide theoretical guarantees and a proof-of-concept empirical
evaluation to demonstrate the effectiveness of our proposed approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Explainable AI for medical imaging: Explaining pneumothorax diagnoses with Bayesian Teaching. (arXiv:2106.04684v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Folke_T/0/1/0/all/0/1">Tomas Folke</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Scott Cheng-Hsin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Anderson_S/0/1/0/all/0/1">Sean Anderson</a>, <a href="http://arxiv.org/find/cs/1/au:+Shafto_P/0/1/0/all/0/1">Patrick Shafto</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04684">
                                    <div class="article-summary-box-inner">
                                        <span>Limited expert time is a key bottleneck in medical imaging. Due to advances
in image classification, AI can now serve as decision-support for medical
experts, with the potential for great gains in radiologist productivity and, by
extension, public health. However, these gains are contingent on building and
maintaining experts&#x27; trust in the AI agents. Explainable AI may build such
trust by helping medical experts to understand the AI decision processes behind
diagnostic judgements. Here we introduce and evaluate explanations based on
Bayesian Teaching, a formal account of explanation rooted in the cognitive
science of human learning. We find that medical experts exposed to explanations
generated by Bayesian Teaching successfully predict the AI&#x27;s diagnostic
decisions and are more likely to certify the AI for cases when the AI is
correct than when it is wrong, indicating appropriate trust. These results show
that Explainable AI can be used to support human-AI collaboration in medical
imaging.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Joint System-Wise Optimization for Pipeline Goal-Oriented Dialog System. (arXiv:2106.04835v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zichuan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bowen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaodong He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tengyu Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04835">
                                    <div class="article-summary-box-inner">
                                        <span>Recent work (Takanobu et al., 2020) proposed the system-wise evaluation on
dialog systems and found that improvement on individual components (e.g., NLU,
policy) in prior work may not necessarily bring benefit to pipeline systems in
system-wise evaluation. To improve the system-wise performance, in this paper,
we propose new joint system-wise optimization techniques for the pipeline
dialog system. First, we propose a new data augmentation approach which
automates the labeling process for NLU training. Second, we propose a novel
stochastic policy parameterization with Poisson distribution that enables
better exploration and offers a principled way to compute policy gradient.
Third, we propose a reward bonus to help policy explore successful dialogs. Our
approaches outperform the competitive pipeline systems from Takanobu et al.
(2020) by big margins of 12% success rate in automatic system-wise evaluation
and of 16% success rate in human evaluation on the standard multi-domain
benchmark dataset MultiWOZ 2.1, and also outperform the recent state-of-the-art
end-to-end trained model from DSTC9.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fractal Structure and Generalization Properties of Stochastic Optimization Algorithms. (arXiv:2106.04881v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Camuto_A/0/1/0/all/0/1">Alexander Camuto</a>, <a href="http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1">George Deligiannidis</a>, <a href="http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1">Murat A. Erdogdu</a>, <a href="http://arxiv.org/find/stat/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1">Mert G&#xfc;rb&#xfc;zbalaban</a>, <a href="http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1">Umut &#x15e;im&#x15f;ekli</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhu_L/0/1/0/all/0/1">Lingjiong Zhu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04881">
                                    <div class="article-summary-box-inner">
                                        <span>Understanding generalization in deep learning has been one of the major
challenges in statistical learning theory over the last decade. While recent
work has illustrated that the dataset and the training algorithm must be taken
into account in order to obtain meaningful generalization bounds, it is still
theoretically not clear which properties of the data and the algorithm
determine the generalization performance. In this study, we approach this
problem from a dynamical systems theory perspective and represent stochastic
optimization algorithms as random iterated function systems (IFS). Well studied
in the dynamical systems literature, under mild assumptions, such IFSs can be
shown to be ergodic with an invariant measure that is often supported on sets
with a fractal structure. As our main contribution, we prove that the
generalization error of a stochastic optimization algorithm can be bounded
based on the &#x60;complexity&#x27; of the fractal structure that underlies its invariant
measure. Leveraging results from dynamical systems theory, we show that the
generalization error can be explicitly linked to the choice of the algorithm
(e.g., stochastic gradient descent -- SGD), algorithm hyperparameters (e.g.,
step-size, batch-size), and the geometry of the problem (e.g., Hessian of the
loss). We further specialize our results to specific problems (e.g.,
linear/logistic regression, one hidden-layered neural networks) and algorithms
(e.g., SGD and preconditioned variants), and obtain analytical estimates for
our bound.For modern neural networks, we develop an efficient algorithm to
compute the developed bound and support our theory with various experiments on
neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scale Free Adversarial Multi Armed Bandits. (arXiv:2106.04700v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Putta_S/0/1/0/all/0/1">Sudeep Raja Putta</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1">Shipra Agrawal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04700">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the Scale-Free Adversarial Multi Armed Bandit(MAB) problem, where
the player only knows the number of arms $n$ and not the scale or magnitude of
the losses. It sees bandit feedback about the loss vectors $l_1,\dots, l_T \in
\mathbb{R}^n$. The goal is to bound its regret as a function of $n$ and
$l_1,\dots, l_T$. We design a Follow The Regularized Leader(FTRL) algorithm,
which comes with the first scale-free regret guarantee for MAB. It uses the log
barrier regularizer, the importance weighted estimator, an adaptive learning
rate, and an adaptive exploration parameter. In the analysis, we introduce a
simple, unifying technique for obtaining regret inequalities for FTRL and
Online Mirror Descent(OMD) on the probability simplex using Potential Functions
and Mixed Bregmans. We also develop a new technique for obtaining local-norm
lower bounds for Bregman Divergences, which are crucial in bandit regret
bounds. These tools could be of independent interest.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Bi-Level Framework for Learning to Solve Combinatorial Optimization on Graphs. (arXiv:2106.04927v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Runzhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_Z/0/1/0/all/0/1">Zhigang Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Gan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiayi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1">Feng Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shuang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaokang Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04927">
                                    <div class="article-summary-box-inner">
                                        <span>Combinatorial Optimization (CO) has been a long-standing challenging research
topic featured by its NP-hard nature. Traditionally such problems are
approximately solved with heuristic algorithms which are usually fast but may
sacrifice the solution quality. Currently, machine learning for combinatorial
optimization (MLCO) has become a trending research topic, but most existing
MLCO methods treat CO as a single-level optimization by directly learning the
end-to-end solutions, which are hard to scale up and mostly limited by the
capacity of ML models given the high complexity of CO. In this paper, we
propose a hybrid approach to combine the best of the two worlds, in which a
bi-level framework is developed with an upper-level learning method to optimize
the graph (e.g. add, delete or modify edges in a graph), fused with a
lower-level heuristic algorithm solving on the optimized graph. Such a bi-level
approach simplifies the learning on the original hard CO and can effectively
mitigate the demand for model capacity. The experiments and results on several
popular CO problems like Directed Acyclic Graph scheduling, Graph Edit Distance
and Hamiltonian Cycle Problem show its effectiveness over manually designed
heuristics and single-level learning methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural Supervised Domain Adaptation by Augmenting Pre-trained Models with Random Units. (arXiv:2106.04935v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Meftah_S/0/1/0/all/0/1">Sara Meftah</a>, <a href="http://arxiv.org/find/cs/1/au:+Semmar_N/0/1/0/all/0/1">Nasredine Semmar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamaazousti_Y/0/1/0/all/0/1">Youssef Tamaazousti</a>, <a href="http://arxiv.org/find/cs/1/au:+Essafi_H/0/1/0/all/0/1">Hassane Essafi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadat_F/0/1/0/all/0/1">Fatiha Sadat</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04935">
                                    <div class="article-summary-box-inner">
                                        <span>Neural Transfer Learning (TL) is becoming ubiquitous in Natural Language
Processing (NLP), thanks to its high performance on many tasks, especially in
low-resourced scenarios. Notably, TL is widely used for neural domain
adaptation to transfer valuable knowledge from high-resource to low-resource
domains. In the standard fine-tuning scheme of TL, a model is initially
pre-trained on a source domain and subsequently fine-tuned on a target domain
and, therefore, source and target domains are trained using the same
architecture. In this paper, we show through interpretation methods that such
scheme, despite its efficiency, is suffering from a main limitation. Indeed,
although capable of adapting to new domains, pre-trained neurons struggle with
learning certain patterns that are specific to the target domain. Moreover, we
shed light on the hidden negative transfer occurring despite the high
relatedness between source and target domains, which may mitigate the final
gain brought by transfer learning. To address these problems, we propose to
augment the pre-trained model with normalised, weighted and randomly
initialised units that foster a better adaptation while maintaining the
valuable source knowledge. We show that our approach exhibits significant
improvements to the standard fine-tuning scheme for neural domain adaptation
from the news domain to the social media domain on four NLP tasks:
part-of-speech tagging, chunking, named entity recognition and morphosyntactic
tagging.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Energy-Based Models for Code Generation under Compilability Constraints. (arXiv:2106.04985v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1">Tomasz Korbak</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1">Hady Elsahar</a>, <a href="http://arxiv.org/find/cs/1/au:+Dymetman_M/0/1/0/all/0/1">Marc Dymetman</a>, <a href="http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1">Germ&#xe1;n Kruszewski</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04985">
                                    <div class="article-summary-box-inner">
                                        <span>Neural language models can be successfully trained on source code, leading to
applications such as code completion. However, their versatile autoregressive
self-supervision objective overlooks important global sequence-level features
that are present in the data such as syntactic correctness or compilability. In
this work, we pose the problem of learning to generate compilable code as
constraint satisfaction. We define an Energy-Based Model (EBM) representing a
pre-trained generative model with an imposed constraint of generating only
compilable sequences. We then use the KL-Adaptive Distributional Policy
Gradient algorithm (Khalifa et al., 2021) to train a generative model
approximating the EBM. We conduct experiments showing that our proposed
approach is able to improve compilability rates without sacrificing diversity
and complexity of the generated samples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Influence-Augmented Online Planning for Complex Environments. (arXiv:2010.11038v2 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jinke He</a>, <a href="http://arxiv.org/find/cs/1/au:+Suau_M/0/1/0/all/0/1">Miguel Suau</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliehoek_F/0/1/0/all/0/1">Frans A. Oliehoek</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11038">
                                    <div class="article-summary-box-inner">
                                        <span>How can we plan efficiently in real time to control an agent in a complex
environment that may involve many other agents? While existing sample-based
planners have enjoyed empirical success in large POMDPs, their performance
heavily relies on a fast simulator. However, real-world scenarios are complex
in nature and their simulators are often computationally demanding, which
severely limits the performance of online planners. In this work, we propose
influence-augmented online planning, a principled method to transform a
factored simulator of the entire environment into a local simulator that
samples only the state variables that are most relevant to the observation and
reward of the planning agent and captures the incoming influence from the rest
of the environment using machine learning methods. Our main experimental
results show that planning on this less accurate but much faster local
simulator with POMCP leads to higher real-time planning performance than
planning on the simulator that models the entire environment.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Handcrafted Backdoors in Deep Neural Networks. (arXiv:2106.04690v1 [cs.CR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1">Sanghyun Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1">Nicholas Carlini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1">Alexey Kurakin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04690">
                                    <div class="article-summary-box-inner">
                                        <span>Deep neural networks (DNNs), while accurate, are expensive to train. Many
practitioners, therefore, outsource the training process to third parties or
use pre-trained DNNs. This practice makes DNNs vulnerable to $backdoor$
$attacks$: the third party who trains the model may act maliciously to inject
hidden behaviors into the otherwise accurate model. Until now, the mechanism to
inject backdoors has been limited to $poisoning$.

We argue that such a supply-chain attacker has more attack techniques
available. To study this hypothesis, we introduce a handcrafted attack that
directly manipulates the parameters of a pre-trained model to inject backdoors.
Our handcrafted attacker has more degrees of freedom in manipulating model
parameters than poisoning. This makes it difficult for a defender to identify
or remove the manipulations with straightforward methods, such as statistical
analysis, adding random noises to model parameters, or clipping their values
within a certain range. Further, our attacker can combine the handcrafting
process with additional techniques, $e.g.$, jointly optimizing a trigger
pattern, to inject backdoors into complex networks effectively$-$the
meet-in-the-middle attack.

In evaluations, our handcrafted backdoors remain effective across four
datasets and four network architectures with a success rate above 96%. Our
backdoored models are resilient to both parameter-level backdoor removal
techniques and can evade existing defenses by slightly changing the backdoor
attack configurations. Moreover, we demonstrate the feasibility of suppressing
unwanted behaviors otherwise caused by poisoning. Our results suggest that
further research is needed for understanding the complete space of supply-chain
backdoor attacks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Loss function based second-order Jensen inequality and its application to particle variational inference. (arXiv:2106.05010v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Futami_F/0/1/0/all/0/1">Futoshi Futami</a>, <a href="http://arxiv.org/find/stat/1/au:+Iwata_T/0/1/0/all/0/1">Tomoharu Iwata</a>, <a href="http://arxiv.org/find/stat/1/au:+Ueda_N/0/1/0/all/0/1">Naonori Ueda</a>, <a href="http://arxiv.org/find/stat/1/au:+Sato_I/0/1/0/all/0/1">Issei Sato</a>, <a href="http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1">Masashi Sugiyama</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05010">
                                    <div class="article-summary-box-inner">
                                        <span>Bayesian model averaging, obtained as the expectation of a likelihood
function by a posterior distribution, has been widely used for prediction,
evaluation of uncertainty, and model selection. Various approaches have been
developed to efficiently capture the information in the posterior distribution;
one such approach is the optimization of a set of models simultaneously with
interaction to ensure the diversity of the individual models in the same way as
ensemble learning. A representative approach is particle variational inference
(PVI), which uses an ensemble of models as an empirical approximation for the
posterior distribution. PVI iteratively updates each model with a repulsion
force to ensure the diversity of the optimized models. However, despite its
promising performance, a theoretical understanding of this repulsion and its
association with the generalization ability remains unclear. In this paper, we
tackle this problem in light of PAC-Bayesian analysis. First, we provide a new
second-order Jensen inequality, which has the repulsion term based on the loss
function. Thanks to the repulsion term, it is tighter than the standard Jensen
inequality. Then, we derive a novel generalization error bound and show that it
can be reduced by enhancing the diversity of models. Finally, we derive a new
PVI that optimizes the generalization error bound directly. Numerical
experiments demonstrate that the performance of the proposed PVI compares
favorably with existing methods in the experiment.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Understanding Softmax Confidence and Uncertainty. (arXiv:2106.04972v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1">Tim Pearce</a>, <a href="http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1">Alexandra Brintrup</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04972">
                                    <div class="article-summary-box-inner">
                                        <span>It is often remarked that neural networks fail to increase their uncertainty
when predicting on data far from the training distribution. Yet naively using
softmax confidence as a proxy for uncertainty achieves modest success in tasks
exclusively testing for this, e.g., out-of-distribution (OOD) detection. This
paper investigates this contradiction, identifying two implicit biases that do
encourage softmax confidence to correlate with epistemic uncertainty: 1)
Approximately optimal decision boundary structure, and 2) Filtering effects of
deep networks. It describes why low-dimensional intuitions about softmax
confidence are misleading. Diagnostic experiments quantify reasons softmax
confidence can fail, finding that extrapolations are less to blame than overlap
between training and OOD data in final-layer representations.
Pre-trained/fine-tuned networks reduce this overlap.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Offline Inverse Reinforcement Learning. (arXiv:2106.05068v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jarboui_F/0/1/0/all/0/1">Firas Jarboui</a>, <a href="http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1">Vianney Perchet</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05068">
                                    <div class="article-summary-box-inner">
                                        <span>The objective of offline RL is to learn optimal policies when a fixed
exploratory demonstrations data-set is available and sampling additional
observations is impossible (typically if this operation is either costly or
rises ethical questions). In order to solve this problem, off the shelf
approaches require a properly defined cost function (or its evaluation on the
provided data-set), which are seldom available in practice. To circumvent this
issue, a reasonable alternative is to query an expert for few optimal
demonstrations in addition to the exploratory data-set. The objective is then
to learn an optimal policy w.r.t. the expert&#x27;s latent cost function. Current
solutions either solve a behaviour cloning problem (which does not leverage the
exploratory data) or a reinforced imitation learning problem (using a fixed
cost function that discriminates available exploratory trajectories from expert
ones). Inspired by the success of IRL techniques in achieving state of the art
imitation performances in online settings, we exploit GAN based data
augmentation procedures to construct the first offline IRL algorithm. The
obtained policies outperformed the aforementioned solutions on multiple OpenAI
gym environments.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data. (arXiv:2106.05001v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1">Mi Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Fei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1">Dapeng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1">Jian Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jiashi Feng</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05001">
                                    <div class="article-summary-box-inner">
                                        <span>A central challenge in training classification models in the real-world
federated system is learning with non-IID data. To cope with this, most of the
existing works involve enforcing regularization in local optimization or
improving the model aggregation scheme at the server. Other works also share
public datasets or synthesized samples to supplement the training of
under-represented classes or introduce a certain level of personalization.
Though effective, they lack a deep understanding of how the data heterogeneity
affects each layer of a deep classification model. In this paper, we bridge
this gap by performing an experimental analysis of the representations learned
by different layers. Our observations are surprising: (1) there exists a
greater bias in the classifier than other layers, and (2) the classification
performance can be significantly improved by post-calibrating the classifier
after federated training. Motivated by the above findings, we propose a novel
and simple algorithm called Classifier Calibration with Virtual Representations
(CCVR), which adjusts the classifier using virtual representations sampled from
an approximated gaussian mixture model. Experimental results demonstrate that
CCVR achieves state-of-the-art performance on popular federated learning
benchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple
yet effective method can shed some light on the future research of federated
learning with non-IID data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Initialization Matters: Regularizing Manifold-informed Initialization for Neural Recommendation Systems. (arXiv:2106.04993v1 [cs.IR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yinan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1">Chunyan Miao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04993">
                                    <div class="article-summary-box-inner">
                                        <span>Proper initialization is crucial to the optimization and the generalization
of neural networks. However, most existing neural recommendation systems
initialize the user and item embeddings randomly. In this work, we propose a
new initialization scheme for user and item embeddings called Laplacian
Eigenmaps with Popularity-based Regularization for Isolated Data (LEPORID).
LEPORID endows the embeddings with information regarding multi-scale
neighborhood structures on the data manifold and performs adaptive
regularization to compensate for high embedding variance on the tail of the
data distribution. Exploiting matrix sparsity, LEPORID embeddings can be
computed efficiently. We evaluate LEPORID in a wide range of neural
recommendation models. In contrast to the recent surprising finding that the
simple K-nearest-neighbor (KNN) method often outperforms neural recommendation
systems, we show that existing neural systems initialized with LEPORID often
perform on par or better than KNN. To maximize the effects of the
initialization, we propose the Dual-Loss Residual Recommendation (DLR2)
network, which, when initialized with LEPORID, substantially outperforms both
traditional and state-of-the-art neural recommender systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">It Takes Two to Tango: Mixup for Deep Metric Learning. (arXiv:2106.04990v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Venkataramanan_S/0/1/0/all/0/1">Shashanka Venkataramanan</a>, <a href="http://arxiv.org/find/cs/1/au:+Psomas_B/0/1/0/all/0/1">Bill Psomas</a>, <a href="http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1">Yannis Avrithis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1">Ewa Kijak</a>, <a href="http://arxiv.org/find/cs/1/au:+Amsaleg_L/0/1/0/all/0/1">Laurent Amsaleg</a>, <a href="http://arxiv.org/find/cs/1/au:+Karantzalos_K/0/1/0/all/0/1">Konstantinos Karantzalos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04990">
                                    <div class="article-summary-box-inner">
                                        <span>Metric learning involves learning a discriminative representation such that
embeddings of similar classes are encouraged to be close, while embeddings of
dissimilar classes are pushed far apart. State-of-the-art methods focus mostly
on sophisticated loss functions or mining strategies. On the one hand, metric
learning losses consider two or more examples at a time. On the other hand,
modern data augmentation methods for classification consider two or more
examples at a time. The combination of the two ideas is under-studied.

In this work, we aim to bridge this gap and improve representations using
mixup, which is a powerful data augmentation approach interpolating two or more
examples and corresponding target labels at a time. This task is challenging
because, unlike classification, the loss functions used in metric learning are
not additive over examples, so the idea of interpolating target labels is not
straightforward. To the best of our knowledge, we are the first to investigate
mixing examples and target labels for deep metric learning. We develop a
generalized formulation that encompasses existing metric learning loss
functions and modify it to accommodate for mixup, introducing Metric Mix, or
Metrix. We show that mixing inputs, intermediate representations or embeddings
along with target labels significantly improves representations and outperforms
state-of-the-art metric learning methods on four benchmark datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Enhance Convolutional Neural Networks with Noise Incentive Block. (arXiv:2012.12109v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Menghan Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1">Chu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1">Tien-Tsin Wong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.12109">
                                    <div class="article-summary-box-inner">
                                        <span>As a generic modeling tool, Convolutional Neural Networks (CNNs) have been
widely employed in image generation and translation tasks. However, when fed
with a flat input, current CNN models may fail to generate vivid results due to
the spatially shared convolution kernels. We call it the flatness degradation
of CNNs. Unfortunately, such degradation is the greatest obstacles to generate
a spatially-variant output from a flat input, which has been barely discussed
in the previous literature. To tackle this problem, we propose a model agnostic
solution, i.e. Noise Incentive Block (NIB), which serves as a generic plug-in
for any CNN generation model. The key idea is to break the flat input condition
while keeping the intactness of the original information. Specifically, the NIB
perturbs the input data symmetrically with a noise map and reassembles them in
the feature domain as driven by the objective function. Extensive experiments
show that existing CNN models equipped with NIB survive from the flatness
degradation and are able to generate visually better results with richer
details in some specific image generation tasks given flat inputs, e.g.
semantic image synthesis, data-hidden image generation, and deep neural
dithering.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adaptive Inference through Early-Exit Networks: Design, Challenges and Directions. (arXiv:2106.05022v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Laskaridis_S/0/1/0/all/0/1">Stefanos Laskaridis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kouris_A/0/1/0/all/0/1">Alexandros Kouris</a>, <a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1">Nicholas D. Lane</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05022">
                                    <div class="article-summary-box-inner">
                                        <span>DNNs are becoming less and less over-parametrised due to recent advances in
efficient model design, through careful hand-crafted or NAS-based methods.
Relying on the fact that not all inputs require the same amount of computation
to yield a confident prediction, adaptive inference is gaining attention as a
prominent approach for pushing the limits of efficient deployment.
Particularly, early-exit networks comprise an emerging direction for tailoring
the computation depth of each input sample at runtime, offering complementary
performance gains to other efficiency optimisations. In this paper, we
decompose the design methodology of early-exit networks to its key components
and survey the recent advances in each one of them. We also position
early-exiting against other efficient inference solutions and provide our
insights on the current challenges and most promising future directions for
research in the field.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bayesian Bellman Operators. (arXiv:2106.05012v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Fellows_M/0/1/0/all/0/1">Matthew Fellows</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartikainen_K/0/1/0/all/0/1">Kristian Hartikainen</a>, <a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1">Shimon Whiteson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05012">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a novel perspective on Bayesian reinforcement learning (RL);
whereas existing approaches infer a posterior over the transition distribution
or Q-function, we characterise the uncertainty in the Bellman operator. Our
Bayesian Bellman operator (BBO) framework is motivated by the insight that when
bootstrapping is introduced, model-free approaches actually infer a posterior
over Bellman operators, not value functions. In this paper, we use BBO to
provide a rigorous theoretical analysis of model-free Bayesian RL to better
understand its relationshipto established frequentist RL methodologies. We
prove that Bayesian solutions are consistent with frequentist RL solutions,
even when approximate inference isused, and derive conditions for which
convergence properties hold. Empirically, we demonstrate that algorithms
derived from the BBO framework have sophisticated deep exploration properties
that enable them to solve continuous control tasks at which state-of-the-art
regularised actor-critic algorithms fail catastrophically</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoqing Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1">Yan Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1">Jiansheng Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1">Yanwu Xu</a>, <a href="http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1">Risa Higashita</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1">Jiang Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04830">
                                    <div class="article-summary-box-inner">
                                        <span>Cataract is one of the leading causes of reversible visual impairment and
blindness globally. Over the years, researchers have achieved significant
progress in developing state-of-the-art artificial intelligence techniques for
automatic cataract classification and grading, helping clinicians prevent and
treat cataract in time. This paper provides a comprehensive survey of recent
advances in machine learning for cataract classification and grading based on
ophthalmic images. We summarize existing literature from two research
directions: conventional machine learning techniques and deep learning
techniques. This paper also provides insights into existing works of both
merits and limitations. In addition, we discuss several challenges of automatic
cataract classification and grading based on machine learning techniques and
present possible solutions to these challenges for future research.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">NeRF in detail: Learning to sample for view synthesis. (arXiv:2106.05264v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_R/0/1/0/all/0/1">Relja Arandjelovi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1">Andrew Zisserman</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05264">
                                    <div class="article-summary-box-inner">
                                        <span>Neural radiance fields (NeRF) methods have demonstrated impressive novel view
synthesis performance. The core approach is to render individual rays by
querying a neural network at points sampled along the ray to obtain the density
and colour of the sampled points, and integrating this information using the
rendering equation. Since dense sampling is computationally prohibitive, a
common solution is to perform coarse-to-fine sampling.

In this work we address a clear limitation of the vanilla coarse-to-fine
approach -- that it is based on a heuristic and not trained end-to-end for the
task at hand. We introduce a differentiable module that learns to propose
samples and their importance for the fine network, and consider and compare
multiple alternatives for its neural architecture. Training the proposal module
from scratch can be unstable due to lack of supervision, so an effective
pre-training strategy is also put forward. The approach, named &#x60;NeRF in detail&#x27;
(NeRF-ID), achieves superior view synthesis quality over NeRF and the
state-of-the-art on the synthetic Blender benchmark and on par or better
performance on the real LLFF-NeRF scenes. Furthermore, by leveraging the
predicted sample importance, a 25% saving in computation can be achieved
without significantly sacrificing the rendering quality.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">BiFair: Training Fair Models with Bilevel Optimization. (arXiv:2106.04757v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ozdayi_M/0/1/0/all/0/1">Mustafa Safa Ozdayi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kantarcioglu_M/0/1/0/all/0/1">Murat Kantarcioglu</a>, <a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1">Rishabh Iyer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04757">
                                    <div class="article-summary-box-inner">
                                        <span>Prior studies have shown that, training machine learning models via empirical
loss minimization to maximize a utility metric (e.g., accuracy), might yield
models that make discriminatory predictions. To alleviate this issue, we
develop a new training algorithm, named BiFair, which jointly minimizes for a
utility, and a fairness loss of interest. Crucially, we do so without directly
modifying the training objective, e.g., by adding regularization terms. Rather,
we learn a set of weights on the training dataset, such that, training on the
weighted dataset ensures both good utility, and fairness. The dataset weights
are learned in concurrence to the model training, which is done by solving a
bilevel optimization problem using a held-out validation dataset. Overall, this
approach yields models with better fairness-utility trade-offs. Particularly,
we compare our algorithm with three other state-of-the-art fair training
algorithms over three real-world datasets, and demonstrate that, BiFair
consistently performs better, i.e., we reach to better values of a given
fairness metric under same, or higher accuracy. Further, our algorithm is
scalable. It is applicable both to simple models, such as logistic regression,
as well as more complex models, such as deep neural networks, as evidenced by
our experimental analysis.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Harmless Overparametrization in Two-layer Neural Networks. (arXiv:2106.04795v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huiyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Wei Lin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04795">
                                    <div class="article-summary-box-inner">
                                        <span>Overparametrized neural networks, where the number of active parameters is
larger than the sample size, prove remarkably effective in modern deep learning
practice. From the classical perspective, however, much fewer parameters are
sufficient for optimal estimation and prediction, whereas overparametrization
can be harmful even in the presence of explicit regularization. To reconcile
this conflict, we present a generalization theory for overparametrized ReLU
networks by incorporating an explicit regularizer based on the scaled variation
norm. Interestingly, this regularizer is equivalent to the ridge from the angle
of gradient-based optimization, but is similar to the group lasso in terms of
controlling model complexity. By exploiting this ridge-lasso duality, we show
that overparametrization is generally harmless to two-layer ReLU networks. In
particular, the overparametrized estimators are minimax optimal up to a
logarithmic factor. By contrast, we show that overparametrized random feature
models suffer from the curse of dimensionality and thus are suboptimal.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Communication-efficient SGD: From Local SGD to One-Shot Averaging. (arXiv:2106.04759v1 [cs.DC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Spiridonoff_A/0/1/0/all/0/1">Artin Spiridonoff</a>, <a href="http://arxiv.org/find/cs/1/au:+Olshevsky_A/0/1/0/all/0/1">Alex Olshevsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Paschalidis_I/0/1/0/all/0/1">Ioannis Ch. Paschalidis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04759">
                                    <div class="article-summary-box-inner">
                                        <span>We consider speeding up stochastic gradient descent (SGD) by parallelizing it
across multiple workers. We assume the same data set is shared among $N$
workers, who can take SGD steps and coordinate with a central server. While it
is possible to obtain a linear reduction in the variance by averaging all the
stochastic gradients at every step, this requires a lot of communication
between the workers and the server, which can dramatically reduce the gains
from parallelism. The Local SGD method, proposed and analyzed in the earlier
literature, suggests machines should make many local steps between such
communications. While the initial analysis of Local SGD showed it needs $\Omega
( \sqrt{T} )$ communications for $T$ local gradient steps in order for the
error to scale proportionately to $1/(NT)$, this has been successively improved
in a string of papers, with the state-of-the-art requiring $\Omega \left( N
\left( \mbox{ polynomial in log } (T) \right) \right)$ communications. In this
paper, we suggest a Local SGD scheme that communicates less overall by
communicating less frequently as the number of iterations grows. Our analysis
shows that this can achieve an error that scales as $1/(NT)$ with a number of
communications that is completely independent of $T$. In particular, we show
that $\Omega(N)$ communications are sufficient. Empirical evidence suggests
this bound is close to tight as we further show that $\sqrt{N}$ or $N^{3/4}$
communications fail to achieve linear speed-up in simulations. Moreover, we
show that under mild assumptions, the main of which is twice differentiability
on any neighborhood of the optimal solution, one-shot averaging which only uses
a single round of communication can also achieve the optimal convergence rate
asymptotically.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Scaling Up Graph Neural Networks Via Graph Coarsening. (arXiv:2106.05150v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zengfeng Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shengzhong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_C/0/1/0/all/0/1">Chong Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Min Zhou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05150">
                                    <div class="article-summary-box-inner">
                                        <span>Scalability of graph neural networks remains one of the major challenges in
graph machine learning. Since the representation of a node is computed by
recursively aggregating and transforming representation vectors of its
neighboring nodes from previous layers, the receptive fields grow
exponentially, which makes standard stochastic optimization techniques
ineffective. Various approaches have been proposed to alleviate this issue,
e.g., sampling-based methods and techniques based on pre-computation of graph
filters.

In this paper, we take a different approach and propose to use graph
coarsening for scalable training of GNNs, which is generic, extremely simple
and has sublinear memory and time costs during training. We present extensive
theoretical analysis on the effect of using coarsening operations and provides
useful guidance on the choice of coarsening methods. Interestingly, our
theoretical analysis shows that coarsening can also be considered as a type of
regularization and may improve the generalization. Finally, empirical results
on real world datasets show that, simply applying off-the-shelf coarsening
methods, we can reduce the number of nodes by up to a factor of ten without
causing a noticeable downgrade in classification accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Transformers for Modeling Physical Systems. (arXiv:2010.03957v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Geneva_N/0/1/0/all/0/1">Nicholas Geneva</a>, <a href="http://arxiv.org/find/cs/1/au:+Zabaras_N/0/1/0/all/0/1">Nicholas Zabaras</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03957">
                                    <div class="article-summary-box-inner">
                                        <span>Transformers are widely used in natural language processing due to their
ability to model longer-term dependencies in text. Although these models
achieve state-of-the-art performance for many language related tasks, their
applicability outside of the natural language processing field has been
minimal. In this work, we propose the use of transformer models for the
prediction of dynamical systems representative of physical phenomena. The use
of Koopman based embeddings provide a unique and powerful method for projecting
any dynamical system into a vector representation which can then be predicted
by a transformer model. The proposed model is able to accurately predict
various dynamical systems and outperform classical methods that are commonly
used in the scientific machine learning literature.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning. (arXiv:2010.03110v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sontakke_S/0/1/0/all/0/1">Sumedh A. Sontakke</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehrjou_A/0/1/0/all/0/1">Arash Mehrjou</a>, <a href="http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1">Laurent Itti</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03110">
                                    <div class="article-summary-box-inner">
                                        <span>Animals exhibit an innate ability to learn regularities of the world through
interaction. By performing experiments in their environment, they are able to
discern the causal factors of variation and infer how they affect the world&#x27;s
dynamics. Inspired by this, we attempt to equip reinforcement learning agents
with the ability to perform experiments that facilitate a categorization of the
rolled-out trajectories, and to subsequently infer the causal factors of the
environment in a hierarchical manner. We introduce {\em causal curiosity}, a
novel intrinsic reward, and show that it allows our agents to learn optimal
sequences of actions and discover causal factors in the dynamics of the
environment. The learned behavior allows the agents to infer a binary quantized
representation for the ground-truth causal factors in every environment.
Additionally, we find that these experimental behaviors are semantically
meaningful (e.g., our agents learn to lift blocks to categorize them by
weight), and are learnt in a self-supervised manner with approximately 2.5
times less data than conventional supervised planners. We show that these
behaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or
other downstream tasks). Finally, we show that the knowledge of causal factor
representations aids zero-shot learning for more complex tasks. Visit
https://sites.google.com/usc.edu/causal-curiosity/home for website.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for Investigating Learned Representations. (arXiv:1905.13308v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Livezey_J/0/1/0/all/0/1">Jesse A. Livezey</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_A/0/1/0/all/0/1">Ahyeon Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_J/0/1/0/all/0/1">Jacob Yeung</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1">Kristofer E. Bouchard</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.13308">
                                    <div class="article-summary-box-inner">
                                        <span>Hierarchy and compositionality are common latent properties in many natural
and scientific datasets. Determining when a deep network&#x27;s hidden activations
represent hierarchy and compositionality is important both for understanding
deep representation learning and for applying deep networks in domains where
interpretability is crucial. However, current benchmark machine learning
datasets either have little hierarchical or compositional structure, or the
structure is not known. This gap impedes precise analysis of a network&#x27;s
representations and thus hinders development of new methods that can learn such
properties. To address this gap, we developed a new benchmark dataset with
known hierarchical and compositional structure. The Hangul Fonts Dataset (HFD)
is comprised of 35 fonts from the Korean writing system (Hangul), each with
11,172 blocks (syllables) composed from the product of initial consonant,
medial vowel, and final consonant glyphs. All blocks can be grouped into a few
geometric types which induces a hierarchy across blocks. In addition, each
block is composed of individual glyphs with rotations, translations, scalings,
and naturalistic style variation across fonts. We find that both shallow and
deep unsupervised methods only show modest evidence of hierarchy and
compositionality in their representations of the HFD compared to supervised
deep networks. Supervised deep network representations contain structure
related to the geometrical hierarchy of the characters, but the compositional
structure of the data is not evident. Thus, HFD enables the identification of
shortcomings in existing methods, a critical first step toward developing new
machine learning algorithms to extract hierarchical and compositional structure
in the context of naturalistic variability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL. (arXiv:2106.05087v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yanchao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1">Ruijie Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yongyuan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Furong Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05087">
                                    <div class="article-summary-box-inner">
                                        <span>Evaluating the worst-case performance of a reinforcement learning (RL) agent
under the strongest/optimal adversarial perturbations on state observations
(within some constraints) is crucial for understanding the robustness of RL
agents. However, finding the optimal adversary is challenging, in terms of both
whether we can find the optimal attack and how efficiently we can find it.
Existing works on adversarial RL either use heuristics-based methods that may
not find the strongest adversary, or directly train an RL-based adversary by
treating the agent as a part of the environment, which can find the optimal
adversary but may become intractable in a large state space. In this paper, we
propose a novel attacking algorithm which has an RL-based &quot;director&quot; searching
for the optimal policy perturbation, and an &quot;actor&quot; crafting state
perturbations following the directions from the director (i.e. the actor
executes targeted attacks). Our proposed algorithm, PA-AD, is theoretically
optimal against an RL agent and significantly improves the efficiency compared
with prior RL-based works in environments with large or pixel state spaces.
Empirical results show that our proposed PA-AD universally outperforms
state-of-the-art attacking methods in a wide range of environments. Our method
can be easily applied to any RL algorithms to evaluate and improve their
robustness.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On Path Integration of Grid Cells: Group Representation and Isotropic Scaling. (arXiv:2006.10259v5 [q-bio.NC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/q-bio/1/au:+Gao_R/0/1/0/all/0/1">Ruiqi Gao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Xie_J/0/1/0/all/0/1">Jianwen Xie</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wei_X/0/1/0/all/0/1">Xue-Xin Wei</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhu_S/0/1/0/all/0/1">Song-Chun Zhu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wu_Y/0/1/0/all/0/1">Ying Nian Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10259">
                                    <div class="article-summary-box-inner">
                                        <span>Understanding how grid cells perform path integration calculations remains a
fundamental problem. In this paper, we conduct theoretical analysis of a
general representation model of path integration by grid cells, where the 2D
self-position is encoded as a higher dimensional vector, and the 2D self-motion
is represented by a general transformation of the vector. We identify two
conditions on the transformation. One is a group representation condition that
is necessary for path integration. The other is an isotropic scaling condition
that ensures locally conformal embedding, so that the error in the vector
representation translates proportionally to the error in the 2D self-position.
Then we investigate the simplest transformation, i.e., the linear
transformation, uncover its explicit algebraic and geometric structure as
matrix Lie group of rotation, and establish the connection between the
isotropic scaling condition and hexagon grid patterns of grid cells under the
linear transformation. Finally, with our optimization-based approach, we manage
to learn hexagon grid patterns that share similar properties of the grid cells
in the rodent brain. The learned model is capable of accurate long distance
path integration.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Understanding Neural Networks and Individual Neuron Importance via Information-Ordered Cumulative Ablation. (arXiv:1804.06679v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1">Rana Ali Amjad</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kairen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiger_B/0/1/0/all/0/1">Bernhard C. Geiger</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.06679">
                                    <div class="article-summary-box-inner">
                                        <span>In this work, we investigate the use of three information-theoretic
quantities -- entropy, mutual information with the class variable, and a class
selectivity measure based on Kullback-Leibler divergence -- to understand and
study the behavior of already trained fully-connected feed-forward neural
networks. We analyze the connection between these information-theoretic
quantities and classification performance on the test set by cumulatively
ablating neurons in networks trained on MNIST, FashionMNIST, and CIFAR-10. Our
results parallel those recently published by Morcos et al., indicating that
class selectivity is not a good indicator for classification performance.
However, looking at individual layers separately, both mutual information and
class selectivity are positively correlated with classification performance, at
least for networks with ReLU activation functions. We provide explanations for
this phenomenon and conclude that it is ill-advised to compare the proposed
information-theoretic quantities across layers. Furthermore, we show that
cumulative ablation of neurons with ascending or descending
information-theoretic quantities can be used to formulate hypotheses regarding
the joint behavior of multiple neurons, such as redundancy and synergy, with
comparably low computational cost. We also draw connections to the information
bottleneck theory for neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MSTDP: A More Biologically Plausible Learning. (arXiv:1912.00009v2 [cs.NE] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shiyuan Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.00009">
                                    <div class="article-summary-box-inner">
                                        <span>Spike-timing dependent plasticity (STDP) which observed in the brain has
proven to be important in biological learning. On the other hand, artificial
neural networks use a different way to learn, such as Back-Propagation or
Contrastive Hebbian Learning. In this work, we propose a new framework called
mstdp that learn almost the same way biological learning use, it only uses STDP
rules for supervised and unsupervised learning and don&#x27; t need a global loss or
other supervise information. The framework works like an auto-encoder by making
each input neuron also an output neuron. It can make predictions or generate
patterns in one model without additional configuration. We also brought a new
iterative inference method using momentum to make the framework more efficient,
which can be used in training and testing phases. Finally, we verified our
framework on MNIST dataset for classification and generation task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Attacking Adversarial Attacks as A Defense. (arXiv:2106.04938v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1">Boxi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1">Heng Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jindong Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shuai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhifeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1">Deng Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiaofei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04938">
                                    <div class="article-summary-box-inner">
                                        <span>It is well known that adversarial attacks can fool deep neural networks with
imperceptible perturbations. Although adversarial training significantly
improves model robustness, failure cases of defense still broadly exist. In
this work, we find that the adversarial attacks can also be vulnerable to small
perturbations. Namely, on adversarially-trained models, perturbing adversarial
examples with a small random noise may invalidate their misled predictions.
After carefully examining state-of-the-art attacks of various kinds, we find
that all these attacks have this deficiency to different extents. Enlightened
by this finding, we propose to counter attacks by crafting more effective
defensive perturbations. Our defensive perturbations leverage the advantage
that adversarial training endows the ground-truth class with smaller local
Lipschitzness. By simultaneously attacking all the classes, the misled
predictions with larger Lipschitzness can be flipped into correct ones. We
verify our defensive perturbation with both empirical experiments and
theoretical analyses on a linear model. On CIFAR10, it boosts the
state-of-the-art model from 66.16% to 72.66% against the four attacks of
AutoAttack, including 71.76% to 83.30% against the Square attack. On ImageNet,
the top-1 robust accuracy of FastAT is improved from 33.18% to 38.54% under the
100-step PGD attack.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Convolutional Complex Knowledge Graph Embeddings. (arXiv:2008.03130v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1">Caglar Demir</a>, <a href="http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1">Axel-Cyrille Ngonga Ngomo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.03130">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we study the problem of learning continuous vector
representations of knowledge graphs for predicting missing links. We present a
new approach called ConEx, which infers missing links by leveraging the
composition of a 2D convolution with a Hermitian inner product of
complex-valued embedding vectors. We evaluate ConEx against state-of-the-art
approaches on the WN18RR, FB15K-237, KINSHIP and UMLS benchmark datasets. Our
experimental results show that ConEx achieves a performance superior to that of
state-of-the-art approaches such as RotatE, QuatE and TuckER on the link
prediction task on all datasets while requiring at least 8 times fewer
parameters. We ensure the reproducibility of our results by providing an
open-source implementation which includes the training, evaluation scripts
along with pre-trained models at https://github.com/conex-kge/ConEx.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Neural Network Subspaces. (arXiv:2102.10472v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1">Mitchell Wortsman</a>, <a href="http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1">Maxwell Horton</a>, <a href="http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1">Carlos Guestrin</a>, <a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1">Ali Farhadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1">Mohammad Rastegari</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10472">
                                    <div class="article-summary-box-inner">
                                        <span>Recent observations have advanced our understanding of the neural network
optimization landscape, revealing the existence of (1) paths of high accuracy
containing diverse solutions and (2) wider minima offering improved
performance. Previous methods observing diverse paths require multiple training
runs. In contrast we aim to leverage both property (1) and (2) with a single
method and in a single training run. With a similar computational cost as
training one model, we learn lines, curves, and simplexes of high-accuracy
neural networks. These neural network subspaces contain diverse solutions that
can be ensembled, approaching the ensemble performance of independently trained
networks without the training cost. Moreover, using the subspace midpoint
boosts accuracy, calibration, and robustness to label noise, outperforming
Stochastic Weight Averaging.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Phraseformer: Multimodal Key-phrase Extraction using Transformer and Graph Embedding. (arXiv:2106.04939v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nikzad_Khasmakhi_N/0/1/0/all/0/1">Narjes Nikzad-Khasmakhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1">Mohammad-Reza Feizi-Derakhshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Asgari_Chenaghlu_M/0/1/0/all/0/1">Meysam Asgari-Chenaghlu</a>, <a href="http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1">Mohammad-Ali Balafar</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_A/0/1/0/all/0/1">Ali-Reza Feizi-Derakhshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahkar_Farshi_T/0/1/0/all/0/1">Taymaz Rahkar-Farshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1">Majid Ramezani</a>, <a href="http://arxiv.org/find/cs/1/au:+Jahanbakhsh_Nagadeh_Z/0/1/0/all/0/1">Zoleikha Jahanbakhsh-Nagadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zafarani_Moattar_E/0/1/0/all/0/1">Elnaz Zafarani-Moattar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranjbar_Khadivi_M/0/1/0/all/0/1">Mehrdad Ranjbar-Khadivi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04939">
                                    <div class="article-summary-box-inner">
                                        <span>Background: Keyword extraction is a popular research topic in the field of
natural language processing. Keywords are terms that describe the most relevant
information in a document. The main problem that researchers are facing is how
to efficiently and accurately extract the core keywords from a document.
However, previous keyword extraction approaches have utilized the text and
graph features, there is the lack of models that can properly learn and combine
these features in a best way.

Methods: In this paper, we develop a multimodal Key-phrase extraction
approach, namely Phraseformer, using transformer and graph embedding
techniques. In Phraseformer, each keyword candidate is presented by a vector
which is the concatenation of the text and structure learning representations.
Phraseformer takes the advantages of recent researches such as BERT and ExEm to
preserve both representations. Also, the Phraseformer treats the key-phrase
extraction task as a sequence labeling problem solved using classification
task.

Results: We analyze the performance of Phraseformer on three datasets
including Inspec, SemEval2010 and SemEval 2017 by F1-score. Also, we
investigate the performance of different classifiers on Phraseformer method
over Inspec dataset. Experimental results demonstrate the effectiveness of
Phraseformer method over the three datasets used. Additionally, the Random
Forest classifier gain the highest F1-score among all classifiers.

Conclusions: Due to the fact that the combination of BERT and ExEm is more
meaningful and can better represent the semantic of words. Hence, Phraseformer
significantly outperforms single-modality methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Independent mechanism analysis, a new concept?. (arXiv:2106.05200v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Gresele_L/0/1/0/all/0/1">Luigi Gresele</a>, <a href="http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1">Julius von K&#xfc;gelgen</a>, <a href="http://arxiv.org/find/stat/1/au:+Stimper_V/0/1/0/all/0/1">Vincent Stimper</a>, <a href="http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/stat/1/au:+Besserve_M/0/1/0/all/0/1">Michel Besserve</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05200">
                                    <div class="article-summary-box-inner">
                                        <span>Independent component analysis provides a principled framework for
unsupervised representation learning, with solid theory on the identifiability
of the latent code that generated the data, given only observations of mixtures
thereof. Unfortunately, when the mixing is nonlinear, the model is provably
nonidentifiable, since statistical independence alone does not sufficiently
constrain the problem. Identifiability can be recovered in settings where
additional, typically observed variables are included in the generative
process. We investigate an alternative path and consider instead including
assumptions reflecting the principle of independent causal mechanisms exploited
in the field of causality. Specifically, our approach is motivated by thinking
of each source as independently influencing the mixing process. This gives rise
to a framework which we term independent mechanism analysis. We provide
theoretical and empirical evidence that our approach circumvents a number of
nonidentifiability issues arising in nonlinear blind source separation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DiffPD: Differentiable Projective Dynamics. (arXiv:2101.05917v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1">Tao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1">Pingchuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wah_S/0/1/0/all/0/1">Sebastien Wah</a>, <a href="http://arxiv.org/find/cs/1/au:+Spielberg_A/0/1/0/all/0/1">Andrew Spielberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1">Daniela Rus</a>, <a href="http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1">Wojciech Matusik</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05917">
                                    <div class="article-summary-box-inner">
                                        <span>We present a novel, fast differentiable simulator for soft-body learning and
control applications. Existing differentiable soft-body simulators can be
classified into two categories based on their time integration methods:
Simulators using explicit time-stepping scheme require tiny time steps to avoid
numerical instabilities in gradient computation, and simulators using implicit
time integration typically compute gradients by employing the adjoint method
and solving the expensive linearized dynamics. Inspired by Projective Dynamics
(PD), we present Differentiable Projective Dynamics (DiffPD), an efficient
differentiable soft-body simulator based on PD with implicit time integration.
The key idea in DiffPD is to speed up backpropagation by exploiting the
prefactorized Cholesky decomposition in forward PD simulation. In terms of
contact handling, DiffPD supports two types of contacts: a penalty-based model
describing contact and friction forces and a complementarity-based model
enforcing non-penetration conditions and static friction. We evaluate the
performance of DiffPD and observe it is 4-19 times faster compared to the
standard Newton&#x27;s method in various applications including system
identification, inverse design problems, trajectory optimization, and
closed-loop control. We also apply DiffPD in a real-to-sim example with contact
and collisions and show its capability of reconstructing a digital twin of
real-world scenes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Crosslingual Embeddings are Essential in UNMT for Distant Languages: An English to IndoAryan Case Study. (arXiv:2106.04995v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1">Tamali Banerjee</a>, <a href="http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1">Rudra Murthy V</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1">Pushpak Bhattacharyya</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04995">
                                    <div class="article-summary-box-inner">
                                        <span>Recent advances in Unsupervised Neural Machine Translation (UNMT) have
minimized the gap between supervised and unsupervised machine translation
performance for closely related language pairs. However, the situation is very
different for distant language pairs. Lack of lexical overlap and low syntactic
similarities such as between English and Indo-Aryan languages leads to poor
translation quality in existing UNMT systems. In this paper, we show that
initializing the embedding layer of UNMT models with cross-lingual embeddings
shows significant improvements in BLEU score over existing approaches with
embeddings randomly initialized. Further, static embeddings (freezing the
embedding layer weights) lead to better gains compared to updating the
embedding layer weights during training (non-static). We experimented using
Masked Sequence to Sequence (MASS) and Denoising Autoencoder (DAE) UNMT
approaches for three distant language pairs. The proposed cross-lingual
embedding initialization yields BLEU score improvement of as much as ten times
over the baseline for English-Hindi, English-Bengali, and English-Gujarati. Our
analysis shows the importance of cross-lingual embedding, comparisons between
approaches, and the scope of improvements in these systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Unifying Behavioral and Response Diversity for Open-ended Learning in Zero-sum Games. (arXiv:2106.04958v1 [cs.MA])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiangyu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1">Hangtian Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1">Ying Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yujing Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yingfeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1">Changjie Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1">Zhipeng Hu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04958">
                                    <div class="article-summary-box-inner">
                                        <span>Measuring and promoting policy diversity is critical for solving games with
strong non-transitive dynamics where strategic cycles exist, and there is no
consistent winner (e.g., Rock-Paper-Scissors). With that in mind, maintaining a
pool of diverse policies via open-ended learning is an attractive solution,
which can generate auto-curricula to avoid being exploited. However, in
conventional open-ended learning algorithms, there are no widely accepted
definitions for diversity, making it hard to construct and evaluate the diverse
policies. In this work, we summarize previous concepts of diversity and work
towards offering a unified measure of diversity in multi-agent open-ended
learning to include all elements in Markov games, based on both Behavioral
Diversity (BD) and Response Diversity (RD). At the trajectory distribution
level, we re-define BD in the state-action space as the discrepancies of
occupancy measures. For the reward dynamics, we propose RD to characterize
diversity through the responses of policies when encountering different
opponents. We also show that many current diversity measures fall in one of the
categories of BD or RD but not both. With this unified diversity measure, we
design the corresponding diversity-promoting objective and population
effectivity when seeking the best responses in open-ended learning. We validate
our methods in both relatively simple games like matrix game, non-transitive
mixture model, and the complex \textit{Google Research Football} environment.
The population found by our methods reveals the lowest exploitability, highest
population effectivity in matrix game and non-transitive mixture model, as well
as the largest goal difference when interacting with opponents of various
levels in \textit{Google Research Football}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Realizing GANs via a Tunable Loss Function. (arXiv:2106.05232v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kurri_G/0/1/0/all/0/1">Gowtham R. Kurri</a>, <a href="http://arxiv.org/find/cs/1/au:+Sypherd_T/0/1/0/all/0/1">Tyler Sypherd</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankar_L/0/1/0/all/0/1">Lalitha Sankar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05232">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a tunable GAN, called $\alpha$-GAN, parameterized by $\alpha \in
(0,\infty]$, which interpolates between various $f$-GANs and Integral
Probability Metric based GANs (under constrained discriminator set). We
construct $\alpha$-GAN using a supervised loss function, namely, $\alpha$-loss,
which is a tunable loss function capturing several canonical losses. We show
that $\alpha$-GAN is intimately related to the Arimoto divergence, which was
first proposed by \&quot;{O}sterriecher (1996), and later studied by Liese and Vajda
(2006). We posit that the holistic understanding that $\alpha$-GAN introduces
will have practical benefits of addressing both the issues of vanishing
gradients and mode collapse.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cooperative Online Learning. (arXiv:2106.04982v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1">Tommaso R. Cesari</a>, <a href="http://arxiv.org/find/cs/1/au:+Vecchia_R/0/1/0/all/0/1">Riccardo Della Vecchia</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04982">
                                    <div class="article-summary-box-inner">
                                        <span>In this preliminary (and unpolished) version of the paper, we study an
asynchronous online learning setting with a network of agents. At each time
step, some of the agents are activated, requested to make a prediction, and pay
the corresponding loss. Some feedback is then revealed to these agents and is
later propagated through the network. We consider the case of full, bandit, and
semi-bandit feedback. In particular, we construct a reduction to delayed
single-agent learning that applies to both the full and the bandit feedback
case and allows to obtain regret guarantees for both settings. We complement
these results with a near-matching lower bound.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Modeling massive highly-multivariate nonstationary spatial data with the basis graphical lasso. (arXiv:2101.02404v2 [stat.ME] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Krock_M/0/1/0/all/0/1">Mitchell Krock</a>, <a href="http://arxiv.org/find/stat/1/au:+Kleiber_W/0/1/0/all/0/1">William Kleiber</a>, <a href="http://arxiv.org/find/stat/1/au:+Hammerling_D/0/1/0/all/0/1">Dorit Hammerling</a>, <a href="http://arxiv.org/find/stat/1/au:+Becker_S/0/1/0/all/0/1">Stephen Becker</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02404">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a new modeling framework for highly-multivariate spatial processes
that synthesizes ideas from recent multiscale and spectral approaches with
graphical models. The basis graphical lasso writes a univariate Gaussian
process as a linear combination of basis functions weighted with entries of a
Gaussian graphical vector whose graph is estimated from optimizing an $\ell_1$
penalized likelihood. This paper extends the setting to a multivariate Gaussian
process where the basis functions are weighted with Gaussian graphical vectors.
We motivate a model where the basis functions represent different levels of
resolution and the graphical vectors for each level are assumed to be
independent. Using an orthogonal basis grants linear complexity and memory
usage in the number of spatial locations, the number of basis functions, and
the number of realizations. An additional fusion penalty encourages a
parsimonious conditional independence structure in the multilevel graphical
model. We illustrate our method on a large climate ensemble from the National
Center for Atmospheric Research&#x27;s Community Atmosphere Model that involves 40
spatial processes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks. (arXiv:2011.02511v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1">Julia Kreutzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1">Stefan Riezler</a>, <a href="http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1">Carolin Lawrence</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.02511">
                                    <div class="article-summary-box-inner">
                                        <span>Large volumes of interaction logs can be collected from NLP systems that are
deployed in the real world. How can this wealth of information be leveraged?
Using such interaction logs in an offline reinforcement learning (RL) setting
is a promising approach. However, due to the nature of NLP tasks and the
constraints of production systems, a series of challenges arise. We present a
concise overview of these challenges and discuss possible solutions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Single-Server Private Linear Transformation: The Individual Privacy Case. (arXiv:2106.05222v1 [cs.IT])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Heidarzadeh_A/0/1/0/all/0/1">Anoosheh Heidarzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Esmati_N/0/1/0/all/0/1">Nahid Esmati</a>, <a href="http://arxiv.org/find/cs/1/au:+Sprintson_A/0/1/0/all/0/1">Alex Sprintson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05222">
                                    <div class="article-summary-box-inner">
                                        <span>This paper considers the single-server Private Linear Transformation (PLT)
problem with individual privacy guarantees. In this problem, there is a user
that wishes to obtain $L$ independent linear combinations of a $D$-subset of
messages belonging to a dataset of $K$ messages stored on a single server. The
goal is to minimize the download cost while keeping the identity of each
message required for the computation individually private. The individual
privacy requirement ensures that the identity of each individual message
required for the computation is kept private. This is in contrast to the
stricter notion of joint privacy that protects the entire set of identities of
all messages used for the computation, including the correlations between these
identities. The notion of individual privacy captures a broad set of practical
applications. For example, such notion is relevant when the dataset contains
information about individuals, each of them requires privacy guarantees for
their data access patterns. We focus on the setting in which the required
linear transformation is associated with a maximum distance separable (MDS)
matrix. In particular, we require that the matrix of coefficients pertaining to
the required linear combinations is the generator matrix of an MDS code. We
establish lower and upper bounds on the capacity of PLT with individual
privacy, where the capacity is defined as the supremum of all achievable
download rates. We show that our bounds are tight under certain conditions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Canonical Transform for Strengthening the Local $L^p$-Type Universal Approximation Property. (arXiv:2006.14378v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1">Anastasis Kratsios</a>, <a href="http://arxiv.org/find/cs/1/au:+Zamanlooy_B/0/1/0/all/0/1">Behnoosh Zamanlooy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14378">
                                    <div class="article-summary-box-inner">
                                        <span>Most $L^p$-type universal approximation theorems guarantee that a given
machine learning model class $\mathscr{F}\subseteq
C(\mathbb{R}^d,\mathbb{R}^D)$ is dense in
$L^p_{\mu}(\mathbb{R}^d,\mathbb{R}^D)$ for any suitable finite Borel measure
$\mu$ on $\mathbb{R}^d$. Unfortunately, this means that the model&#x27;s
approximation quality can rapidly degenerate outside some compact subset of
$\mathbb{R}^d$, as any such measure is largely concentrated on some bounded
subset of $\mathbb{R}^d$. This paper proposes a generic solution to this
approximation theoretic problem by introducing a canonical transformation which
&quot;upgrades $\mathscr{F}$&#x27;s approximation property&quot; in the following sense. The
transformed model class, denoted by $\mathscr{F}\text{-tope}$, is shown to be
dense in $L^p_{\mu,\text{strict}}(\mathbb{R}^d,\mathbb{R}^D)$ which is a
topological space whose elements are locally $p$-integrable functions and whose
topology is much finer than usual norm topology on
$L^p_{\mu}(\mathbb{R}^d,\mathbb{R}^D)$; here $\mu$ is any suitable
$\sigma$-finite Borel measure $\mu$ on $\mathbb{R}^d$. Next, we show that if
$\mathscr{F}$ is any family of analytic functions then there is always a strict
&quot;gap&quot; between $\mathscr{F}\text{-tope}$&#x27;s expressibility and that of
$\mathscr{F}$, since we find that $\mathscr{F}$ can never dense in
$L^p_{\mu,\text{strict}}(\mathbb{R}^d,\mathbb{R}^D)$. In the general case,
where $\mathscr{F}$ may contain non-analytic functions, we provide an abstract
form of these results guaranteeing that there always exists some function space
in which $\mathscr{F}\text{-tope}$ is dense but $\mathscr{F}$ is not, while,
the converse is never possible. Applications to feedforward networks,
convolutional neural networks, and polynomial bases are explored.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Implicit field learning for unsupervised anomaly detection in medical images. (arXiv:2106.05214v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Marimont_S/0/1/0/all/0/1">Sergio Naval Marimont</a>, <a href="http://arxiv.org/find/eess/1/au:+Tarroni_G/0/1/0/all/0/1">Giacomo Tarroni</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05214">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a novel unsupervised out-of-distribution detection method for
medical images based on implicit fields image representations. In our approach,
an auto-decoder feed-forward neural network learns the distribution of healthy
images in the form of a mapping between spatial coordinates and probabilities
over a proxy for tissue types. At inference time, the learnt distribution is
used to retrieve, from a given test image, a restoration, i.e. an image
maximally consistent with the input one but belonging to the healthy
distribution. Anomalies are localized using the voxel-wise probability
predicted by our model for the restored image. We tested our approach in the
task of unsupervised localization of gliomas on brain MR images and compared it
to several other VAE-based anomaly detection methods. Results show that the
proposed technique substantially outperforms them (average DICE 0.640 vs 0.518
for the best performing VAE-based alternative) while also requiring
considerably less computing time.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Stochastic Subset Selection for Efficient Training and Inference of Neural Networks. (arXiv:2006.14222v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Andreis_B/0/1/0/all/0/1">Bruno Andreis</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1">A. Tuan Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seanie Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Juho Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1">Eunho Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14222">
                                    <div class="article-summary-box-inner">
                                        <span>Current machine learning algorithms are designed to work with huge volumes of
high dimensional data such as images. However, these algorithms are being
increasingly deployed to resource constrained systems such as mobile devices
and embedded systems. Even in cases where large computing infrastructure is
available, the size of each data instance, as well as datasets, can be a
bottleneck in data transfer across communication channels. Also, there is a
huge incentive both in energy and monetary terms in reducing both the
computational and memory requirements of these algorithms. For nonparametric
models that require to leverage the stored training data at inference time, the
increased cost in memory and computation could be even more problematic. In
this work, we aim to reduce the volume of data these algorithms must process
through an end-to-end two-stage neural subset selection model. We first
efficiently obtain a subset of candidate elements by sampling a mask from a
conditionally independent Bernoulli distribution, and then autoregressivley
construct a subset consisting of the most task relevant elements via sampling
the elements from a conditional Categorical distribution. We validate our
method on set reconstruction and classification tasks with feature selection as
well as the selection of representative samples from a given dataset, on which
our method outperforms relevant baselines. We also show in our experiments that
our method enhances scalability of nonparametric models such as Neural
Processes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration. (arXiv:2006.01419v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1">Seungyul Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1">Youngchul Sung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.01419">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, sample-aware policy entropy regularization is proposed to
enhance the conventional policy entropy regularization for better exploration.
Exploiting the sample distribution obtainable from the replay buffer, the
proposed sample-aware entropy regularization maximizes the entropy of the
weighted sum of the policy action distribution and the sample action
distribution from the replay buffer for sample-efficient exploration. A
practical algorithm named diversity actor-critic (DAC) is developed by applying
policy iteration to the objective function with the proposed sample-aware
entropy regularization. Numerical results show that DAC significantly
outperforms existing recent algorithms for reinforcement learning.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Class-Transductive Intent Representations for Zero-shot Intent Detection. (arXiv:2012.01721v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1">Qingyi Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuanxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1">Peng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiangnan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiping Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01721">
                                    <div class="article-summary-box-inner">
                                        <span>Zero-shot intent detection (ZSID) aims to deal with the continuously emerging
intents without annotated training data. However, existing ZSID systems suffer
from two limitations: 1) They are not good at modeling the relationship between
seen and unseen intents. 2) They cannot effectively recognize unseen intents
under the generalized intent detection (GZSID) setting. A critical problem
behind these limitations is that the representations of unseen intents cannot
be learned in the training stage. To address this problem, we propose a novel
framework that utilizes unseen class labels to learn Class-Transductive Intent
Representations (CTIR). Specifically, we allow the model to predict unseen
intents during training, with the corresponding label names serving as input
utterances. On this basis, we introduce a multi-task learning objective, which
encourages the model to learn the distinctions among intents, and a similarity
scorer, which estimates the connections among intents more accurately. CTIR is
easy to implement and can be integrated with existing methods. Experiments on
two real-world datasets show that CTIR brings considerable improvement to the
baseline systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-Facet Clustering Variational Autoencoders. (arXiv:2106.05241v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Falck_F/0/1/0/all/0/1">Fabian Falck</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1">Haoting Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Willetts_M/0/1/0/all/0/1">Matthew Willetts</a>, <a href="http://arxiv.org/find/stat/1/au:+Nicholson_G/0/1/0/all/0/1">George Nicholson</a>, <a href="http://arxiv.org/find/stat/1/au:+Yau_C/0/1/0/all/0/1">Christopher Yau</a>, <a href="http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1">Christopher C Holmes</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05241">
                                    <div class="article-summary-box-inner">
                                        <span>Work in deep clustering focuses on finding a single partition of data.
However, high-dimensional data, such as images, typically feature multiple
interesting characteristics one could cluster over. For example, images of
objects against a background could be clustered over the shape of the object
and separately by the colour of the background. In this paper, we introduce
Multi-Facet Clustering Variational Autoencoders (MFCVAE), a novel class of
variational autoencoders with a hierarchy of latent variables, each with a
Mixture-of-Gaussians prior, that learns multiple clusterings simultaneously,
and is trained fully unsupervised and end-to-end. MFCVAE uses a
progressively-trained ladder architecture which leads to highly stable
performance. We provide novel theoretical results for optimising the ELBO
analytically with respect to the categorical variational posterior
distribution, and corrects earlier influential theoretical work. On image
benchmarks, we demonstrate that our approach separates out and clusters over
different aspects of the data in a disentangled manner. We also show other
advantages of our model: the compositionality of its latent space and that it
provides controlled generation of samples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neural UpFlow: A Scene Flow Learning Approach to Increase the Apparent Resolution of Particle-Based Liquids. (arXiv:2106.05143v1 [cs.GR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1">Bruno Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Poulin_P/0/1/0/all/0/1">Pierre Poulin</a>, <a href="http://arxiv.org/find/cs/1/au:+Paquette_E/0/1/0/all/0/1">Eric Paquette</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05143">
                                    <div class="article-summary-box-inner">
                                        <span>We present a novel up-resing technique for generating high-resolution liquids
based on scene flow estimation using deep neural networks. Our approach infers
and synthesizes small- and large-scale details solely from a low-resolution
particle-based liquid simulation. The proposed network leverages neighborhood
contributions to encode inherent liquid properties throughout convolutions. We
also propose a particle-based approach to interpolate between liquids generated
from varying simulation discretizations using a state-of-the-art bidirectional
optical flow solver method for fluids in addition to a novel key-event
topological alignment constraint. In conjunction with the neighborhood
contributions, our loss formulation allows the inference model throughout
epochs to reward important differences in regard to significant gaps in
simulation discretizations. Even when applied in an untested simulation setup,
our approach is able to generate plausible high-resolution details. Using this
interpolation approach and the predicted displacements, our approach combines
the input liquid properties with the predicted motion to infer semi-Lagrangian
advection. We furthermore showcase how the proposed interpolation approach can
facilitate generating large simulation datasets with a subset of initial
condition parameters.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An ordinal CNN approach for the assessment of neurological damage in Parkinson&#x27;s disease patients. (arXiv:2106.05230v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Barbero_Gomez_J/0/1/0/all/0/1">Javier Barbero-G&#xf3;mez</a>, <a href="http://arxiv.org/find/cs/1/au:+Gutierrez_P/0/1/0/all/0/1">Pedro-Antonio Guti&#xe9;rrez</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_V/0/1/0/all/0/1">V&#xed;ctor-Manuel Vargas</a>, <a href="http://arxiv.org/find/cs/1/au:+Vallejo_Casas_J/0/1/0/all/0/1">Juan-Antonio Vallejo-Casas</a>, <a href="http://arxiv.org/find/cs/1/au:+Hervas_Martinez_C/0/1/0/all/0/1">C&#xe9;sar Herv&#xe1;s-Mart&#xed;nez</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05230">
                                    <div class="article-summary-box-inner">
                                        <span>3D image scans are an assessment tool for neurological damage in Parkinson&#x27;s
disease (PD) patients. This diagnosis process can be automatized to help
medical staff through Decision Support Systems (DSSs), and Convolutional Neural
Networks (CNNs) are good candidates, because they are effective when applied to
spatial data. This paper proposes a 3D CNN ordinal model for assessing the
level or neurological damage in PD patients. Given that CNNs need large
datasets to achieve acceptable performance, a data augmentation method is
adapted to work with spatial data. We consider the Ordinal Graph-based
Oversampling via Shortest Paths (OGO-SP) method, which applies a gamma
probability distribution for inter-class data generation. A modification of
OGO-SP is proposed, the OGO-SP-$\beta$ algorithm, which applies the beta
distribution for generating synthetic samples in the inter-class region, a
better suited distribution when compared to gamma. The evaluation of the
different methods is based on a novel 3D image dataset provided by the Hospital
Universitario &#x27;Reina Sof\&#x27;ia&#x27; (C\&#x27;ordoba, Spain). We show how the ordinal
methodology improves the performance with respect to the nominal one, and how
OGO-SP-$\beta$ yields better performance than OGO-SP.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1">Wang Yifan</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1">Lukas Rahmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1">Olga Sorkine-Hornung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05187">
                                    <div class="article-summary-box-inner">
                                        <span>We present implicit displacement fields, a novel representation for detailed
3D geometry. Inspired by a classic surface deformation technique, displacement
mapping, our method represents a complex surface as a smooth base surface plus
a displacement along the base&#x27;s normal directions, resulting in a
frequency-based shape decomposition, where the high frequency signal is
constrained geometrically by the low frequency signal. Importantly, this
disentanglement is unsupervised thanks to a tailored architectural design that
has an innate frequency hierarchy by construction. We explore implicit
displacement field surface reconstruction and detail transfer and demonstrate
superior representational power, training stability and generalizability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bayesian Attention Belief Networks. (arXiv:2106.05251v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shujian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1">Xinjie Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Bo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1">Mingyuan Zhou</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05251">
                                    <div class="article-summary-box-inner">
                                        <span>Attention-based neural networks have achieved state-of-the-art results on a
wide range of tasks. Most such models use deterministic attention while
stochastic attention is less explored due to the optimization difficulties or
complicated model design. This paper introduces Bayesian attention belief
networks, which construct a decoder network by modeling unnormalized attention
weights with a hierarchy of gamma distributions, and an encoder network by
stacking Weibull distributions with a deterministic-upward-stochastic-downward
structure to approximate the posterior. The resulting auto-encoding networks
can be optimized in a differentiable way with a variational lower bound. It is
simple to convert any models with deterministic attention, including pretrained
ones, to the proposed Bayesian attention belief networks. On a variety of
language understanding tasks, we show that our method outperforms deterministic
attention and state-of-the-art stochastic attention in accuracy, uncertainty
estimation, generalization across domains, and robustness to adversarial
attacks. We further demonstrate the general applicability of our method on
neural machine translation and visual question answering, showing great
potential of incorporating our method into various attention-related tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Mixture weights optimisation for Alpha-Divergence Variational Inference. (arXiv:2106.05114v1 [math.ST])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Daudel_K/0/1/0/all/0/1">Kam&#xe9;lia Daudel</a>, <a href="http://arxiv.org/find/math/1/au:+Douc_R/0/1/0/all/0/1">Randal Douc</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05114">
                                    <div class="article-summary-box-inner">
                                        <span>This paper focuses on $\alpha$-divergence minimisation methods for
Variational Inference. More precisely, we are interested in algorithms
optimising the mixture weights of any given mixture model, without any
information on the underlying distribution of its mixture components
parameters. The Power Descent, defined for all $\alpha \neq 1$, is one such
algorithm and we establish in our work the full proof of its convergence
towards the optimal mixture weights when $\alpha &lt;1$. Since the
$\alpha$-divergence recovers the widely-used forward Kullback-Leibler when
$\alpha \to 1$, we then extend the Power Descent to the case $\alpha &#x3D; 1$ and
show that we obtain an Entropic Mirror Descent. This leads us to investigate
the link between Power Descent and Entropic Mirror Descent: first-order
approximations allow us to introduce the Renyi Descent, a novel algorithm for
which we prove an $O(1/N)$ convergence rate. Lastly, we compare numerically the
behavior of the unbiased Power Descent and of the biased Renyi Descent and we
discuss the potential advantages of one algorithm over the other.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning normal form autoencoders for data-driven discovery of universal,parameter-dependent governing equations. (arXiv:2106.05102v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kalia_M/0/1/0/all/0/1">Manu Kalia</a>, <a href="http://arxiv.org/find/cs/1/au:+Brunton_S/0/1/0/all/0/1">Steven L. Brunton</a>, <a href="http://arxiv.org/find/cs/1/au:+Meijer_H/0/1/0/all/0/1">Hil G.E. Meijer</a>, <a href="http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1">Christoph Brune</a>, <a href="http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1">J. Nathan Kutz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05102">
                                    <div class="article-summary-box-inner">
                                        <span>Complex systems manifest a small number of instabilities and bifurcations
that are canonical in nature, resulting in universal pattern forming
characteristics as a function of some parametric dependence. Such parametric
instabilities are mathematically characterized by their universal un-foldings,
or normal form dynamics, whereby a parsimonious model can be used to represent
the dynamics. Although center manifold theory guarantees the existence of such
low-dimensional normal forms, finding them has remained a long standing
challenge. In this work, we introduce deep learning autoencoders to discover
coordinate transformations that capture the underlying parametric dependence of
a dynamical system in terms of its canonical normal form, allowing for a simple
representation of the parametric dependence and bifurcation structure. The
autoencoder constrains the latent variable to adhere to a given normal form,
thus allowing it to learn the appropriate coordinate transformation. We
demonstrate the method on a number of example problems, showing that it can
capture a diverse set of normal forms associated with Hopf, pitchfork,
transcritical and/or saddle node bifurcations. This method shows how normal
forms can be leveraged as canonical and universal building blocks in deep
learning approaches for model discovery and reduced-order modeling.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Lipschitz Constant of Self-Attention. (arXiv:2006.04710v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Kim_H/0/1/0/all/0/1">Hyunjik Kim</a>, <a href="http://arxiv.org/find/stat/1/au:+Papamakarios_G/0/1/0/all/0/1">George Papamakarios</a>, <a href="http://arxiv.org/find/stat/1/au:+Mnih_A/0/1/0/all/0/1">Andriy Mnih</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04710">
                                    <div class="article-summary-box-inner">
                                        <span>Lipschitz constants of neural networks have been explored in various contexts
in deep learning, such as provable adversarial robustness, estimating
Wasserstein distance, stabilising training of GANs, and formulating invertible
neural networks. Such works have focused on bounding the Lipschitz constant of
fully connected or convolutional networks, composed of linear maps and
pointwise non-linearities. In this paper, we investigate the Lipschitz constant
of self-attention, a non-linear neural network module widely used in sequence
modelling. We prove that the standard dot-product self-attention is not
Lipschitz for unbounded input domain, and propose an alternative L2
self-attention that is Lipschitz. We derive an upper bound on the Lipschitz
constant of L2 self-attention and provide empirical evidence for its asymptotic
tightness. To demonstrate the practical relevance of our theoretical work, we
formulate invertible self-attention and use it in a Transformer-based
architecture for a character-level language modelling task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Lyapunov-Based Methodology for Constrained Optimization with Bandit Feedback. (arXiv:2106.05165v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1">Semih Cayci</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yilin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Eryilmaz_A/0/1/0/all/0/1">Atilla Eryilmaz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05165">
                                    <div class="article-summary-box-inner">
                                        <span>In a wide variety of applications including online advertising, contractual
hiring, and wireless scheduling, the controller is constrained by a stringent
budget constraint on the available resources, which are consumed in a random
amount by each action, and a stochastic feasibility constraint that may impose
important operational limitations on decision-making. In this work, we consider
a general model to address such problems, where each action returns a random
reward, cost, and penalty from an unknown joint distribution, and the
decision-maker aims to maximize the total reward under a budget constraint $B$
on the total cost and a stochastic constraint on the time-average penalty. We
propose a novel low-complexity algorithm based on Lyapunov optimization
methodology, named ${\tt LyOn}$, and prove that it achieves $O(\sqrt{B\log B})$
regret and $O(\log B/B)$ constraint-violation. The low computational cost and
sharp performance bounds of ${\tt LyOn}$ suggest that Lyapunov-based algorithm
design methodology can be effective in solving constrained bandit optimization
problems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Distilling Image Classifiers in Object Detectors. (arXiv:2106.05209v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shuxuan Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1">Jose M. Alvarez</a>, <a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1">Mathieu Salzmann</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05209">
                                    <div class="article-summary-box-inner">
                                        <span>Knowledge distillation constitutes a simple yet effective way to improve the
performance of a compact student network by exploiting the knowledge of a more
powerful teacher. Nevertheless, the knowledge distillation literature remains
limited to the scenario where the student and the teacher tackle the same task.
Here, we investigate the problem of transferring knowledge not only across
architectures but also across tasks. To this end, we study the case of object
detection and, instead of following the standard detector-to-detector
distillation approach, introduce a classifier-to-detector knowledge transfer
framework. In particular, we propose strategies to exploit the classification
teacher to improve both the detector&#x27;s recognition accuracy and localization
performance. Our experiments on several detectors with different backbones
demonstrate the effectiveness of our approach, allowing us to outperform the
state-of-the-art detector-to-detector distillation methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Analysis of convolutional neural network image classifiers in a hierarchical max-pooling model with additional local pooling. (arXiv:2106.05233v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Walter_B/0/1/0/all/0/1">Benjamin Walter</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05233">
                                    <div class="article-summary-box-inner">
                                        <span>Image classification is considered, and a hierarchical max-pooling model with
additional local pooling is introduced. Here the additional local pooling
enables the hierachical model to combine parts of the image which have a
variable relative distance towards each other. Various convolutional neural
network image classifiers are introduced and compared in view of their rate of
convergence. The finite sample size performance of the estimates is analyzed by
applying them to simulated and real data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Massively Parallel and Asynchronous Tsetlin Machine Architecture Supporting Almost Constant-Time Scaling. (arXiv:2009.04861v4 [cs.AI] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Abeyrathna_K/0/1/0/all/0/1">K. Darshana Abeyrathna</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1">Bimal Bhattarai</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1">Morten Goodwin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorji_S/0/1/0/all/0/1">Saeed Gorji</a>, <a href="http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1">Ole-Christoffer Granmo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1">Lei Jiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1">Rupsa Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1">Rohan K. Yadav</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.04861">
                                    <div class="article-summary-box-inner">
                                        <span>Using logical clauses to represent patterns, Tsetlin Machines (TMs) have
recently obtained competitive performance in terms of accuracy, memory
footprint, energy, and learning speed on several benchmarks. Each TM clause
votes for or against a particular class, with classification resolved using a
majority vote. While the evaluation of clauses is fast, being based on binary
operators, the voting makes it necessary to synchronize the clause evaluation,
impeding parallelization. In this paper, we propose a novel scheme for
desynchronizing the evaluation of clauses, eliminating the voting bottleneck.
In brief, every clause runs in its own thread for massive native parallelism.
For each training example, we keep track of the class votes obtained from the
clauses in local voting tallies. The local voting tallies allow us to detach
the processing of each clause from the rest of the clauses, supporting
decentralized learning. This means that the TM most of the time will operate on
outdated voting tallies. We evaluated the proposed parallelization across
diverse learning tasks and it turns out that our decentralized TM learning
algorithm copes well with working on outdated data, resulting in no significant
loss in learning accuracy. Furthermore, we show that the proposed approach
provides up to 50 times faster learning. Finally, learning time is almost
constant for reasonable clause amounts (employing from 20 to 7,000 clauses on a
Tesla V100 GPU). For sufficiently large clause numbers, computation time
increases approximately proportionally. Our parallel and asynchronous
architecture thus allows processing of massive datasets and operating with more
clauses for higher accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">To Bag is to Prune. (arXiv:2008.07063v4 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Coulombe_P/0/1/0/all/0/1">Philippe Goulet Coulombe</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.07063">
                                    <div class="article-summary-box-inner">
                                        <span>It is notoriously difficult to build a bad Random Forest (RF). Concurrently,
RF blatantly overfits in-sample without any apparent consequence out-of-sample.
Standard arguments, like the classic bias-variance trade-off or double descent,
cannot rationalize this paradox. I propose a new explanation: bootstrap
aggregation and model perturbation as implemented by RF automatically prune a
latent &quot;true&quot; tree. More generally, randomized ensembles of greedily optimized
learners implicitly perform optimal early stopping out-of-sample. So there is
no need to tune the stopping point. By construction, novel variants of Boosting
and MARS are also eligible for automatic tuning. I empirically demonstrate the
property, with simulated and real data, by reporting that these new completely
overfitting ensembles perform similarly to their tuned counterparts -- or
better.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech to Standard German Text Corpus. (arXiv:2010.02810v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pluss_M/0/1/0/all/0/1">Michel Pl&#xfc;ss</a>, <a href="http://arxiv.org/find/cs/1/au:+Neukom_L/0/1/0/all/0/1">Lukas Neukom</a>, <a href="http://arxiv.org/find/cs/1/au:+Scheller_C/0/1/0/all/0/1">Christian Scheller</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogel_M/0/1/0/all/0/1">Manfred Vogel</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.02810">
                                    <div class="article-summary-box-inner">
                                        <span>We present the Swiss Parliaments Corpus (SPC), an automatically aligned Swiss
German speech to Standard German text corpus. This first version of the corpus
is based on publicly available data of the Bernese cantonal parliament and
consists of 293 hours of data. It was created using a novel forced sentence
alignment procedure and an alignment quality estimator, which can be used to
trade off corpus size and quality. We trained Automatic Speech Recognition
(ASR) models as baselines on different subsets of the data and achieved a Word
Error Rate (WER) of 0.278 and a BLEU score of 0.586 on the SPC test set. The
corpus is freely available for download.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning. (arXiv:2006.10412v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1">Arrasy Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Hopner_N/0/1/0/all/0/1">Niklas H&#xf6;pner</a>, <a href="http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1">Filippos Christianos</a>, <a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1">Stefano V. Albrecht</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10412">
                                    <div class="article-summary-box-inner">
                                        <span>Ad hoc teamwork is the challenging problem of designing an autonomous agent
which can adapt quickly to collaborate with teammates without prior
coordination mechanisms, including joint training. Prior work in this area has
focused on closed teams in which the number of agents is fixed. In this work,
we consider open teams by allowing agents with different fixed policies to
enter and leave the environment without prior notification. Our solution builds
on graph neural networks to learn agent models and joint-action value models
under varying team compositions. We contribute a novel action-value computation
that integrates the agent model and joint-action value model to produce
action-value estimates. We empirically demonstrate that our approach
successfully models the effects other agents have on the learner, leading to
policies that robustly adapt to dynamic team compositions and significantly
outperform several alternative methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation. (arXiv:2106.05093v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1">Cunxiao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhaopeng Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jing Jiang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05093">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a new training objective named order-agnostic cross entropy (OaXE)
for fully non-autoregressive translation (NAT) models. OaXE improves the
standard cross-entropy loss to ameliorate the effect of word reordering, which
is a common source of the critical multimodality problem in NAT. Concretely,
OaXE removes the penalty for word order errors, and computes the cross entropy
loss based on the best possible alignment between model predictions and target
tokens. Since the log loss is very sensitive to invalid references, we leverage
cross entropy initialization and loss truncation to ensure the model focuses on
a good part of the search space. Extensive experiments on major WMT benchmarks
show that OaXE substantially improves translation performance, setting new
state of the art for fully NAT models. Further analyses show that OaXE
alleviates the multimodality problem by reducing token repetitions and
increasing prediction confidence. Our code, data, and trained models are
available at https://github.com/tencent-ailab/ICML21_OAXE.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback. (arXiv:2106.05203v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1">Peter Richt&#xe1;rik</a>, <a href="http://arxiv.org/find/cs/1/au:+Sokolov_I/0/1/0/all/0/1">Igor Sokolov</a>, <a href="http://arxiv.org/find/cs/1/au:+Fatkhullin_I/0/1/0/all/0/1">Ilyas Fatkhullin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05203">
                                    <div class="article-summary-box-inner">
                                        <span>Error feedback (EF), also known as error compensation, is an immensely
popular convergence stabilization mechanism in the context of distributed
training of supervised machine learning models enhanced by the use of
contractive communication compression mechanisms, such as Top-$k$. First
proposed by Seide et al (2014) as a heuristic, EF resisted any theoretical
understanding until recently [Stich et al., 2018, Alistarh et al., 2018].
However, all existing analyses either i) apply to the single node setting only,
ii) rely on very strong and often unreasonable assumptions, such global
boundedness of the gradients, or iterate-dependent assumptions that cannot be
checked a-priori and may not hold in practice, or iii) circumvent these issues
via the introduction of additional unbiased compressors, which increase the
communication cost. In this work we fix all these deficiencies by proposing and
analyzing a new EF mechanism, which we call EF21, which consistently and
substantially outperforms EF in practice. Our theoretical analysis relies on
standard assumptions only, works in the distributed heterogeneous data setting,
and leads to better and more meaningful rates. In particular, we prove that
EF21 enjoys a fast $O(1/T)$ convergence rate for smooth nonconvex problems,
beating the previous bound of $O(1/T^{2/3})$, which was shown a bounded
gradients assumption. We further improve this to a fast linear rate for PL
functions, which is the first linear convergence result for an EF-type method
not relying on unbiased compressors. Since EF has a large number of
applications where it reigns supreme, we believe that our 2021 variant, EF21,
can a large impact on the practice of communication efficient distributed
learning.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Concave Utility Reinforcement Learning: the Mean-field Game viewpoint. (arXiv:2106.03787v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1">Matthieu Geist</a>, <a href="http://arxiv.org/find/cs/1/au:+Perolat_J/0/1/0/all/0/1">Julien P&#xe9;rolat</a>, <a href="http://arxiv.org/find/cs/1/au:+Lauriere_M/0/1/0/all/0/1">Mathieu Lauri&#xe8;re</a>, <a href="http://arxiv.org/find/cs/1/au:+Elie_R/0/1/0/all/0/1">Romuald Elie</a>, <a href="http://arxiv.org/find/cs/1/au:+Perrin_S/0/1/0/all/0/1">Sarah Perrin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1">Olivier Bachem</a>, <a href="http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1">R&#xe9;mi Munos</a>, <a href="http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1">Olivier Pietquin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03787">
                                    <div class="article-summary-box-inner">
                                        <span>Concave Utility Reinforcement Learning (CURL) extends RL from linear to
concave utilities in the occupancy measure induced by the agent&#x27;s policy. This
encompasses not only RL but also imitation learning and exploration, among
others. Yet, this more general paradigm invalidates the classical Bellman
equations, and calls for new algorithms. Mean-field Games (MFGs) are a
continuous approximation of many-agent RL. They consider the limit case of a
continuous distribution of identical agents, anonymous with symmetric
interests, and reduce the problem to the study of a single representative agent
in interaction with the full population. Our core contribution consists in
showing that CURL is a subclass of MFGs. We think this important to bridge
together both communities. It also allows to shed light on aspects of both
fields: we show the equivalence between concavity in CURL and monotonicity in
the associated MFG, between optimality conditions in CURL and Nash equilibrium
in MFG, or that Fictitious Play (FP) for this class of MFGs is simply
Frank-Wolfe, bringing the first convergence rate for discrete-time FP for MFGs.
We also experimentally demonstrate that, using algorithms recently introduced
for solving MFGs, we can address the CURL problem more efficiently.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Neighborhood Contrastive Learning Applied to Online Patient Monitoring. (arXiv:2106.05142v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yeche_H/0/1/0/all/0/1">Hugo Y&#xe8;che</a>, <a href="http://arxiv.org/find/cs/1/au:+Dresdner_G/0/1/0/all/0/1">Gideon Dresdner</a>, <a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1">Francesco Locatello</a>, <a href="http://arxiv.org/find/cs/1/au:+Huser_M/0/1/0/all/0/1">Matthias H&#xfc;ser</a>, <a href="http://arxiv.org/find/cs/1/au:+Ratsch_G/0/1/0/all/0/1">Gunnar R&#xe4;tsch</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05142">
                                    <div class="article-summary-box-inner">
                                        <span>Intensive care units (ICU) are increasingly looking towards machine learning
for methods to provide online monitoring of critically ill patients. In machine
learning, online monitoring is often formulated as a supervised learning
problem. Recently, contrastive learning approaches have demonstrated promising
improvements over competitive supervised benchmarks. These methods rely on
well-understood data augmentation techniques developed for image data which do
not apply to online monitoring. In this work, we overcome this limitation by
supplementing time-series data augmentation techniques with a novel contrastive
learning objective which we call neighborhood contrastive learning (NCL). Our
objective explicitly groups together contiguous time segments from each patient
while maintaining state-specific information. Our experiments demonstrate a
marked improvement over existing work applying contrastive methods to medical
time-series.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Densely connected multidilated convolutional networks for dense prediction tasks. (arXiv:2011.11844v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Takahashi_N/0/1/0/all/0/1">Naoya Takahashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1">Yuki Mitsufuji</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11844">
                                    <div class="article-summary-box-inner">
                                        <span>Tasks that involve high-resolution dense prediction require a modeling of
both local and global patterns in a large input field. Although the local and
global structures often depend on each other and their simultaneous modeling is
important, many convolutional neural network (CNN)-based approaches interchange
representations in different resolutions only a few times. In this paper, we
claim the importance of a dense simultaneous modeling of multiresolution
representation and propose a novel CNN architecture called densely connected
multidilated DenseNet (D3Net). D3Net involves a novel multidilated convolution
that has different dilation factors in a single layer to model different
resolutions simultaneously. By combining the multidilated convolution with the
DenseNet architecture, D3Net incorporates multiresolution learning with an
exponentially growing receptive field in almost all layers, while avoiding the
aliasing problem that occurs when we naively incorporate the dilated
convolution in DenseNet. Experiments on the image semantic segmentation task
using Cityscapes and the audio source separation task using MUSDB18 show that
the proposed method has superior performance over state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Efficient Active Search for Combinatorial Optimization Problems. (arXiv:2106.05126v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hottung_A/0/1/0/all/0/1">Andr&#xe9; Hottung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1">Yeong-Dae Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Tierney_K/0/1/0/all/0/1">Kevin Tierney</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05126">
                                    <div class="article-summary-box-inner">
                                        <span>Recently numerous machine learning based methods for combinatorial
optimization problems have been proposed that learn to construct solutions in a
sequential decision process via reinforcement learning. While these methods can
be easily combined with search strategies like sampling and beam search, it is
not straightforward to integrate them into a high-level search procedure
offering strong search guidance. Bello et al. (2016) propose active search,
which adjusts the weights of a (trained) model with respect to a single
instance at test time using reinforcement learning. While active search is
simple to implement, it is not competitive with state-of-the-art methods
because adjusting all model weights for each test instance is very time and
memory intensive. Instead of updating all model weights, we propose and
evaluate three efficient active search strategies that only update a subset of
parameters during the search. The proposed methods offer a simple way to
significantly improve the search performance of a given model and outperform
state-of-the-art machine learning based methods on combinatorial problems, even
surpassing the well-known heuristic solver LKH3 on the capacitated vehicle
routing problem. Finally, we show that (efficient) active search enables
learned models to effectively solve instances that are much larger than those
seen during training.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MLPF: Efficient machine-learned particle-flow reconstruction using graph neural networks. (arXiv:2101.08578v3 [physics.data-an] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/physics/1/au:+Pata_J/0/1/0/all/0/1">Joosep Pata</a>, <a href="http://arxiv.org/find/physics/1/au:+Duarte_J/0/1/0/all/0/1">Javier Duarte</a>, <a href="http://arxiv.org/find/physics/1/au:+Vlimant_J/0/1/0/all/0/1">Jean-Roch Vlimant</a>, <a href="http://arxiv.org/find/physics/1/au:+Pierini_M/0/1/0/all/0/1">Maurizio Pierini</a>, <a href="http://arxiv.org/find/physics/1/au:+Spiropulu_M/0/1/0/all/0/1">Maria Spiropulu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08578">
                                    <div class="article-summary-box-inner">
                                        <span>In general-purpose particle detectors, the particle-flow algorithm may be
used to reconstruct a comprehensive particle-level view of the event by
combining information from the calorimeters and the trackers, significantly
improving the detector resolution for jets and the missing transverse momentum.
In view of the planned high-luminosity upgrade of the CERN Large Hadron
Collider (LHC), it is necessary to revisit existing reconstruction algorithms
and ensure that both the physics and computational performance are sufficient
in an environment with many simultaneous proton-proton interactions (pileup).
Machine learning may offer a prospect for computationally efficient event
reconstruction that is well-suited to heterogeneous computing platforms, while
significantly improving the reconstruction quality over rule-based algorithms
for granular detectors. We introduce MLPF, a novel, end-to-end trainable,
machine-learned particle-flow algorithm based on parallelizable,
computationally efficient, and scalable graph neural networks optimized using a
multi-task objective on simulated events. We report the physics and
computational performance of the MLPF algorithm on a Monte Carlo dataset of top
quark-antiquark pairs produced in proton-proton collisions in conditions
similar to those expected for the high-luminosity LHC. The MLPF algorithm
improves the physics response with respect to a rule-based benchmark algorithm
and demonstrates computationally scalable particle-flow reconstruction in a
high-pileup environment.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SMG: A Shuffling Gradient-Based Method with Momentum. (arXiv:2011.11884v3 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Tran_T/0/1/0/all/0/1">Trang H. Tran</a>, <a href="http://arxiv.org/find/math/1/au:+Nguyen_L/0/1/0/all/0/1">Lam M. Nguyen</a>, <a href="http://arxiv.org/find/math/1/au:+Tran_Dinh_Q/0/1/0/all/0/1">Quoc Tran-Dinh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11884">
                                    <div class="article-summary-box-inner">
                                        <span>We combine two advanced ideas widely used in optimization for machine
learning: shuffling strategy and momentum technique to develop a novel
shuffling gradient-based method with momentum, coined Shuffling Momentum
Gradient (SMG), for non-convex finite-sum optimization problems. While our
method is inspired by momentum techniques, its update is fundamentally
different from existing momentum-based methods. We establish state-of-the-art
convergence rates of SMG for any shuffling strategy using either constant or
diminishing learning rate under standard assumptions (i.e.$L$-smoothness and
bounded variance). When the shuffling strategy is fixed, we develop another new
algorithm that is similar to existing momentum methods, and prove the same
convergence rates for this algorithm under the $L$-smoothness and bounded
gradient assumptions. We demonstrate our algorithms via numerical simulations
on standard datasets and compare them with existing shuffling methods. Our
tests have shown encouraging performance of the new algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Local Algorithms for Finding Densely Connected Clusters. (arXiv:2106.05245v1 [cs.DS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Macgregor_P/0/1/0/all/0/1">Peter Macgregor</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">He Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05245">
                                    <div class="article-summary-box-inner">
                                        <span>Local graph clustering is an important algorithmic technique for analysing
massive graphs, and has been widely applied in many research fields of data
science. While the objective of most (local) graph clustering algorithms is to
find a vertex set of low conductance, there has been a sequence of recent
studies that highlight the importance of the inter-connection between clusters
when analysing real-world datasets. Following this line of research, in this
work we study local algorithms for finding a pair of vertex sets defined with
respect to their inter-connection and their relationship with the rest of the
graph. The key to our analysis is a new reduction technique that relates the
structure of multiple sets to a single vertex set in the reduced graph. Among
many potential applications, we show that our algorithms successfully recover
densely connected clusters in the Interstate Disputes Dataset and the US
Migration Dataset.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Pretrained Encoders are All You Need. (arXiv:2106.05139v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1">Mina Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivatsa_P/0/1/0/all/0/1">P Srivatsa</a>, <a href="http://arxiv.org/find/cs/1/au:+Rane_A/0/1/0/all/0/1">Advait Rane</a>, <a href="http://arxiv.org/find/cs/1/au:+Chenniappa_S/0/1/0/all/0/1">Shriram Chenniappa</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_R/0/1/0/all/0/1">Rishabh Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Ozair_S/0/1/0/all/0/1">Sherjil Ozair</a>, <a href="http://arxiv.org/find/cs/1/au:+Maes_P/0/1/0/all/0/1">Pattie Maes</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05139">
                                    <div class="article-summary-box-inner">
                                        <span>Data-efficiency and generalization are key challenges in deep learning and
deep reinforcement learning as many models are trained on large-scale,
domain-specific, and expensive-to-label datasets. Self-supervised models
trained on large-scale uncurated datasets have shown successful transfer to
diverse settings. We investigate using pretrained image representations and
spatio-temporal attention for state representation learning in Atari. We also
explore fine-tuning pretrained representations with self-supervised techniques,
i.e., contrastive predictive coding, spatio-temporal contrastive learning, and
augmentations. Our results show that pretrained representations are at par with
state-of-the-art self-supervised methods trained on domain-specific data.
Pretrained representations, thus, yield data and compute-efficient state
representations. https://github.com/PAL-ML/PEARL_v1</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Knowledge distillation: A good teacher is patient and consistent. (arXiv:2106.05237v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1">Lucas Beyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1">Xiaohua Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Royer_A/0/1/0/all/0/1">Am&#xe9;lie Royer</a>, <a href="http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1">Larisa Markeeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1">Rohan Anil</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1">Alexander Kolesnikov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05237">
                                    <div class="article-summary-box-inner">
                                        <span>There is a growing discrepancy in computer vision between large-scale models
that achieve state-of-the-art performance and models that are affordable in
practical applications. In this paper we address this issue and significantly
bridge the gap between these two types of models. Throughout our empirical
investigation we do not aim to necessarily propose a new method, but strive to
identify a robust and effective recipe for making state-of-the-art large scale
models affordable in practice. We demonstrate that, when performed correctly,
knowledge distillation can be a powerful tool for reducing the size of large
models without compromising their performance. In particular, we uncover that
there are certain implicit design choices, which may drastically affect the
effectiveness of distillation. Our key contribution is the explicit
identification of these design choices, which were not previously articulated
in the literature. We back up our findings by a comprehensive empirical study,
demonstrate compelling results on a wide range of vision datasets and, in
particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which
achieves 82.8\% top-1 accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Rethink Transfer Learning in Medical Image Classification. (arXiv:2106.05152v1 [eess.IV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1">Le Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1">Hengyue Liang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1">Taihui Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1">Ju Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05152">
                                    <div class="article-summary-box-inner">
                                        <span>Transfer learning (TL) with deep convolutional neural networks (DCNNs) has
proved successful in medical image classification (MIC). However, the current
practice is puzzling, as MIC typically relies only on low- and/or mid-level
features that are learned in the bottom layers of DCNNs. Following this
intuition, we question the current strategies of TL in MIC. In this paper, we
perform careful experimental comparisons between shallow and deep networks for
classification on two chest x-ray datasets, using different TL strategies. We
find that deep models are not always favorable, and finetuning truncated deep
models almost always yields the best performance, especially in data-poor
regimes.

Project webpage:
https://github.com/sun-umn/Transfer-Learning-in-Medical-Imaging

Keywords: Transfer learning, Medical image classification, Feature hierarchy,
Medical imaging, Evaluation metrics, Imbalanced data</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-armed Bandit Requiring Monotone Arm Sequences. (arXiv:2106.03790v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1">Ningyuan Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03790">
                                    <div class="article-summary-box-inner">
                                        <span>In many online learning or multi-armed bandit problems, the taken actions or
pulled arms are ordinal and required to be monotone over time. Examples include
dynamic pricing, in which the firms use markup pricing policies to please early
adopters and deter strategic waiting, and clinical trials, in which the dose
allocation usually follows the dose escalation principle to prevent dose
limiting toxicities. We consider the continuum-armed bandit problem when the
arm sequence is required to be monotone. We show that when the unknown
objective function is Lipschitz continuous, the regret is $O(T)$. When in
addition the objective function is unimodal or quasiconcave, the regret is
$\tilde O(T^{3/4})$ under the proposed algorithm, which is also shown to be the
optimal rate. This deviates from the optimal rate $\tilde O(T^{2/3})$ in the
continuous-armed bandit literature and demonstrates the cost to the learning
efficiency brought by the monotonicity requirement.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">How Framelets Enhance Graph Neural Networks. (arXiv:2102.06986v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xuebin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bingxin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Junbin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Guang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Lio</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Ming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1">Guido Montufar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06986">
                                    <div class="article-summary-box-inner">
                                        <span>This paper presents a new approach for assembling graph neural networks based
on framelet transforms. The latter provides a multi-scale representation for
graph-structured data. We decompose an input graph into low-pass and high-pass
frequencies coefficients for network training, which then defines a
framelet-based graph convolution. The framelet decomposition naturally induces
a graph pooling strategy by aggregating the graph feature into low-pass and
high-pass spectra, which considers both the feature values and geometry of the
graph data and conserves the total information. The graph neural networks with
the proposed framelet convolution and pooling achieve state-of-the-art
performance in many node and graph prediction tasks. Moreover, we propose
shrinkage as a new activation for the framelet convolution, which thresholds
high-frequency information at different scales. Compared to ReLU, shrinkage
activation improves model performance on denoising and signal compression:
noises in both node and structure can be significantly reduced by accurately
cutting off the high-pass coefficients from framelet decomposition, and the
signal can be compressed to less than half its original size with
well-preserved prediction performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Do Transformers Really Perform Bad for Graph Representation?. (arXiv:2106.05234v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1">Chengxuan Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Tianle Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1">Shengjie Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Shuxin Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1">Guolin Ke</a>, <a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1">Di He</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yanming Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tie-Yan Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05234">
                                    <div class="article-summary-box-inner">
                                        <span>The Transformer architecture has become a dominant choice in many domains,
such as natural language processing and computer vision. Yet, it has not
achieved competitive performance on popular leaderboards of graph-level
prediction compared to mainstream GNN variants. Therefore, it remains a mystery
how Transformers could perform well for graph representation learning. In this
paper, we solve this mystery by presenting Graphormer, which is built upon the
standard Transformer architecture, and could attain excellent results on a
broad range of graph representation learning tasks, especially on the recent
OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the
graph is the necessity of effectively encoding the structural information of a
graph into the model. To this end, we propose several simple yet effective
structural encoding methods to help Graphormer better model graph-structured
data. Besides, we mathematically characterize the expressive power of
Graphormer and exhibit that with our ways of encoding the structural
information of graphs, many popular GNN variants could be covered as the
special cases of Graphormer.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Clustering based Fair Outlier Detection. (arXiv:2106.05127v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1">Hanyu Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peizhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongfu Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05127">
                                    <div class="article-summary-box-inner">
                                        <span>In this paper, we focus on the fairness issues regarding unsupervised outlier
detection. Traditional algorithms, without a specific design for algorithmic
fairness, could implicitly encode and propagate statistical bias in data and
raise societal concerns. To correct such unfairness and deliver a fair set of
potential outlier candidates, we propose Deep Clustering based Fair Outlier
Detection (DCFOD) that learns a good representation for utility maximization
while enforcing the learnable representation to be subgroup-invariant on the
sensitive attribute. Considering the coupled and reciprocal nature between
clustering and outlier detection, we leverage deep clustering to discover the
intrinsic cluster structure and out-of-structure instances. Meanwhile, an
adversarial training erases the sensitive pattern for instances for fairness
adaptation. Technically, we propose an instance-level weighted representation
learning strategy to enhance the joint deep clustering and outlier detection,
where the dynamic weight module re-emphasizes contributions of likely-inliers
while mitigating the negative impact from outliers. Demonstrated by experiments
on eight datasets comparing to 17 outlier detection algorithms, our DCFOD
method consistently achieves superior performance on both the outlier detection
validity and two types of fairness notions in outlier detection.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning to Generate Noise for Multi-Attack Robustness. (arXiv:2006.12135v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1">Divyam Madaan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1">Jinwoo Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12135">
                                    <div class="article-summary-box-inner">
                                        <span>Adversarial learning has emerged as one of the successful techniques to
circumvent the susceptibility of existing methods against adversarial
perturbations. However, the majority of existing defense methods are tailored
to defend against a single category of adversarial perturbation (e.g.
$\ell_\infty$-attack). In safety-critical applications, this makes these
methods extraneous as the attacker can adopt diverse adversaries to deceive the
system. Moreover, training on multiple perturbations simultaneously
significantly increases the computational overhead during training. To address
these challenges, we propose a novel meta-learning framework that explicitly
learns to generate noise to improve the model&#x27;s robustness against multiple
types of attacks. Its key component is Meta Noise Generator (MNG) that outputs
optimal noise to stochastically perturb a given sample, such that it helps
lower the error on diverse adversarial perturbations. By utilizing samples
generated by MNG, we train a model by enforcing the label consistency across
multiple perturbations. We validate the robustness of models trained by our
scheme on various datasets and against a wide variety of perturbations,
demonstrating that it significantly outperforms the baselines across multiple
perturbations with a marginal computational cost.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-domain Speech Recognition with Unsupervised Character-level Distribution Matching. (arXiv:2104.07491v3 [cs.SD] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1">Wenxin Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1">Xu Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1">Tao Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1">Takahiro Shinozaki</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07491">
                                    <div class="article-summary-box-inner">
                                        <span>End-to-end automatic speech recognition (ASR) can achieve promising
performance with large-scale training data. However, it is known that domain
mismatch between training and testing data often leads to a degradation of
recognition accuracy. In this work, we focus on the unsupervised domain
adaptation for ASR and propose CMatch, a Character-level distribution matching
method to perform fine-grained adaptation between each character in two
domains. First, to obtain labels for the features belonging to each character,
we achieve frame-level label assignment using the Connectionist Temporal
Classification (CTC) pseudo labels. Then, we match the character-level
distributions using Maximum Mean Discrepancy. We train our algorithm using the
self-training technique. Experiments on the Libri-Adapt dataset show that our
proposed approach achieves 14.39% and 16.50% relative Word Error Rate (WER)
reduction on both cross-device and cross-environment ASR. We also
comprehensively analyze the different strategies for frame-level label
assignment and Transformer adaptations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-Diagnosing GAN: Diagnosing Underrepresented Samples in Generative Adversarial Networks. (arXiv:2102.12033v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jinhee Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1">Haeri Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Youngkyu Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1">Hye Won Chung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12033">
                                    <div class="article-summary-box-inner">
                                        <span>Despite remarkable performance in producing realistic samples, Generative
Adversarial Networks (GANs) often produce low-quality samples near low-density
regions of the data manifold, especially for samples with minor features. Many
techniques have been developed to improve the quality of generated samples,
either by post-processing generated samples or by pre-processing the empirical
data distribution, but at the cost of reduced diversity. To promote diversity
in sample generation without degrading the overall quality, we propose a simple
yet effective method to diagnose and emphasize underrepresented samples during
training of a GAN. The main idea is to use the statistics of the discrepancy
between the data distribution and the model distribution at each data instance.
Based on the observation that the underrepresented samples have a high average
discrepancy or high variability in discrepancy, we propose a method to
emphasize those samples during training of a GAN. Our experimental results
demonstrate that the proposed method improves GAN performance on various
datasets, and it is especially effective in improving the quality and diversity
of generated samples with minor features.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Crowdsourced Labeling for Worker-Task Specialization Model. (arXiv:2004.00101v2 [cs.HC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Doyeon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1">Hye Won Chung</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.00101">
                                    <div class="article-summary-box-inner">
                                        <span>We consider crowdsourced labeling under a $d$-type worker-task specialization
model, where each worker and task is associated with one particular type among
a finite set of types and a worker provides a more reliable answer to tasks of
the matched type than to tasks of unmatched types. We design an inference
algorithm that recovers binary task labels (up to any given recovery accuracy)
by using worker clustering, worker skill estimation and weighted majority
voting. The designed inference algorithm does not require any information about
worker/task types, and achieves any targeted recovery accuracy with the best
known performance (minimum number of queries per task).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">XBNet : An Extremely Boosted Neural Network. (arXiv:2106.05239v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sarkar_T/0/1/0/all/0/1">Tushar Sarkar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05239">
                                    <div class="article-summary-box-inner">
                                        <span>Neural networks have proved to be very robust at processing unstructured data
like images, text, videos, and audio. However, it has been observed that their
performance is not up to the mark in tabular data; hence tree-based models are
preferred in such scenarios. A popular model for tabular data is boosted trees,
a highly efficacious and extensively used machine learning method, and it also
provides good interpretability compared to neural networks. In this paper, we
describe a novel architecture XBNet, which tries to combine tree-based models
with that of neural networks to create a robust architecture trained by using a
novel optimization technique, Boosted Gradient Descent for Tabular Data which
increases its interpretability and performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">MACE: A Flexible Framework for Membership Privacy Estimation in Generative Models. (arXiv:2009.05683v3 [cs.CR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yixi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1">Shruti Tople</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1">Sumit Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1">Juan Lavista Ferres</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.05683">
                                    <div class="article-summary-box-inner">
                                        <span>In this work, we formally study the membership privacy risk of generative
models and propose a membership privacy estimation framework. We formulate the
membership privacy risk as a statistical divergence between training samples
and hold-out samples, and propose sample-based methods to estimate this
divergence. Unlike previous works, our proposed metric and estimators make
realistic and flexible assumptions. First, we offer a generalizable metric as
an alternative to accuracy for imbalanced datasets. Second, our estimators are
capable of estimating the membership privacy risk given any scalar or vector
valued attributes from the learned model, while prior work require access to
specific attributes. This allows our framework to provide data-driven
certificates for trained generative models in terms of membership privacy risk.
Finally, we show a connection to differential privacy, which allows our
proposed estimators to be used to understand the privacy budget &#x27;epsilon&#x27;
needed for differentially private generative models. We demonstrate the utility
of our framework through experimental demonstrations on different generative
models using various model attributes yielding some new insights about
membership leakage and vulnerabilities of models.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">TF-IDF vs Word Embeddings for Morbidity Identification in Clinical Notes: An Initial Study. (arXiv:2105.09632v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dessi_D/0/1/0/all/0/1">Danilo Dessi</a>, <a href="http://arxiv.org/find/cs/1/au:+Helaoui_R/0/1/0/all/0/1">Rim Helaoui</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Vivek Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Recupero_D/0/1/0/all/0/1">Diego Reforgiato Recupero</a>, <a href="http://arxiv.org/find/cs/1/au:+Riboni_D/0/1/0/all/0/1">Daniele Riboni</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09632">
                                    <div class="article-summary-box-inner">
                                        <span>Today, we are seeing an ever-increasing number of clinical notes that contain
clinical results, images, and textual descriptions of patient&#x27;s health state.
All these data can be analyzed and employed to cater novel services that can
help people and domain experts with their common healthcare tasks. However,
many technologies such as Deep Learning and tools like Word Embeddings have
started to be investigated only recently, and many challenges remain open when
it comes to healthcare domain applications. To address these challenges, we
propose the use of Deep Learning and Word Embeddings for identifying sixteen
morbidity types within textual descriptions of clinical records. For this
purpose, we have used a Deep Learning model based on Bidirectional Long-Short
Term Memory (LSTM) layers which can exploit state-of-the-art vector
representations of data such as Word Embeddings. We have employed pre-trained
Word Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained
on the target domain. Furthermore, we have compared the performances of the
deep learning approaches against the traditional tf-idf using Support Vector
Machine and Multilayer perceptron (our baselines). From the obtained results it
seems that the latter outperforms the combination of Deep Learning approaches
using any word embeddings. Our preliminary results indicate that there are
specific features that make the dataset biased in favour of traditional machine
learning approaches.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Data-Driven Robust Optimization using Unsupervised Deep Learning. (arXiv:2011.09769v2 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Goerigk_M/0/1/0/all/0/1">Marc Goerigk</a>, <a href="http://arxiv.org/find/math/1/au:+Kurtz_J/0/1/0/all/0/1">Jannis Kurtz</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09769">
                                    <div class="article-summary-box-inner">
                                        <span>Robust optimization has been established as a leading methodology to approach
decision problems under uncertainty. To derive a robust optimization model, a
central ingredient is to identify a suitable model for uncertainty, which is
called the uncertainty set, containing all scenarios against which we wish to
protect. An ongoing challenge in the recent literature is to derive uncertainty
sets from given historical data.

In this paper we use an unsupervised deep learning method to construct
non-convex uncertainty sets from data, which have a more complex structure than
the typically considered sets. We prove that most of the classical uncertainty
classes are special cases of our derived sets and that optimizing over it is
strongly NP-hard. Nevertheless we show that the trained neural networks can be
integrated into a robust optimization model by formulating the adversarial
problem as a convex quadratic mixed-integer program. This allows us to derive
robust solutions through an iterative scenario generation process. We prove
that our class of uncertainty sets contains In extensive computational
experiments, we compare this approach to a similar approach, which derives
uncertainty sets by kernel-based support vector clustering. We find that
uncertainty sets derived by the unsupervised deep learning method can give a
better description of data, leading to robust solutions that often outperform
the comparison method both with respect to objective value and feasibility.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multistep Electric Vehicle Charging Station Occupancy Prediction using Mixed LSTM Neural Networks. (arXiv:2106.04986v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1">Tai-Yu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Faye_S/0/1/0/all/0/1">S&#xe9;bastien Faye</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04986">
                                    <div class="article-summary-box-inner">
                                        <span>Public charging station occupancy prediction plays key importance in
developing a smart charging strategy to reduce electric vehicle (EV) operator
and user inconvenience. However, existing studies are mainly based on
conventional econometric or time series methodologies with limited accuracy. We
propose a new mixed long short-term memory neural network incorporating both
historical charging state sequences and time-related features for multistep
discrete charging occupancy state prediction. Unlike the existing LSTM
networks, the proposed model separates different types of features and handles
them differently with mixed neural network architecture. The model is compared
to a number of state-of-the-art machine learning and deep learning approaches
based on the EV charging data obtained from the open data portal of the city of
Dundee, UK. The results show that the proposed method produces very accurate
predictions (99.99% and 81.87% for 1 step (10 minutes) and 6 step (1 hour)
ahead, respectively, and outperforms the benchmark approaches significantly
(+22.4% for one-step-ahead prediction and +6.2% for 6 steps ahead). A
sensitivity analysis is conducted to evaluate the impact of the model
parameters on prediction accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Non-Parametric Stochastic Sequential Assignment With Random Arrival Times. (arXiv:2106.04944v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dervovic_D/0/1/0/all/0/1">Danial Dervovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassanzadeh_P/0/1/0/all/0/1">Parisa Hassanzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Assefa_S/0/1/0/all/0/1">Samuel Assefa</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_P/0/1/0/all/0/1">Prashant Reddy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04944">
                                    <div class="article-summary-box-inner">
                                        <span>We consider a problem wherein jobs arrive at random times and assume random
values. Upon each job arrival, the decision-maker must decide immediately
whether or not to accept the job and gain the value on offer as a reward, with
the constraint that they may only accept at most $n$ jobs over some reference
time period. The decision-maker only has access to $M$ independent realisations
of the job arrival process. We propose an algorithm, Non-Parametric Sequential
Allocation (NPSA), for solving this problem. Moreover, we prove that the
expected reward returned by the NPSA algorithm converges in probability to
optimality as $M$ grows large. We demonstrate the effectiveness of the
algorithm empirically on synthetic data and on public fraud-detection datasets,
from where the motivation for this work is derived.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Parameter-Efficient Transfer Learning with Diff Pruning. (arXiv:2012.07463v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Demi Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1">Alexander M. Rush</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yoon Kim</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07463">
                                    <div class="article-summary-box-inner">
                                        <span>While task-specific finetuning of pretrained networks has led to significant
empirical advances in NLP, the large size of networks makes finetuning
difficult to deploy in multi-task, memory-constrained settings. We propose diff
pruning as a simple approach to enable parameter-efficient transfer learning
within the pretrain-finetune framework. This approach views finetuning as
learning a task-specific diff vector that is applied on top of the pretrained
parameter vector, which remains fixed and is shared across different tasks. The
diff vector is adaptively pruned during training with a differentiable
approximation to the L0-norm penalty to encourage sparsity. Diff pruning
becomes parameter-efficient as the number of tasks increases, as it requires
storing only the nonzero positions and weights of the diff vector for each
task, while the cost of storing the shared pretrained model remains constant.
It further does not require access to all tasks during training, which makes it
attractive in settings where tasks arrive in stream or the set of tasks is
unknown. We find that models finetuned with diff pruning can match the
performance of fully finetuned baselines on the GLUE benchmark while only
modifying 0.5% of the pretrained model&#x27;s parameters per task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Transient Chaos in BERT. (arXiv:2106.03181v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1">Katsuma Inoue</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohara_S/0/1/0/all/0/1">Soh Ohara</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1">Yasuo Kuniyoshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakajima_K/0/1/0/all/0/1">Kohei Nakajima</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03181">
                                    <div class="article-summary-box-inner">
                                        <span>Language is an outcome of our complex and dynamic human-interactions and the
technique of natural language processing (NLP) is hence built on human
linguistic activities. Bidirectional Encoder Representations from Transformers
(BERT) has recently gained its popularity by establishing the state-of-the-art
scores in several NLP benchmarks. A Lite BERT (ALBERT) is literally
characterized as a lightweight version of BERT, in which the number of BERT
parameters is reduced by repeatedly applying the same neural network called
Transformer&#x27;s encoder layer. By pre-training the parameters with a massive
amount of natural language data, ALBERT can convert input sentences into
versatile high-dimensional vectors potentially capable of solving multiple NLP
tasks. In that sense, ALBERT can be regarded as a well-designed
high-dimensional dynamical system whose operator is the Transformer&#x27;s encoder,
and essential structures of human language are thus expected to be encapsulated
in its dynamics. In this study, we investigated the embedded properties of
ALBERT to reveal how NLP tasks are effectively solved by exploiting its
dynamics. We thereby aimed to explore the nature of human language from the
dynamical expressions of the NLP model. Our short-term analysis clarified that
the pre-trained model stably yields trajectories with higher dimensionality,
which would enhance the expressive capacity required for NLP tasks. Also, our
long-term analysis revealed that ALBERT intrinsically shows transient chaos, a
typical nonlinear phenomenon showing chaotic dynamics only in its transient,
and the pre-trained ALBERT model tends to produce the chaotic trajectory for a
significantly longer time period compared to a randomly-initialized one. Our
results imply that local chaoticity would contribute to improving NLP
performance, uncovering a novel aspect in the role of chaotic dynamics in human
language behaviors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling. (arXiv:2106.05223v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1">Chuizheng Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1">Sirisha Rambhatla</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05223">
                                    <div class="article-summary-box-inner">
                                        <span>Vast amount of data generated from networks of sensors, wearables, and the
Internet of Things (IoT) devices underscores the need for advanced modeling
techniques that leverage the spatio-temporal structure of decentralized data
due to the need for edge computation and licensing (data access) issues. While
federated learning (FL) has emerged as a framework for model training without
requiring direct data sharing and exchange, effectively modeling the complex
spatio-temporal dependencies to improve forecasting capabilities still remains
an open problem. On the other hand, state-of-the-art spatio-temporal
forecasting models assume unfettered access to the data, neglecting constraints
on data sharing. To bridge this gap, we propose a federated spatio-temporal
model -- Cross-Node Federated Graph Neural Network (CNFGNN) -- which explicitly
encodes the underlying graph structure using graph neural network (GNN)-based
architecture under the constraint of cross-node federated learning, which
requires that data in a network of nodes is generated locally on each node and
remains decentralized. CNFGNN operates by disentangling the temporal dynamics
modeling on devices and spatial dynamics on the server, utilizing alternating
optimization to reduce the communication cost, facilitating computations on the
edge devices. Experiments on the traffic flow forecasting task show that CNFGNN
achieves the best forecasting performance in both transductive and inductive
learning settings with no extra computation cost on edge devices, while
incurring modest communication cost.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-Paced Context Evaluation for Contextual Reinforcement Learning. (arXiv:2106.05110v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Eimer_T/0/1/0/all/0/1">Theresa Eimer</a>, <a href="http://arxiv.org/find/cs/1/au:+Biedenkapp_A/0/1/0/all/0/1">Andr&#xe9; Biedenkapp</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1">Marius Lindauer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05110">
                                    <div class="article-summary-box-inner">
                                        <span>Reinforcement learning (RL) has made a lot of advances for solving a single
problem in a given environment; but learning policies that generalize to unseen
variations of a problem remains challenging. To improve sample efficiency for
learning on such instances of a problem domain, we present Self-Paced Context
Evaluation (SPaCE). Based on self-paced learning, \spc automatically generates
\task curricula online with little computational overhead. To this end, SPaCE
leverages information contained in state values during training to accelerate
and improve training performance as well as generalization capabilities to new
instances from the same problem domain. Nevertheless, SPaCE is independent of
the problem domain at hand and can be applied on top of any RL agent with
state-value function approximation. We demonstrate SPaCE&#x27;s ability to speed up
learning of different value-based RL agents on two environments, showing better
generalization capabilities and up to 10x faster learning compared to naive
approaches such as round robin or SPDRL, as the closest state-of-the-art
approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">EMA2S: An End-to-End Multimodal Articulatory-to-Speech System. (arXiv:2102.03786v2 [eess.AS] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1">Yu-Wen Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Hung_K/0/1/0/all/0/1">Kuo-Hsuan Hung</a>, <a href="http://arxiv.org/find/eess/1/au:+Chuang_S/0/1/0/all/0/1">Shang-Yi Chuang</a>, <a href="http://arxiv.org/find/eess/1/au:+Sherman_J/0/1/0/all/0/1">Jonathan Sherman</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1">Wen-Chin Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1">Xugang Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1">Yu Tsao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03786">
                                    <div class="article-summary-box-inner">
                                        <span>Synthesized speech from articulatory movements can have real-world use for
patients with vocal cord disorders, situations requiring silent speech, or in
high-noise environments. In this work, we present EMA2S, an end-to-end
multimodal articulatory-to-speech system that directly converts articulatory
movements to speech signals. We use a neural-network-based vocoder combined
with multimodal joint-training, incorporating spectrogram, mel-spectrogram, and
deep features. The experimental results confirm that the multimodal approach of
EMA2S outperforms the baseline system in terms of both objective evaluation and
subjective evaluation metrics. Moreover, results demonstrate that joint
mel-spectrogram and deep feature loss training can effectively improve system
performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks. (arXiv:2012.11654v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Nguyen_Q/0/1/0/all/0/1">Quynh Nguyen</a>, <a href="http://arxiv.org/find/stat/1/au:+Mondelli_M/0/1/0/all/0/1">Marco Mondelli</a>, <a href="http://arxiv.org/find/stat/1/au:+Montufar_G/0/1/0/all/0/1">Guido Montufar</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11654">
                                    <div class="article-summary-box-inner">
                                        <span>A recent line of work has analyzed the theoretical properties of deep neural
networks via the Neural Tangent Kernel (NTK). In particular, the smallest
eigenvalue of the NTK has been related to the memorization capacity, the global
convergence of gradient descent algorithms and the generalization of deep nets.
However, existing results either provide bounds in the two-layer setting or
assume that the spectrum of the NTK matrices is bounded away from 0 for
multi-layer networks. In this paper, we provide tight bounds on the smallest
eigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of
infinite widths and for finite widths. In the finite-width setting, the network
architectures we consider are fairly general: we require the existence of a
wide layer with roughly order of $N$ neurons, $N$ being the number of data
samples; and the scaling of the remaining layer widths is arbitrary (up to
logarithmic factors). To obtain our results, we analyze various quantities of
independent interest: we give lower bounds on the smallest singular value of
hidden feature matrices, and upper bounds on the Lipschitz constant of
input-output feature maps.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A general approach for Explanations in terms of Middle Level Features. (arXiv:2106.05037v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Apicella_A/0/1/0/all/0/1">Andrea Apicella</a>, <a href="http://arxiv.org/find/cs/1/au:+Isgro_F/0/1/0/all/0/1">Francesco Isgr&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Prevete_R/0/1/0/all/0/1">Roberto Prevete</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05037">
                                    <div class="article-summary-box-inner">
                                        <span>Nowadays, it is growing interest to make Machine Learning (ML) systems more
understandable and trusting to general users. Thus, generating explanations for
ML system behaviours that are understandable to human beings is a central
scientific and technological issue addressed by the rapidly growing research
area of eXplainable Artificial Intelligence (XAI). Recently, it is becoming
more and more evident that new directions to create better explanations should
take into account what a good explanation is to a human user, and consequently,
develop XAI solutions able to provide user-centred explanations. This paper
suggests taking advantage of developing an XAI general approach that allows
producing explanations for an ML system behaviour in terms of different and
user-selected input features, i.e., explanations composed of input properties
that the human user can select according to his background knowledge and goals.
To this end, we propose an XAI general approach which is able: 1) to construct
explanations in terms of input features that represent more salient and
understandable input properties for a user, which we call here Middle-Level
input Features (MLFs), 2) to be applied to different types of MLFs. We
experimentally tested our approach on two different datasets and using three
different types of MLFs. The results seem encouraging.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Avoiding Traps in Nonconvex Problems. (arXiv:2106.05206v1 [math.OC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Deyo_S/0/1/0/all/0/1">Sean Deyo</a>, <a href="http://arxiv.org/find/math/1/au:+Elser_V/0/1/0/all/0/1">Veit Elser</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05206">
                                    <div class="article-summary-box-inner">
                                        <span>Iterative projection methods may become trapped at non-solutions when the
constraint sets are nonconvex. Two kinds of parameters are available to help
avoid this behavior and this study gives examples of both. The first kind of
parameter, called a hyperparameter, includes any kind of parameter that appears
in the definition of the iteration rule itself. The second kind comprises
metric parameters in the definition of the constraint sets, a feature that
arises when the problem to be solved has two or more kinds of variables.
Through examples we show the importance of properly tuning both kinds of
parameters and offer heuristic interpretations of the observed behavior.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self Normalizing Flows. (arXiv:2011.07248v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Keller_T/0/1/0/all/0/1">T. Anderson Keller</a>, <a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1">Jorn W.T. Peters</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaini_P/0/1/0/all/0/1">Priyank Jaini</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1">Emiel Hoogeboom</a>, <a href="http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1">Patrick Forr&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1">Max Welling</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07248">
                                    <div class="article-summary-box-inner">
                                        <span>Efficient gradient computation of the Jacobian determinant term is a core
problem in many machine learning settings, and especially so in the normalizing
flow framework. Most proposed flow models therefore either restrict to a
function class with easy evaluation of the Jacobian determinant, or an
efficient estimator thereof. However, these restrictions limit the performance
of such density models, frequently requiring significant depth to reach desired
performance levels. In this work, we propose Self Normalizing Flows, a flexible
framework for training normalizing flows by replacing expensive terms in the
gradient by learned approximate inverses at each layer. This reduces the
computational complexity of each layer&#x27;s exact update from $\mathcal{O}(D^3)$
to $\mathcal{O}(D^2)$, allowing for the training of flow architectures which
were otherwise computationally infeasible, while also providing efficient
sampling. We show experimentally that such models are remarkably stable and
optimize to similar data likelihood values as their exact gradient
counterparts, while training more quickly and surpassing the performance of
functionally constrained counterparts.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">An Efficient Point of Gaze Estimator for Low-Resolution Imaging Systems Using Extracted Ocular Features Based Neural Architecture. (arXiv:2106.05106v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sahay_A/0/1/0/all/0/1">Atul Sahay</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_I/0/1/0/all/0/1">Imon Mukherjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Arya_K/0/1/0/all/0/1">Kavi Arya</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05106">
                                    <div class="article-summary-box-inner">
                                        <span>A user&#x27;s eyes provide means for Human Computer Interaction (HCI) research as
an important modal. The time to time scientific explorations of the eye has
already seen an upsurge of the benefits in HCI applications from gaze
estimation to the measure of attentiveness of a user looking at a screen for a
given time period. The eye tracking system as an assisting, interactive tool
can be incorporated by physically disabled individuals, fitted best for those
who have eyes as only a limited set of communication. The threefold objective
of this paper is - 1. To introduce a neural network based architecture to
predict users&#x27; gaze at 9 positions displayed in the 11.31{\deg} visual range on
the screen, through a low resolution based system such as a webcam in real time
by learning various aspects of eyes as an ocular feature set. 2.A collection of
coarsely supervised feature set obtained in real time which is also validated
through the user case study presented in the paper for 21 individuals ( 17 men
and 4 women ) from whom a 35k set of instances was derived with an accuracy
score of 82.36% and f1_score of 82.2% and 3.A detailed study over applicability
and underlying challenges of such systems. The experimental results verify the
feasibility and validity of the proposed eye gaze tracking model.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Single-Server Private Linear Transformation: The Joint Privacy Case. (arXiv:2106.05220v1 [cs.IT])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Heidarzadeh_A/0/1/0/all/0/1">Anoosheh Heidarzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Esmati_N/0/1/0/all/0/1">Nahid Esmati</a>, <a href="http://arxiv.org/find/cs/1/au:+Sprintson_A/0/1/0/all/0/1">Alex Sprintson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05220">
                                    <div class="article-summary-box-inner">
                                        <span>This paper introduces the problem of Private Linear Transformation (PLT)
which generalizes the problems of private information retrieval and private
linear computation. The PLT problem includes one or more remote server(s)
storing (identical copies of) $K$ messages and a user who wants to compute $L$
independent linear combinations of a $D$-subset of messages. The objective of
the user is to perform the computation by downloading minimum possible amount
of information from the server(s), while protecting the identities of the $D$
messages required for the computation. In this work, we focus on the
single-server setting of the PLT problem when the identities of the $D$
messages required for the computation must be protected jointly. We consider
two different models, depending on whether the coefficient matrix of the
required $L$ linear combinations generates a Maximum Distance Separable (MDS)
code. We prove that the capacity for both models is given by $L/(K-D+L)$, where
the capacity is defined as the supremum of all achievable download rates. Our
converse proofs are based on linear-algebraic and information-theoretic
arguments that establish connections between PLT schemes and linear codes. We
also present an achievability scheme for each of the models being considered.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">More than meets the eye: Self-supervised depth reconstruction from brain activity. (arXiv:2106.05113v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gaziv_G/0/1/0/all/0/1">Guy Gaziv</a>, <a href="http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1">Michal Irani</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05113">
                                    <div class="article-summary-box-inner">
                                        <span>In the past few years, significant advancements were made in reconstruction
of observed natural images from fMRI brain recordings using deep-learning
tools. Here, for the first time, we show that dense 3D depth maps of observed
2D natural images can also be recovered directly from fMRI brain recordings. We
use an off-the-shelf method to estimate the unknown depth maps of natural
images. This is applied to both: (i) the small number of images presented to
subjects in an fMRI scanner (images for which we have fMRI recordings -
referred to as &quot;paired&quot; data), and (ii) a very large number of natural images
with no fMRI recordings (&quot;unpaired data&quot;). The estimated depth maps are then
used as an auxiliary reconstruction criterion to train for depth reconstruction
directly from fMRI. We propose two main approaches: Depth-only recovery and
joint image-depth RGBD recovery. Because the number of available &quot;paired&quot;
training data (images with fMRI) is small, we enrich the training data via
self-supervised cycle-consistent training on many &quot;unpaired&quot; data (natural
images &amp; depth maps without fMRI). This is achieved using our newly defined and
trained Depth-based Perceptual Similarity metric as a reconstruction criterion.
We show that predicting the depth map directly from fMRI outperforms its
indirect sequential recovery from the reconstructed images. We further show
that activations from early cortical visual areas dominate our depth
reconstruction results, and propose means to characterize fMRI voxels by their
degree of depth-information tuning. This work adds an important layer of
decoded information, extending the current envelope of visual brain decoding
capabilities.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Significance tests of feature relevance for a blackbox learner. (arXiv:2103.04985v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Dai_B/0/1/0/all/0/1">Ben Dai</a>, <a href="http://arxiv.org/find/stat/1/au:+Shen_X/0/1/0/all/0/1">Xiaotong Shen</a>, <a href="http://arxiv.org/find/stat/1/au:+Pan_W/0/1/0/all/0/1">Wei Pan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04985">
                                    <div class="article-summary-box-inner">
                                        <span>An exciting recent development is the uptake of deep learning in many
scientific fields, where the objective is seeking novel scientific insights and
discoveries. To interpret a learning outcome, researchers perform hypothesis
testing for explainable features to advance scientific domain knowledge. In
such a situation, testing for a blackbox learner poses a severe challenge
because of intractable models, unknown limiting distributions of parameter
estimates, and high computational constraints. In this article, we derive two
consistent tests for the feature relevance of a blackbox learner. The first one
evaluates a loss difference with perturbation on an inference sample, which is
independent of an estimation sample used for parameter estimation in model
fitting. The second further splits the inference sample into two but does not
require data perturbation. Also, we develop their combined versions by
aggregating the order statistics of the $p$-values based on repeated sample
splitting. To estimate the splitting ratio and the perturbation size, we
develop adaptive splitting schemes for suitably controlling the Type \rom{1}
error subject to computational constraints. By deflating the
\textit{bias-sd-ratio}, we establish asymptotic null distributions of the test
statistics and their consistency in terms of statistical power. Our theoretical
power analysis and simulations indicate that the one-split test is more
powerful than the two-split test, though the latter is easier to apply for
large datasets. Moreover, the combined tests are more stable while compensating
for a power loss by repeated sample splitting. Numerically, we demonstrate the
utility of the proposed tests on two benchmark examples. Accompanying this
paper is our Python library {\tt dnn-inference}
https://dnn-inference.readthedocs.io/en/latest/ that implements the proposed
tests.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Recovering AES Keys with a Deep Cold Boot Attack. (arXiv:2106.04876v1 [cs.CR])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zimerman_I/0/1/0/all/0/1">Itamar Zimerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1">Eliya Nachmani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1">Lior Wolf</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04876">
                                    <div class="article-summary-box-inner">
                                        <span>Cold boot attacks inspect the corrupted random access memory soon after the
power has been shut down. While most of the bits have been corrupted, many
bits, at random locations, have not. Since the keys in many encryption schemes
are being expanded in memory into longer keys with fixed redundancies, the keys
can often be restored. In this work, we combine a novel cryptographic variant
of a deep error correcting code technique with a modified SAT solver scheme to
apply the attack on AES keys. Even though AES consists of Rijndael S-box
elements, that are specifically designed to be resistant to linear and
differential cryptanalysis, our method provides a novel formalization of the
AES key scheduling as a computational graph, which is implemented by a neural
message passing network. Our results show that our methods outperform the state
of the art attack methods by a very large margin.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bias-Robust Bayesian Optimization via Dueling Bandits. (arXiv:2105.11802v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Kirschner_J/0/1/0/all/0/1">Johannes Kirschner</a>, <a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11802">
                                    <div class="article-summary-box-inner">
                                        <span>We consider Bayesian optimization in settings where observations can be
adversarially biased, for example by an uncontrolled hidden confounder. Our
first contribution is a reduction of the confounded setting to the dueling
bandit model. Then we propose a novel approach for dueling bandits based on
information-directed sampling (IDS). Thereby, we obtain the first efficient
kernelized algorithm for dueling bandits that comes with cumulative regret
guarantees. Our analysis further generalizes a previously proposed
semi-parametric linear bandit model to non-linear reward functions, and
uncovers interesting links to doubly-robust estimation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-Supervised Graph Learning with Hyperbolic Embedding for Temporal Health Event Prediction. (arXiv:2106.04751v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chang Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1">Chandan K. Reddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1">Yue Ning</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04751">
                                    <div class="article-summary-box-inner">
                                        <span>Electronic Health Records (EHR) have been heavily used in modern healthcare
systems for recording patients&#x27; admission information to hospitals. Many
data-driven approaches employ temporal features in EHR for predicting specific
diseases, readmission times, or diagnoses of patients. However, most existing
predictive models cannot fully utilize EHR data, due to an inherent lack of
labels in supervised training for some temporal events. Moreover, it is hard
for existing works to simultaneously provide generic and personalized
interpretability. To address these challenges, we first propose a hyperbolic
embedding method with information flow to pre-train medical code
representations in a hierarchical structure. We incorporate these pre-trained
representations into a graph neural network to detect disease complications,
and design a multi-level attention method to compute the contributions of
particular diseases and admissions, thus enhancing personalized
interpretability. We present a new hierarchy-enhanced historical prediction
proxy task in our self-supervised learning framework to fully utilize EHR data
and exploit medical domain knowledge. We conduct a comprehensive set of
experiments and case studies on widely used publicly available EHR datasets to
verify the effectiveness of our model. The results demonstrate our model&#x27;s
strengths in both predictive tasks and interpretable abilities.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Which transformer architecture fits my data? A vocabulary bottleneck in self-attention. (arXiv:2105.03928v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1">Noam Wies</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1">Yoav Levine</a>, <a href="http://arxiv.org/find/cs/1/au:+Jannai_D/0/1/0/all/0/1">Daniel Jannai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1">Amnon Shashua</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03928">
                                    <div class="article-summary-box-inner">
                                        <span>After their successful debut in natural language processing, Transformer
architectures are now becoming the de-facto standard in many domains. An
obstacle for their deployment over new modalities is the architectural
configuration: the optimal depth-to-width ratio has been shown to dramatically
vary across data types (e.g., $10$x larger over images than over language). We
theoretically predict the existence of an embedding rank bottleneck that limits
the contribution of self-attention width to the Transformer expressivity. We
thus directly tie the input vocabulary size and rank to the optimal
depth-to-width ratio, since a small vocabulary size or rank dictates an added
advantage of depth over width. We empirically demonstrate the existence of this
bottleneck and its implications on the depth-to-width interplay of Transformer
architectures, linking the architecture variability across domains to the often
glossed-over usage of different vocabulary sizes or embedding ranks in
different domains. As an additional benefit, our rank bottlenecking framework
allows us to identify size redundancies of $25\%-50\%$ in leading NLP models
such as ALBERT and T5.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Music Generation using Three-layered LSTM. (arXiv:2105.09046v3 [cs.SD] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ingale_V/0/1/0/all/0/1">Vaishali Ingale</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1">Anush Mohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Adlakha_D/0/1/0/all/0/1">Divit Adlakha</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1">Krishan Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1">Mohit Gupta</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09046">
                                    <div class="article-summary-box-inner">
                                        <span>This paper explores the idea of utilising Long Short-Term Memory neural
networks (LSTMNN) for the generation of musical sequences in ABC notation. The
proposed approach takes ABC notations from the Nottingham dataset and encodes
it to be fed as input for the neural networks. The primary objective is to
input the neural networks with an arbitrary note, let the network process and
augment a sequence based on the note until a good piece of music is produced.
Multiple calibrations have been done to amend the parameters of the network for
optimal generation. The output is assessed on the basis of rhythm, harmony, and
grammar accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Nonlinear Invariant Risk Minimization: A Causal Approach. (arXiv:2102.12353v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chaochao Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuhuai Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1">Jo&#x15b;e Miguel Hern&#xe1;ndez-Lobato</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12353">
                                    <div class="article-summary-box-inner">
                                        <span>Due to spurious correlations, machine learning systems often fail to
generalize to environments whose distributions differ from the ones used at
training time. Prior work addressing this, either explicitly or implicitly,
attempted to find a data representation that has an invariant relationship with
the target. This is done by leveraging a diverse set of training environments
to reduce the effect of spurious features and build an invariant predictor.
However, these methods have generalization guarantees only when both data
representation and classifiers come from a linear model class. We propose
invariant Causal Representation Learning (iCaRL), an approach that enables
out-of-distribution (OOD) generalization in the nonlinear setting (i.e.,
nonlinear representations and nonlinear classifiers). It builds upon a
practical and general assumption: the prior over the data representation (i.e.,
a set of latent variables encoding the data) given the target and the
environment belongs to general exponential family distributions. Based on this,
we show that it is possible to identify the data representation up to simple
transformations. We also prove that all direct causes of the target can be
fully discovered, which further enables us to obtain generalization guarantees
in the nonlinear setting. Extensive experiments on both synthetic and
real-world datasets show that our approach outperforms a variety of baseline
methods. Finally, in the discussion, we further explore the aforementioned
assumption and propose a more general hypothesis, called the Agnostic
Hypothesis: there exist a set of hidden causal factors affecting both inputs
and outcomes. The Agnostic Hypothesis can provide a unifying view of machine
learning. More importantly, it can inspire a new direction to explore a general
theory for identifying hidden causal factors, which is key to enabling the OOD
generalization guarantees.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Periodic-GP: Learning Periodic World with Gaussian Process Bandits. (arXiv:2105.14422v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Hengrui Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cen_Z/0/1/0/all/0/1">Zhihao Cen</a>, <a href="http://arxiv.org/find/cs/1/au:+Leng_L/0/1/0/all/0/1">Ling Leng</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1">Rui Song</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14422">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the sequential decision optimization on the periodic environment,
that occurs in a wide variety of real-world applications when the data involves
seasonality, such as the daily demand of drivers in ride-sharing and dynamic
traffic patterns in transportation. In this work, we focus on learning the
stochastic periodic world by leveraging this seasonal law. To deal with the
general action space, we use the bandit based on Gaussian process (GP) as the
base model due to its flexibility and generality, and propose the Periodic-GP
method with a temporal periodic kernel based on the upper confidence bound.
Theoretically, we provide a new regret bound of the proposed method, by
explicitly characterizing the periodic kernel in the periodic stationary model.
Empirically, the proposed algorithm significantly outperforms the existing
methods in both synthetic data experiments and a real data application on
Madrid traffic pollution.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Memory-based Optimization Methods for Model-Agnostic Meta-Learning. (arXiv:2106.04911v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bokun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Zhuoning Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1">Yiming Ying</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tianbao Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04911">
                                    <div class="article-summary-box-inner">
                                        <span>Recently, model-agnostic meta-learning (MAML) has garnered tremendous
attention. However, stochastic optimization of MAML is still immature. Existing
algorithms for MAML are based on the &#x60;&#x60;episode&quot; idea by sampling a number of
tasks and a number of data points for each sampled task at each iteration for
updating the meta-model. However, they either do not necessarily guarantee
convergence with a constant mini-batch size or require processing a larger
number of tasks at every iteration, which is not viable for continual learning
or cross-device federated learning where only a small number of tasks are
available per-iteration or per-round. This paper addresses these issues by (i)
proposing efficient memory-based stochastic algorithms for MAML with a
diminishing convergence error, which only requires sampling a constant number
of tasks and a constant number of examples per-task per-iteration; (ii)
proposing communication-efficient distributed memory-based MAML algorithms for
personalized federated learning in both the cross-device (w/ client sampling)
and the cross-silo (w/o client sampling) settings. The key novelty of the
proposed algorithms is to maintain an individual personalized model (aka
memory) for each task besides the meta-model and only update them for the
sampled tasks by a momentum method that incorporates historical updates at each
iteration. The theoretical results significantly improve the optimization
theory for MAML and the empirical results also corroborate the theory.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images. (arXiv:2106.00116v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1">Mehdi Cherti</a>, <a href="http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1">Jenia Jitsev</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00116">
                                    <div class="article-summary-box-inner">
                                        <span>Transfer learning aims to exploit pre-trained models for more efficient
follow-up training on wide range of downstream tasks and datasets, enabling
successful training also on small data. Recent line of work posits strong
benefits for model generalization and transfer when model size, data size, and
compute budget are increased for the pre-training. It remains however still
largely unclear whether the observed transfer improvement due to increase in
scale also holds when source and target data distributions are far apart from
each other. In this work we conduct large-scale pre-training on large source
datasets of either natural (ImageNet-21k/1k) or medical chest X-Ray images and
compare full and few-shot transfer using different target datasets from both
natural and medical imaging domains. Our observations provide evidence that
while pre-training and transfer on closely related datasets do show clear
benefit of increasing model and data size during pre-training, such benefits
are not clearly visible when source and target datasets are further apart.
These observations hold across both full and few-shot transfer and indicate
that scaling laws pointing to improvement of generalization and transfer with
increasing model and data size are incomplete and should be revised by taking
into account the type and proximity of the source and target data, to correctly
predict the effect of model and data scale during pre-training on transfer.
Remarkably, in full shot transfer to a large X-Ray chest imaging target
(PadChest), the largest model pre-trained on ImageNet-21k slightly outperforms
best models pre-trained on large X-Ray chest imaging data. This indicates
possibility to obtain high quality models for domain-specific transfer even
without access to large domain-specific data, by pre-training instead on
comparably very large, generic source data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">TempoRL: Learning When to Act. (arXiv:2106.05262v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Biedenkapp_A/0/1/0/all/0/1">Andr&#xe9; Biedenkapp</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajan_R/0/1/0/all/0/1">Raghu Rajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1">Frank Hutter</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1">Marius Lindauer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05262">
                                    <div class="article-summary-box-inner">
                                        <span>Reinforcement learning is a powerful approach to learn behaviour through
interactions with an environment. However, behaviours are usually learned in a
purely reactive fashion, where an appropriate action is selected based on an
observation. In this form, it is challenging to learn when it is necessary to
execute new decisions. This makes learning inefficient, especially in
environments that need various degrees of fine and coarse control. To address
this, we propose a proactive setting in which the agent not only selects an
action in a state but also for how long to commit to that action. Our TempoRL
approach introduces skip connections between states and learns a skip-policy
for repeating the same action along these skips. We demonstrate the
effectiveness of TempoRL on a variety of traditional and deep RL environments,
showing that our approach is capable of learning successful policies up to an
order of magnitude faster than vanilla Q-learning.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Linear Transformers Are Secretly Fast Weight Programmers. (arXiv:2102.11174v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Schlag_I/0/1/0/all/0/1">Imanol Schlag</a>, <a href="http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1">Kazuki Irie</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1">J&#xfc;rgen Schmidhuber</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11174">
                                    <div class="article-summary-box-inner">
                                        <span>We show the formal equivalence of linearised self-attention mechanisms and
fast weight controllers from the early &#x27;90s, where a &#x60;&#x60;slow&quot; neural net learns
by gradient descent to program the &#x60;&#x60;fast weights&quot; of another net through
sequences of elementary programming instructions which are additive outer
products of self-invented activation patterns (today called keys and values).
Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a
finite memory and dynamically interact with it. We infer a memory capacity
limitation of recent linearised softmax attention variants, and replace the
purely additive outer products by a delta rule-like programming instruction,
such that the FWP can more easily learn to correct the current mapping from
keys to values. The FWP also learns to compute dynamically changing learning
rates. We also propose a new kernel function to linearise attention which
balances simplicity and effectiveness. We conduct experiments on synthetic
retrieval problems as well as standard machine translation and language
modelling tasks which demonstrate the benefits of our methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">GP-ConvCNP: Better Generalization for Convolutional Conditional Neural Processes on Time Series Data. (arXiv:2106.04967v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1">Jens Petersen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kohler_G/0/1/0/all/0/1">Gregor K&#xf6;hler</a>, <a href="http://arxiv.org/find/cs/1/au:+Zimmerer_D/0/1/0/all/0/1">David Zimmerer</a>, <a href="http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1">Fabian Isensee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jager_P/0/1/0/all/0/1">Paul F. J&#xe4;ger</a>, <a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_K/0/1/0/all/0/1">Klaus H. Maier-Hein</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04967">
                                    <div class="article-summary-box-inner">
                                        <span>Neural Processes (NPs) are a family of conditional generative models that are
able to model a distribution over functions, in a way that allows them to
perform predictions at test time conditioned on a number of context points. A
recent addition to this family, Convolutional Conditional Neural Processes
(ConvCNP), have shown remarkable improvement in performance over prior art, but
we find that they sometimes struggle to generalize when applied to time series
data. In particular, they are not robust to distribution shifts and fail to
extrapolate observed patterns into the future. By incorporating a Gaussian
Process into the model, we are able to remedy this and at the same time improve
performance within distribution. As an added benefit, the Gaussian Process
reintroduces the possibility to sample from the model, a key feature of other
members in the NP family.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Contrastive Learning for Many-to-many Multilingual Neural Machine Translation. (arXiv:2105.09501v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xiao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mingxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Liwei Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09501">
                                    <div class="article-summary-box-inner">
                                        <span>Existing multilingual machine translation approaches mainly focus on
English-centric directions, while the non-English directions still lag behind.
In this work, we aim to build a many-to-many translation system with an
emphasis on the quality of non-English language directions. Our intuition is
based on the hypothesis that a universal cross-language representation leads to
better multilingual translation performance. To this end, we propose mRASP2, a
training method to obtain a single unified multilingual translation model.
mRASP2 is empowered by two techniques: a) a contrastive learning scheme to
close the gap among representations of different languages, and b) data
augmentation on both multiple parallel and monolingual data to further align
token representations. For English-centric directions, mRASP2 outperforms
existing best unified model and achieves competitive or even better performance
than the pre-trained and fine-tuned model mBART on tens of WMT&#x27;s translation
directions. For non-English directions, mRASP2 achieves an improvement of
average 10+ BLEU compared with the multilingual Transformer baseline. Code,
data and trained models are available at https://github.com/PANXiao1994/mRASP2.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust Binary Neural Network Operation from 233 K to 398 K via Gate Stack and Bias Optimization of Ferroelectric FinFET Synapses. (arXiv:2103.03111v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1">Sourav De</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1">Hoang-Hiep Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1">Bo-Han Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Baig_M/0/1/0/all/0/1">Md. Aftab Baig</a>, <a href="http://arxiv.org/find/cs/1/au:+Sung_P/0/1/0/all/0/1">Po-Jung Sung</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1">Chung Jun Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yao-Jen Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1">Darsen D. Lu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03111">
                                    <div class="article-summary-box-inner">
                                        <span>A synergistic approach for optimizing devices, circuits, and neural network
architectures was used to abate junction-temperature-change-induced performance
degradation of a Fe-FinFET-based artificial neural network. We demonstrated
that the digital nature of the binarized neural network, with the &quot;0&quot; state
programmed deep in the subthreshold and the &quot;1&quot; state in strong inversion, is
crucial for robust DNN inference. The performance of a purely software-based
binary neural network (BNN), with 96.1% accuracy for Modified National
Institute of Standards and Technology (MNIST) handwritten digit recognition,
was used as a baseline. The Fe-FinFET-based BNN (including device-to-device
variation at 300 K) achieved 95.7% inference accuracy on the MNIST dataset.
Although substantial inference accuracy degradation with temperature change was
observed in a nonbinary neural network, the BNN with optimized Fe-FinFETs as
synaptic devices had excellent resistance to temperature change effects and
maintained a minimum inference accuracy of 95.2% within a temperature range of
-233K to 398K after gate stack and bias optimization. However, reprogramming to
adjust device conductance was necessary for temperatures higher than 398K.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Autobahn: Automorphism-based Graph Neural Nets. (arXiv:2103.01710v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Thiede_E/0/1/0/all/0/1">Erik Henning Thiede</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wenda Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Kondor_R/0/1/0/all/0/1">Risi Kondor</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01710">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce Automorphism-based graph neural networks (Autobahn), a new
family of graph neural networks. In an Autobahn, we decompose the graph into a
collection of subgraphs and apply local convolutions that are equivariant to
each subgraph&#x27;s automorphism group. Specific choices of local neighborhoods and
subgraphs recover existing architectures such as message passing neural
networks. Our formalism also encompasses novel architectures: as an example, we
introduce a graph neural network that decomposes the graph into paths and
cycles. The resulting convolutions reflect the natural way that parts of the
graph can transform, preserving the intuitive meaning of convolution without
sacrificing global permutation equivariance. We validate our approach by
applying Autobahn to molecular graphs, where it achieves state-of-the-art
results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Quantum Annealing for Automated Feature Selection in Stress Detection. (arXiv:2106.05134v1 [quant-ph])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/quant-ph/1/au:+Nath_R/0/1/0/all/0/1">Rajdeep Kumar Nath</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Thapliyal_H/0/1/0/all/0/1">Himanshu Thapliyal</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Humble_T/0/1/0/all/0/1">Travis S. Humble</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05134">
                                    <div class="article-summary-box-inner">
                                        <span>We present a novel methodology for automated feature subset selection from a
pool of physiological signals using Quantum Annealing (QA). As a case study, we
will investigate the effectiveness of QA-based feature selection techniques in
selecting the optimal feature subset for stress detection. Features are
extracted from four signal sources: foot EDA, hand EDA, ECG, and respiration.
The proposed method embeds the feature variables extracted from the
physiological signals in a binary quadratic model. The bias of the feature
variable is calculated using the Pearson correlation coefficient between the
feature variable and the target variable. The weight of the edge connecting the
two feature variables is calculated using the Pearson correlation coefficient
between two feature variables in the binary quadratic model. Subsequently,
D-Wave&#x27;s clique sampler is used to sample cliques from the binary quadratic
model. The underlying solution is then re-sampled to obtain multiple good
solutions and the clique with the lowest energy is returned as the optimal
solution. The proposed method is compared with commonly used feature selection
techniques for stress detection. Results indicate that QA-based feature subset
selection performed equally as that of classical techniques. However, under
data uncertainty conditions such as limited training data, the performance of
quantum annealing for selecting optimum features remained unaffected, whereas a
significant decrease in performance is observed with classical feature
selection techniques. Preliminary results show the promise of quantum annealing
in optimizing the training phase of a machine learning classifier, especially
under data uncertainty conditions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fast and More Powerful Selective Inference for Sparse High-order Interaction Model. (arXiv:2106.04929v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Das_D/0/1/0/all/0/1">Diptesh Das</a>, <a href="http://arxiv.org/find/stat/1/au:+Duy_V/0/1/0/all/0/1">Vo Nguyen Le Duy</a>, <a href="http://arxiv.org/find/stat/1/au:+Hanada_H/0/1/0/all/0/1">Hiroyuki Hanada</a>, <a href="http://arxiv.org/find/stat/1/au:+Tsuda_K/0/1/0/all/0/1">Koji Tsuda</a>, <a href="http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1">Ichiro Takeuchi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04929">
                                    <div class="article-summary-box-inner">
                                        <span>Automated high-stake decision-making such as medical diagnosis requires
models with high interpretability and reliability. As one of the interpretable
and reliable models with good prediction ability, we consider Sparse High-order
Interaction Model (SHIM) in this study. However, finding statistically
significant high-order interactions is challenging due to the intrinsic high
dimensionality of the combinatorial effects. Another problem in data-driven
modeling is the effect of &quot;cherry-picking&quot; a.k.a. selection bias. Our main
contribution is to extend the recently developed parametric programming
approach for selective inference to high-order interaction models. Exhaustive
search over the cherry tree (all possible interactions) can be daunting and
impractical even for a small-sized problem. We introduced an efficient pruning
strategy and demonstrated the computational efficiency and statistical power of
the proposed method using both synthetic and real data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Is it Enough to Optimize CNN Architectures on ImageNet?. (arXiv:2103.09108v2 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tuggener_L/0/1/0/all/0/1">Lukas Tuggener</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1">J&#xfc;rgen Schmidhuber</a>, <a href="http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1">Thilo Stadelmann</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09108">
                                    <div class="article-summary-box-inner">
                                        <span>An implicit but pervasive hypothesis of modern computer vision research is
that convolutional neural network (CNN) architectures that perform better on
ImageNet will also perform better on other vision datasets. We challenge this
hypothesis through an extensive empirical study for which we train 500 sampled
CNN architectures on ImageNet as well as 8 other image classification datasets
from a wide array of application domains. The relationship between architecture
and performance varies wildly, depending on the datasets. For some of them, the
performance correlation with ImageNet is even negative. Clearly, it is not
enough to optimize architectures solely for ImageNet when aiming for progress
that is relevant for all applications. Therefore, we identify two
dataset-specific performance indicators: the cumulative width across layers as
well as the total depth of the network. Lastly, we show that the range of
dataset variability covered by ImageNet can be significantly extended by adding
ImageNet subsets restricted to few classes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FedDR -- Randomized Douglas-Rachford Splitting Algorithms for Nonconvex Federated Composite Optimization. (arXiv:2103.03452v2 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Tran_Dinh_Q/0/1/0/all/0/1">Quoc Tran-Dinh</a>, <a href="http://arxiv.org/find/stat/1/au:+Pham_N/0/1/0/all/0/1">Nhan H. Pham</a>, <a href="http://arxiv.org/find/stat/1/au:+Phan_D/0/1/0/all/0/1">Dzung T. Phan</a>, <a href="http://arxiv.org/find/stat/1/au:+Nguyen_L/0/1/0/all/0/1">Lam M. Nguyen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03452">
                                    <div class="article-summary-box-inner">
                                        <span>We develop two new algorithms, called, FedDR and asyncFedDR, for solving a
fundamental nonconvex composite optimization problem in federated learning. Our
algorithms rely on a novel combination between a nonconvex Douglas-Rachford
splitting method, randomized block-coordinate strategies, and asynchronous
implementation. They can also handle convex regularizers. Unlike recent methods
in the literature, e.g., FedSplit and FedPD, our algorithms update only a
subset of users at each communication round, and possibly in an asynchronous
manner, making them more practical. These new algorithms also achieve
communication efficiency and more importantly can handle statistical and system
heterogeneity, which are the two main challenges in federated learning. Our
convergence analysis shows that the new algorithms match the communication
complexity lower bound up to a constant factor under standard assumptions. Our
numerical experiments illustrate the advantages of our methods compared to
existing ones on several datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Generating Reliable Process Event Streams and Time Series Data based on Neural Networks. (arXiv:2103.05462v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Herbert_T/0/1/0/all/0/1">Tobias Herbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Mangler_J/0/1/0/all/0/1">Juergen Mangler</a>, <a href="http://arxiv.org/find/cs/1/au:+Rinderle_Ma_S/0/1/0/all/0/1">Stefanie Rinderle-Ma</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05462">
                                    <div class="article-summary-box-inner">
                                        <span>Domains such as manufacturing and medicine crave for continuous monitoring
and analysis of their processes, especially in combination with time series as
produced by sensors. Time series data can be exploited to, for example, explain
and predict concept drifts during runtime. Generally, a certain data volume is
required in order to produce meaningful analysis results. However, reliable
data sets are often missing, for example, if event streams and times series
data are collected separately, in case of a new process, or if it is too
expensive to obtain a sufficient data volume. Additional challenges arise with
preparing time series data from multiple event sources, variations in data
collection frequency, and concept drift. This paper proposes the GENLOG
approach to generate reliable event and time series data that follows the
distribution of the underlying input data set. GENLOG employs data resampling
and enables the user to select different parts of the log data to orchestrate
the training of a recurrent neural network for stream generation. The generated
data is sampled back to its original sample rate and is embedded into the
originating log data file. Overall, GENLOG can boost small data sets and
consequently the application of online process mining.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Objective Robustness in Deep Reinforcement Learning. (arXiv:2105.14111v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1">Jack Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1">Lauro Langosco</a>, <a href="http://arxiv.org/find/cs/1/au:+Pfau_J/0/1/0/all/0/1">Jacob Pfau</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_J/0/1/0/all/0/1">James Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharkey_L/0/1/0/all/0/1">Lee Sharkey</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14111">
                                    <div class="article-summary-box-inner">
                                        <span>We study objective robustness failures, a type of out-of-distribution
robustness failure in reinforcement learning (RL). Objective robustness
failures occur when an RL agent retains its capabilities out-of-distribution
yet pursues the wrong objective. This kind of failure presents different risks
than the robustness problems usually considered in the literature, since it
involves agents that leverage their capabilities to pursue the wrong objective
rather than simply failing to do anything useful. We provide the first explicit
empirical demonstrations of objective robustness failures and present a partial
characterization of its causes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SPINN: Sparse, Physics-based, and Interpretable Neural Networks for PDEs. (arXiv:2102.13037v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ramabathiran_A/0/1/0/all/0/1">Amuthan A. Ramabathiran</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramachandran_P/0/1/0/all/0/1">Prabhu Ramachandran</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.13037">
                                    <div class="article-summary-box-inner">
                                        <span>We introduce a class of Sparse, Physics-based, and Interpretable Neural
Networks (SPINN) for solving ordinary and partial differential equations
(PDEs). By reinterpreting a traditional meshless representation of solutions of
PDEs we develop a class of sparse neural network architectures that are
interpretable. The SPINN model we propose here serves as a seamless bridge
between two extreme modeling tools for PDEs, namely dense neural network based
methods like Physics Informed Neural Networks (PINNs) and traditional mesh-free
numerical methods, thereby providing a novel means to develop a new class of
hybrid algorithms that build on the best of both these viewpoints. A unique
feature of the SPINN model that distinguishes it from other neural network
based approximations proposed earlier is that it is (i) interpretable, and (ii)
sparse in the sense that it has much fewer connections than typical dense
neural networks used for PDEs. Further, the SPINN algorithm implicitly encodes
mesh adaptivity and is able to handle discontinuities in the solutions. In
addition, we demonstrate that Fourier series representations can also be
expressed as a special class of SPINN and propose generalized neural network
analogues of Fourier representations. We illustrate the utility of the proposed
method with a variety of examples involving ordinary differential equations,
elliptic, parabolic, hyperbolic and nonlinear partial differential equations,
and an example in fluid dynamics.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fully differentiable model discovery. (arXiv:2106.04886v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Both_G/0/1/0/all/0/1">Gert-Jan Both</a>, <a href="http://arxiv.org/find/stat/1/au:+Kusters_R/0/1/0/all/0/1">Remy Kusters</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04886">
                                    <div class="article-summary-box-inner">
                                        <span>Model discovery aims at autonomously discovering differential equations
underlying a dataset. Approaches based on Physics Informed Neural Networks
(PINNs) have shown great promise, but a fully-differentiable model which
explicitly learns the equation has remained elusive. In this paper we propose
such an approach by combining neural network based surrogates with Sparse
Bayesian Learning (SBL). We start by reinterpreting PINNs as multitask models,
applying multitask learning using uncertainty, and show that this leads to a
natural framework for including Bayesian regression techniques. We then
construct a robust model discovery algorithm by using SBL, which we showcase on
various datasets. Concurrently, the multitask approach allows the use of
probabilistic approximators, and we show a proof of concept using normalizing
flows to directly learn a density model from single particle data. Our work
expands PINNs to various types of neural network architectures, and connects
neural network-based surrogates to the rich field of Bayesian parameter
inference.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">WGAN with an Infinitely Wide Generator Has No Spurious Stationary Points. (arXiv:2102.07541v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+No_A/0/1/0/all/0/1">Albert No</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_T/0/1/0/all/0/1">TaeHo Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1">Sehyun Kwon</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryu_E/0/1/0/all/0/1">Ernest K. Ryu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07541">
                                    <div class="article-summary-box-inner">
                                        <span>Generative adversarial networks (GAN) are a widely used class of deep
generative models, but their minimax training dynamics are not understood very
well. In this work, we show that GANs with a 2-layer infinite-width generator
and a 2-layer finite-width discriminator trained with stochastic gradient
ascent-descent have no spurious stationary points. We then show that when the
width of the generator is finite but wide, there are no spurious stationary
points within a ball whose radius becomes arbitrarily large (to cover the
entire parameter space) as the width goes to infinity.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Does the Adam Optimizer Exacerbate Catastrophic Forgetting?. (arXiv:2102.07686v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ashley_D/0/1/0/all/0/1">Dylan R. Ashley</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghiassian_S/0/1/0/all/0/1">Sina Ghiassian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1">Richard S. Sutton</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07686">
                                    <div class="article-summary-box-inner">
                                        <span>Catastrophic forgetting remains a severe hindrance to the broad application
of artificial neural networks (ANNs), however, it continues to be a poorly
understood phenomenon. Despite the extensive amount of work on catastrophic
forgetting, we argue that it is still unclear how exactly the phenomenon should
be quantified, and, moreover, to what degree all of the choices we make when
designing learning systems affect the amount of catastrophic forgetting. We use
various testbeds from the reinforcement learning and supervised learning
literature to (1) provide evidence that the choice of which modern
gradient-based optimization algorithm is used to train an ANN has a significant
impact on the amount of catastrophic forgetting and show that-surprisingly-in
many instances classical algorithms such as vanilla SGD experience less
catastrophic forgetting than the more modern algorithms such as Adam. We
empirically compare four different existing metrics for quantifying
catastrophic forgetting and (2) show that the degree to which the learning
systems experience catastrophic forgetting is sufficiently sensitive to the
metric used that a change from one principled metric to another is enough to
change the conclusions of a study dramatically. Our results suggest that a much
more rigorous experimental methodology is required when looking at catastrophic
forgetting. Based on our results, we recommend inter-task forgetting in
supervised learning must be measured with both retention and relearning metrics
concurrently, and intra-task forgetting in reinforcement learning must-at the
very least-be measured with pairwise interference.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sparse Algorithms for Markovian Gaussian Processes. (arXiv:2103.10710v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Wilkinson_W/0/1/0/all/0/1">William J. Wilkinson</a>, <a href="http://arxiv.org/find/stat/1/au:+Solin_A/0/1/0/all/0/1">Arno Solin</a>, <a href="http://arxiv.org/find/stat/1/au:+Adam_V/0/1/0/all/0/1">Vincent Adam</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10710">
                                    <div class="article-summary-box-inner">
                                        <span>Approximate Bayesian inference methods that scale to very large datasets are
crucial in leveraging probabilistic models for real-world time series. Sparse
Markovian Gaussian processes combine the use of inducing variables with
efficient Kalman filter-like recursions, resulting in algorithms whose
computational and memory requirements scale linearly in the number of inducing
points, whilst also enabling parallel parameter updates and stochastic
optimisation. Under this paradigm, we derive a general site-based approach to
approximate inference, whereby we approximate the non-Gaussian likelihood with
local Gaussian terms, called sites. Our approach results in a suite of novel
sparse extensions to algorithms from both the machine learning and signal
processing literature, including variational inference, expectation
propagation, and the classical nonlinear Kalman smoothers. The derived methods
are suited to large time series, and we also demonstrate their applicability to
spatio-temporal data, where the model has separate inducing points in both time
and space.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Domain Invariant Representations by Joint Wasserstein Distance Minimization. (arXiv:2106.04923v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Andeol_L/0/1/0/all/0/1">L&#xe9;o And&#xe9;ol</a>, <a href="http://arxiv.org/find/stat/1/au:+Kawakami_Y/0/1/0/all/0/1">Yusei Kawakami</a>, <a href="http://arxiv.org/find/stat/1/au:+Wada_Y/0/1/0/all/0/1">Yuichiro Wada</a>, <a href="http://arxiv.org/find/stat/1/au:+Kanamori_T/0/1/0/all/0/1">Takafumi Kanamori</a>, <a href="http://arxiv.org/find/stat/1/au:+Muller_K/0/1/0/all/0/1">Klaus-Robert M&#xfc;ller</a>, <a href="http://arxiv.org/find/stat/1/au:+Montavon_G/0/1/0/all/0/1">Gr&#xe9;goire Montavon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04923">
                                    <div class="article-summary-box-inner">
                                        <span>Domain shifts in the training data are common in practical applications of
machine learning, they occur for instance when the data is coming from
different sources. Ideally, a ML model should work well independently of these
shifts, for example, by learning a domain-invariant representation. Moreover,
privacy concerns regarding the source also require a domain-invariant
representation. In this work, we provide theoretical results that link domain
invariant representations -- measured by the Wasserstein distance on the joint
distributions -- to a practical semi-supervised learning objective based on a
cross-entropy classifier and a novel domain critic. Quantitative experiments
demonstrate that the proposed approach is indeed able to practically learn such
an invariant representation (between two domains), and the latter also supports
models with higher predictive accuracy on both domains, comparing favorably to
existing techniques.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Orthogonal Least Squares Based Fast Feature Selection for Linear Classification. (arXiv:2101.08539v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Sikai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lang_Z/0/1/0/all/0/1">Zi-Qiang Lang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08539">
                                    <div class="article-summary-box-inner">
                                        <span>An Orthogonal Least Squares (OLS) based feature selection method is proposed
for both binomial and multinomial classification. The novel Squared Orthogonal
Correlation Coefficient (SOCC) is defined based on Error Reduction Ratio (ERR)
in OLS and used as the feature ranking criterion. The equivalence between the
canonical correlation coefficient, Fisher&#x27;s criterion, and the sum of the SOCCs
is revealed, which unveils the statistical implication of ERR in OLS for the
first time. It is also shown that the OLS based feature selection method has
speed advantages when applied for greedy search. The proposed method is
comprehensively compared with the mutual information based feature selection
methods in 2 synthetic and 7 real world datasets. The results show that the
proposed method is always in the top 5 among the 10 candidate methods. Besides,
the proposed method can be directly applied to continuous features without
discretisation, which is another significant advantage over mutual information
based methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Nonlinear Hawkes Processes in Time-Varying System. (arXiv:2106.04844v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1">Feng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1">Quyu Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yixuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1">Cheng Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04844">
                                    <div class="article-summary-box-inner">
                                        <span>Hawkes processes are a class of point processes that have the ability to
model the self- and mutual-exciting phenomena. Although the classic Hawkes
processes cover a wide range of applications, their expressive ability is
limited due to three key hypotheses: parametric, linear and homogeneous. Recent
work has attempted to address these limitations separately. This work aims to
overcome all three assumptions simultaneously by proposing the flexible
state-switching Hawkes processes: a flexible, nonlinear and nonhomogeneous
variant where a state process is incorporated to interact with the point
processes. The proposed model empowers Hawkes processes to be applied to
time-varying systems. For inference, we utilize the latent variable
augmentation technique to design two efficient Bayesian inference algorithms:
Gibbs sampler and mean-field variational inference, with analytical iterative
updates to estimate the posterior. In experiments, our model achieves superior
performance compared to the state-of-the-art competitors.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Stable High-order Tuner for General Convex Functions. (arXiv:2011.09996v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Moreu_J/0/1/0/all/0/1">Jos&#xe9; M. Moreu</a>, <a href="http://arxiv.org/find/cs/1/au:+Annaswamy_A/0/1/0/all/0/1">Anuradha M. Annaswamy</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09996">
                                    <div class="article-summary-box-inner">
                                        <span>Iterative gradient-based algorithms have been increasingly applied for the
training of a broad variety of machine learning models including large
neural-nets. In particular, momentum-based methods, with accelerated learning
guarantees, have received a lot of attention due to their provable guarantees
of fast learning in certain classes of problems and multiple algorithms have
been derived. However, properties for these methods hold only for constant
regressors. When time-varying regressors occur, which is commonplace in dynamic
systems, many of these momentum-based methods cannot guarantee stability.
Recently, a new High-order Tuner (HT) was developed for linear regression
problems and shown to have 1) stability and asymptotic convergence for
time-varying regressors and 2) non-asymptotic accelerated learning guarantees
for constant regressors. In this paper, we extend and discuss the results of
this same HT for general convex loss functions. Through the exploitation of
convexity and smoothness definitions, we establish similar stability and
asymptotic convergence guarantees. Finally, we provide numerical simulations
supporting the satisfactory behavior of the HT algorithm as well as an
accelerated learning property.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Explainable Abnormal Infant Movements Identification: A Body-part Based Prediction and Visualisation Framework. (arXiv:2106.04966v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+McCay_K/0/1/0/all/0/1">Kevin D. McCay</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1">Edmond S. L. Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakkos_D/0/1/0/all/0/1">Dimitrios Sakkos</a>, <a href="http://arxiv.org/find/cs/1/au:+Woo_W/0/1/0/all/0/1">Wai Lok Woo</a>, <a href="http://arxiv.org/find/cs/1/au:+Marcroft_C/0/1/0/all/0/1">Claire Marcroft</a>, <a href="http://arxiv.org/find/cs/1/au:+Dulson_P/0/1/0/all/0/1">Patricia Dulson</a>, <a href="http://arxiv.org/find/cs/1/au:+Embleton_N/0/1/0/all/0/1">Nicholas D. Embleton</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04966">
                                    <div class="article-summary-box-inner">
                                        <span>Providing early diagnosis of cerebral palsy (CP) is key to enhancing the
developmental outcomes for those affected. Diagnostic tools such as the General
Movements Assessment (GMA), have produced promising results in early diagnosis,
however these manual methods can be laborious.

In this paper, we propose a new framework for the automated classification of
infant body movements, based upon the GMA, which unlike previous methods, also
incorporates a visualization framework to aid with interpretability. Our
proposed framework segments extracted features to detect the presence of
Fidgety Movements (FMs) associated with the GMA spatiotemporally. These
features are then used to identify the body-parts with the greatest
contribution towards a classification decision and highlight the related
body-part segment providing visual feedback to the user.

We quantitatively compare the proposed framework&#x27;s classification performance
with several other methods from the literature and qualitatively evaluate the
visualization&#x27;s veracity. Our experimental results show that the proposed
method performs more robustly than comparable techniques in this setting whilst
simultaneously providing relevant visual interpretability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Densely connected normalizing flows. (arXiv:2106.04627v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Grcic_M/0/1/0/all/0/1">Matej Grci&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1">Ivan Grubi&#x161;i&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1">Sini&#x161;a &#x160;egvi&#x107;</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04627">
                                    <div class="article-summary-box-inner">
                                        <span>Normalizing flows are bijective mappings between inputs and latent
representations with a fully factorized distribution. They are very attractive
due to exact likelihood evaluation and efficient sampling. However, their
effective capacity is often insufficient since the bijectivity constraint
limits the model width. We address this issue by incrementally padding
intermediate representations with noise. We precondition the noise in
accordance with previous invertible units, which we describe as cross-unit
coupling. Our invertible glow-like modules express intra-unit affine coupling
as a fusion of a densely connected block and Nystr\&quot;om self-attention. We refer
to our architecture as DenseFlow since both cross-unit and intra-unit couplings
rely on dense connectivity. Experiments show significant improvements due to
the proposed contributions, and reveal state-of-the-art density estimation
among all generative models under moderate computing budgets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Adversarial Tracking Control via Strongly Adaptive Online Learning with Memory. (arXiv:2102.01623v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1">Ashok Cutkosky</a>, <a href="http://arxiv.org/find/cs/1/au:+Paschalidis_I/0/1/0/all/0/1">Ioannis Ch. Paschalidis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01623">
                                    <div class="article-summary-box-inner">
                                        <span>We consider tracking adversarial targets in a delayed time-varying linear
system with adversarial disturbances and loss functions, which significantly
generalizes earlier work. To this end, we develop three techniques that each
could be of independent interest. First, we propose a black-box reduction from
adversarial tracking control to strongly adaptive online learning with memory.
Any solution to the latter translates to a tracking controller that pursues the
best action on any time interval. Second, for the resulting online learning
problem we develop a novel approach that further adapts to the observed
gradients. Third, we propose a new algorithm for unconstrained online linear
optimization: for all (unknown) $T\in\mathbb{N}_+$, the cumulative loss and
movement on the time horizon $[1:T]$ is upper-bounded by a user-specified
constant. Combining these individual techniques, we propose a tracking
controller with a sensible performance guarantee even when the adversarial
target has a large range of movement.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robust normalizing flows using Bernstein-type polynomials. (arXiv:2102.03509v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1">Sameera Ramasinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Fernando_K/0/1/0/all/0/1">Kasun Fernando</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1">Salman Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1">Nick Barnes</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03509">
                                    <div class="article-summary-box-inner">
                                        <span>Modeling real-world distributions can often be challenging due to sample data
that are subjected to perturbations, e.g., instrumentation errors, or added
random noise. Since flow models are typically nonlinear algorithms, they
amplify these initial errors, leading to poor generalizations. This paper
proposes a framework to construct Normalizing Flows (NF), which demonstrates
higher robustness against such initial errors. To this end, we utilize
Bernstein-type polynomials inspired by the optimal stability of the Bernstein
basis. Further, compared to the existing NF frameworks, our method provides
compelling advantages like theoretical upper bounds for the approximation
error, higher interpretability, suitability for compactly supported densities,
and the ability to employ higher degree polynomials without training
instability. We conduct a thorough theoretical analysis and empirically
demonstrate the efficacy of the proposed technique using experiments on both
real-world and synthetic datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning. (arXiv:2106.04895v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tengyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1">Nan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1">Caiming Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yu Bai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04895">
                                    <div class="article-summary-box-inner">
                                        <span>Recent theoretical work studies sample-efficient reinforcement learning (RL)
extensively in two settings: learning interactively in the environment (online
RL), or learning from an offline dataset (offline RL). However, existing
algorithms and theories for learning near-optimal policies in these two
settings are rather different and disconnected. Towards bridging this gap, this
paper initiates the theoretical study of policy finetuning, that is, online RL
where the learner has additional access to a &quot;reference policy&quot; $\mu$ close to
the optimal policy $\pi_\star$ in a certain sense. We consider the policy
finetuning problem in episodic Markov Decision Processes (MDPs) with $S$
states, $A$ actions, and horizon length $H$. We first design a sharp offline
reduction algorithm -- which simply executes $\mu$ and runs offline policy
optimization on the collected dataset -- that finds an $\varepsilon$
near-optimal policy within $\widetilde{O}(H^3SC^\star/\varepsilon^2)$ episodes,
where $C^\star$ is the single-policy concentrability coefficient between $\mu$
and $\pi_\star$. This offline result is the first that matches the sample
complexity lower bound in this setting, and resolves a recent open question in
offline RL. We then establish an $\Omega(H^3S\min\{C^\star, A\}/\varepsilon^2)$
sample complexity lower bound for any policy finetuning algorithm, including
those that can adaptively explore the environment. This implies that -- perhaps
surprisingly -- the optimal policy finetuning algorithm is either offline
reduction or a purely online RL algorithm that does not use $\mu$. Finally, we
design a new hybrid offline/online algorithm for policy finetuning that
achieves better sample complexity than both vanilla offline reduction and
purely online RL algorithms, in a relaxed setting where $\mu$ only satisfies
concentrability partially up to a certain time step.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Pretraining Representations for Data-Efficient Reinforcement Learning. (arXiv:2106.04799v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Schwarzer_M/0/1/0/all/0/1">Max Schwarzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajkumar_N/0/1/0/all/0/1">Nitarshan Rajkumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Noukhovitch_M/0/1/0/all/0/1">Michael Noukhovitch</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1">Ankesh Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Charlin_L/0/1/0/all/0/1">Laurent Charlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hjelm_D/0/1/0/all/0/1">Devon Hjelm</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1">Philip Bachman</a>, <a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1">Aaron Courville</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04799">
                                    <div class="article-summary-box-inner">
                                        <span>Data efficiency is a key challenge for deep reinforcement learning. We
address this problem by using unlabeled data to pretrain an encoder which is
then finetuned on a small amount of task-specific data. To encourage learning
representations which capture diverse aspects of the underlying MDP, we employ
a combination of latent dynamics modelling and unsupervised goal-conditioned
RL. When limited to 100k steps of interaction on Atari games (equivalent to two
hours of human experience), our approach significantly surpasses prior work
combining offline representation pretraining with task-specific finetuning, and
compares favourably with other pretraining methods that require orders of
magnitude more data. Our approach shows particular promise when combined with
larger models as well as more diverse, task-aligned observational data --
approaching human-level performance and data-efficiency on Atari in our best
setting. We provide code associated with this work at
https://github.com/mila-iqia/SGI.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On the Lack of Robust Interpretability of Neural Text Classifiers. (arXiv:2106.04631v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1">Muhammad Bilal Zafar</a>, <a href="http://arxiv.org/find/cs/1/au:+Donini_M/0/1/0/all/0/1">Michele Donini</a>, <a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1">Dylan Slack</a>, <a href="http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1">C&#xe9;dric Archambeau</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1">Sanjiv Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1">Krishnaram Kenthapadi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04631">
                                    <div class="article-summary-box-inner">
                                        <span>With the ever-increasing complexity of neural language models, practitioners
have turned to methods for understanding the predictions of these models. One
of the most well-adopted approaches for model interpretability is feature-based
interpretability, i.e., ranking the features in terms of their impact on model
predictions. Several prior studies have focused on assessing the fidelity of
feature-based interpretability methods, i.e., measuring the impact of dropping
the top-ranked features on the model output. However, relatively little work
has been conducted on quantifying the robustness of interpretations. In this
work, we assess the robustness of interpretations of neural text classifiers,
specifically, those based on pretrained Transformer encoders, using two
randomization tests. The first compares the interpretations of two models that
are identical except for their initializations. The second measures whether the
interpretations differ between a model with trained parameters and a model with
random parameters. Both tests show surprising deviations from expected
behavior, raising questions about the extent of insights that practitioners may
draw from interpretations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Reliable Adversarial Distillation with Unreliable Teachers. (arXiv:2106.04928v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jianing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jiangchao Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jingfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1">Gang Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jingren Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jianliang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Hongxia Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04928">
                                    <div class="article-summary-box-inner">
                                        <span>In ordinary distillation, student networks are trained with soft labels (SLs)
given by pretrained teacher networks, and students are expected to improve upon
teachers since SLs are stronger supervision than the original hard labels.
However, when considering adversarial robustness, teachers may become
unreliable and adversarial distillation may not work: teachers are pretrained
on their own adversarial data, and it is too demanding to require that teachers
are also good at every adversarial data queried by students. Therefore, in this
paper, we propose reliable introspective adversarial distillation (IAD) where
students partially instead of fully trust their teachers. Specifically, IAD
distinguishes between three cases given a query of a natural data (ND) and the
corresponding adversarial data (AD): (a) if a teacher is good at AD, its SL is
fully trusted; (b) if a teacher is good at ND but not AD, its SL is partially
trusted and the student also takes its own SL into account; (c) otherwise, the
student only relies on its own SL. Experiments demonstrate the effectiveness of
IAD for improving upon teachers in terms of adversarial robustness.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Probabilistic Deep Learning with Probabilistic Neural Networks and Deep Probabilistic Models. (arXiv:2106.00120v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1">Daniel T. Chang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00120">
                                    <div class="article-summary-box-inner">
                                        <span>Probabilistic deep learning is deep learning that accounts for uncertainty,
both model uncertainty and data uncertainty. It is based on the use of
probabilistic models and deep neural networks. We distinguish two approaches to
probabilistic deep learning: probabilistic neural networks and deep
probabilistic models. The former employs deep neural networks that utilize
probabilistic layers which can represent and process uncertainty; the latter
uses probabilistic models that incorporate deep neural network components which
capture complex non-linear stochastic relationships between the random
variables. We discuss some major examples of each approach including Bayesian
neural networks and mixture density networks (for probabilistic neural
networks), and variational autoencoders, deep Gaussian processes and deep mixed
effects models (for deep probabilistic models). TensorFlow Probability is a
library for probabilistic modeling and inference which can be used for both
approaches of probabilistic deep learning. We include its code examples for
illustration.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-Improved Retrosynthetic Planning. (arXiv:2106.04880v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Junsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1">Sungsoo Ahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hankook Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1">Jinwoo Shin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04880">
                                    <div class="article-summary-box-inner">
                                        <span>Retrosynthetic planning is a fundamental problem in chemistry for finding a
pathway of reactions to synthesize a target molecule. Recently, search
algorithms have shown promising results for solving this problem by using deep
neural networks (DNNs) to expand their candidate solutions, i.e., adding new
reactions to reaction pathways. However, the existing works on this line are
suboptimal; the retrosynthetic planning problem requires the reaction pathways
to be (a) represented by real-world reactions and (b) executable using
&quot;building block&quot; molecules, yet the DNNs expand reaction pathways without fully
incorporating such requirements. Motivated by this, we propose an end-to-end
framework for directly training the DNNs towards generating reaction pathways
with the desirable properties. Our main idea is based on a self-improving
procedure that trains the model to imitate successful trajectories found by
itself. We also propose a novel reaction augmentation scheme based on a forward
reaction model. Our experiments demonstrate that our scheme significantly
improves the success rate of solving the retrosynthetic problem from 86.84% to
96.32% while maintaining the performance of DNN for predicting valid reactions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Deep Industrial Transfer Learning for Anomaly Detection on Time Series Data. (arXiv:2106.04920v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Maschler_B/0/1/0/all/0/1">Benjamin Maschler</a>, <a href="http://arxiv.org/find/cs/1/au:+Knodel_T/0/1/0/all/0/1">Tim Knodel</a>, <a href="http://arxiv.org/find/cs/1/au:+Weyrich_M/0/1/0/all/0/1">Michael Weyrich</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04920">
                                    <div class="article-summary-box-inner">
                                        <span>Deep learning promises performant anomaly detection on time-variant datasets,
but greatly suffers from low availability of suitable training datasets and
frequently changing tasks. Deep transfer learning offers mitigation by letting
algorithms built upon previous knowledge from different tasks or locations. In
this article, a modular deep learning algorithm for anomaly detection on time
series datasets is presented that allows for an easy integration of such
transfer learning capabilities. It is thoroughly tested on a dataset from a
discrete manufacturing process in order to prove its fundamental adequacy
towards deep industrial transfer learning - the transfer of knowledge in
industrial applications&#x27; special environment.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Simulating Continuum Mechanics with Multi-Scale Graph Neural Networks. (arXiv:2106.04900v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lino_M/0/1/0/all/0/1">Mario Lino</a>, <a href="http://arxiv.org/find/cs/1/au:+Cantwell_C/0/1/0/all/0/1">Chris Cantwell</a>, <a href="http://arxiv.org/find/cs/1/au:+Bharath_A/0/1/0/all/0/1">Anil A. Bharath</a>, <a href="http://arxiv.org/find/cs/1/au:+Fotiadis_S/0/1/0/all/0/1">Stathi Fotiadis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04900">
                                    <div class="article-summary-box-inner">
                                        <span>Continuum mechanics simulators, numerically solving one or more partial
differential equations, are essential tools in many areas of science and
engineering, but their performance often limits application in practice. Recent
modern machine learning approaches have demonstrated their ability to
accelerate spatio-temporal predictions, although, with only moderate accuracy
in comparison. Here we introduce MultiScaleGNN, a novel multi-scale graph
neural network model for learning to infer unsteady continuum mechanics.
MultiScaleGNN represents the physical domain as an unstructured set of nodes,
and it constructs one or more graphs, each of them encoding different scales of
spatial resolution. Successive learnt message passing between these graphs
improves the ability of GNNs to capture and forecast the system state in
problems encompassing a range of length scales. Using graph representations,
MultiScaleGNN can impose periodic boundary conditions as an inductive bias on
the edges in the graphs, and achieve independence to the nodes&#x27; positions. We
demonstrate this method on advection problems and incompressible fluid
dynamics. Our results show that the proposed model can generalise from uniform
advection fields to high-gradient fields on complex domains at test time and
infer long-term Navier-Stokes solutions within a range of Reynolds numbers.
Simulations obtained with MultiScaleGNN are between two and four orders of
magnitude faster than the ones on which it was trained.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Expectation Programming. (arXiv:2106.04953v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Reichelt_T/0/1/0/all/0/1">Tim Reichelt</a>, <a href="http://arxiv.org/find/cs/1/au:+Golinski_A/0/1/0/all/0/1">Adam Goli&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ong_L/0/1/0/all/0/1">Luke Ong</a>, <a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1">Tom Rainforth</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04953">
                                    <div class="article-summary-box-inner">
                                        <span>Building on ideas from probabilistic programming, we introduce the concept of
an expectation programming framework (EPF) that automates the calculation of
expectations. Analogous to a probabilistic program, an expectation program is
comprised of a mix of probabilistic constructs and deterministic calculations
that define a conditional distribution over its variables. However, the focus
of the inference engine in an EPF is to directly estimate the resulting
expectation of the program return values, rather than approximate the
conditional distribution itself. This distinction allows us to achieve
substantial performance improvements over the standard probabilistic
programming pipeline by tailoring the inference to the precise expectation we
care about. We realize a particular instantiation of our EPF concept by
extending the probabilistic programming language Turing to allow so-called
target-aware inference to be run automatically, and show that this leads to
significant empirical gains compared to conventional posterior-based inference.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation. (arXiv:2106.04732v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Berthelot_D/0/1/0/all/0/1">David Berthelot</a>, <a href="http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1">Rebecca Roelofs</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kihyuk Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1">Nicholas Carlini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1">Alex Kurakin</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04732">
                                    <div class="article-summary-box-inner">
                                        <span>We extend semi-supervised learning to the problem of domain adaptation to
learn significantly higher-accuracy models that train on one data distribution
and test on a different one. With the goal of generality, we introduce
AdaMatch, a method that unifies the tasks of unsupervised domain adaptation
(UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation
(SSDA). In an extensive experimental study, we compare its behavior with
respective state-of-the-art techniques from SSL, SSDA, and UDA on vision
classification tasks. We find AdaMatch either matches or significantly exceeds
the state-of-the-art in each case using the same hyper-parameters regardless of
the dataset or task. For example, AdaMatch nearly doubles the accuracy compared
to that of the prior state-of-the-art on the UDA task for DomainNet and even
exceeds the accuracy of the prior state-of-the-art obtained with pre-training
by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by
providing AdaMatch with just one labeled example per class from the target
domain (i.e., the SSDA setting), we increase the target accuracy by an
additional 6.1%, and with 5 labeled examples, by 13.6%.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On Margin-Based Cluster Recovery with Oracle Queries. (arXiv:2106.04913v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bressan_M/0/1/0/all/0/1">Marco Bressan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1">Nicol&#xf2; Cesa-Bianchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lattanzi_S/0/1/0/all/0/1">Silvio Lattanzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Paudice_A/0/1/0/all/0/1">Andrea Paudice</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04913">
                                    <div class="article-summary-box-inner">
                                        <span>We study an active cluster recovery problem where, given a set of $n$ points
and an oracle answering queries like &quot;are these two points in the same
cluster?&quot;, the task is to recover exactly all clusters using as few queries as
possible. We begin by introducing a simple but general notion of margin between
clusters that captures, as special cases, the margins used in previous work,
the classic SVM margin, and standard notions of stability for center-based
clusterings. Then, under our margin assumptions we design algorithms that, in a
variety of settings, recover all clusters exactly using only $O(\log n)$
queries. For the Euclidean case, $\mathbb{R}^m$, we give an algorithm that
recovers arbitrary convex clusters, in polynomial time, and with a number of
queries that is lower than the best existing algorithm by $\Theta(m^m)$
factors. For general pseudometric spaces, where clusters might not be convex or
might not have any notion of shape, we give an algorithm that achieves the
$O(\log n)$ query bound, and is provably near-optimal as a function of the
packing number of the space. Finally, for clusterings realized by binary
concept classes, we give a combinatorial characterization of recoverability
with $O(\log n)$ queries, and we show that, for many concept classes in
Euclidean spaces, this characterization is equivalent to our margin condition.
Our results show a deep connection between cluster margins and active cluster
recoverability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Regret and Cumulative Constraint Violation Analysis for Online Convex Optimization with Long Term Constraints. (arXiv:2106.05135v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1">Xinlei Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiuxian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Lihua Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_T/0/1/0/all/0/1">Tianyou Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Johansson_K/0/1/0/all/0/1">Karl H. Johansson</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05135">
                                    <div class="article-summary-box-inner">
                                        <span>This paper considers online convex optimization with long term constraints,
where constraints can be violated in intermediate rounds, but need to be
satisfied in the long run. The cumulative constraint violation is used as the
metric to measure constraint violations, which excludes the situation that
strictly feasible constraints can compensate the effects of violated
constraints. A novel algorithm is first proposed and it achieves an
$\mathcal{O}(T^{\max\{c,1-c\}})$ bound for static regret and an
$\mathcal{O}(T^{(1-c)/2})$ bound for cumulative constraint violation, where
$c\in(0,1)$ is a user-defined trade-off parameter, and thus has improved
performance compared with existing results. Both static regret and cumulative
constraint violation bounds are reduced to $\mathcal{O}(\log(T))$ when the loss
functions are strongly convex, which also improves existing results. %In order
to bound the regret with respect to any comparator sequence, In order to
achieve the optimal regret with respect to any comparator sequence, another
algorithm is then proposed and it achieves the optimal
$\mathcal{O}(\sqrt{T(1+P_T)})$ regret and an $\mathcal{O}(\sqrt{T})$ cumulative
constraint violation, where $P_T$ is the path-length of the comparator
sequence. Finally, numerical simulations are provided to illustrate the
effectiveness of the theoretical results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Predicting the Success of Domain Adaptation in Text Similarity. (arXiv:2106.04641v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Pogrebnyakov_N/0/1/0/all/0/1">Nicolai Pogrebnyakov</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaghaghian_S/0/1/0/all/0/1">Shohreh Shaghaghian</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04641">
                                    <div class="article-summary-box-inner">
                                        <span>Transfer learning methods, and in particular domain adaptation, help exploit
labeled data in one domain to improve the performance of a certain task in
another domain. However, it is still not clear what factors affect the success
of domain adaptation. This paper models adaptation success and selection of the
most suitable source domains among several candidates in text similarity. We
use descriptive domain information and cross-domain similarity metrics as
predictive features. While mostly positive, the results also point to some
domains where adaptation success was difficult to predict.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">EMFlow: Data Imputation in Latent Space via EM and Deep Flow Models. (arXiv:2106.04804v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1">Qi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Sujit K. Ghosh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04804">
                                    <div class="article-summary-box-inner">
                                        <span>High dimensional incomplete data can be found in a wide range of systems. Due
to the fact that most of the data mining techniques and machine learning
algorithms require complete observations, data imputation is vital for
down-stream analysis. In this work, we introduce an imputation approach, called
EMFlow, that performs imputation in an latent space via an online version of
Expectation-Maximization (EM) algorithm and connects the latent space and the
data space via the normalizing flow (NF). The inference of EMFlow is iterative,
involving updating the parameters of online EM and NF alternatively. Extensive
experimental results on multivariate and image datasets show that the proposed
EMFlow has superior performance to competing methods in terms of both
imputation quality and convergence speed.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Symmetric Spaces for Graph Embeddings: A Finsler-Riemannian Approach. (arXiv:2106.04941v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lopez_F/0/1/0/all/0/1">Federico L&#xf3;pez</a>, <a href="http://arxiv.org/find/cs/1/au:+Pozzetti_B/0/1/0/all/0/1">Beatrice Pozzetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Trettel_S/0/1/0/all/0/1">Steve Trettel</a>, <a href="http://arxiv.org/find/cs/1/au:+Strube_M/0/1/0/all/0/1">Michael Strube</a>, <a href="http://arxiv.org/find/cs/1/au:+Wienhard_A/0/1/0/all/0/1">Anna Wienhard</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04941">
                                    <div class="article-summary-box-inner">
                                        <span>Learning faithful graph representations as sets of vertex embeddings has
become a fundamental intermediary step in a wide range of machine learning
applications. We propose the systematic use of symmetric spaces in
representation learning, a class encompassing many of the previously used
embedding targets. This enables us to introduce a new method, the use of
Finsler metrics integrated in a Riemannian optimization scheme, that better
adapts to dissimilar structures in the graph. We develop a tool to analyze the
embeddings and infer structural properties of the data sets. For
implementation, we choose Siegel spaces, a versatile family of symmetric
spaces. Our approach outperforms competitive baselines for graph reconstruction
tasks on various synthetic and real-world datasets. We further demonstrate its
applicability on two downstream tasks, recommender systems and node
classification.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Phase Retrieval using Single-Instance Deep Generative Prior. (arXiv:2106.04812v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1">Kshitij Tayal</a>, <a href="http://arxiv.org/find/cs/1/au:+Manekar_R/0/1/0/all/0/1">Raunak Manekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1">Zhong Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">David Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Vipin Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmann_F/0/1/0/all/0/1">Felix Hofmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Ju Sun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04812">
                                    <div class="article-summary-box-inner">
                                        <span>Several deep learning methods for phase retrieval exist, but most of them
fail on realistic data without precise support information. We propose a novel
method based on single-instance deep generative prior that works well on
complex-valued crystal data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning Pseudo-Backdoors for Mixed Integer Programs. (arXiv:2106.05080v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ferber_A/0/1/0/all/0/1">Aaron Ferber</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jialin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1">Bistra Dilkina</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1">Yisong Yue</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05080">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a machine learning approach for quickly solving Mixed Integer
Programs (MIP) by learning to prioritize a set of decision variables, which we
call pseudo-backdoors, for branching that results in faster solution times.
Learning-based approaches have seen success in the area of solving
combinatorial optimization problems by being able to flexibly leverage common
structures in a given distribution of problems. Our approach takes inspiration
from the concept of strong backdoors, which corresponds to a small set of
variables such that only branching on these variables yields an optimal
integral solution and a proof of optimality. Our notion of pseudo-backdoors
corresponds to a small set of variables such that only branching on them leads
to faster solve time (which can be solver dependent). A key advantage of
pseudo-backdoors over strong backdoors is that they are much amenable to
data-driven identification or prediction. Our proposed method learns to
estimate the solver performance of a proposed pseudo-backdoor, using a labeled
dataset collected on a set of training MIP instances. This model can then be
used to identify high-quality pseudo-backdoors on new MIP instances from the
same distribution. We evaluate our method on the generalized independent set
problems and find that our approach can efficiently identify high-quality
pseudo-backdoors. In addition, we compare our learned approach against Gurobi,
a state-of-the-art MIP solver, demonstrating that our method can be used to
improve solver performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Labeled Data Generation with Inexact Supervision. (arXiv:2106.04716v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dai_E/0/1/0/all/0/1">Enyan Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1">Kai Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yiwei Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Suhang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04716">
                                    <div class="article-summary-box-inner">
                                        <span>The recent advanced deep learning techniques have shown the promising results
in various domains such as computer vision and natural language processing. The
success of deep neural networks in supervised learning heavily relies on a
large amount of labeled data. However, obtaining labeled data with target
labels is often challenging due to various reasons such as cost of labeling and
privacy issues, which challenges existing deep models. In spite of that, it is
relatively easy to obtain data with \textit{inexact supervision}, i.e., having
labels/tags related to the target task. For example, social media platforms are
overwhelmed with billions of posts and images with self-customized tags, which
are not the exact labels for target classification tasks but are usually
related to the target labels. It is promising to leverage these tags (inexact
supervision) and their relations with target classes to generate labeled data
to facilitate the downstream classification tasks. However, the work on this is
rather limited. Therefore, we study a novel problem of labeled data generation
with inexact supervision. We propose a novel generative framework named as
ADDES which can synthesize high-quality labeled data for target classification
tasks by learning from data with inexact supervision and the relations between
inexact supervision and target classes. Experimental results on image and text
datasets demonstrate the effectiveness of the proposed ADDES for generating
realistic labeled data from inexact supervision to facilitate the target
classification task.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Robustness in Compressed Neural Networks for Object Detection. (arXiv:2102.05509v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1">Sebastian Cygert</a>, <a href="http://arxiv.org/find/cs/1/au:+Czyzewski_A/0/1/0/all/0/1">Andrzej Czy&#x17c;ewski</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05509">
                                    <div class="article-summary-box-inner">
                                        <span>Model compression techniques allow to significantly reduce the computational
cost associated with data processing by deep neural networks with only a minor
decrease in average accuracy. Simultaneously, reducing the model size may have
a large effect on noisy cases or objects belonging to less frequent classes. It
is a crucial problem from the perspective of the models&#x27; safety, especially for
object detection in the autonomous driving setting, which is considered in this
work. It was shown in the paper that the sensitivity of compressed models to
different distortion types is nuanced, and some of the corruptions are heavily
impacted by the compression methods (i.e., additive noise), while others (blur
effect) are only slightly affected. A common way to improve the robustness of
models is to use data augmentation, which was confirmed to positively affect
models&#x27; robustness, also for highly compressed models. It was further shown
that while data imbalance methods brought only a slight increase in accuracy
for the baseline model (without compression), the impact was more striking at
higher compression rates for the structured pruning. Finally, methods for
handling data imbalance brought a significant improvement of the pruned models&#x27;
worst-detected class accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Tracking by Joint Local and Global Search: A Target-aware Attention based Approach. (arXiv:2106.04840v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jin Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1">Bin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yonghong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Feng Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04840">
                                    <div class="article-summary-box-inner">
                                        <span>Tracking-by-detection is a very popular framework for single object tracking
which attempts to search the target object within a local search window for
each frame. Although such local search mechanism works well on simple videos,
however, it makes the trackers sensitive to extremely challenging scenarios,
such as heavy occlusion and fast motion. In this paper, we propose a novel and
general target-aware attention mechanism (termed TANet) and integrate it with
tracking-by-detection framework to conduct joint local and global search for
robust tracking. Specifically, we extract the features of target object patch
and continuous video frames, then we concatenate and feed them into a decoder
network to generate target-aware global attention maps. More importantly, we
resort to adversarial training for better attention prediction. The appearance
and motion discriminator networks are designed to ensure its consistency in
spatial and temporal views. In the tracking procedure, we integrate the
target-aware attention with multiple trackers by exploring candidate search
regions for robust tracking. Extensive experiments on both short-term and
long-term tracking benchmark datasets all validated the effectiveness of our
algorithm. The project page of this paper can be found at
\url{https://sites.google.com/view/globalattentiontracking/home/extend}.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Intermittent Speech Recovery. (arXiv:2106.05229v1 [cs.SD])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yu-Chen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_T/0/1/0/all/0/1">Tsun-An Hsieh</a>, <a href="http://arxiv.org/find/cs/1/au:+Hung_K/0/1/0/all/0/1">Kuo-Hsuan Hung</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Cheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Garudadri_H/0/1/0/all/0/1">Harinath Garudadri</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1">Yu Tsao</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuo_T/0/1/0/all/0/1">Tei-Wei Kuo</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05229">
                                    <div class="article-summary-box-inner">
                                        <span>A large number of Internet of Things (IoT) devices today are powered by
batteries, which are often expensive to maintain and may cause serious
environmental pollution. To avoid these problems, researchers have begun to
consider the use of energy systems based on energy-harvesting units for such
devices. However, the power harvested from an ambient source is fundamentally
small and unstable, resulting in frequent power failures during the operation
of IoT applications involving, for example, intermittent speech signals and the
streaming of videos. This paper presents a deep-learning-based speech recovery
system that reconstructs intermittent speech signals from self-powered IoT
devices. Our intermittent speech recovery system (ISR) consists of three
stages: interpolation, recovery, and combination. The experimental results show
that our recovery system increases speech quality by up to 707.1%, while
increasing speech intelligibility by up to 92.1%. Most importantly, our ISR
system also enhances the WER scores by up to 65.6%. To the best of our
knowledge, this study is one of the first to reconstruct intermittent speech
signals from self-powered-sensing IoT devices. These promising results suggest
that even though self powered microphone devices function with weak energy
sources, our ISR system can still maintain the performance of most
speech-signal-based applications.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DIGRAC: Digraph Clustering with Flow Imbalance. (arXiv:2106.05194v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+He_Y/0/1/0/all/0/1">Yixuan He</a>, <a href="http://arxiv.org/find/stat/1/au:+Reinert_G/0/1/0/all/0/1">Gesine Reinert</a>, <a href="http://arxiv.org/find/stat/1/au:+Cucuringu_M/0/1/0/all/0/1">Mihai Cucuringu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05194">
                                    <div class="article-summary-box-inner">
                                        <span>Node clustering is a powerful tool in the analysis of networks. Here, we
introduce a graph neural network framework with a novel scalable Directed Mixed
Path Aggregation(DIMPA) scheme to obtain node embeddings for directed networks
in a self-supervised manner, including a novel probabilistic imbalance loss.
The method is end-to-end in combining embedding generation and clustering
without an intermediate step. In contrast to standard approaches in the
literature, in this paper, directionality is not treated as a nuisance, but
rather contains the main signal. In particular, we leverage the recently
introduced cut flow imbalance measure, which is tightly related to
directionality; cut flow imbalance is optimized without resorting to spectral
methods or cluster labels. Experimental results on synthetic data, in the form
of directed stochastic block models and real-world data at different scales,
demonstrate that our method attains state-of-the-art results on directed
clustering, for a wide range of noise and sparsity levels, as well as graph
structures.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DPER: Efficient Parameter Estimation for Randomly Missing Data. (arXiv:2106.05190v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Nguyen_T/0/1/0/all/0/1">Thu Nguyen</a>, <a href="http://arxiv.org/find/stat/1/au:+Nguyen_Duy_K/0/1/0/all/0/1">Khoi Minh Nguyen-Duy</a>, <a href="http://arxiv.org/find/stat/1/au:+Nguyen_D/0/1/0/all/0/1">Duy Ho Minh Nguyen</a>, <a href="http://arxiv.org/find/stat/1/au:+Nguyen_B/0/1/0/all/0/1">Binh T. Nguyen</a>, <a href="http://arxiv.org/find/stat/1/au:+Wade_B/0/1/0/all/0/1">Bruce Alan Wade</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05190">
                                    <div class="article-summary-box-inner">
                                        <span>The missing data problem has been broadly studied in the last few decades and
has various applications in different areas such as statistics or
bioinformatics. Even though many methods have been developed to tackle this
challenge, most of those are imputation techniques that require multiple
iterations through the data before yielding convergence. In addition, such
approaches may introduce extra biases and noises to the estimated parameters.
In this work, we propose novel algorithms to find the maximum likelihood
estimates (MLEs) for a one-class/multiple-class randomly missing data set under
some mild assumptions. As the computation is direct without any imputation, our
algorithms do not require multiple iterations through the data, thus promising
to be less time-consuming than other methods while maintaining superior
estimation performance. We validate these claims by empirical results on
various data sets of different sizes and release all codes in a GitHub
repository to contribute to the research community related to this problem.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">I Don&#x27;t Need $\mathbf{u}$: Identifiable Non-Linear ICA Without Side Information. (arXiv:2106.05238v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Willetts_M/0/1/0/all/0/1">Matthew Willetts</a>, <a href="http://arxiv.org/find/cs/1/au:+Paige_B/0/1/0/all/0/1">Brooks Paige</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05238">
                                    <div class="article-summary-box-inner">
                                        <span>In this work we introduce a new approach for identifiable non-linear ICA
models. Recently there has been a renaissance in identifiability results in
deep generative models, not least for non-linear ICA. These prior works,
however, have assumed access to a sufficiently-informative auxiliary set of
observations, denoted $\mathbf{u}$. We show here how identifiability can be
obtained in the absence of this side-information, rendering possible
fully-unsupervised identifiable non-linear ICA. While previous theoretical
results have established the impossibility of identifiable non-linear ICA in
the presence of infinitely-flexible universal function approximators, here we
rely on the intrinsically-finite modelling capacity of any particular chosen
parameterisation of a deep generative model. In particular, we focus on
generative models which perform clustering in their latent space -- a model
structure which matches previous identifiable models, but with the learnt
clustering providing a synthetic form of auxiliary information. We evaluate our
proposals using VAEs, on synthetic and image datasets, and find that the
learned clusterings function effectively: deep generative models with latent
clusterings are empirically identifiable, to the same degree as models which
rely on side information.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Implicit Regularization in Tensor Factorization. (arXiv:2102.09972v3 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Razin_N/0/1/0/all/0/1">Noam Razin</a>, <a href="http://arxiv.org/find/cs/1/au:+Maman_A/0/1/0/all/0/1">Asaf Maman</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1">Nadav Cohen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09972">
                                    <div class="article-summary-box-inner">
                                        <span>Recent efforts to unravel the mystery of implicit regularization in deep
learning have led to a theoretical focus on matrix factorization -- matrix
completion via linear neural network. As a step further towards practical deep
learning, we provide the first theoretical analysis of implicit regularization
in tensor factorization -- tensor completion via certain type of non-linear
neural network. We circumvent the notorious difficulty of tensor problems by
adopting a dynamical systems perspective, and characterizing the evolution
induced by gradient descent. The characterization suggests a form of greedy low
tensor rank search, which we rigorously prove under certain conditions, and
empirically demonstrate under others. Motivated by tensor rank capturing the
implicit regularization of a non-linear neural network, we empirically explore
it as a measure of complexity, and find that it captures the essence of
datasets on which neural networks generalize. This leads us to believe that
tensor rank may pave way to explaining both implicit regularization in deep
learning, and the properties of real-world data translating this implicit
regularization to generalization.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">What causes the test error? Going beyond bias-variance via ANOVA. (arXiv:2010.05170v3 [stat.ML] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Lin_L/0/1/0/all/0/1">Licong Lin</a>, <a href="http://arxiv.org/find/stat/1/au:+Dobriban_E/0/1/0/all/0/1">Edgar Dobriban</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05170">
                                    <div class="article-summary-box-inner">
                                        <span>Modern machine learning methods are often overparametrized, allowing
adaptation to the data at a fine level. This can seem puzzling; in the worst
case, such models do not need to generalize. This puzzle inspired a great
amount of work, arguing when overparametrization reduces test error, in a
phenomenon called &quot;double descent&quot;. Recent work aimed to understand in greater
depth why overparametrization is helpful for generalization. This leads to
discovering the unimodality of variance as a function of the level of
parametrization, and to decomposing the variance into that arising from label
noise, initialization, and randomness in the training data to understand the
sources of the error.

In this work we develop a deeper understanding of this area. Specifically, we
propose using the analysis of variance (ANOVA) to decompose the variance in the
test error in a symmetric way, for studying the generalization performance of
certain two-layer linear and non-linear networks. The advantage of the analysis
of variance is that it reveals the effects of initialization, label noise, and
training data more clearly than prior approaches. Moreover, we also study the
monotonicity and unimodality of the variance components. While prior work
studied the unimodality of the overall variance, we study the properties of
each term in variance decomposition.

One key insight is that in typical settings, the interaction between training
samples and initialization can dominate the variance; surprisingly being larger
than their marginal effect. Also, we characterize &quot;phase transitions&quot; where the
variance changes from unimodal to monotone. On a technical level, we leverage
advanced deterministic equivalent techniques for Haar random matrices, that --
to our knowledge -- have not yet been used in the area. We also verify our
results in numerical simulations and on empirical data examples.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Multi-layered Network Exploration via Random Walks: From Offline Optimization to Online Learning. (arXiv:2106.05065v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xutong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1">Jinhang Zuo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaowei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lui_J/0/1/0/all/0/1">John C.S. Lui</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05065">
                                    <div class="article-summary-box-inner">
                                        <span>Multi-layered network exploration (MuLaNE) problem is an important problem
abstracted from many applications. In MuLaNE, there are multiple network layers
where each node has an importance weight and each layer is explored by a random
walk. The MuLaNE task is to allocate total random walk budget $B$ into each
network layer so that the total weights of the unique nodes visited by random
walks are maximized. We systematically study this problem from offline
optimization to online learning. For the offline optimization setting where the
network structure and node weights are known, we provide greedy based
constant-ratio approximation algorithms for overlapping networks, and greedy or
dynamic-programming based optimal solutions for non-overlapping networks. For
the online learning setting, neither the network structure nor the node weights
are known initially. We adapt the combinatorial multi-armed bandit framework
and design algorithms to learn random walk related parameters and node weights
while optimizing the budget allocation in multiple rounds, and prove that they
achieve logarithmic regret bounds. Finally, we conduct experiments on a
real-world social network dataset to validate our theoretical results.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards Open-World Recommendation: An Inductive Model-based Collaborative Filtering Approach. (arXiv:2007.04833v2 [cs.IR] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qitian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hengrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xiaofeng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1">Hongyuan Zha</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.04833">
                                    <div class="article-summary-box-inner">
                                        <span>Recommendation models can effectively estimate underlying user interests and
predict one&#x27;s future behaviors by factorizing an observed user-item rating
matrix into products of two sets of latent factors. However, the user-specific
embedding factors can only be learned in a transductive way, making it
difficult to handle new users on-the-fly. In this paper, we propose an
inductive collaborative filtering framework that contains two representation
models. The first model follows conventional matrix factorization which
factorizes a group of key users&#x27; rating matrix to obtain meta latents. The
second model resorts to attention-based structure learning that estimates
hidden relations from query to key users and learns to leverage meta latents to
inductively compute embeddings for query users via neural message passing. Our
model enables inductive representation learning for users and meanwhile
guarantees equivalent representation capacity as matrix factorization.
Experiments demonstrate that our model achieves promising results for
recommendation on few-shot users with limited training ratings and new unseen
users which are commonly encountered in open-world recommender systems.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Maximum Probability Theorem: A Framework for Probabilistic Learning. (arXiv:1910.09417v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Marvasti_A/0/1/0/all/0/1">Amir Emad Marvasti</a>, <a href="http://arxiv.org/find/cs/1/au:+Marvasti_E/0/1/0/all/0/1">Ehsan Emad Marvasti</a>, <a href="http://arxiv.org/find/cs/1/au:+Foroosh_H/0/1/0/all/0/1">Hassan Foroosh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.09417">
                                    <div class="article-summary-box-inner">
                                        <span>We present a theoretical framework of probabilistic learning derived by
Maximum Probability (MP) Theorem shown in the current paper. In this
probabilistic framework, a model is defined as an event in the probability
space, and a model or the associated event - either the true underlying model
or the parameterized model - have a quantified probability measure. This
quantification of a model&#x27;s probability measure is derived by the MP Theorem,
in which we have shown that an event&#x27;s probability measure has an upper-bound
given its conditional distribution on an arbitrary random variable. Through
this alternative framework, the notion of model parameters is encompassed in
the definition of the model or the associated event. Therefore, this framework
deviates from the conventional approach of assuming a prior on the model
parameters. Instead, the regularizing effects of assuming prior over parameters
is seen through maximizing probabilities of models or according to information
theory, minimizing the information content of a model. The probability of a
model in our framework is invariant to reparameterization and is solely
dependent on the model&#x27;s likelihood function. Also, rather than maximizing the
posterior in a conventional Bayesian setting, the objective function in our
alternative framework is defined as the probability of set operations (e.g.
intersection) on the event of the true underlying model and the event of the
model at hand. Our theoretical framework, as a derivation of MP theorem, adds
clarity to probabilistic learning through solidifying the definition of
probabilistic models, quantifying their probabilities, and providing a visual
understanding of objective functions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">The Adaptive Doubly Robust Estimator for Policy Evaluation in Adaptive Experiments and a Paradox Concerning Logging Policy. (arXiv:2010.03792v4 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1">Masahiro Kato</a>, <a href="http://arxiv.org/find/cs/1/au:+Yasui_S/0/1/0/all/0/1">Shota Yasui</a>, <a href="http://arxiv.org/find/cs/1/au:+McAlinn_K/0/1/0/all/0/1">Kenichiro McAlinn</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03792">
                                    <div class="article-summary-box-inner">
                                        <span>The doubly robust (DR) estimator, which consists of two nuisance parameters,
the conditional mean outcome and the logging policy (the probability of
choosing an action), is crucial in causal inference. This paper proposes a DR
estimator for dependent samples obtained from adaptive experiments. To obtain
an asymptotically normal semiparametric estimator from dependent samples with
non-Donsker nuisance estimators, we propose adaptive-fitting as a variant of
sample-splitting. We also report an empirical paradox that our proposed DR
estimator tends to show better performances compared to other estimators
utilizing the true logging policy. While a similar phenomenon is known for
estimators with i.i.d. samples, traditional explanations based on asymptotic
efficiency cannot elucidate our case with dependent samples. We confirm this
hypothesis through simulation studies.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Quickest change detection with unknown parameters: Constant complexity and near optimality. (arXiv:2106.05061v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Jarboui_F/0/1/0/all/0/1">Firas Jarboui</a>, <a href="http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1">Viannet Perchet</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05061">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the quickest change detection problem where both the parameters
of pre- and post- change distributions are unknown, which prevents the use of
classical simple hypothesis testing. Without additional assumptions, optimal
solutions are not tractable as they rely on some minimax and robust variant of
the objective. As a consequence, change points might be detected too late for
practical applications (in economics, health care or maintenance for instance).
Available constant complexity techniques typically solve a relaxed version of
the problem, deeply relying on very specific probability distributions and/or
some very precise additional knowledge. We consider a totally different
approach that leverages the theoretical asymptotic properties of optimal
solutions to derive a new scalable approximate algorithm with near optimal
performance that runs~in~$\mathcal{O}(1)$, adapted to even more complex
Markovian settings.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dimensionwise Separable 2-D Graph Convolution for Unsupervised and Semi-Supervised Learning on Graphs. (arXiv:1909.12038v5 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qimai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaotong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Han Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1">Quanyu Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1">Xiao-Ming Wu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.12038">
                                    <div class="article-summary-box-inner">
                                        <span>Graph convolutional neural networks (GCN) have been the model of choice for
graph representation learning, which is mainly due to the effective design of
graph convolution that computes the representation of a node by aggregating
those of its neighbors. However, existing GCN variants commonly use 1-D graph
convolution that solely operates on the object link graph without exploring
informative relational information among object attributes. This significantly
limits their modeling capability and may lead to inferior performance on noisy
and sparse real-world networks. In this paper, we explore 2-D graph convolution
to jointly model object links and attribute relations for graph representation
learning. Specifically, we propose a computationally efficient dimensionwise
separable 2-D graph convolution (DSGC) for filtering node features.
Theoretically, we show that DSGC can reduce intra-class variance of node
features on both the object dimension and the attribute dimension to learn more
effective representations. Empirically, we demonstrate that by modeling
attribute relations, DSGC achieves significant performance gain over
state-of-the-art methods for node classification and clustering on a variety of
real-world networks. The source code for reproducing the experimental results
is available at https://github.com/liqimai/DSGC.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Diffusion Source Identification on Networks with Statistical Confidence. (arXiv:2106.04800v1 [cs.SI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dawkins_Q/0/1/0/all/0/1">Quinlan Dawkins</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianxi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Haifeng Xu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04800">
                                    <div class="article-summary-box-inner">
                                        <span>Diffusion source identification on networks is a problem of fundamental
importance in a broad class of applications, including rumor controlling and
virus identification. Though this problem has received significant recent
attention, most studies have focused only on very restrictive settings and lack
theoretical guarantees for more realistic networks. We introduce a statistical
framework for the study of diffusion source identification and develop a
confidence set inference approach inspired by hypothesis testing. Our method
efficiently produces a small subset of nodes, which provably covers the source
node with any pre-specified confidence level without restrictive assumptions on
network structures. Moreover, we propose multiple Monte Carlo strategies for
the inference procedure based on network topology and the probabilistic
properties that significantly improve the scalability. To our knowledge, this
is the first diffusion source identification method with a practically useful
theoretical guarantee on general networks. We demonstrate our approach via
extensive synthetic experiments on well-known random network models and a
mobility network between cities concerning the COVID-19 spreading.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bayesian Optimization over Hybrid Spaces. (arXiv:2106.04682v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Deshwal_A/0/1/0/all/0/1">Aryan Deshwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Belakaria_S/0/1/0/all/0/1">Syrine Belakaria</a>, <a href="http://arxiv.org/find/cs/1/au:+Doppa_J/0/1/0/all/0/1">Janardhan Rao Doppa</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04682">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of optimizing hybrid structures (mixture of discrete
and continuous input variables) via expensive black-box function evaluations.
This problem arises in many real-world applications. For example, in materials
design optimization via lab experiments, discrete and continuous variables
correspond to the presence/absence of primitive elements and their relative
concentrations respectively. The key challenge is to accurately model the
complex interactions between discrete and continuous variables. In this paper,
we propose a novel approach referred as Hybrid Bayesian Optimization (HyBO) by
utilizing diffusion kernels, which are naturally defined over continuous and
discrete variables. We develop a principled approach for constructing diffusion
kernels over hybrid spaces by utilizing the additive kernel formulation, which
allows additive interactions of all orders in a tractable manner. We
theoretically analyze the modeling strength of additive hybrid kernels and
prove that it has the universal approximation property. Our experiments on
synthetic and six diverse real-world benchmarks show that HyBO significantly
outperforms the state-of-the-art methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">FastSeq: Make Sequence Generation Faster. (arXiv:2106.04718v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1">Fei Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiusheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1">Nikhil Bhendawade</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1">Ting Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yeyun Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1">Nan Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_D/0/1/0/all/0/1">Desheng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1">Bingyu Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruifei Zhang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04718">
                                    <div class="article-summary-box-inner">
                                        <span>Transformer-based models have made tremendous impacts in natural language
generation. However the inference speed is a bottleneck due to large model size
and intensive computing involved in auto-regressive decoding process. We
develop FastSeq framework to accelerate sequence generation without accuracy
loss. The proposed optimization techniques include an attention cache
optimization, an efficient algorithm for detecting repeated n-grams, and an
asynchronous generation pipeline with parallel I/O. These optimizations are
general enough to be applicable to Transformer-based models (e.g., T5, GPT2,
and UniLM). Our benchmark results on a set of widely used and diverse models
demonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use
with a simple one-line code change. The source code is available at
https://github.com/microsoft/fastseq.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On a class of data-driven combinatorial optimization problems under uncertainty: a distributionally robust approach. (arXiv:2105.14139v2 [math.OC] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Ketkov_S/0/1/0/all/0/1">Sergey S. Ketkov</a>, <a href="http://arxiv.org/find/math/1/au:+Shilov_A/0/1/0/all/0/1">Andrei S. Shilov</a>, <a href="http://arxiv.org/find/math/1/au:+Prokopyev_O/0/1/0/all/0/1">Oleg A. Prokopyev</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14139">
                                    <div class="article-summary-box-inner">
                                        <span>In this study we analyze linear combinatorial optimization problems where the
cost vector is not known a priori, but is only observable through a finite data
set. In contrast to the related studies, we presume that the number of
observations with respect to particular components of the cost vector may vary.
The goal is to find a procedure that transforms the data set into an estimate
of the expected value of the objective function (which is referred to as a
prediction rule) and a procedure that retrieves a candidate decision (which is
referred to as a prescription rule). We aim at finding the least conservative
prediction and prescription rules, which satisfy some specified asymptotic
guarantees. We demonstrate that the resulting vector optimization problems
admit a weakly optimal solution, which can be obtained by solving a particular
distributionally robust optimization problem. Specifically, the decision-maker
may optimize the worst-case expected loss across all probability distributions
with given component-wise relative entropy distances from the empirical
marginal distributions. Finally, we perform numerical experiments to analyze
the out-of-sample performance of the proposed solution approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Incentivizing Efficient Equilibria in Traffic Networks with Mixed Autonomy. (arXiv:2106.04678v1 [cs.MA])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1">Erdem B&#x131;y&#x131;k</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazar_D/0/1/0/all/0/1">Daniel A. Lazar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedarsani_R/0/1/0/all/0/1">Ramtin Pedarsani</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1">Dorsa Sadigh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04678">
                                    <div class="article-summary-box-inner">
                                        <span>Traffic congestion has large economic and social costs. The introduction of
autonomous vehicles can potentially reduce this congestion by increasing road
capacity via vehicle platooning and by creating an avenue for influencing
people&#x27;s choice of routes. We consider a network of parallel roads with two
modes of transportation: (i) human drivers, who will choose the quickest route
available to them, and (ii) a ride hailing service, which provides an array of
autonomous vehicle route options, each with different prices, to users. We
formalize a model of vehicle flow in mixed autonomy and a model of how
autonomous service users make choices between routes with different prices and
latencies. Developing an algorithm to learn the preferences of the users, we
formulate a planning optimization that chooses prices to maximize a social
objective. We demonstrate the benefit of the proposed scheme by comparing the
results to theoretical benchmarks which we show can be efficiently calculated.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Ex uno plures: Splitting One Model into an Ensemble of Subnetworks. (arXiv:2106.04767v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhilu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1">Vianne R. Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sabuncu_M/0/1/0/all/0/1">Mert R. Sabuncu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04767">
                                    <div class="article-summary-box-inner">
                                        <span>Monte Carlo (MC) dropout is a simple and efficient ensembling method that can
improve the accuracy and confidence calibration of high-capacity deep neural
network models. However, MC dropout is not as effective as more
compute-intensive methods such as deep ensembles. This performance gap can be
attributed to the relatively poor quality of individual models in the MC
dropout ensemble and their lack of diversity. These issues can in turn be
traced back to the coupled training and substantial parameter sharing of the
dropout models. Motivated by this perspective, we propose a strategy to compute
an ensemble of subnetworks, each corresponding to a non-overlapping dropout
mask computed via a pruning strategy and trained independently. We show that
the proposed subnetwork ensembling method can perform as well as standard deep
ensembles in both accuracy and uncertainty estimates, yet with a computational
efficiency similar to MC dropout. Lastly, using several computer vision
datasets like CIFAR10/100, CUB200, and Tiny-Imagenet, we experimentally
demonstrate that subnetwork ensembling also consistently outperforms recently
proposed approaches that efficiently ensemble neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Controlling False Discovery Rates under Cross-Sectional Correlations. (arXiv:2102.07826v2 [stat.ME] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Komiyama_J/0/1/0/all/0/1">Junpei Komiyama</a>, <a href="http://arxiv.org/find/stat/1/au:+Abe_M/0/1/0/all/0/1">Masaya Abe</a>, <a href="http://arxiv.org/find/stat/1/au:+Nakagawa_K/0/1/0/all/0/1">Kei Nakagawa</a>, <a href="http://arxiv.org/find/stat/1/au:+McAlinn_K/0/1/0/all/0/1">Kenichiro McAlinn</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07826">
                                    <div class="article-summary-box-inner">
                                        <span>We consider controlling the false discovery rate for testing many time series
with an unknown cross-sectional correlation structure. Given a large number of
hypotheses, false and missing discoveries can plague an analysis. While many
procedures have been proposed to control false discovery, most of them either
assume independent hypotheses or lack statistical power. A problem of
particular interest is in financial asset pricing, where the goal is to
determine which &#x60;&#x60;factors&quot; lead to excess returns out of a large number of
potential factors. Our contribution is two-fold. First, we show the consistency
of Fama and French&#x27;s prominent method under multiple testing. Second, we
propose a novel method for false discovery control using double bootstrapping.
We achieve superior statistical power to existing methods and prove that the
false discovery rate is controlled. Simulations and a real data application
illustrate the efficacy of our method over existing methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On Feature Collapse and Deep Kernel Learning for Single Forward Pass Uncertainty. (arXiv:2102.11409v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Amersfoort_J/0/1/0/all/0/1">Joost van Amersfoort</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1">Lewis Smith</a>, <a href="http://arxiv.org/find/cs/1/au:+Jesson_A/0/1/0/all/0/1">Andrew Jesson</a>, <a href="http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1">Oscar Key</a>, <a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1">Yarin Gal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11409">
                                    <div class="article-summary-box-inner">
                                        <span>Gaussian processes are often considered a gold standard in uncertainty
estimation with low dimensional data, but they have difficulty scaling to high
dimensional inputs. Deep Kernel Learning (DKL) was introduced as a solution to
this problem: a deep feature extractor is used to transform the inputs over
which a Gaussian process&#x27; kernel is defined. However, DKL has been shown to
provide unreliable uncertainty estimates in practice. We study why, and show
that for certain feature extractors, &quot;far-away&quot; data points are mapped to the
same features as those of training-set points. With this insight we propose to
constrain DKL&#x27;s feature extractor to approximately preserve distances through a
bi-Lipschitz constraint, resulting in a feature space favorable to DKL. We
obtain a model, DUE, which demonstrates uncertainty quality outperforming
previous DKL and single forward pass uncertainty methods, while maintaining the
speed and accuracy of softmax neural networks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Marginalizable Density Models. (arXiv:2106.04741v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Gilboa_D/0/1/0/all/0/1">Dar Gilboa</a>, <a href="http://arxiv.org/find/stat/1/au:+Pakman_A/0/1/0/all/0/1">Ari Pakman</a>, <a href="http://arxiv.org/find/stat/1/au:+Vatter_T/0/1/0/all/0/1">Thibault Vatter</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04741">
                                    <div class="article-summary-box-inner">
                                        <span>Probability density models based on deep networks have achieved remarkable
success in modeling complex high-dimensional datasets. However, unlike kernel
density estimators, modern neural models do not yield marginals or conditionals
in closed form, as these quantities require the evaluation of seldom tractable
integrals. In this work, we present the Marginalizable Density Model
Approximator (MDMA), a novel deep network architecture which provides closed
form expressions for the probabilities, marginals and conditionals of any
subset of the variables. The MDMA learns deep scalar representations for each
individual variable and combines them via learned hierarchical tensor
decompositions into a tractable yet expressive CDF, from which marginals and
conditional densities are easily obtained. We illustrate the advantage of exact
marginalizability in several tasks that are out of reach of previous deep
network-based density estimation models, such as estimating mutual information
between arbitrary subsets of variables, inferring causality by testing for
conditional independence, and inference with missing data without the need for
data imputation, outperforming state-of-the-art models on these tasks. The
model also allows for parallelized sampling with only a logarithmic dependence
of the time complexity on the number of variables.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Programmable 3D snapshot microscopy with Fourier convolutional networks. (arXiv:2104.10611v2 [eess.IV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Deb_D/0/1/0/all/0/1">Diptodip Deb</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiao_Z/0/1/0/all/0/1">Zhenfei Jiao</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1">Alex B. Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Ahrens_M/0/1/0/all/0/1">Misha B. Ahrens</a>, <a href="http://arxiv.org/find/eess/1/au:+Podgorski_K/0/1/0/all/0/1">Kaspar Podgorski</a>, <a href="http://arxiv.org/find/eess/1/au:+Turaga_S/0/1/0/all/0/1">Srinivas C. Turaga</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10611">
                                    <div class="article-summary-box-inner">
                                        <span>3D snapshot microscopy enables fast volumetric imaging by capturing a 3D
volume in a single 2D camera image, and has found a variety of biological
applications such as whole brain imaging of fast neural activity in larval
zebrafish. The optimal microscope design for this optical 3D-to-2D encoding is
both sample- and task-dependent, with no general solution known. Highly
programmable optical elements create new possibilities for sample-specific
computational optimization of microscope parameters, e.g. tuning the collection
of light for a given sample structure. We perform such optimization with deep
learning, using a differentiable wave-optics simulation of light propagation
through a programmable microscope and a neural network to reconstruct volumes
from the microscope image. We introduce a class of global kernel Fourier
convolutional neural networks which can efficiently decode information from
multiple depths in the volume, globally encoded across a 3D snapshot image. We
show that our proposed networks succeed in large field of view volume
reconstruction and microscope parameter optimization where traditional networks
fail. We also show that our networks outperform the state-of-the-art learned
reconstruction algorithms for lensless computational photography.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning. (arXiv:2106.03760v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Hazimeh_H/0/1/0/all/0/1">Hussein Hazimeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1">Aakanksha Chowdhery</a>, <a href="http://arxiv.org/find/cs/1/au:+Sathiamoorthy_M/0/1/0/all/0/1">Maheswaran Sathiamoorthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yihua Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazumder_R/0/1/0/all/0/1">Rahul Mazumder</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Lichan Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1">Ed H. Chi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03760">
                                    <div class="article-summary-box-inner">
                                        <span>The Mixture-of-experts (MoE) architecture is showing promising results in
multi-task learning (MTL) and in scaling high-capacity neural networks.
State-of-the-art MoE models use a trainable sparse gate to select a subset of
the experts for each input example. While conceptually appealing, existing
sparse gates, such as Top-k, are not smooth. The lack of smoothness can lead to
convergence and statistical performance issues when training with
gradient-based methods. In this paper, we develop DSelect-k: the first,
continuously differentiable and sparse gate for MoE, based on a novel binary
encoding formulation. Our gate can be trained using first-order methods, such
as stochastic gradient descent, and offers explicit control over the number of
experts to select. We demonstrate the effectiveness of DSelect-k in the context
of MTL, on both synthetic and real datasets with up to 128 tasks. Our
experiments indicate that MoE models based on DSelect-k can achieve
statistically significant improvements in predictive and expert selection
performance. Notably, on a real-world large-scale recommender system, DSelect-k
achieves over 22% average improvement in predictive performance compared to the
Top-k gate. We provide an open-source TensorFlow implementation of our gate.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Probabilistic task modelling for meta-learning. (arXiv:2106.04802v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cuong C. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1">Thanh-Toan Do</a>, <a href="http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1">Gustavo Carneiro</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04802">
                                    <div class="article-summary-box-inner">
                                        <span>We propose probabilistic task modelling -- a generative probabilistic model
for collections of tasks used in meta-learning. The proposed model combines
variational auto-encoding and latent Dirichlet allocation to model each task as
a mixture of Gaussian distribution in an embedding space. Such modelling
provides an explicit representation of a task through its task-theme mixture.
We present an efficient approximation inference technique based on variational
inference method for empirical Bayes parameter estimation. We perform empirical
evaluations to validate the task uncertainty and task distance produced by the
proposed method through correlation diagrams of the prediction accuracy on
testing tasks. We also carry out experiments of task selection in meta-learning
to demonstrate how the task relatedness inferred from the proposed model help
to facilitate meta-learning algorithms.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Ghosts in Neural Networks: Existence, Structure and Role of Infinite-Dimensional Null Space. (arXiv:2106.04770v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Sonoda_S/0/1/0/all/0/1">Sho Sonoda</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishikawa_I/0/1/0/all/0/1">Isao Ishikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ikeda_M/0/1/0/all/0/1">Masahiro Ikeda</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04770">
                                    <div class="article-summary-box-inner">
                                        <span>Overparametrization has been remarkably successful for deep learning studies.
This study investigates an overlooked but important aspect of overparametrized
neural networks, that is, the null components in the parameters of neural
networks, or the ghosts. Since deep learning is not explicitly regularized,
typical deep learning solutions contain null components. In this paper, we
present a structure theorem of the null space for a general class of neural
networks. Specifically, we show that any null element can be uniquely written
by the linear combination of ridgelet transforms. In general, it is quite
difficult to fully characterize the null space of an arbitrarily given
operator. Therefore, the structure theorem is a great advantage for
understanding a complicated landscape of neural network parameters. As
applications, we discuss the roles of ghosts on the generalization performance
of deep learning.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Operationalizing Complex Causes:A Pragmatic View of Mediation. (arXiv:2106.05074v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gultchin_L/0/1/0/all/0/1">Limor Gultchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Watson_D/0/1/0/all/0/1">David S. Watson</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1">Matt J. Kusner</a>, <a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1">Ricardo Silva</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05074">
                                    <div class="article-summary-box-inner">
                                        <span>We examine the problem of causal response estimation for complex objects
(e.g., text, images, genomics). In this setting, classical \emph{atomic}
interventions are often not available (e.g., changes to characters, pixels, DNA
base-pairs). Instead, we only have access to indirect or \emph{crude}
interventions (e.g., enrolling in a writing program, modifying a scene,
applying a gene therapy). In this work, we formalize this problem and provide
an initial solution. Given a collection of candidate mediators, we propose (a)
a two-step method for predicting the causal responses of crude interventions;
and (b) a testing procedure to identify mediators of crude interventions. We
demonstrate, on a range of simulated and real-world-inspired examples, that our
approach allows us to efficiently estimate the effect of crude interventions
with limited data from new treatment regimes.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Online Optimization in Games via Control Theory: Connecting Regret, Passivity and Poincar\&#x27;e Recurrence. (arXiv:2106.04748v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Cheung_Y/0/1/0/all/0/1">Yun Kuen Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1">Georgios Piliouras</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04748">
                                    <div class="article-summary-box-inner">
                                        <span>We present a novel control-theoretic understanding of online optimization and
learning in games, via the notion of passivity. Passivity is a fundamental
concept in control theory, which abstracts energy conservation and dissipation
in physical systems. It has become a standard tool in analysis of general
feedback systems, to which game dynamics belong. Our starting point is to show
that all continuous-time Follow-the-Regularized-Leader (FTRL) dynamics, which
includes the well-known Replicator Dynamic, are lossless, i.e. it is passive
with no energy dissipation. Interestingly, we prove that passivity implies
bounded regret, connecting two fundamental primitives of control theory and
online optimization.

The observation of energy conservation in FTRL inspires us to present a
family of lossless learning dynamics, each of which has an underlying energy
function with a simple gradient structure. This family is closed under convex
combination; as an immediate corollary, any convex combination of FTRL dynamics
is lossless and thus has bounded regret. This allows us to extend the framework
of Fox and Shamma (Games, 2013) to prove not just global asymptotic stability
results for game dynamics, but Poincar\&#x27;e recurrence results as well.
Intuitively, when a lossless game (e.g. graphical constant-sum game) is coupled
with lossless learning dynamic, their interconnection is also lossless, which
results in a pendulum-like energy-preserving recurrent behavior, generalizing
the results of Piliouras and Shamma (SODA, 2014) and Mertikopoulos,
Papadimitriou and Piliouras (SODA, 2018).</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Sentence Embeddings using Supervised Contrastive Learning. (arXiv:2106.04791v1 [cs.CL])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1">Danqi Liao</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04791">
                                    <div class="article-summary-box-inner">
                                        <span>Sentence embeddings encode sentences in fixed dense vectors and have played
an important role in various NLP tasks and systems. Methods for building
sentence embeddings include unsupervised learning such as Quick-Thoughts and
supervised learning such as InferSent. With the success of pretrained NLP
models, recent research shows that fine-tuning pretrained BERT on SNLI and
Multi-NLI data creates state-of-the-art sentence embeddings, outperforming
previous sentence embeddings methods on various evaluation benchmarks. In this
paper, we propose a new method to build sentence embeddings by doing supervised
contrastive learning. Specifically our method fine-tunes pretrained BERT on
SNLI data, incorporating both supervised crossentropy loss and supervised
contrastive loss. Compared with baseline where fine-tuning is only done with
supervised cross-entropy loss similar to current state-of-the-art method SBERT,
our supervised contrastive method improves 2.8% in average on Semantic Textual
Similarity (STS) benchmarks and 1.05% in average on various sentence transfer
tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL. (arXiv:2103.09815v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Romac_C/0/1/0/all/0/1">Cl&#xe9;ment Romac</a>, <a href="http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1">R&#xe9;my Portelas</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1">Katja Hofmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1">Pierre-Yves Oudeyer</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09815">
                                    <div class="article-summary-box-inner">
                                        <span>Training autonomous agents able to generalize to multiple tasks is a key
target of Deep Reinforcement Learning (DRL) research. In parallel to improving
DRL algorithms themselves, Automatic Curriculum Learning (ACL) study how
teacher algorithms can train DRL agents more efficiently by adapting task
selection to their evolving abilities. While multiple standard benchmarks exist
to compare DRL agents, there is currently no such thing for ACL algorithms.
Thus, comparing existing approaches is difficult, as too many experimental
parameters differ from paper to paper. In this work, we identify several key
challenges faced by ACL algorithms. Based on these, we present TeachMyAgent
(TA), a benchmark of current ACL algorithms leveraging procedural task
generation. It includes 1) challenge-specific unit-tests using variants of a
procedural Box2D bipedal walker environment, and 2) a new procedural Parkour
environment combining most ACL challenges, making it ideal for global
performance assessment. We then use TeachMyAgent to conduct a comparative study
of representative existing approaches, showcasing the competitiveness of some
ACL algorithms that do not use expert knowledge. We also show that the Parkour
environment remains an open problem. We open-source our environments, all
studied ACL algorithms (collected from open-source code or re-implemented), and
DRL students in a Python package available at
https://github.com/flowersteam/TeachMyAgent.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Contextual Recommendations and Low-Regret Cutting-Plane Algorithms. (arXiv:2106.04819v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Gollapudi_S/0/1/0/all/0/1">Sreenivas Gollapudi</a>, <a href="http://arxiv.org/find/cs/1/au:+Guruganesh_G/0/1/0/all/0/1">Guru Guruganesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kollias_K/0/1/0/all/0/1">Kostas Kollias</a>, <a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1">Pasin Manurangsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Leme_R/0/1/0/all/0/1">Renato Paes Leme</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Jon Schneider</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04819">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the following variant of contextual linear bandits motivated by
routing applications in navigational engines and recommendation systems. We
wish to learn a hidden $d$-dimensional value $w^*$. Every round, we are
presented with a subset $\mathcal{X}_t \subseteq \mathbb{R}^d$ of possible
actions. If we choose (i.e. recommend to the user) action $x_t$, we obtain
utility $\langle x_t, w^* \rangle$ but only learn the identity of the best
action $\arg\max_{x \in \mathcal{X}_t} \langle x, w^* \rangle$. We design
algorithms for this problem which achieve regret $O(d\log T)$ and $\exp(O(d
\log d))$. To accomplish this, we design novel cutting-plane algorithms with
low &quot;regret&quot; -- the total distance between the true point $w^*$ and the
hyperplanes the separation oracle returns. We also consider the variant where
we are allowed to provide a list of several recommendations. In this variant,
we give an algorithm with $O(d^2 \log d)$ regret and list size
$\mathrm{poly}(d)$. Finally, we construct nearly tight algorithms for a weaker
variant of this problem where the learner only learns the identity of an action
that is better than the recommendation. Our results rely on new algorithmic
techniques in convex geometry (including a variant of Steiner&#x27;s formula for the
centroid of a convex set) which may be of independent interest.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Vector Quantized Models for Planning. (arXiv:2106.04615v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Ozair_S/0/1/0/all/0/1">Sherjil Ozair</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yazhe Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Razavi_A/0/1/0/all/0/1">Ali Razavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Antonoglou_I/0/1/0/all/0/1">Ioannis Antonoglou</a>, <a href="http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1">A&#xe4;ron van den Oord</a>, <a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1">Oriol Vinyals</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04615">
                                    <div class="article-summary-box-inner">
                                        <span>Recent developments in the field of model-based RL have proven successful in
a range of environments, especially ones where planning is essential. However,
such successes have been limited to deterministic fully-observed environments.
We present a new approach that handles stochastic and partially-observable
environments. Our key insight is to use discrete autoencoders to capture the
multiple possible effects of an action in a stochastic environment. We use a
stochastic variant of \emph{Monte Carlo tree search} to plan over both the
agent&#x27;s actions and the discrete latent variables representing the
environment&#x27;s response. Our approach significantly outperforms an offline
version of MuZero on a stochastic interpretation of chess where the opponent is
considered part of the environment. We also show that our approach scales to
\emph{DeepMind Lab}, a first-person 3D environment with large visual
observations and partial observability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Bayesian Boosting for Linear Mixed Models. (arXiv:2106.04862v1 [stat.ME])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Zhang_B/0/1/0/all/0/1">Boyao Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Griesbach_C/0/1/0/all/0/1">Colin Griesbach</a>, <a href="http://arxiv.org/find/stat/1/au:+Kim_C/0/1/0/all/0/1">Cora Kim</a>, <a href="http://arxiv.org/find/stat/1/au:+Muller_Voggel_N/0/1/0/all/0/1">Nadia M&#xfc;ller-Voggel</a>, <a href="http://arxiv.org/find/stat/1/au:+Bergherr_E/0/1/0/all/0/1">Elisabeth Bergherr</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04862">
                                    <div class="article-summary-box-inner">
                                        <span>Boosting methods are widely used in statistical learning to deal with
high-dimensional data due to their variable selection feature. However, those
methods lack straightforward ways to construct estimators for the precision of
the parameters such as variance or confidence interval, which can be achieved
by conventional statistical methods like Bayesian inference. In this paper, we
propose a new inference method &quot;BayesBoost&quot; that combines boosting and Bayesian
for linear mixed models to make the uncertainty estimation for the random
effects possible on the one hand. On the other hand, the new method overcomes
the shortcomings of Bayesian inference in giving precise and unambiguous
guidelines for the selection of covariates by benefiting from boosting
techniques. The implementation of Bayesian inference leads to the randomness of
model selection criteria like the conditional AIC (cAIC), so we also propose a
cAIC-based model selection criteria that focus on the stabilized regions
instead of the global minimum. The effectiveness of the new approach can be
observed via simulation and in a data example from the field of neurophysiology
focussing on the mechanisms in the brain while listening to unpleasant sounds.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Rate-Distortion Theoretic Model Compression: Successive Refinement for Pruning. (arXiv:2102.08329v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Isik_B/0/1/0/all/0/1">Berivan Isik</a>, <a href="http://arxiv.org/find/cs/1/au:+No_A/0/1/0/all/0/1">Albert No</a>, <a href="http://arxiv.org/find/cs/1/au:+Weissman_T/0/1/0/all/0/1">Tsachy Weissman</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08329">
                                    <div class="article-summary-box-inner">
                                        <span>We study the neural network (NN) compression problem, viewing the tension
between the compression ratio and NN performance through the lens of
rate-distortion theory. We choose a distortion metric that reflects the effect
of NN compression on the model output and then derive the tradeoff between rate
(compression ratio) and distortion. In addition to characterizing theoretical
limits of NN compression, this formulation shows that \emph{pruning},
implicitly or explicitly, must be a part of a good compression algorithm. This
observation bridges a gap between parts of the literature pertaining to NN and
data compression, respectively, providing insight into the empirical success of
pruning for NN compression. Finally, we propose a novel pruning strategy
derived from our information-theoretic formulation and show that it outperforms
the relevant baselines on CIFAR-10 and ImageNet datasets.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Towards the Memorization Effect of Neural Networks in Adversarial Training. (arXiv:2106.04794v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Han Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaorui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wentao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1">Wenbiao Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhongqin Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zitao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Anil Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04794">
                                    <div class="article-summary-box-inner">
                                        <span>Recent studies suggest that &#x60;&#x60;memorization&#x27;&#x27; is one important factor for
overparameterized deep neural networks (DNNs) to achieve optimal performance.
Specifically, the perfectly fitted DNNs can memorize the labels of many
atypical samples, generalize their memorization to correctly classify test
atypical samples and enjoy better test performance. While, DNNs which are
optimized via adversarial training algorithms can also achieve perfect training
performance by memorizing the labels of atypical samples, as well as the
adversarially perturbed atypical samples. However, adversarially trained models
always suffer from poor generalization, with both relatively low clean accuracy
and robustness on the test set. In this work, we study the effect of
memorization in adversarial trained DNNs and disclose two important findings:
(a) Memorizing atypical samples is only effective to improve DNN&#x27;s accuracy on
clean atypical samples, but hardly improve their adversarial robustness and (b)
Memorizing certain atypical samples will even hurt the DNN&#x27;s performance on
typical samples. Based on these two findings, we propose Benign Adversarial
Training (BAT) which can facilitate adversarial training to avoid fitting
&#x60;&#x60;harmful&#x27;&#x27; atypical samples and fit as more &#x60;&#x60;benign&#x27;&#x27; atypical samples as
possible. In our experiments, we validate the effectiveness of BAT, and show it
can achieve better clean accuracy vs. robustness trade-off than baseline
methods, in benchmark datasets such as CIFAR100 and Tiny~ImageNet.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">On the Evolution of Neuron Communities in a Deep Learning Architecture. (arXiv:2106.04693v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mostafa_S/0/1/0/all/0/1">Sakib Mostafa</a>, <a href="http://arxiv.org/find/cs/1/au:+Mondal_D/0/1/0/all/0/1">Debajyoti Mondal</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04693">
                                    <div class="article-summary-box-inner">
                                        <span>Deep learning techniques are increasingly being adopted for classification
tasks over the past decade, yet explaining how deep learning architectures can
achieve state-of-the-art performance is still an elusive goal. While all the
training information is embedded deeply in a trained model, we still do not
understand much about its performance by only analyzing the model. This paper
examines the neuron activation patterns of deep learning-based classification
models and explores whether the models&#x27; performances can be explained through
neurons&#x27; activation behavior. We propose two approaches: one that models
neurons&#x27; activation behavior as a graph and examines whether the neurons form
meaningful communities, and the other examines the predictability of neurons&#x27;
behavior using entropy. Our comprehensive experimental study reveals that both
the community quality (modularity) and entropy are closely related to the deep
learning models&#x27; performances, thus paves a novel way of explaining deep
learning models directly from the neurons&#x27; activation pattern.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Embedding Physics to Learn Spatiotemporal Dynamics from Sparse Data. (arXiv:2106.04781v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Rao_C/0/1/0/all/0/1">Chengping Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yang Liu</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04781">
                                    <div class="article-summary-box-inner">
                                        <span>Modeling nonlinear spatiotemporal dynamical systems has primarily relied on
partial differential equations (PDEs) that are typically derived from first
principles. However, the explicit formulation of PDEs for many underexplored
processes, such as climate systems, biochemical reaction and epidemiology,
remains uncertain or partially unknown, where very sparse measurement data is
yet available. To tackle this challenge, we propose a novel deep learning
architecture that forcibly embedded known physics knowledge in a
residual-recurrent $\Pi$-block network, to facilitate the learning of the
spatiotemporal dynamics in a data-driven manner. The coercive embedding
mechanism of physics, fundamentally different from physics-informed neural
networks based on loss penalty, ensures the network to rigorously obey given
physics. Numerical experiments demonstrate that the resulting learning paradigm
that embeds physics possesses remarkable accuracy, robustness, interpretability
and generalizability for learning spatiotemporal dynamics.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Practical Machine Learning Safety: A Survey and Primer. (arXiv:2106.04823v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mohseni_S/0/1/0/all/0/1">Sina Mohseni</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haotao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhiding Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1">Chaowei Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhangyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadawa_J/0/1/0/all/0/1">Jay Yadawa</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04823">
                                    <div class="article-summary-box-inner">
                                        <span>The open-world deployment of Machine Learning (ML) algorithms in
safety-critical applications such as autonomous vehicles needs to address a
variety of ML vulnerabilities such as interpretability, verifiability, and
performance limitations. Research explores different approaches to improve ML
dependability by proposing new models and training techniques to reduce
generalization error, achieve domain adaptation, and detect outlier examples
and adversarial attacks. In this paper, we review and organize practical ML
techniques that can improve the safety and dependability of ML algorithms and
therefore ML-based software. Our organization maps state-of-the-art ML
techniques to safety strategies in order to enhance the dependability of the ML
algorithm from different aspects, and discuss research gaps as well as
promising solutions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Job Dispatching Policies for Queueing Systems with Unknown Service Rates. (arXiv:2106.04707v1 [eess.SY])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Choudhury_T/0/1/0/all/0/1">Tuhinangshu Choudhury</a>, <a href="http://arxiv.org/find/eess/1/au:+Joshi_G/0/1/0/all/0/1">Gauri Joshi</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1">Weina Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Shakkottai_S/0/1/0/all/0/1">Sanjay Shakkottai</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04707">
                                    <div class="article-summary-box-inner">
                                        <span>In multi-server queueing systems where there is no central queue holding all
incoming jobs, job dispatching policies are used to assign incoming jobs to the
queue at one of the servers. Classic job dispatching policies such as
join-the-shortest-queue and shortest expected delay assume that the service
rates and queue lengths of the servers are known to the dispatcher. In this
work, we tackle the problem of job dispatching without the knowledge of service
rates and queue lengths, where the dispatcher can only obtain noisy estimates
of the service rates by observing job departures. This problem presents a novel
exploration-exploitation trade-off between sending jobs to all the servers to
estimate their service rates, and exploiting the currently known fastest
servers to minimize the expected queueing delay. We propose a bandit-based
exploration policy that learns the service rates from observed job departures.
Unlike the standard multi-armed bandit problem where only one out of a finite
set of actions is optimal, here the optimal policy requires identifying the
optimal fraction of incoming jobs to be sent to each server. We present a
regret analysis and simulations to demonstrate the effectiveness of the
proposed bandit-based exploration policy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Submodular + Concave. (arXiv:2106.04769v1 [math.OC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Mitra_S/0/1/0/all/0/1">Siddharth Mitra</a>, <a href="http://arxiv.org/find/math/1/au:+Feldman_M/0/1/0/all/0/1">Moran Feldman</a>, <a href="http://arxiv.org/find/math/1/au:+Karbasi_A/0/1/0/all/0/1">Amin Karbasi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04769">
                                    <div class="article-summary-box-inner">
                                        <span>It has been well established that first order optimization methods can
converge to the maximal objective value of concave functions and provide
constant factor approximation guarantees for (non-convex/non-concave)
continuous submodular functions. In this work, we initiate the study of the
maximization of functions of the form $F(x) &#x3D; G(x) +C(x)$ over a solvable
convex body $P$, where $G$ is a smooth DR-submodular function and $C$ is a
smooth concave function. This class of functions is a strict extension of both
concave and continuous DR-submodular functions for which no theoretical
guarantee is known. We provide a suite of Frank-Wolfe style algorithms, which,
depending on the nature of the objective function (i.e., if $G$ and $C$ are
monotone or not, and non-negative or not) and on the nature of the set $P$
(i.e., whether it is downward closed or not), provide $1-1/e$, $1/e$, or $1/2$
approximation guarantees. We then use our algorithms to get a framework to
smoothly interpolate between choosing a diverse set of elements from a given
ground set (corresponding to the mode of a determinantal point process) and
choosing a clustered set of elements (corresponding to the maxima of a suitable
concave function). Additionally, we apply our algorithms to various functions
in the above class (DR-submodular + concave) in both constrained and
unconstrained settings, and show that our algorithms consistently outperform
natural baselines.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Drones for Medical Delivery Considering Different Demands Classes: A Markov Decision Process Approach for Managing Health Centers Dispatching Medical Products. (arXiv:2106.04729v1 [math.OC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/math/1/au:+Asadi_A/0/1/0/all/0/1">Amin Asadi</a>, <a href="http://arxiv.org/find/math/1/au:+Pinkley_S/0/1/0/all/0/1">Sarah Nurre Pinkley</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04729">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of optimizing the distribution operations of a hub
using drones to deliver medical supplies to different geographic regions.
Drones are an innovative method with many benefits including low-contact
delivery thereby reducing the spread of pandemic and vaccine-preventable
diseases. While we focus on medical supply delivery for this work, it is
applicable to drone delivery for many other applications, including food,
postal items, and e-commerce delivery. In this paper, our goal is to address
drone delivery challenges by optimizing the distribution operations at a drone
hub that dispatch drones to different geographic locations generating
stochastic demands for medical supplies. By considering different geographic
locations, we consider different classes of demand that require different
flight ranges, which is directly related to the amount of charge held in a
drone battery. We classify the stochastic demands based on their distance from
the drone hub, use a Markov decision process to model the problem, and perform
computational tests using realistic data representing a prominent drone
delivery company. We solve the problem using a reinforcement learning method
and show its high performance compared with the exact solution found using
dynamic programming. Finally, we analyze the results and provide insights for
managing the drone hub operations.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Dynamic Instance-Wise Classification in Correlated Feature Spaces. (arXiv:2106.04668v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liyanage_Y/0/1/0/all/0/1">Yasitha Warahena Liyanage</a>, <a href="http://arxiv.org/find/cs/1/au:+Zois_D/0/1/0/all/0/1">Daphney-Stavroula Zois</a>, <a href="http://arxiv.org/find/cs/1/au:+Chelmis_C/0/1/0/all/0/1">Charalampos Chelmis</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04668">
                                    <div class="article-summary-box-inner">
                                        <span>In a typical supervised machine learning setting, the predictions on all test
instances are based on a common subset of features discovered during model
training. However, using a different subset of features that is most
informative for each test instance individually may not only improve prediction
accuracy, but also the overall interpretability of the model. At the same time,
feature selection methods for classification have been known to be the most
effective when many features are irrelevant and/or uncorrelated. In fact,
feature selection ignoring correlations between features can lead to poor
classification performance. In this work, a Bayesian network is utilized to
model feature dependencies. Using the dependency network, a new method is
proposed that sequentially selects the best feature to evaluate for each test
instance individually, and stops the selection process to make a prediction
once it determines that no further improvement can be achieved with respect to
classification accuracy. The optimum number of features to acquire and the
optimum classification strategy are derived for each test instance. The
theoretical properties of the optimum solution are analyzed, and a new
algorithm is proposed that takes advantage of these properties to implement a
robust and scalable solution for high dimensional settings. The effectiveness,
generalizability, and scalability of the proposed method is illustrated on a
variety of real-world datasets from diverse application domains.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Probabilistic Neural Network to Quantify Uncertainty of Wind Power Estimation. (arXiv:2106.04656v1 [cs.NE])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Karami_F/0/1/0/all/0/1">Farzad Karami</a>, <a href="http://arxiv.org/find/cs/1/au:+Kehtarnavaz_N/0/1/0/all/0/1">Nasser Kehtarnavaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rotea_M/0/1/0/all/0/1">Mario Rotea</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04656">
                                    <div class="article-summary-box-inner">
                                        <span>Each year a growing number of wind farms are being added to power grids to
generate electricity. The power curve of a wind turbine, which exhibits the
relationship between generated power and wind speed, plays a major role in
assessing the performance of a wind farm. Neural networks have been used for
power curve estimation. However, they do not produce a confidence measure for
their output, unless computationally prohibitive Bayesian methods are used. In
this paper, a probabilistic neural network with Monte Carlo dropout is
considered to quantify the model (epistemic) uncertainty of the power curve
estimation. This approach offers a minimal increase in computational complexity
over deterministic approaches. Furthermore, by incorporating a probabilistic
loss function, the noise or aleatoric uncertainty in the data is estimated. The
developed network captures both model and noise uncertainty which is found to
be useful tools in assessing performance. Also, the developed network is
compared with existing ones across a public domain dataset showing superior
performance in terms of prediction accuracy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning subtree pattern importance for Weisfeiler-Lehmanbased graph kernels. (arXiv:2106.04739v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1">Dai Hai Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Canh Hao Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mamitsuka_H/0/1/0/all/0/1">Hiroshi Mamitsuka</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04739">
                                    <div class="article-summary-box-inner">
                                        <span>Graph is an usual representation of relational data, which are ubiquitous in
manydomains such as molecules, biological and social networks. A popular
approach to learningwith graph structured data is to make use of graph kernels,
which measure the similaritybetween graphs and are plugged into a kernel
machine such as a support vector machine.Weisfeiler-Lehman (WL) based graph
kernels, which employ WL labeling scheme to extract subtree patterns and
perform node embedding, are demonstrated to achieve great performance while
being efficiently computable. However, one of the main drawbacks of ageneral
kernel is the decoupling of kernel construction and learning process. For
moleculargraphs, usual kernels such as WL subtree, based on substructures of
the molecules, consider all available substructures having the same importance,
which might not be suitable inpractice. In this paper, we propose a method to
learn the weights of subtree patterns in the framework of WWL kernels, the
state of the art method for graph classification task [14]. To overcome the
computational issue on large scale data sets, we present an efficient learning
algorithm and also derive a generalization gap bound to show its convergence.
Finally, through experiments on synthetic and real-world data sets, we
demonstrate the effectiveness of our proposed method for learning the weights
of subtree patterns.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Streaming Belief Propagation for Community Detection. (arXiv:2106.04805v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1">Yuchen Wu</a>, <a href="http://arxiv.org/find/stat/1/au:+Bateni_M/0/1/0/all/0/1">MohammadHossein Bateni</a>, <a href="http://arxiv.org/find/stat/1/au:+Linhares_A/0/1/0/all/0/1">Andre Linhares</a>, <a href="http://arxiv.org/find/stat/1/au:+Almeida_F/0/1/0/all/0/1">Filipe Miguel Goncalves de Almeida</a>, <a href="http://arxiv.org/find/stat/1/au:+Montanari_A/0/1/0/all/0/1">Andrea Montanari</a>, <a href="http://arxiv.org/find/stat/1/au:+Norouzi_Fard_A/0/1/0/all/0/1">Ashkan Norouzi-Fard</a>, <a href="http://arxiv.org/find/stat/1/au:+Tardos_J/0/1/0/all/0/1">Jakab Tardos</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04805">
                                    <div class="article-summary-box-inner">
                                        <span>The community detection problem requires to cluster the nodes of a network
into a small number of well-connected &quot;communities&quot;. There has been substantial
recent progress in characterizing the fundamental statistical limits of
community detection under simple stochastic block models. However, in
real-world applications, the network structure is typically dynamic, with nodes
that join over time. In this setting, we would like a detection algorithm to
perform only a limited number of updates at each node arrival. While standard
voting approaches satisfy this constraint, it is unclear whether they exploit
the network information optimally. We introduce a simple model for networks
growing over time which we refer to as streaming stochastic block model
(StSBM). Within this model, we prove that voting algorithms have fundamental
limitations. We also develop a streaming belief-propagation (StreamBP)
approach, for which we prove optimality in certain regimes. We validate our
theoretical findings on synthetic and real data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">General Rough Modeling of Cluster Analysis. (arXiv:2106.04683v1 [cs.AI])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Mani_A/0/1/0/all/0/1">A. Mani</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04683">
                                    <div class="article-summary-box-inner">
                                        <span>In this research, a general theoretical framework for clustering is proposed
over specific partial algebraic systems by the present author. Her theory helps
in isolating minimal assumptions necessary for different concepts of clustering
information in any form to be realized in a situation (and therefore in a
semantics). \emph{It is well-known that of the limited number of proofs in the
theory of hard and soft clustering that are known to exist, most involve
statistical assumptions}. Many methods seem to work because they seem to work
in specific empirical practice. A new general rough method of analyzing
clusterings is invented, and this opens the subject to clearer conceptions and
contamination-free theoretical proofs. Numeric ideas of validation are also
proposed to be replaced by those based on general rough approximation. The
essence of the approach is explained in brief and supported by an example.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Predicting Deep Neural Network Generalization with Perturbation Response Curves. (arXiv:2106.04765v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Schiff_Y/0/1/0/all/0/1">Yair Schiff</a>, <a href="http://arxiv.org/find/cs/1/au:+Quanz_B/0/1/0/all/0/1">Brian Quanz</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1">Payel Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Pin-Yu Chen</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04765">
                                    <div class="article-summary-box-inner">
                                        <span>The field of Deep Learning is rich with empirical evidence of human-like
performance on a variety of prediction tasks. However, despite these successes,
the recent Predicting Generalization in Deep Learning (PGDL) NeurIPS 2020
competition suggests that there is a need for more robust and efficient
measures of network generalization. In this work, we propose a new framework
for evaluating the generalization capabilities of trained networks. We use
perturbation response (PR) curves that capture the accuracy change of a given
network as a function of varying levels of training sample perturbation. From
these PR curves, we derive novel statistics that capture generalization
capability. Specifically, we introduce two new measures for accurately
predicting generalization gaps: the Gi-score and Pal-score, that are inspired
by the Gini coefficient and Palma ratio (measures of income inequality), that
accurately predict generalization gaps. Using our framework applied to intra
and inter class sample mixup, we attain better predictive scores than the
current state-of-the-art measures on a majority of tasks in the PGDL
competition. In addition, we show that our framework and the proposed
statistics can be used to capture to what extent a trained network is invariant
to a given parametric input transformation, such as rotation or translation.
Therefore, these generalization gap prediction statistics also provide a useful
means for selecting the optimal network architectures and hyperparameters that
are invariant to a certain perturbation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ParChain: A Framework for Parallel Hierarchical Agglomerative Clustering using Nearest-Neighbor Chain. (arXiv:2106.04727v1 [cs.DS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shangdi Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiqiu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dhulipala_L/0/1/0/all/0/1">Laxman Dhulipala</a>, <a href="http://arxiv.org/find/cs/1/au:+Shun_J/0/1/0/all/0/1">Julian Shun</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04727">
                                    <div class="article-summary-box-inner">
                                        <span>This paper studies the hierarchical clustering problem, where the goal is to
produce a dendrogram that represents clusters at varying scales of a data set.
We propose the ParChain framework for designing parallel hierarchical
agglomerative clustering (HAC) algorithms, and using the framework we obtain
novel parallel algorithms for the complete linkage, average linkage, and Ward&#x27;s
linkage criteria. Compared to most previous parallel HAC algorithms, which
require quadratic memory, our new algorithms require only linear memory, and
are scalable to large data sets. ParChain is based on our parallelization of
the nearest-neighbor chain algorithm, and enables multiple clusters to be
merged on every round. We introduce two key optimizations that are critical for
efficiency: a range query optimization that reduces the number of distance
computations required when finding nearest neighbors of clusters, and a caching
optimization that stores a subset of previously computed distances, which are
likely to be reused.

Experimentally, we show that our highly-optimized implementations using 48
cores with two-way hyper-threading achieve 5.8--110.1x speedup over
state-of-the-art parallel HAC algorithms and achieve 13.75--54.23x
self-relative speedup. Compared to state-of-the-art algorithms, our algorithms
require up to 237.3x less space. Our algorithms are able to scale to data set
sizes with tens of millions of points, which existing algorithms are not able
to handle.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">CoAtNet: Marrying Convolution and Attention for All Data Sizes. (arXiv:2106.04803v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1">Zihang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hanxiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1">Quoc V. Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1">Mingxing Tan</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04803">
                                    <div class="article-summary-box-inner">
                                        <span>Transformers have attracted increasing interests in computer vision, but they
still fall behind state-of-the-art convolutional networks. In this work, we
show that while Transformers tend to have larger model capacity, their
generalization can be worse than convolutional networks due to the lack of the
right inductive bias. To effectively combine the strengths from both
architectures, we present CoAtNets(pronounced &quot;coat&quot; nets), a family of hybrid
models built from two key insights:(1) depthwise Convolution and self-Attention
can be naturally unified via simple relative attention; (2) vertically stacking
convolution layers and attention layers in a principled way is surprisingly
effective in improving generalization, capacity and efficiency. Experiments
show that our CoAtNets achieve state-of-the-art performance under different
resource constraints across various datasets. For example, CoAtNet achieves
86.0% ImageNet top-1 accuracy without extra data, and 89.77% with extra JFT
data, outperforming prior arts of both convolutional networks and Transformers.
Notably, when pre-trained with 13M images fromImageNet-21K, our CoAtNet
achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images
from JFT while using 23x less data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ChaCha for Online AutoML. (arXiv:2106.04815v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1">Qingyun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1">John Langford</a>, <a href="http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1">Paul Mineiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossi_M/0/1/0/all/0/1">Marco Rossi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04815">
                                    <div class="article-summary-box-inner">
                                        <span>We propose the ChaCha (Champion-Challengers) algorithm for making an online
choice of hyperparameters in online learning settings. ChaCha handles the
process of determining a champion and scheduling a set of &#x60;live&#x27; challengers
over time based on sample complexity bounds. It is guaranteed to have sublinear
regret after the optimal configuration is added into consideration by an
application-dependent oracle based on the champions. Empirically, we show that
ChaCha provides good performance across a wide array of datasets when
optimizing over featurization and hyperparameter decisions.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PAM: Understanding Product Images in Cross Product Category Attribute Extraction. (arXiv:2106.04630v1 [cs.CV])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1">Rongmei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xiang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jie Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1">Nasser Zalmout</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yan Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Li Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xin Luna Dong</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04630">
                                    <div class="article-summary-box-inner">
                                        <span>Understanding product attributes plays an important role in improving online
shopping experience for customers and serves as an integral part for
constructing a product knowledge graph. Most existing methods focus on
attribute extraction from text description or utilize visual information from
product images such as shape and color. Compared to the inputs considered in
prior works, a product image in fact contains more information, represented by
a rich mixture of words and visual clues with a layout carefully designed to
impress customers. This work proposes a more inclusive framework that fully
utilizes these different modalities for attribute extraction. Inspired by
recent works in visual question answering, we use a transformer based sequence
to sequence model to fuse representations of product text, Optical Character
Recognition (OCR) tokens and visual objects detected in the product image. The
framework is further extended with the capability to extract attribute value
across multiple product categories with a single model, by training the decoder
to predict both product category and attribute value and conditioning its
output on product category. The model provides a unified attribute extraction
solution desirable at an e-commerce platform that offers numerous product
categories with a diverse body of product attributes. We evaluated the model on
two product attributes, one with many possible values and one with a small set
of possible values, over 14 product categories and found the model could
achieve 15% gain on the Recall and 10% gain on the F1 score compared to
existing methods using text-only features.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Accelerating Neural Architecture Search via Proxy Data. (arXiv:2106.04784v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1">Byunggook Na</a>, <a href="http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1">Jisoo Mok</a>, <a href="http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1">Hyeokjun Choe</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sungroh Yoon</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04784">
                                    <div class="article-summary-box-inner">
                                        <span>Despite the increasing interest in neural architecture search (NAS), the
significant computational cost of NAS is a hindrance to researchers. Hence, we
propose to reduce the cost of NAS using proxy data, i.e., a representative
subset of the target data, without sacrificing search performance. Even though
data selection has been used across various fields, our evaluation of existing
selection methods for NAS algorithms offered by NAS-Bench-1shot1 reveals that
they are not always appropriate for NAS and a new selection method is
necessary. By analyzing proxy data constructed using various selection methods
through data entropy, we propose a novel proxy data selection method tailored
for NAS. To empirically demonstrate the effectiveness, we conduct thorough
experiments across diverse datasets, search spaces, and NAS algorithms.
Consequently, NAS algorithms with the proposed selection discover architectures
that are competitive with those obtained using the entire dataset. It
significantly reduces the search cost: executing DARTS with the proposed
selection requires only 40 minutes on CIFAR-10 and 7.5 hours on ImageNet with a
single GPU. Additionally, when the architecture searched on ImageNet using the
proposed selection is inversely transferred to CIFAR-10, a state-of-the-art
test error of 2.4\% is yielded. Our code is available at
https://github.com/nabk89/NAS-with-Proxy-data.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Boolean Matrix Factorization via Nonnegative Auxiliary Optimization. (arXiv:2106.04708v1 [cs.DS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Truong_D/0/1/0/all/0/1">Duc P. Truong</a>, <a href="http://arxiv.org/find/cs/1/au:+Skau_E/0/1/0/all/0/1">Erik Skau</a>, <a href="http://arxiv.org/find/cs/1/au:+Desantis_D/0/1/0/all/0/1">Derek Desantis</a>, <a href="http://arxiv.org/find/cs/1/au:+Alexandrov_B/0/1/0/all/0/1">Boian Alexandrov</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04708">
                                    <div class="article-summary-box-inner">
                                        <span>A novel approach to Boolean matrix factorization (BMF) is presented. Instead
of solving the BMF problem directly, this approach solves a nonnegative
optimization problem with the constraint over an auxiliary matrix whose Boolean
structure is identical to the initial Boolean data. Then the solution of the
nonnegative auxiliary optimization problem is thresholded to provide a solution
for the BMF problem. We provide the proofs for the equivalencies of the two
solution spaces under the existence of an exact solution. Moreover, the
nonincreasing property of the algorithm is also proven. Experiments on
synthetic and real datasets are conducted to show the effectiveness and
complexity of the algorithm compared to other current methods.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">NRGNN: Learning a Label Noise-Resistant Graph Neural Network on Sparsely and Noisily Labeled Graphs. (arXiv:2106.04714v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Dai_E/0/1/0/all/0/1">Enyan Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1">Charu Aggarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Suhang Wang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04714">
                                    <div class="article-summary-box-inner">
                                        <span>Graph Neural Networks (GNNs) have achieved promising results for
semi-supervised learning tasks on graphs such as node classification. Despite
the great success of GNNs, many real-world graphs are often sparsely and
noisily labeled, which could significantly degrade the performance of GNNs, as
the noisy information could propagate to unlabeled nodes via graph structure.
Thus, it is important to develop a label noise-resistant GNN for
semi-supervised node classification. Though extensive studies have been
conducted to learn neural networks with noisy labels, they mostly focus on
independent and identically distributed data and assume a large number of noisy
labels are available, which are not directly applicable for GNNs. Thus, we
investigate a novel problem of learning a robust GNN with noisy and limited
labels. To alleviate the negative effects of label noise, we propose to link
the unlabeled nodes with labeled nodes of high feature similarity to bring more
clean label information. Furthermore, accurate pseudo labels could be obtained
by this strategy to provide more supervision and further reduce the effects of
label noise. Our theoretical and empirical analysis verify the effectiveness of
these two strategies under mild conditions. Extensive experiments on real-world
datasets demonstrate the effectiveness of the proposed method in learning a
robust GNN with noisy and limited labels.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style. (arXiv:2106.04619v1 [stat.ML])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1">Julius von K&#xfc;gelgen</a>, <a href="http://arxiv.org/find/stat/1/au:+Sharma_Y/0/1/0/all/0/1">Yash Sharma</a>, <a href="http://arxiv.org/find/stat/1/au:+Gresele_L/0/1/0/all/0/1">Luigi Gresele</a>, <a href="http://arxiv.org/find/stat/1/au:+Brendel_W/0/1/0/all/0/1">Wieland Brendel</a>, <a href="http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/stat/1/au:+Besserve_M/0/1/0/all/0/1">Michel Besserve</a>, <a href="http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1">Francesco Locatello</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04619">
                                    <div class="article-summary-box-inner">
                                        <span>Self-supervised representation learning has shown remarkable success in a
number of domains. A common practice is to perform data augmentation via
hand-crafted transformations intended to leave the semantics of the data
invariant. We seek to understand the empirical success of this approach from a
theoretical perspective. We formulate the augmentation process as a latent
variable model by postulating a partition of the latent representation into a
content component, which is assumed invariant to augmentation, and a style
component, which is allowed to change. Unlike prior work on disentanglement and
independent component analysis, we allow for both nontrivial statistical and
causal dependencies in the latent space. We study the identifiability of the
latent representation based on pairs of views of the observations and prove
sufficient conditions that allow us to identify the invariant content partition
up to an invertible mapping in both generative and discriminative settings. We
find numerical simulations with dependent latent variables are consistent with
our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional,
visually complex images with rich causal dependencies, which we use to study
the effect of data augmentations performed in practice.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1">MohammadJavad Azizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1">Branislav Kveton</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1">Mohammad Ghavamzadeh</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04763">
                                    <div class="article-summary-box-inner">
                                        <span>We study the problem of best-arm identification (BAI) in contextual bandits
in the fixed-budget setting. We propose a general successive elimination
algorithm that proceeds in stages and eliminates a fixed fraction of suboptimal
arms in each stage. This design takes advantage of the strengths of static and
adaptive allocations. We analyze the algorithm in linear models and obtain a
better error bound than prior work. We also apply it to generalized linear
models (GLMs) and bound its error. This is the first BAI algorithm for GLMs in
the fixed-budget setting. Our extensive numerical experiments show that our
algorithm outperforms the state of art.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Curriculum Design for Teaching via Demonstrations: Theory and Applications. (arXiv:2106.04696v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yengera_G/0/1/0/all/0/1">Gaurav Yengera</a>, <a href="http://arxiv.org/find/cs/1/au:+Devidze_R/0/1/0/all/0/1">Rati Devidze</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamalaruban_P/0/1/0/all/0/1">Parameswaran Kamalaruban</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1">Adish Singla</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04696">
                                    <div class="article-summary-box-inner">
                                        <span>We consider the problem of teaching via demonstrations in sequential
decision-making settings. In particular, we study how to design a personalized
curriculum over demonstrations to speed up the learner&#x27;s convergence. We
provide a unified curriculum strategy for two popular learner models: Maximum
Causal Entropy Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy
Behavioral Cloning (CrossEnt-BC). Our unified strategy induces a ranking over
demonstrations based on a notion of difficulty scores computed w.r.t. the
teacher&#x27;s optimal policy and the learner&#x27;s current policy. Compared to the
state of the art, our strategy doesn&#x27;t require access to the learner&#x27;s internal
dynamics and still enjoys similar convergence guarantees under mild technical
conditions. Furthermore, we adapt our curriculum strategy to teach a learner
using domain knowledge in the form of task-specific difficulty scores when the
teacher&#x27;s optimal policy is unknown. Experiments on a car driving simulator
environment and shortest path problems in a grid-world environment demonstrate
the effectiveness of our proposed curriculum strategy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Learning to Price Against a Moving Target. (arXiv:2106.04689v1 [cs.GT])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Leme_R/0/1/0/all/0/1">Renato Paes Leme</a>, <a href="http://arxiv.org/find/cs/1/au:+Sivan_B/0/1/0/all/0/1">Balasubramanian Sivan</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1">Yifeng Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Worah_P/0/1/0/all/0/1">Pratik Worah</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04689">
                                    <div class="article-summary-box-inner">
                                        <span>In the Learning to Price setting, a seller posts prices over time with the
goal of maximizing revenue while learning the buyer&#x27;s valuation. This problem
is very well understood when values are stationary (fixed or iid). Here we
study the problem where the buyer&#x27;s value is a moving target, i.e., they change
over time either by a stochastic process or adversarially with bounded
variation. In either case, we provide matching upper and lower bounds on the
optimal revenue loss. Since the target is moving, any information learned soon
becomes out-dated, which forces the algorithms to keep switching between
exploring and exploiting phases.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">OODIn: An Optimised On-Device Inference Framework for Heterogeneous Mobile Devices. (arXiv:2106.04723v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1">Stylianos I. Venieris</a>, <a href="http://arxiv.org/find/cs/1/au:+Panopoulos_I/0/1/0/all/0/1">Ioannis Panopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Venieris_I/0/1/0/all/0/1">Iakovos S. Venieris</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04723">
                                    <div class="article-summary-box-inner">
                                        <span>Radical progress in the field of deep learning (DL) has led to unprecedented
accuracy in diverse inference tasks. As such, deploying DL models across mobile
platforms is vital to enable the development and broad availability of the
next-generation intelligent apps. Nevertheless, the wide and optimised
deployment of DL models is currently hindered by the vast system heterogeneity
of mobile devices, the varying computational cost of different DL models and
the variability of performance needs across DL applications. This paper
proposes OODIn, a framework for the optimised deployment of DL apps across
heterogeneous mobile devices. OODIn comprises a novel DL-specific software
architecture together with an analytical framework for modelling DL
applications that: (1) counteract the variability in device resources and DL
models by means of a highly parametrised multi-layer design; and (2) perform a
principled optimisation of both model- and system-level parameters through a
multi-objective formulation, designed for DL inference apps, in order to adapt
the deployment to the user-specified performance requirements and device
capabilities. Quantitative evaluation shows that the proposed framework
consistently outperforms status-quo designs across heterogeneous devices and
delivers up to 4.3x and 3.5x performance gain over highly optimised platform-
and model-aware designs respectively, while effectively adapting execution to
dynamic changes in resource availability.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Provably Faster Algorithms for Bilevel Optimization. (arXiv:2106.04692v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Junjie Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1">Kaiyi Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1">Yingbin Liang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04692">
                                    <div class="article-summary-box-inner">
                                        <span>Bilevel optimization has been widely applied in many important machine
learning applications such as hyperparameter optimization and meta-learning.
Recently, several momentum-based algorithms have been proposed to solve bilevel
optimization problems faster. However, those momentum-based algorithms do not
achieve provably better computational complexity than
$\mathcal{O}(\epsilon^{-2})$ of the SGD-based algorithm. In this paper, we
propose two new algorithms for bilevel optimization, where the first algorithm
adopts momentum-based recursive iterations, and the second algorithm adopts
recursive gradient estimations in nested loops to decrease the variance. We
show that both algorithms achieve the complexity of
$\mathcal{O}(\epsilon^{-1.5})$, which outperforms all existing algorithms by
the order of magnitude. Our experiments validate our theoretical results and
demonstrate the superior empirical performance of our algorithms in
hyperparameter applications. Our codes for MRBO, VRBO and other benchmarks are
available $\text{online}^1$.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Self-Adaptive Swarm System (SASS). (arXiv:2106.04679v1 [cs.MA])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qin Yang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04679">
                                    <div class="article-summary-box-inner">
                                        <span>Distributed artificial intelligence (DAI) studies artificial intelligence
entities working together to reason, plan, solve problems, organize behaviors
and strategies, make collective decisions and learn. This Ph.D. research
proposes a principled Multi-Agent Systems (MAS) cooperation framework,
Self-Adaptive Swarm System (SASS), to bridge the fourth level automation gap
between perception, communication, planning, execution, decision-making, and
learning.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning. (arXiv:2106.04590v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1">Seng Pei Liew</a>, <a href="http://arxiv.org/find/cs/1/au:+Takahashi_T/0/1/0/all/0/1">Tsubasa Takahashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ueno_M/0/1/0/all/0/1">Michihiko Ueno</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04590">
                                    <div class="article-summary-box-inner">
                                        <span>We propose a new framework of synthesizing data using deep generative models
in a differentially private manner. Within our framework, sensitive data are
sanitized with rigorous privacy guarantees in a one-shot fashion, such that
training deep generative models is possible without re-using the original data.
Hence, no extra privacy costs or model constraints are incurred, in contrast to
popular approaches such as Differentially Private Stochastic Gradient Descent
(DP-SGD), which, among other issues, causes degradation in privacy guarantees
as the training iteration increases. We demonstrate a realization of our
framework by making use of the characteristic function and an adversarial
re-weighting objective, which are of independent interest as well. Our proposal
has theoretical guarantees of performance, and empirical evaluations on
multiple datasets show that our approach outperforms other methods at
reasonable levels of privacy.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">SpeechBrain: A General-Purpose Speech Toolkit. (arXiv:2106.04624v1 [eess.AS])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/eess/1/au:+Ravanelli_M/0/1/0/all/0/1">Mirco Ravanelli</a>, <a href="http://arxiv.org/find/eess/1/au:+Parcollet_T/0/1/0/all/0/1">Titouan Parcollet</a>, <a href="http://arxiv.org/find/eess/1/au:+Plantinga_P/0/1/0/all/0/1">Peter Plantinga</a>, <a href="http://arxiv.org/find/eess/1/au:+Rouhe_A/0/1/0/all/0/1">Aku Rouhe</a>, <a href="http://arxiv.org/find/eess/1/au:+Cornell_S/0/1/0/all/0/1">Samuele Cornell</a>, <a href="http://arxiv.org/find/eess/1/au:+Lugosch_L/0/1/0/all/0/1">Loren Lugosch</a>, <a href="http://arxiv.org/find/eess/1/au:+Subakan_C/0/1/0/all/0/1">Cem Subakan</a>, <a href="http://arxiv.org/find/eess/1/au:+Dawalatabad_N/0/1/0/all/0/1">Nauman Dawalatabad</a>, <a href="http://arxiv.org/find/eess/1/au:+Heba_A/0/1/0/all/0/1">Abdelwahab Heba</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhong_J/0/1/0/all/0/1">Jianyuan Zhong</a>, <a href="http://arxiv.org/find/eess/1/au:+Chou_J/0/1/0/all/0/1">Ju-Chieh Chou</a>, <a href="http://arxiv.org/find/eess/1/au:+Yeh_S/0/1/0/all/0/1">Sung-Lin Yeh</a>, <a href="http://arxiv.org/find/eess/1/au:+Fu_S/0/1/0/all/0/1">Szu-Wei Fu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liao_C/0/1/0/all/0/1">Chien-Feng Liao</a>, <a href="http://arxiv.org/find/eess/1/au:+Rastorgueva_E/0/1/0/all/0/1">Elena Rastorgueva</a>, <a href="http://arxiv.org/find/eess/1/au:+Grondin_F/0/1/0/all/0/1">Fran&#xe7;ois Grondin</a>, <a href="http://arxiv.org/find/eess/1/au:+Aris_W/0/1/0/all/0/1">William Aris</a>, <a href="http://arxiv.org/find/eess/1/au:+Na_H/0/1/0/all/0/1">Hwidong Na</a>, <a href="http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1">Yan Gao</a>, <a href="http://arxiv.org/find/eess/1/au:+Mori_R/0/1/0/all/0/1">Renato De Mori</a>, <a href="http://arxiv.org/find/eess/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04624">
                                    <div class="article-summary-box-inner">
                                        <span>SpeechBrain is an open-source and all-in-one speech toolkit. It is designed
to facilitate the research and development of neural speech processing
technologies by being simple, flexible, user-friendly, and well-documented.
This paper describes the core architecture designed to support several tasks of
common interest, allowing users to naturally conceive, compare and share novel
speech processing pipelines. SpeechBrain achieves competitive or
state-of-the-art performance in a wide range of speech benchmarks. It also
provides training recipes, pretrained models, and inference scripts for popular
speech datasets, as well as tutorials which allow anyone with basic Python
proficiency to familiarize themselves with speech technologies.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">EXPObench: Benchmarking Surrogate-based Optimisation Algorithms on Expensive Black-box Functions. (arXiv:2106.04618v1 [cs.LG])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Bliek_L/0/1/0/all/0/1">Laurens Bliek</a>, <a href="http://arxiv.org/find/cs/1/au:+Guijt_A/0/1/0/all/0/1">Arthur Guijt</a>, <a href="http://arxiv.org/find/cs/1/au:+Karlsson_R/0/1/0/all/0/1">Rickard Karlsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Verwer_S/0/1/0/all/0/1">Sicco Verwer</a>, <a href="http://arxiv.org/find/cs/1/au:+Weerdt_M/0/1/0/all/0/1">Mathijs de Weerdt</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04618">
                                    <div class="article-summary-box-inner">
                                        <span>Surrogate algorithms such as Bayesian optimisation are especially designed
for black-box optimisation problems with expensive objectives, such as
hyperparameter tuning or simulation-based optimisation. In the literature,
these algorithms are usually evaluated with synthetic benchmarks which are well
established but have no expensive objective, and only on one or two real-life
applications which vary wildly between papers. There is a clear lack of
standardisation when it comes to benchmarking surrogate algorithms on
real-life, expensive, black-box objective functions. This makes it very
difficult to draw conclusions on the effect of algorithmic contributions. A new
benchmark library, EXPObench, provides first steps towards such a
standardisation. The library is used to provide an extensive comparison of six
different surrogate algorithms on four expensive optimisation problems from
different real-life applications. This has led to new insights regarding the
relative importance of exploration, the evaluation time of the objective, and
the used model. A further contribution is that we make the algorithms and
benchmark problem instances publicly available, contributing to more uniform
analysis of surrogate algorithms. Most importantly, we include the performance
of the six algorithms on all evaluated problem instances. This results in a
unique new dataset that lowers the bar for researching new methods as the
number of expensive evaluations required for comparison is significantly
reduced.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Launchpad: A Programming Model for Distributed Machine Learning Research. (arXiv:2106.04516v1 [cs.DC])</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1">Fan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Barth_Maron_G/0/1/0/all/0/1">Gabriel Barth-Maron</a>, <a href="http://arxiv.org/find/cs/1/au:+Stanczyk_P/0/1/0/all/0/1">Piotr Sta&#x144;czyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoffman_M/0/1/0/all/0/1">Matthew Hoffman</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Siqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kroiss_M/0/1/0/all/0/1">Manuel Kroiss</a>, <a href="http://arxiv.org/find/cs/1/au:+Pope_A/0/1/0/all/0/1">Aedan Pope</a>, <a href="http://arxiv.org/find/cs/1/au:+Rrustemi_A/0/1/0/all/0/1">Alban Rrustemi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04516">
                                    <div class="article-summary-box-inner">
                                        <span>A major driver behind the success of modern machine learning algorithms has
been their ability to process ever-larger amounts of data. As a result, the use
of distributed systems in both research and production has become increasingly
prevalent as a means to scale to this growing data. At the same time, however,
distributing the learning process can drastically complicate the implementation
of even simple algorithms. This is especially problematic as many machine
learning practitioners are not well-versed in the design of distributed
systems, let alone those that have complicated communication topologies. In
this work we introduce Launchpad, a programming model that simplifies the
process of defining and launching distributed systems that is specifically
tailored towards a machine learning audience. We describe our framework, its
design philosophy and implementation, and give a number of examples of common
learning algorithms whose designs are greatly simplified by this approach.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.MM"">cs.MM updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Domain Generalization for Document Authentication against Practical Recapturing Attacks. (arXiv:2101.01404v2 [cs.MM] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changsheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuzheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_F/0/1/0/all/0/1">Fengbo Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiwu Huang</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.01404">
                                    <div class="article-summary-box-inner">
                                        <span>Recapturing attack can be employed as a simple but effective anti-forensic
tool for digital document images. Inspired by the document inspection process
that compares a questioned document against a reference sample, we proposed a
document recapture detection scheme by employing Siamese network to compare and
extract distinct features in a recapture document image. The proposed algorithm
takes advantages of both metric learning and image forensic techniques. Instead
of adopting Euclidean distance-based loss function, we integrate the forensic
similarity function with a triplet loss and a normalized softmax loss. After
training with the proposed triplet selection strategy, the resulting feature
embedding clusters the genuine samples near the reference while pushes the
recaptured samples apart. In the experiment, we consider practical domain
generalization problems, such as the variations in printing/imaging devices,
substrates, recapturing channels, and document types. To evaluate the
robustness of different approaches, we benchmark some popular off-the-shelf
machine learning-based approaches, a state-of-the-art document image detection
scheme, and the proposed schemes with different network backbones under various
experimental protocols. Experimental results show that the proposed schemes
with different network backbones have consistently outperformed the
state-of-the-art approaches under different experimental settings.
Specifically, under the most challenging scenario in our experiment, i.e.,
evaluation across different types of documents that produced by different
devices, we have achieved less than 5.00% APCER (Attack Presentation
Classification Error Rate) and 5.56% BPCER (Bona Fide Presentation
Classification Error Rate) by the proposed network with ResNeXt101 backbone at
5% BPCER decision threshold.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
        </ul>
    </section>
    <section class="daily-content">
        <h2 class="daily-heading"><time datatime="2021-06-09">2021-06-09</time></h2>
        <ul class="sources card">
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.CL"">cs.CL updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Intrinsic Bias Metrics Do Not Correlate with Application Bias. (arXiv:2012.15859v5 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Goldfarb_Tarrant_S/0/1/0/all/0/1">Seraphina Goldfarb-Tarrant</a>, <a href="http://arxiv.org/find/cs/1/au:+Marchant_R/0/1/0/all/0/1">Rebecca Marchant</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1">Ricardo Mu&#xf1;oz Sanchez</a>, <a href="http://arxiv.org/find/cs/1/au:+Pandya_M/0/1/0/all/0/1">Mugdha Pandya</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1">Adam Lopez</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15859">
                                    <div class="article-summary-box-inner">
                                        <span>Natural Language Processing (NLP) systems learn harmful societal biases that
cause them to amplify inequality as they are deployed in more and more
situations. To guide efforts at debiasing these systems, the NLP community
relies on a variety of metrics that quantify bias in models. Some of these
metrics are intrinsic, measuring bias in word embedding spaces, and some are
extrinsic, measuring bias in downstream tasks that the word embeddings enable.
Do these intrinsic and extrinsic metrics correlate with each other? We compare
intrinsic and extrinsic metrics across hundreds of trained models covering
different tasks and experimental conditions. Our results show no reliable
correlation between these metrics that holds in all scenarios across tasks and
languages. We urge researchers working on debiasing to focus on extrinsic
measures of bias, and to make using these measures more feasible via creation
of new challenge sets and annotated test data. To aid this effort, we release
code, a new intrinsic metric, and an annotated test set focused on gender bias
in hate speech.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">ARBERT &amp; MARBERT: Deep Bidirectional Transformers for Arabic. (arXiv:2101.01785v3 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1">Muhammad Abdul-Mageed</a>, <a href="http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1">AbdelRahim Elmadany</a>, <a href="http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1">El Moatez Billah Nagoudi</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.01785">
                                    <div class="article-summary-box-inner">
                                        <span>Pre-trained language models (LMs) are currently integral to many natural
language processing systems. Although multilingual LMs were also introduced to
serve many languages, these have limitations such as being costly at inference
time and the size and diversity of non-English data involved in their
pre-training. We remedy these issues for a collection of diverse Arabic
varieties by introducing two powerful deep bidirectional transformer-based
models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a
new benchmark for multi-dialectal Arabic language understanding evaluation.
ARLUE is built using 42 datasets targeting six different task clusters,
allowing us to offer a series of standardized experiments under rich
conditions. When fine-tuned on ARLUE, our models collectively achieve new
state-of-the-art results across the majority of tasks (37 out of 48
classification tasks, on the 42 datasets). Our best model acquires the highest
ARLUE score (77.40) across all six task clusters, outperforming all other
models including XLM-R Large (~ 3.4 x larger size). Our models are publicly
available at https://github.com/UBC-NLP/marbert and ARLUE will be released
through the same repository.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Is Sparse Attention more Interpretable?. (arXiv:2106.01087v2 [cs.CL] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1">Clara Meister</a>, <a href="http://arxiv.org/find/cs/1/au:+Lazov_S/0/1/0/all/0/1">Stefan Lazov</a>, <a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1">Isabelle Augenstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01087">
                                    <div class="article-summary-box-inner">
                                        <span>Sparse attention has been claimed to increase model interpretability under
the assumption that it highlights influential inputs. Yet the attention
distribution is typically over representations internal to the model rather
than the inputs themselves, suggesting this assumption may not have merit. We
build on the recent work exploring the interpretability of attention; we design
a set of experiments to help us understand how sparsity affects our ability to
use attention as an explainability tool. On three text classification tasks, we
verify that only a weak relationship between inputs and co-indexed intermediate
representations exists -- under sparse attention and otherwise. Further, we do
not find any plausible mappings from sparse attention distributions to a sparse
set of influential inputs through other avenues. Rather, we observe in this
setting that inducing sparsity may make it less plausible that attention can be
used as a tool for understanding model behavior.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.CV"">cs.CV updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Active Surface Models. (arXiv:2011.08826v5 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1">Udaranga Wickramasinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Knott_G/0/1/0/all/0/1">Graham Knott</a>, <a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1">Pascal Fua</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08826">
                                    <div class="article-summary-box-inner">
                                        <span>Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
            <li class="source">
                <section>
                    <h3 class="source-name"><a class="source-name__link" href="http://export.arxiv.org/rss/cs.LG"">cs.LG updates on arXiv.org</a></h3>
                    <section class="articles-per-source">
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Information Theoretic Measures for Fairness-aware Feature Selection. (arXiv:2106.00772v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Khodadadian_S/0/1/0/all/0/1">Sajad Khodadadian</a>, <a href="http://arxiv.org/find/cs/1/au:+Nafea_M/0/1/0/all/0/1">Mohamed Nafea</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghassami_A/0/1/0/all/0/1">AmirEmad Ghassami</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1">Negar Kiyavash</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00772">
                                    <div class="article-summary-box-inner">
                                        <span>Machine learning algorithms are increasingly used for consequential decision
making regarding individuals based on their relevant features. Features that
are relevant for accurate decisions may however lead to either explicit or
implicit forms of discrimination against unprivileged groups, such as those of
certain race or gender. This happens due to existing biases in the training
data, which are often replicated or even exacerbated by the learning algorithm.
Identifying and measuring these biases at the data level is a challenging
problem due to the interdependence among the features, and the decision
outcome. In this work, we develop a framework for fairness-aware feature
selection which takes into account the correlation among the features and the
decision outcome, and is based on information theoretic measures for the
accuracy and discriminatory impacts of features. In particular, we first
propose information theoretic measures which quantify the impact of different
subsets of features on the accuracy and discrimination of the decision
outcomes. We then deduce the marginal impact of each feature using Shapley
value function; a solution concept in cooperative game theory used to estimate
marginal contributions of players in a coalitional game. Finally, we design a
fairness utility score for each feature (for feature selection) which
quantifies how this feature influences accurate as well as nondiscriminatory
decisions. Our framework depends on the joint statistics of the data rather
than a particular classifier design. We examine our proposed framework on real
and synthetic data to evaluate its performance.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">Deep Active Surface Models. (arXiv:2011.08826v5 [cs.CV] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1">Udaranga Wickramasinghe</a>, <a href="http://arxiv.org/find/cs/1/au:+Knott_G/0/1/0/all/0/1">Graham Knott</a>, <a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1">Pascal Fua</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08826">
                                    <div class="article-summary-box-inner">
                                        <span>Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                        <article>
                            <details class="article-expander">
                                <summary class="article-expander__title">A Generalizable Approach to Learning Optimizers. (arXiv:2106.00958v2 [cs.LG] UPDATED)</summary>
                                <div class="article-authors">
                                    <a href="http://arxiv.org/find/cs/1/au:+Almeida_D/0/1/0/all/0/1">Diogo Almeida</a>, <a href="http://arxiv.org/find/cs/1/au:+Winter_C/0/1/0/all/0/1">Clemens Winter</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaremba_W/0/1/0/all/0/1">Wojciech Zaremba</a>
                                </div>
                                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00958">
                                    <div class="article-summary-box-inner">
                                        <span>A core issue with learning to optimize neural networks has been the lack of
generalization to real world problems. To address this, we describe a system
designed from a generalization-first perspective, learning to update optimizer
hyperparameters instead of model parameters directly using novel features,
actions, and a reward function. This system outperforms Adam at all neural
network tasks including on modalities not seen during training. We achieve 2x
speedups on ImageNet, and a 2.5x speedup on a language modeling task using over
5 orders of magnitude more compute than the training tasks.</span>
                                    </div>
                                </a>
                            </details>
                        </article>
                    </section>
            </li>
            <br>
        </ul>
    </section>

    <footer>
        <time id="build-timestamp" datetime="2021-06-15T23:08:11.784Z">2021-06-15T23:08:11.784Z</time>
    </footer>
    <script src="https://code.jquery.com/jquery-3.6.0.slim.min.js" integrity="sha256-u7e5khyithlIdTpu22PHhENmPcRdFiHRjhAuHcs05RI=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/handlebars@latest/dist/handlebars.js"></script>
    <script src="highlightRegex.js"></script>
    <script src="index.js"></script>
    <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=386&t=tt&d=sDvlbgmeTw_E_GoVDGdggVOFT21w54hFtP9VETatnEM&cmo=ff4242&cmn=3dd13d"></script>
    <!-- %before-body-end.html% -->
</body>

</html>